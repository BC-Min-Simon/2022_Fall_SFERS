{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Won/Dollar Exchange Rate"
      ],
      "metadata": {
        "id": "axxfDT4n8ao9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Modules"
      ],
      "metadata": {
        "id": "xK4r9qhU8mQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas_datareader.data as pdr\n",
        "import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import argparse\n",
        "from copy import deepcopy # Add Deepcopy for args\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import time\n",
        "import scipy.signal\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__)\n",
        "%matplotlib inline\n",
        "%pylab inline\n",
        "pylab.rcParams['figure.figsize'] = (15, 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEvf-ev6kQmm",
        "outputId": "3927ed2a-e300-4cb5-a79b-baacdc3ec176"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n",
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset"
      ],
      "metadata": {
        "id": "LVKITzPP8pdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "4tgs6x4p8zyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "myfile = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "8XvLAwzjJmUB",
        "outputId": "0e6b9322-80c5-4d2d-c9ee-77ee31eea395"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-123997dd-cb74-472c-a765-4b6ccc19bfe6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-123997dd-cb74-472c-a765-4b6ccc19bfe6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving wondollarweek.csv to wondollarweek.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM87S5qfaKgs",
        "outputId": "7f0f08af-fd7f-4694-da6d-a4719688c08f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'wondollarweek.csv': b'\\xef\\xbb\\xbf1186,1174,1181.5,1180,1174.5,1175.4,1181.9,1224,1242,1233.8,1220.8,1223,1224.6,1223.1,1204,1186,1190.1,1198,1200.5,1187,1187,1178.5,1165.9,1159.4,1157,1168,1176,1193,1203.1,1198.3,1199,1205.8,1199,1184.6,1193.5,1192.3,1204.5,1216.4,1202,1208.5,1205.4,1199.7,1184.9,1173.1,1173.2,1158.2,1150,1131,1131.3,1130.7,1138,1133.9,1121,1124,1123.2,1129.3,1127,1134,1131,1119,1118.3,1113,1109.4,1112,1111.3,1108.5,1109.1,1109.4,1115,1133,1129.4,1115.3,1115,1118.8,1114.9,1117.2,1111.9,1112.8,1116.6,1115.2,1115.4,1114.5,1111.8,1104.4,1115.1,1128.1,1115.8,1116.2,1128.6,1128.5,1135.9,1133.6,1134.6,1141.8,1188.1,1209.5,1190.3,1207,1237,1276.4,1255.1,1283.5,1280.3,1249.5,1261.6,1243.5,1248.5,1273.3,1278.4,1299.2,1310.5,1348.8,1334.1,1324.1,1307,1302.8,1301.8,1309.3,1285,1289.5,1286.1,1291.7,1302.9,1303.2,1296.6,1300.7,1305.6,1300.8,1288.7,1283.5,1286.1,1280.9,1280.1,1290.3,1298.7,1307.2,1313.1,1302,1298.5,1297,1298,1285.1,1283.3,1276.8,1269,1274.4,1274.6,1300.2,1323,1302,1312.3,1320.4,1321.3,1317.8,1316.2,1318.8,1323.8,1314.3,1322.8,1328.4,1325.9,1328.2,1327.9,1308.8,1293.6,1279.5,1279.3,1254.6,1237.5,1220.1,1233.6,1224.8,1202.9,1204.9,1182.8,1165.6,1200,1198,1201,1195.3,1200,1200.7,1198.4,1218.6,1225.5,1230.4,1257.8,1240.6,1232,1221.6,1217.1,1212,1213.1,1207.8,1226.2,1212,1204.7,1197,1186.5,1176.7,1180,1170.3,1174.2,1199.3,1199.1,1188.5,1199.2,1245,1256.8,1244.3,1254,1249.8,1215,1218.9,1215.3,1195.2,1193,1199,1205.4,1194.5,1184,1189.8,1190,1180.2,1176,1180.9,1180,1184.6,1179.7,1173,1178,1173.1,1170,1150.1,1150.1,1149.9,1170.5,1181.3,1178.3,1182.5,1173,1176.8,1202.4,1195.3,1186,1188.6,1199.6,1195,1181.9,1186,1171.6,1168.4,1160.6,1152.2,1170.2,1170.1,1169,1159.8,1157,1141.2,1143.9,1153.8,1155,1171.8,1188.5,1182,1177.7,1164.8,1154.2,1157.1,1159.7,1155.5,1152,1153,1159,1167.4,1167.6,1159.7,1157.8,1156.4,1151.8,1149.7,1144.2,1144.2,1149.6,1146,1143.5,1135,1119,1105.3,1092,1065.3,1047,1040.9,1061.9,1058.2,1046.4,1038.1,1053.7,1035.7,1032.2,1025.6,1026.2,1025.6,1006.5,1008.2,1000.3,1004.9,1014.4,1008,1015,1019.7,998.9,1001.2,999.1,1004.5,1000.7,1007.7,1005.7,1012.2,1010.4,1026.3,1047.8,1036.9,1042.4,1031.1,1017.4,1014.7,1018.7,1024.3,1036.4,1025.7,1024.7,1031,1041.1,1037.9,1047.4,1058.1,1040.2,1049.5,1037.8,1039.3,1038.5,1036.3,1033.7,1017.7,1011.8,1005.4,982.1,989.2,975.5,961.1,970.8,974,974.7,969.1,982.2,975,975.6,976.1,953.5,961.5,948.4,944.9,927.9,943.7,952.3,947.4,948.2,962,955.8,957.7,946.7,948.8,951.9,952.2,965.7,957.9,958.6,961.9,960.5,956.3,956.3,946.8,946.2,958.2,955.4,955.7,939.5,935.2,941.5,934.1,930.5,916.4,922.6,925.8,929.8,938.2,938.6,940.1,940.5,935.8,937.6,938.5,938.8,948.8,946.3,938,939.1,936.3,932.2,928.2,926.7,926.9,925.1,928.1,928.3,928.4,932.5,928.3,926.6,921.7,920.9,918.3,914.1,919.3,922.9,932.5,944.2,942.1,938.1,932.5,926.7,913.7,918.5,917.5,918,907,907.8,918.7,922.2,929.3,922.5,924,939.4,940.2,940.5,937.4,948.5,949.8,942.8,945.3,948.4,936.5,949.6,982.4,1010,987.8,975.6,975.7,1000.7,996,1014.5,1047.2,1042.2,1037,1017.3,1034,1026,1036.6,1045,1002.9,1012.9,1007.1,1012.2,1016.5,1039.8,1062.5,1089,1117.8,1109.1,1140.3,1188.8,1328.1,1208,1320.1,1467.8,1288,1329.9,1448,1502.3,1464.5,1447,1349.6,1338,1321,1343,1358,1390.9,1389.5,1382.9,1455.5,1516.3,1552.4,1511.5,1408.5,1383.5,1383.5,1322.5,1323.5,1349.5,1356.8,1262.3,1267.2,1248.6,1256.9,1251,1253,1266.3,1288.8,1269.5,1279,1265.7,1248.7,1236.8,1222.5,1237.3,1246.9,1248.7,1246,1224.5,1204.8,1195.7,1178.3,1164.5,1164.5,1181.5,1182.5,1168,1160.3,1159,1175.5,1153,1164,1176.2,1170.2,1136.4,1125.5,1138.2,1159.9,1149,1160.3,1150.5,1163.4,1140.1,1128.3,1132.7,1138.7,1126,1118.2,1110.3,1108.7,1108.4,1132.1,1153.8,1250,1216.5,1248.7,1210.9,1187.5,1222.2,1223,1202.5,1204.6,1184.1,1168.4,1182.5,1174.2,1196,1184.7,1172.8,1160.9,1148.2,1122.3,1116.7,1119.3,1116.3,1116.6,1113.5,1131.9,1125.7,1152.5,1133.2,1146,1150.2,1149,1121,1125.1,1116.6,1118.1,1116.9,1128.6,1112.1,1126.5,1119.2,1129.7,1124.9,1114.4,1086.6,1084.3,1088.4,1081.3,1065,1074.9,1087.9,1101.8,1074.6,1082.9,1089.9,1076.9,1067.7,1064.1,1058.4,1054.6,1051.7,1061.7,1081.8,1087.4,1081.8,1063,1077.3,1148.4,1173.1,1190.4,1166.7,1131.9,1132.3,1121.8,1117.4,1136.6,1152,1143,1126.1,1156.2,1147.7,1156,1152.7,1158.2,1137.1,1127.3,1120.8,1121.9,1123.5,1129.1,1122.9,1121.5,1124.9,1134.2,1121.8,1139.6,1137.3,1141.3,1129,1142.5,1162.9,1180.5,1177.7,1165.9,1157.1,1161.7,1146.1,1141.1,1147,1146.6,1137.6,1129,1131.1,1131.1,1136.7,1133.1,1128.2,1118.3,1119.3,1113.8,1114.3,1104.3,1098.2,1092.3,1089.3,1086.7,1085.9,1084.1,1083,1073,1074.3,1063.5,1061.7,1058.7,1066.2,1085.5,1088.1,1083.8,1086.2,1083,1090.3,1110.3,1119.3,1111.1,1131.8,1129.1,1116.3,1112.3,1094.5,1111.7,1110.6,1126.9,1122.2,1133.6,1130.8,1154.5,1143.7,1135.8,1121.6,1112.7,1123.5,1118.7,1118.7,1123,1109.7,1098.4,1085,1072.2,1073.5,1073.6,1063.7,1061,1060.7,1061.4,1067.9,1062.9,1061.5,1059.6,1051,1060.1,1053.9,1068.3,1059.1,1065.3,1081.2,1079,1066.4,1072.2,1068.8,1064.1,1069,1076.2,1071.5,1057.9,1040.2,1038.8,1039.2,1030.3,1022.1,1025.3,1023.2,1023.1,1017.7,1018.7,1016.2,1008.5,1013.4,1029.1,1028.6,1027.9,1037.6,1021.2,1017.7,1014,1024.2,1034.9,1039.9,1062.7,1070.5,1065.9,1057.5,1068.5,1093.7,1100.5,1113.8,1107.9,1114.1,1103.1,1102,1097.8,1099.9,1082.2,1083.4,1084.5,1084.1,1097.7,1108.7,1100.8,1112.1,1131.5,1114.6,1104.9,1084.8,1098.6,1079.2,1073,1080,1099.7,1096,1105.8,1113.9,1108.8,1107.1,1110,1125,1133.9,1149.2,1165.1,1168.4,1166.2,1174,1195,1173.6,1193.4,1184.5,1162.8,1194.7,1165.9,1146.8,1132.5,1131,1132,1154.9,1172.2,1143.4,1164.3,1179.3,1176.2,1173.1,1187.7,1209.8,1210.9,1194.2,1200.5,1202.5,1227.4,1238.8,1203.4,1193.1,1162.5,1169.2,1154.2,1153.8,1150.2,1147.8,1137.8,1167.6,1182.6,1182.5,1193,1156,1171.4,1150.2,1151.8,1154.6,1137.4,1135.9,1124.4,1114,1099.5,1117.6,1113.7,1117.2,1098.4,1120.1,1096.8,1112.2,1135.9,1127.5,1142.5,1139.6,1150.6,1175.9,1180.1,1167.6,1158.5,1178.5,1199.1,1207.7,1193,1174.7,1169.2,1162.1,1144.3,1137.4,1146.1,1130.7,1145.5,1143.6,1123.3,1114.2,1124.4,1141.4,1140.2,1125.1,1131.4,1116,1124.2,1125.1,1124,1123.9,1144,1144,1150.5,1145.1,1120.6,1121.8,1124,1135.2,1137.2,1127.9,1127.8,1129.4,1132.6,1132.7,1149.1,1128.9,1131,1130.5,1113.8,1117.1,1097.5,1085.4,1086.4,1093.3,1089.8,1079.7,1064.5,1071.9,1069.3,1070.2,1067.9,1086.6,1077.2,1079,1082,1065.2,1071.6,1081.1,1056.6,1067.1,1074,1069,1068,1080.9,1077.6,1079.6,1077.7,1075.9,1104.8,1117.2,1120,1112.2,1129.2,1131.4,1120.2,1124,1133.9,1118.4,1110,1114.9,1125.3,1123.2,1109.3,1132.7,1128,1137.6,1139.2,1123.8,1133.3,1125.8,1129.4,1105.3,1130.1,1129.6,1125.4,1124.5,1116.4,1121.9,1121.3,1118.8,1121.7,1123.5,1119.1,1129,1134.8,1127.7,1136.8,1136.3,1139.4,1136.9,1160.5,1170,1187.5,1194.2,1184.5,1182.1,1180.4,1185.8,1156.2,1166,1180.5,1177.6,1178.9,1181.6,1215.3,1222.2,1202.5,1213.9,1208.2,1191,1188,1199.9,1196.6,1185.2,1169.7,1163,1157.5,1160.8,1167.6,1176.7,1187.2,1191.3,1166.2,1163.9,1167.1,1161.3,1159.4,1176.7,1187.4,1181.6,1189.5,1210.3,1195.2,1193.2,1243.5,1249.6,1217.4,1221.2,1217.3,1232.2,1218.2,1220.5,1232.4,1244.2,1225,1204.8,1216,1215.8,1198.6,1195.8,1200.9,1203.2,1196.1,1193.4,1185.6,1183.7,1185.1,1183,1186.4,1179,1165,1169.5,1146.8,1142,1127.7,1133.6,1113.9,1109.3,1110.4,1106.5,1082.1,1091.8,1102.7,1092.1,1087.3,1098,1098.2,1119.6,1118.5,1101.4,1110.4,1124,1140.3,1129.7,1129.7,1133.6,1119.6,1125.9,1112.3,1110.4,1122.6,1124.7,1132,1118.1,1113.6,1115.8,1130.4,1134.9,1133.1,1145,1141.5,1149.9,1146.5,1143.7,1161.2,1179.6,1169.2,1157,1169.1,1175,1181.8,1190.4,1182.4,1177.1,1168.6,1185.2,1179.6,1185.3,1193.3,1180.1,1181.3,1180.9,1186.6,1191.8,1199.1,1192.7,1196.1,1206.4,1196.5,1197.1,1202.4,1214.2,1242.3,1216.3,1227.3,1214.4,1233.1,1234.4,1249.9,1265.1,1276.4,1275,1266.2,1237.2,1256.9,1285.6,1301.8,1298.4,1299.8,1312.1,1307.7,1296.1,1310.1,1303,1325.9,1331.3,1362.6,1373.6,1389.5,1421.5,1410.1,1431.3,1433.3,1417'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prac_data = str(myfile.values()).replace(\"dict_values([b'\\\\xef\\\\xbb\\\\xbf\", '').replace(' ,,', ' ,').replace(\"'])\", '').split(',')\n",
        "float_lst = []\n",
        "for item in prac_data:\n",
        "    float_lst.append(float(item))\n",
        "    \n",
        "print(float_lst)\n",
        "print(len(float_lst))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTXHZgzUyigL",
        "outputId": "d618461e-903e-4a04-f47d-82250e341584"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1186.0, 1174.0, 1181.5, 1180.0, 1174.5, 1175.4, 1181.9, 1224.0, 1242.0, 1233.8, 1220.8, 1223.0, 1224.6, 1223.1, 1204.0, 1186.0, 1190.1, 1198.0, 1200.5, 1187.0, 1187.0, 1178.5, 1165.9, 1159.4, 1157.0, 1168.0, 1176.0, 1193.0, 1203.1, 1198.3, 1199.0, 1205.8, 1199.0, 1184.6, 1193.5, 1192.3, 1204.5, 1216.4, 1202.0, 1208.5, 1205.4, 1199.7, 1184.9, 1173.1, 1173.2, 1158.2, 1150.0, 1131.0, 1131.3, 1130.7, 1138.0, 1133.9, 1121.0, 1124.0, 1123.2, 1129.3, 1127.0, 1134.0, 1131.0, 1119.0, 1118.3, 1113.0, 1109.4, 1112.0, 1111.3, 1108.5, 1109.1, 1109.4, 1115.0, 1133.0, 1129.4, 1115.3, 1115.0, 1118.8, 1114.9, 1117.2, 1111.9, 1112.8, 1116.6, 1115.2, 1115.4, 1114.5, 1111.8, 1104.4, 1115.1, 1128.1, 1115.8, 1116.2, 1128.6, 1128.5, 1135.9, 1133.6, 1134.6, 1141.8, 1188.1, 1209.5, 1190.3, 1207.0, 1237.0, 1276.4, 1255.1, 1283.5, 1280.3, 1249.5, 1261.6, 1243.5, 1248.5, 1273.3, 1278.4, 1299.2, 1310.5, 1348.8, 1334.1, 1324.1, 1307.0, 1302.8, 1301.8, 1309.3, 1285.0, 1289.5, 1286.1, 1291.7, 1302.9, 1303.2, 1296.6, 1300.7, 1305.6, 1300.8, 1288.7, 1283.5, 1286.1, 1280.9, 1280.1, 1290.3, 1298.7, 1307.2, 1313.1, 1302.0, 1298.5, 1297.0, 1298.0, 1285.1, 1283.3, 1276.8, 1269.0, 1274.4, 1274.6, 1300.2, 1323.0, 1302.0, 1312.3, 1320.4, 1321.3, 1317.8, 1316.2, 1318.8, 1323.8, 1314.3, 1322.8, 1328.4, 1325.9, 1328.2, 1327.9, 1308.8, 1293.6, 1279.5, 1279.3, 1254.6, 1237.5, 1220.1, 1233.6, 1224.8, 1202.9, 1204.9, 1182.8, 1165.6, 1200.0, 1198.0, 1201.0, 1195.3, 1200.0, 1200.7, 1198.4, 1218.6, 1225.5, 1230.4, 1257.8, 1240.6, 1232.0, 1221.6, 1217.1, 1212.0, 1213.1, 1207.8, 1226.2, 1212.0, 1204.7, 1197.0, 1186.5, 1176.7, 1180.0, 1170.3, 1174.2, 1199.3, 1199.1, 1188.5, 1199.2, 1245.0, 1256.8, 1244.3, 1254.0, 1249.8, 1215.0, 1218.9, 1215.3, 1195.2, 1193.0, 1199.0, 1205.4, 1194.5, 1184.0, 1189.8, 1190.0, 1180.2, 1176.0, 1180.9, 1180.0, 1184.6, 1179.7, 1173.0, 1178.0, 1173.1, 1170.0, 1150.1, 1150.1, 1149.9, 1170.5, 1181.3, 1178.3, 1182.5, 1173.0, 1176.8, 1202.4, 1195.3, 1186.0, 1188.6, 1199.6, 1195.0, 1181.9, 1186.0, 1171.6, 1168.4, 1160.6, 1152.2, 1170.2, 1170.1, 1169.0, 1159.8, 1157.0, 1141.2, 1143.9, 1153.8, 1155.0, 1171.8, 1188.5, 1182.0, 1177.7, 1164.8, 1154.2, 1157.1, 1159.7, 1155.5, 1152.0, 1153.0, 1159.0, 1167.4, 1167.6, 1159.7, 1157.8, 1156.4, 1151.8, 1149.7, 1144.2, 1144.2, 1149.6, 1146.0, 1143.5, 1135.0, 1119.0, 1105.3, 1092.0, 1065.3, 1047.0, 1040.9, 1061.9, 1058.2, 1046.4, 1038.1, 1053.7, 1035.7, 1032.2, 1025.6, 1026.2, 1025.6, 1006.5, 1008.2, 1000.3, 1004.9, 1014.4, 1008.0, 1015.0, 1019.7, 998.9, 1001.2, 999.1, 1004.5, 1000.7, 1007.7, 1005.7, 1012.2, 1010.4, 1026.3, 1047.8, 1036.9, 1042.4, 1031.1, 1017.4, 1014.7, 1018.7, 1024.3, 1036.4, 1025.7, 1024.7, 1031.0, 1041.1, 1037.9, 1047.4, 1058.1, 1040.2, 1049.5, 1037.8, 1039.3, 1038.5, 1036.3, 1033.7, 1017.7, 1011.8, 1005.4, 982.1, 989.2, 975.5, 961.1, 970.8, 974.0, 974.7, 969.1, 982.2, 975.0, 975.6, 976.1, 953.5, 961.5, 948.4, 944.9, 927.9, 943.7, 952.3, 947.4, 948.2, 962.0, 955.8, 957.7, 946.7, 948.8, 951.9, 952.2, 965.7, 957.9, 958.6, 961.9, 960.5, 956.3, 956.3, 946.8, 946.2, 958.2, 955.4, 955.7, 939.5, 935.2, 941.5, 934.1, 930.5, 916.4, 922.6, 925.8, 929.8, 938.2, 938.6, 940.1, 940.5, 935.8, 937.6, 938.5, 938.8, 948.8, 946.3, 938.0, 939.1, 936.3, 932.2, 928.2, 926.7, 926.9, 925.1, 928.1, 928.3, 928.4, 932.5, 928.3, 926.6, 921.7, 920.9, 918.3, 914.1, 919.3, 922.9, 932.5, 944.2, 942.1, 938.1, 932.5, 926.7, 913.7, 918.5, 917.5, 918.0, 907.0, 907.8, 918.7, 922.2, 929.3, 922.5, 924.0, 939.4, 940.2, 940.5, 937.4, 948.5, 949.8, 942.8, 945.3, 948.4, 936.5, 949.6, 982.4, 1010.0, 987.8, 975.6, 975.7, 1000.7, 996.0, 1014.5, 1047.2, 1042.2, 1037.0, 1017.3, 1034.0, 1026.0, 1036.6, 1045.0, 1002.9, 1012.9, 1007.1, 1012.2, 1016.5, 1039.8, 1062.5, 1089.0, 1117.8, 1109.1, 1140.3, 1188.8, 1328.1, 1208.0, 1320.1, 1467.8, 1288.0, 1329.9, 1448.0, 1502.3, 1464.5, 1447.0, 1349.6, 1338.0, 1321.0, 1343.0, 1358.0, 1390.9, 1389.5, 1382.9, 1455.5, 1516.3, 1552.4, 1511.5, 1408.5, 1383.5, 1383.5, 1322.5, 1323.5, 1349.5, 1356.8, 1262.3, 1267.2, 1248.6, 1256.9, 1251.0, 1253.0, 1266.3, 1288.8, 1269.5, 1279.0, 1265.7, 1248.7, 1236.8, 1222.5, 1237.3, 1246.9, 1248.7, 1246.0, 1224.5, 1204.8, 1195.7, 1178.3, 1164.5, 1164.5, 1181.5, 1182.5, 1168.0, 1160.3, 1159.0, 1175.5, 1153.0, 1164.0, 1176.2, 1170.2, 1136.4, 1125.5, 1138.2, 1159.9, 1149.0, 1160.3, 1150.5, 1163.4, 1140.1, 1128.3, 1132.7, 1138.7, 1126.0, 1118.2, 1110.3, 1108.7, 1108.4, 1132.1, 1153.8, 1250.0, 1216.5, 1248.7, 1210.9, 1187.5, 1222.2, 1223.0, 1202.5, 1204.6, 1184.1, 1168.4, 1182.5, 1174.2, 1196.0, 1184.7, 1172.8, 1160.9, 1148.2, 1122.3, 1116.7, 1119.3, 1116.3, 1116.6, 1113.5, 1131.9, 1125.7, 1152.5, 1133.2, 1146.0, 1150.2, 1149.0, 1121.0, 1125.1, 1116.6, 1118.1, 1116.9, 1128.6, 1112.1, 1126.5, 1119.2, 1129.7, 1124.9, 1114.4, 1086.6, 1084.3, 1088.4, 1081.3, 1065.0, 1074.9, 1087.9, 1101.8, 1074.6, 1082.9, 1089.9, 1076.9, 1067.7, 1064.1, 1058.4, 1054.6, 1051.7, 1061.7, 1081.8, 1087.4, 1081.8, 1063.0, 1077.3, 1148.4, 1173.1, 1190.4, 1166.7, 1131.9, 1132.3, 1121.8, 1117.4, 1136.6, 1152.0, 1143.0, 1126.1, 1156.2, 1147.7, 1156.0, 1152.7, 1158.2, 1137.1, 1127.3, 1120.8, 1121.9, 1123.5, 1129.1, 1122.9, 1121.5, 1124.9, 1134.2, 1121.8, 1139.6, 1137.3, 1141.3, 1129.0, 1142.5, 1162.9, 1180.5, 1177.7, 1165.9, 1157.1, 1161.7, 1146.1, 1141.1, 1147.0, 1146.6, 1137.6, 1129.0, 1131.1, 1131.1, 1136.7, 1133.1, 1128.2, 1118.3, 1119.3, 1113.8, 1114.3, 1104.3, 1098.2, 1092.3, 1089.3, 1086.7, 1085.9, 1084.1, 1083.0, 1073.0, 1074.3, 1063.5, 1061.7, 1058.7, 1066.2, 1085.5, 1088.1, 1083.8, 1086.2, 1083.0, 1090.3, 1110.3, 1119.3, 1111.1, 1131.8, 1129.1, 1116.3, 1112.3, 1094.5, 1111.7, 1110.6, 1126.9, 1122.2, 1133.6, 1130.8, 1154.5, 1143.7, 1135.8, 1121.6, 1112.7, 1123.5, 1118.7, 1118.7, 1123.0, 1109.7, 1098.4, 1085.0, 1072.2, 1073.5, 1073.6, 1063.7, 1061.0, 1060.7, 1061.4, 1067.9, 1062.9, 1061.5, 1059.6, 1051.0, 1060.1, 1053.9, 1068.3, 1059.1, 1065.3, 1081.2, 1079.0, 1066.4, 1072.2, 1068.8, 1064.1, 1069.0, 1076.2, 1071.5, 1057.9, 1040.2, 1038.8, 1039.2, 1030.3, 1022.1, 1025.3, 1023.2, 1023.1, 1017.7, 1018.7, 1016.2, 1008.5, 1013.4, 1029.1, 1028.6, 1027.9, 1037.6, 1021.2, 1017.7, 1014.0, 1024.2, 1034.9, 1039.9, 1062.7, 1070.5, 1065.9, 1057.5, 1068.5, 1093.7, 1100.5, 1113.8, 1107.9, 1114.1, 1103.1, 1102.0, 1097.8, 1099.9, 1082.2, 1083.4, 1084.5, 1084.1, 1097.7, 1108.7, 1100.8, 1112.1, 1131.5, 1114.6, 1104.9, 1084.8, 1098.6, 1079.2, 1073.0, 1080.0, 1099.7, 1096.0, 1105.8, 1113.9, 1108.8, 1107.1, 1110.0, 1125.0, 1133.9, 1149.2, 1165.1, 1168.4, 1166.2, 1174.0, 1195.0, 1173.6, 1193.4, 1184.5, 1162.8, 1194.7, 1165.9, 1146.8, 1132.5, 1131.0, 1132.0, 1154.9, 1172.2, 1143.4, 1164.3, 1179.3, 1176.2, 1173.1, 1187.7, 1209.8, 1210.9, 1194.2, 1200.5, 1202.5, 1227.4, 1238.8, 1203.4, 1193.1, 1162.5, 1169.2, 1154.2, 1153.8, 1150.2, 1147.8, 1137.8, 1167.6, 1182.6, 1182.5, 1193.0, 1156.0, 1171.4, 1150.2, 1151.8, 1154.6, 1137.4, 1135.9, 1124.4, 1114.0, 1099.5, 1117.6, 1113.7, 1117.2, 1098.4, 1120.1, 1096.8, 1112.2, 1135.9, 1127.5, 1142.5, 1139.6, 1150.6, 1175.9, 1180.1, 1167.6, 1158.5, 1178.5, 1199.1, 1207.7, 1193.0, 1174.7, 1169.2, 1162.1, 1144.3, 1137.4, 1146.1, 1130.7, 1145.5, 1143.6, 1123.3, 1114.2, 1124.4, 1141.4, 1140.2, 1125.1, 1131.4, 1116.0, 1124.2, 1125.1, 1124.0, 1123.9, 1144.0, 1144.0, 1150.5, 1145.1, 1120.6, 1121.8, 1124.0, 1135.2, 1137.2, 1127.9, 1127.8, 1129.4, 1132.6, 1132.7, 1149.1, 1128.9, 1131.0, 1130.5, 1113.8, 1117.1, 1097.5, 1085.4, 1086.4, 1093.3, 1089.8, 1079.7, 1064.5, 1071.9, 1069.3, 1070.2, 1067.9, 1086.6, 1077.2, 1079.0, 1082.0, 1065.2, 1071.6, 1081.1, 1056.6, 1067.1, 1074.0, 1069.0, 1068.0, 1080.9, 1077.6, 1079.6, 1077.7, 1075.9, 1104.8, 1117.2, 1120.0, 1112.2, 1129.2, 1131.4, 1120.2, 1124.0, 1133.9, 1118.4, 1110.0, 1114.9, 1125.3, 1123.2, 1109.3, 1132.7, 1128.0, 1137.6, 1139.2, 1123.8, 1133.3, 1125.8, 1129.4, 1105.3, 1130.1, 1129.6, 1125.4, 1124.5, 1116.4, 1121.9, 1121.3, 1118.8, 1121.7, 1123.5, 1119.1, 1129.0, 1134.8, 1127.7, 1136.8, 1136.3, 1139.4, 1136.9, 1160.5, 1170.0, 1187.5, 1194.2, 1184.5, 1182.1, 1180.4, 1185.8, 1156.2, 1166.0, 1180.5, 1177.6, 1178.9, 1181.6, 1215.3, 1222.2, 1202.5, 1213.9, 1208.2, 1191.0, 1188.0, 1199.9, 1196.6, 1185.2, 1169.7, 1163.0, 1157.5, 1160.8, 1167.6, 1176.7, 1187.2, 1191.3, 1166.2, 1163.9, 1167.1, 1161.3, 1159.4, 1176.7, 1187.4, 1181.6, 1189.5, 1210.3, 1195.2, 1193.2, 1243.5, 1249.6, 1217.4, 1221.2, 1217.3, 1232.2, 1218.2, 1220.5, 1232.4, 1244.2, 1225.0, 1204.8, 1216.0, 1215.8, 1198.6, 1195.8, 1200.9, 1203.2, 1196.1, 1193.4, 1185.6, 1183.7, 1185.1, 1183.0, 1186.4, 1179.0, 1165.0, 1169.5, 1146.8, 1142.0, 1127.7, 1133.6, 1113.9, 1109.3, 1110.4, 1106.5, 1082.1, 1091.8, 1102.7, 1092.1, 1087.3, 1098.0, 1098.2, 1119.6, 1118.5, 1101.4, 1110.4, 1124.0, 1140.3, 1129.7, 1129.7, 1133.6, 1119.6, 1125.9, 1112.3, 1110.4, 1122.6, 1124.7, 1132.0, 1118.1, 1113.6, 1115.8, 1130.4, 1134.9, 1133.1, 1145.0, 1141.5, 1149.9, 1146.5, 1143.7, 1161.2, 1179.6, 1169.2, 1157.0, 1169.1, 1175.0, 1181.8, 1190.4, 1182.4, 1177.1, 1168.6, 1185.2, 1179.6, 1185.3, 1193.3, 1180.1, 1181.3, 1180.9, 1186.6, 1191.8, 1199.1, 1192.7, 1196.1, 1206.4, 1196.5, 1197.1, 1202.4, 1214.2, 1242.3, 1216.3, 1227.3, 1214.4, 1233.1, 1234.4, 1249.9, 1265.1, 1276.4, 1275.0, 1266.2, 1237.2, 1256.9, 1285.6, 1301.8, 1298.4, 1299.8, 1312.1, 1307.7, 1296.1, 1310.1, 1303.0, 1325.9, 1331.3, 1362.6, 1373.6, 1389.5, 1421.5, 1410.1, 1431.3, 1433.3, 1417.0]\n",
            "1179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mydf = pd.DataFrame(float_lst)"
      ],
      "metadata": {
        "id": "ICgg6C0e4a38"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, x_frames, y_frames, data, start, end):\n",
        "        \n",
        "        self.x_frames = x_frames\n",
        "        self.y_frames = y_frames\n",
        "        self.data = data\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "\n",
        "        self.data = data.iloc[start : end]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data) - (self.x_frames + self.y_frames) + 1\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        idx += self.x_frames\n",
        "        data = self.data.iloc[idx-self.x_frames:idx+self.y_frames].values\n",
        "        X = data[:self.x_frames]\n",
        "        y = data[self.x_frames:]\n",
        "        \n",
        "        return X, y"
      ],
      "metadata": {
        "id": "t7hfIxSWGdBY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydf.plot(grid=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "e03BNnXww0d9",
        "outputId": "d1e020de-ff4f-4033-e523-1238927a63aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8efefb8f50>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x648 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering & Nomalizing Dataset"
      ],
      "metadata": {
        "id": "nUfe8HWD87PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sklearn에서 제공하는 Scaler에는 크게 세 가지가 있음.\n",
        "#### Standard, MinMax, Robust\n",
        "#### 이상치가 있는 시계열 자료에 가장 적합한 것이 Robust Scaler: 3분위 값, 1분위 값 기준으로 scaling"
      ],
      "metadata": {
        "id": "4tAmPHur2IPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Denoising Filter를 사용하지 않으면, Lagging Problem이 발생\n",
        "#### Moving Average가 잘 알려져 있는데, 이 방법으로는 위 문제를 해결할 수 없음.\n",
        "#### Savitzky-Golay Filter 사용"
      ],
      "metadata": {
        "id": "MuKV8epZ2vQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = RobustScaler()\n",
        "df_robust = scaler.fit_transform(mydf[-1000:])"
      ],
      "metadata": {
        "id": "kE2dEQSvBYtN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 계산의 편의를 위해 5000일의 데이터를 사용"
      ],
      "metadata": {
        "id": "omv1QC_e2htO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_robust.shape)\n",
        "mydf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzs-sb4T5BN8",
        "outputId": "a79acfc1-c1bc-4132-e7f5-1cf32fd120f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1179, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_s = scipy.signal.savgol_filter(np.ravel(df_robust), 15, 2)\n",
        "mydata = pd.DataFrame(df_s)\n",
        "mydata.plot(grid = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "trrfRQgS66ru",
        "outputId": "be1896df-9df9-45d8-e36c-de14ed84378b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8efefee7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x648 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(df_robust)\n",
        "plt.plot(mydata.to_numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "21fQomEI_R3t",
        "outputId": "dcbab56e-d115-47cf-cf07-3dfdc2895328"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8efd16f9d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x648 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filter와 Scaler를 적용한 값 = 주황색\n",
        "#### Scaler만 적용한 값 = 파란색\n",
        "#### -> Filter 적용시 denoising이 일어났음을 확인할 수 있음."
      ],
      "metadata": {
        "id": "6CjkiKtN3LWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Necessary Classes"
      ],
      "metadata": {
        "id": "UR8VWNBP9df6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTM 모형 사용: 시계열 데이터 예측에서 가장 널리 쓰이는 딥러닝 방법론"
      ],
      "metadata": {
        "id": "KYTTOV9K3WXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout, use_bn):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.input_dim = input_dim \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.dropout = dropout\n",
        "        self.use_bn = use_bn \n",
        "        \n",
        "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
        "        self.hidden = self.init_hidden()\n",
        "        self.regressor = self.make_regressor()\n",
        "        \n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
        "    \n",
        "    def make_regressor(self):\n",
        "        layers = []\n",
        "        if self.use_bn:\n",
        "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
        "        layers.append(nn.Dropout(self.dropout))\n",
        "        \n",
        "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
        "        regressor = nn.Sequential(*layers)\n",
        "        return regressor\n",
        "    \n",
        "    def forward(self, x):\n",
        "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
        "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1))\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "JldDK8gMQG-S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LSTM 학습에 필요한 train, validation, test 함수"
      ],
      "metadata": {
        "id": "M5Cb-UvE4EKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, partition, optimizer, loss_fn, args):\n",
        "    trainloader = DataLoader(partition['train'], \n",
        "                             batch_size=args.batch_size, \n",
        "                             shuffle=True, drop_last=True)\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    train_loss = 0.0\n",
        "    for i, (X, y) in enumerate(trainloader):\n",
        "        X = X.transpose(0, 1).float().to(args.device)\n",
        "        y_true = y.float().to(args.device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
        "\n",
        "        y_pred = model(X)\n",
        "        loss = torch.sqrt(loss_fn(y_pred.view(-1), y_true.view(-1)))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    return model, train_loss"
      ],
      "metadata": {
        "id": "YGjGwzN4QjlT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, partition, loss_fn, args):\n",
        "    valloader = DataLoader(partition['val'], \n",
        "                           batch_size=args.batch_size, \n",
        "                           shuffle=False, drop_last=True)\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(valloader):\n",
        "            X = X.transpose(0, 1).float().to(args.device)\n",
        "            y_true = y.float().to(args.device)\n",
        "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
        "\n",
        "            y_pred = model(X)\n",
        "            loss = torch.sqrt(loss_fn(y_pred.view(-1), y_true.view(-1)))\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss = val_loss / len(valloader)\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "YmuCsbwtQm6_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, partition, args):\n",
        "    testloader = DataLoader(partition['test'], \n",
        "                           batch_size=args.batch_size, \n",
        "                           shuffle=False, drop_last=True)\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(testloader):\n",
        "\n",
        "            X = X.transpose(0, 1).float().to(args.device)\n",
        "            y_true = y.float().to(args.device)\n",
        "            y_true1 = y_true[:,-4]\n",
        "            y_true2 = y_true[:,-3]\n",
        "            y_true3 = y_true[:,-2]\n",
        "            y_true4 = y_true[:,-1]\n",
        "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
        "\n",
        "            y_pred = model(X)\n",
        "            y_pred1 = y_pred[:,-4]\n",
        "            y_pred2 = y_pred[:,-3]\n",
        "            y_pred3 = y_pred[:,-2]\n",
        "            y_pred4 = y_pred[:,-1]\n",
        "    return y_true1, y_true2, y_true3, y_true4, y_pred1, y_pred2, y_pred3, y_pred4"
      ],
      "metadata": {
        "id": "_Zo43oTMQpfx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter Tuning을 도와줄 함수"
      ],
      "metadata": {
        "id": "az7sH-Hc4J2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(partition, args):\n",
        "\n",
        "    model = LSTM(args.input_dim, args.hid_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
        "    model.to(args.device)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adagrad':\n",
        "        optimizer = optim.Adagrad(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "        \n",
        "    for epoch in range(args.epoch):\n",
        "        ts = time.time()\n",
        "        model, train_loss = train(model, partition, optimizer, loss_fn, args)\n",
        "        val_loss = validate(model, partition, loss_fn, args)\n",
        "        te = time.time()\n",
        "        train_acc = 0\n",
        "        val_acc = 0\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'.format(epoch, train_loss, val_loss, te-ts))\n",
        "\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    return vars(args), result"
      ],
      "metadata": {
        "id": "rdtsfwC5Qr1c"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter Tuning 과정이 끝난 후, 최종적인 예측을 진행할 함수"
      ],
      "metadata": {
        "id": "VyPW5-ME4UWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(partition, args):\n",
        "\n",
        "    model = LSTM(args.input_dim, args.hid_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
        "    model.to(args.device)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    loss_fn = nn.MSELoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adagrad':\n",
        "        optimizer = optim.Adagrad(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    y_trues1 = []\n",
        "    y_trues2 = []\n",
        "    y_trues3 = []\n",
        "    y_trues4 = []\n",
        "    y_preds1 = []\n",
        "    y_preds2 = []\n",
        "    y_preds3 = []\n",
        "    y_preds4 = []    \n",
        "        \n",
        "    for epoch in range(args.epoch):\n",
        "        ts = time.time()\n",
        "        model, train_loss = train(model, partition, optimizer, loss_fn, args)\n",
        "        val_loss = validate(model, partition, loss_fn, args)\n",
        "        y_true1, y_true2, y_true3, y_true4, y_pred1, y_pred2, y_pred3, y_pred4 = test(model, partition, args)\n",
        "        te = time.time()\n",
        "        train_acc = 0\n",
        "        val_acc = 0\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        y_trues1.append(y_true1)\n",
        "        y_trues2.append(y_true2)\n",
        "        y_trues3.append(y_true3)\n",
        "        y_trues4.append(y_true4)\n",
        "        y_preds1.append(y_pred1)\n",
        "        y_preds2.append(y_pred2)\n",
        "        y_preds3.append(y_pred3)\n",
        "        y_preds4.append(y_pred4)\n",
        "        \n",
        "        print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'.format(epoch, train_loss, val_loss, te-ts))\n",
        "\n",
        "    #print(type(y_preds), type(y_trues))\n",
        "    #print(len(y_preds), len(y_trues))\n",
        "\n",
        "    for i in range(len(y_trues1)):\n",
        "        y_trues1[i] = [item for sublist in y_trues1[i] for item in sublist]\n",
        "    for i in range(len(y_trues2)):\n",
        "        y_trues2[i] = [item for sublist in y_trues2[i] for item in sublist]\n",
        "    for i in range(len(y_trues3)):\n",
        "        y_trues3[i] = [item for sublist in y_trues3[i] for item in sublist]\n",
        "    for i in range(len(y_trues4)):\n",
        "        y_trues4[i] = [item for sublist in y_trues4[i] for item in sublist]\n",
        "\n",
        "    trues_plots1 = []\n",
        "    trues_plots2 = []\n",
        "    trues_plots3 = []\n",
        "    trues_plots4 = []\n",
        "    preds_plots1 = []\n",
        "    preds_plots2 = []\n",
        "    preds_plots3 = []\n",
        "    preds_plots4 = []\n",
        "\n",
        "    for i in range(len(y_preds1[-1])):\n",
        "        preds_plots1.append(y_preds1[-1][i])\n",
        "    for i in range(len(y_preds2[-1])):\n",
        "        preds_plots2.append(y_preds2[-1][i])\n",
        "    for i in range(len(y_preds3[-1])):\n",
        "        preds_plots3.append(y_preds3[-1][i])\n",
        "    for i in range(len(y_preds4[-1])):\n",
        "        preds_plots4.append(y_preds4[-1][i]) \n",
        "\n",
        "    for i in range(len(y_trues1[-1])):\n",
        "        trues_plots1.append(y_trues1[-1][i])\n",
        "    for i in range(len(y_trues2[-1])):\n",
        "        trues_plots2.append(y_trues2[-1][i])\n",
        "    for i in range(len(y_trues3[-1])):\n",
        "        trues_plots3.append(y_trues3[-1][i])\n",
        "    for i in range(len(y_trues4[-1])):\n",
        "        trues_plots4.append(y_trues4[-1][i])\n",
        "    \n",
        "    pp1 = scaler.inverse_transform(np.array(preds_plots1).reshape(-1, 1))\n",
        "    pp2 = scaler.inverse_transform(np.array(preds_plots2).reshape(-1, 1))\n",
        "    pp3 = scaler.inverse_transform(np.array(preds_plots3).reshape(-1, 1))\n",
        "    pp4 = scaler.inverse_transform(np.array(preds_plots4).reshape(-1, 1))\n",
        "    tp1 = scaler.inverse_transform(np.array(trues_plots1).reshape(-1, 1))\n",
        "    tp2 = scaler.inverse_transform(np.array(trues_plots2).reshape(-1, 1))\n",
        "    tp3 = scaler.inverse_transform(np.array(trues_plots3).reshape(-1, 1))\n",
        "    tp4 = scaler.inverse_transform(np.array(trues_plots4).reshape(-1, 1))\n",
        "    plt.figure(figsize = (16, 36))\n",
        "    plt.subplot(4, 1, 1)\n",
        "    plt.plot(pp1, 'b-', pp1, 'bo', markersize = 1, label = '1st Week Predictions')\n",
        "    plt.plot(tp1, 'r-', tp1, 'ro', markersize = 1, label = '1st Week Trues')\n",
        "    #plt.legend()\n",
        "    #plt.show()\n",
        "    plt.subplot(4, 1, 2)\n",
        "    plt.plot(pp2, 'b-', pp2, 'bo', markersize = 1, label = '2nd Week Predictions')\n",
        "    plt.plot(tp2, 'r-', tp2, 'ro', markersize = 1, label = '2nd Week Trues')\n",
        "    #plt.legend()\n",
        "    #plt.show()\n",
        "    plt.subplot(4, 1, 3)\n",
        "    plt.plot(pp3, 'b-', pp3, 'bo', markersize = 1, label = '3rd Week Predictions')\n",
        "    plt.plot(tp3, 'r-', tp3, 'ro', markersize = 1, label = '3rd Week Trues')\n",
        "    #plt.legend()\n",
        "    #plt.show()\n",
        "    plt.subplot(4, 1, 4)\n",
        "    plt.plot(pp4, 'b-', pp4, 'bo', markersize = 1, label = '4th Week Predictions')\n",
        "    plt.plot(tp4, 'r-', tp4, 'ro', markersize = 1, label = '4th Week Trues')\n",
        "    #plt.legend()\n",
        "    #plt.show()\n",
        "    #plt.plot(mydf[-193:].values, color = 'k')\n",
        "\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    return vars(args), result"
      ],
      "metadata": {
        "id": "b5hfyX3UzClt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter Tuning 결과를 저장 및 불러오기"
      ],
      "metadata": {
        "id": "KXhtJCN64rBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "new_dir_path = 'results33'\n",
        "\n",
        "os.mkdir(new_dir_path)"
      ],
      "metadata": {
        "id": "Ypa3qv-BvN9D"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):\n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "    filename = 'results33/{}-{}.json'.format(exp_name, hash_key)\n",
        "    result.update(setting)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)\n",
        "\n",
        "    \n",
        "def load_exp_result(exp_name):\n",
        "    dir_path = 'results33'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        if exp_name in filename:\n",
        "            with open(join(dir_path, filename), 'r') as infile:\n",
        "                results = json.load(infile)\n",
        "                list_result.append(results)\n",
        "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "    return df"
      ],
      "metadata": {
        "id": "JA6cCDcQQwfA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 저장된 결과를 시각화 해줄 함수"
      ],
      "metadata": {
        "id": "MBx2MBvy4_BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_variation(var1, df, **kwargs):\n",
        "\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        row = df.loc[df[var1]==value1]\n",
        "        train_losses = list(row.train_losses)[0]\n",
        "        val_losses = list(row.val_losses)[0]\n",
        "\n",
        "        for epoch, train_loss in enumerate(train_losses):\n",
        "            list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1})\n",
        "        for epoch, val_loss in enumerate(val_losses):\n",
        "            list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train loss vs Val loss')\n",
        "    plt.subplots_adjust(top=0.89)\n",
        "\n",
        "def plot_loss_variation2(var1, var2, df, **kwargs):\n",
        "\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_losses = list(row.train_losses)[0]\n",
        "            val_losses = list(row.val_losses)[0]\n",
        "\n",
        "            for epoch, train_loss in enumerate(train_losses):\n",
        "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_loss in enumerate(val_losses):\n",
        "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train loss vs Val loss')\n",
        "    plt.subplots_adjust(top=0.89)\n",
        "\n",
        "def plot_ypred_ytrue(var3, var4, df, **kwargs):\n",
        "\n",
        "    list_ypred = df[var3].unique()\n",
        "    list_ytrue = df[var4].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for valuep in list_ypred:\n",
        "        for valuet in list_ytrue:\n",
        "            row = df.loc[df[var3]==valuep]\n",
        "            row = row.loc[df[var4]==valuet]\n",
        "\n",
        "            y_preds = list(row.y_preds)[0]\n",
        "            y_trues = list(row.y_trues)[0]\n",
        "\n",
        "            for epoch, y_pred in enumerate(y_preds):\n",
        "                list_data.append({'type':'y_pred', 'pred':y_pred, 'epoch':epoch, var3:valuep, var4:valuet})\n",
        "            for epoch, y_true in enumerate(y_trues):\n",
        "                list_data.append({'type':'y_true', 'loss':y_true, 'epoch':epoch, var3:valuep, var4:valuet})"
      ],
      "metadata": {
        "id": "QtIhb_yUQzts"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "768aHQ5elyQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Optimizer"
      ],
      "metadata": {
        "id": "aPH6dBV9l2XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 666\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp1_optim\"\n",
        "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "args.batch_size = 193\n",
        "args.x_frames = 4\n",
        "args.y_frames = 4\n",
        "trainset = MyDataset(args.x_frames, args.y_frames, mydata, 0, int(0.6*len(mydata)))\n",
        "valset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.6*len(mydata)), int(0.8*len(mydata)))\n",
        "testset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.8*len(mydata)), int(len(mydata)))\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
        "\n",
        "\n",
        "args.input_dim = 1\n",
        "args.hid_dim = 8\n",
        "args.n_layers = 4\n",
        "\n",
        "args.l2 = 0.00001\n",
        "args.dropout = 0.0\n",
        "args.use_bn = True\n",
        "\n",
        "args.optim = 'Adam'\n",
        "args.lr = 0.0001\n",
        "args.epoch = 200\n",
        "\n",
        "name_var1 = 'optim'\n",
        "list_var1 = ['SGD', 'RMSprop', 'Adam', 'Adagrad']\n",
        "\n",
        "for var1 in list_var1:\n",
        "    setattr(args, name_var1, var1)\n",
        "    print(args)\n",
        "                \n",
        "    setting, result = experiment(partition, deepcopy(args))\n",
        "    save_exp_result(setting, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJdGJ900Q6vf",
        "outputId": "069094fb-e20c-4d59-dbdf-f759e1ba2d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=200, exp_name='exp1_optim', hid_dim=8, input_dim=1, l2=1e-05, lr=0.0001, n_layers=4, optim='SGD', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.05109/0.37785. Took 1.13 sec\n",
            "Epoch 1, Loss(train/val) 1.04559/0.37720. Took 0.04 sec\n",
            "Epoch 2, Loss(train/val) 1.05333/0.37675. Took 0.04 sec\n",
            "Epoch 3, Loss(train/val) 1.05092/0.37645. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.04961/0.37627. Took 0.07 sec\n",
            "Epoch 5, Loss(train/val) 1.05089/0.37619. Took 0.07 sec\n",
            "Epoch 6, Loss(train/val) 1.04792/0.37615. Took 0.07 sec\n",
            "Epoch 7, Loss(train/val) 1.04323/0.37615. Took 0.06 sec\n",
            "Epoch 8, Loss(train/val) 1.04812/0.37619. Took 0.04 sec\n",
            "Epoch 9, Loss(train/val) 1.04283/0.37623. Took 0.04 sec\n",
            "Epoch 10, Loss(train/val) 1.03989/0.37629. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.03549/0.37638. Took 0.04 sec\n",
            "Epoch 12, Loss(train/val) 1.04422/0.37646. Took 0.04 sec\n",
            "Epoch 13, Loss(train/val) 1.03892/0.37653. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.02559/0.37661. Took 0.06 sec\n",
            "Epoch 15, Loss(train/val) 1.03603/0.37667. Took 0.07 sec\n",
            "Epoch 16, Loss(train/val) 1.03913/0.37675. Took 0.04 sec\n",
            "Epoch 17, Loss(train/val) 1.03441/0.37683. Took 0.04 sec\n",
            "Epoch 18, Loss(train/val) 1.02181/0.37691. Took 0.04 sec\n",
            "Epoch 19, Loss(train/val) 1.03193/0.37699. Took 0.04 sec\n",
            "Epoch 20, Loss(train/val) 1.03900/0.37692. Took 0.04 sec\n",
            "Epoch 21, Loss(train/val) 1.03335/0.37643. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 1.03505/0.37563. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 1.03446/0.37449. Took 0.06 sec\n",
            "Epoch 24, Loss(train/val) 1.02653/0.37303. Took 0.06 sec\n",
            "Epoch 25, Loss(train/val) 1.02900/0.37153. Took 0.08 sec\n",
            "Epoch 26, Loss(train/val) 1.03032/0.36993. Took 0.06 sec\n",
            "Epoch 27, Loss(train/val) 1.02401/0.36848. Took 0.06 sec\n",
            "Epoch 28, Loss(train/val) 1.02790/0.36683. Took 0.06 sec\n",
            "Epoch 29, Loss(train/val) 1.02748/0.36522. Took 0.07 sec\n",
            "Epoch 30, Loss(train/val) 1.02658/0.36361. Took 0.06 sec\n",
            "Epoch 31, Loss(train/val) 1.02647/0.36222. Took 0.06 sec\n",
            "Epoch 32, Loss(train/val) 1.03028/0.36146. Took 0.06 sec\n",
            "Epoch 33, Loss(train/val) 1.02481/0.36136. Took 0.08 sec\n",
            "Epoch 34, Loss(train/val) 1.02676/0.36105. Took 0.06 sec\n",
            "Epoch 35, Loss(train/val) 1.02735/0.36169. Took 0.06 sec\n",
            "Epoch 36, Loss(train/val) 1.02081/0.36172. Took 0.06 sec\n",
            "Epoch 37, Loss(train/val) 1.02513/0.36285. Took 0.08 sec\n",
            "Epoch 38, Loss(train/val) 1.02180/0.36297. Took 0.06 sec\n",
            "Epoch 39, Loss(train/val) 1.01737/0.36386. Took 0.06 sec\n",
            "Epoch 40, Loss(train/val) 1.02154/0.36390. Took 0.06 sec\n",
            "Epoch 41, Loss(train/val) 1.02435/0.36411. Took 0.06 sec\n",
            "Epoch 42, Loss(train/val) 1.01418/0.36461. Took 0.07 sec\n",
            "Epoch 43, Loss(train/val) 1.00552/0.36461. Took 0.05 sec\n",
            "Epoch 44, Loss(train/val) 1.01538/0.36482. Took 0.04 sec\n",
            "Epoch 45, Loss(train/val) 1.02267/0.36494. Took 0.04 sec\n",
            "Epoch 46, Loss(train/val) 1.02129/0.36496. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 1.00724/0.36481. Took 0.04 sec\n",
            "Epoch 48, Loss(train/val) 1.01685/0.36462. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 1.01002/0.36469. Took 0.04 sec\n",
            "Epoch 50, Loss(train/val) 1.01179/0.36398. Took 0.04 sec\n",
            "Epoch 51, Loss(train/val) 1.01761/0.36395. Took 0.04 sec\n",
            "Epoch 52, Loss(train/val) 1.00811/0.36372. Took 0.04 sec\n",
            "Epoch 53, Loss(train/val) 1.01898/0.36370. Took 0.07 sec\n",
            "Epoch 54, Loss(train/val) 1.01455/0.36382. Took 0.09 sec\n",
            "Epoch 55, Loss(train/val) 1.01539/0.36374. Took 0.06 sec\n",
            "Epoch 56, Loss(train/val) 1.01557/0.36375. Took 0.07 sec\n",
            "Epoch 57, Loss(train/val) 1.01440/0.36368. Took 0.06 sec\n",
            "Epoch 58, Loss(train/val) 1.01639/0.36307. Took 0.07 sec\n",
            "Epoch 59, Loss(train/val) 1.01165/0.36253. Took 0.07 sec\n",
            "Epoch 60, Loss(train/val) 1.01480/0.36248. Took 0.06 sec\n",
            "Epoch 61, Loss(train/val) 1.01359/0.36191. Took 0.06 sec\n",
            "Epoch 62, Loss(train/val) 1.00625/0.36157. Took 0.07 sec\n",
            "Epoch 63, Loss(train/val) 1.00532/0.36099. Took 0.06 sec\n",
            "Epoch 64, Loss(train/val) 1.01696/0.36089. Took 0.07 sec\n",
            "Epoch 65, Loss(train/val) 1.01520/0.36092. Took 0.06 sec\n",
            "Epoch 66, Loss(train/val) 1.01436/0.36064. Took 0.07 sec\n",
            "Epoch 67, Loss(train/val) 1.01201/0.36054. Took 0.06 sec\n",
            "Epoch 68, Loss(train/val) 1.00987/0.36036. Took 0.06 sec\n",
            "Epoch 69, Loss(train/val) 1.00783/0.35991. Took 0.06 sec\n",
            "Epoch 70, Loss(train/val) 1.01060/0.36007. Took 0.06 sec\n",
            "Epoch 71, Loss(train/val) 1.00795/0.35994. Took 0.06 sec\n",
            "Epoch 72, Loss(train/val) 1.00685/0.35986. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 1.01187/0.35981. Took 0.04 sec\n",
            "Epoch 74, Loss(train/val) 1.01523/0.35965. Took 0.04 sec\n",
            "Epoch 75, Loss(train/val) 1.00193/0.35947. Took 0.04 sec\n",
            "Epoch 76, Loss(train/val) 1.01417/0.35952. Took 0.04 sec\n",
            "Epoch 77, Loss(train/val) 1.00417/0.35930. Took 0.04 sec\n",
            "Epoch 78, Loss(train/val) 1.00354/0.35919. Took 0.04 sec\n",
            "Epoch 79, Loss(train/val) 1.00281/0.35907. Took 0.04 sec\n",
            "Epoch 80, Loss(train/val) 1.00191/0.35904. Took 0.03 sec\n",
            "Epoch 81, Loss(train/val) 1.00339/0.35886. Took 0.04 sec\n",
            "Epoch 82, Loss(train/val) 1.00367/0.35893. Took 0.04 sec\n",
            "Epoch 83, Loss(train/val) 1.01208/0.35881. Took 0.04 sec\n",
            "Epoch 84, Loss(train/val) 1.00331/0.35873. Took 0.04 sec\n",
            "Epoch 85, Loss(train/val) 1.00643/0.35867. Took 0.04 sec\n",
            "Epoch 86, Loss(train/val) 0.99717/0.35856. Took 0.04 sec\n",
            "Epoch 87, Loss(train/val) 1.00459/0.35843. Took 0.04 sec\n",
            "Epoch 88, Loss(train/val) 1.00431/0.35831. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 1.00669/0.35836. Took 0.04 sec\n",
            "Epoch 90, Loss(train/val) 1.00611/0.35843. Took 0.04 sec\n",
            "Epoch 91, Loss(train/val) 0.99303/0.35843. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 1.00956/0.35833. Took 0.04 sec\n",
            "Epoch 93, Loss(train/val) 1.00488/0.35835. Took 0.04 sec\n",
            "Epoch 94, Loss(train/val) 0.99832/0.35835. Took 0.04 sec\n",
            "Epoch 95, Loss(train/val) 0.99291/0.35832. Took 0.04 sec\n",
            "Epoch 96, Loss(train/val) 1.00424/0.35818. Took 0.04 sec\n",
            "Epoch 97, Loss(train/val) 1.00157/0.35809. Took 0.04 sec\n",
            "Epoch 98, Loss(train/val) 1.00506/0.35795. Took 0.05 sec\n",
            "Epoch 99, Loss(train/val) 1.00634/0.35801. Took 0.04 sec\n",
            "Epoch 100, Loss(train/val) 1.00539/0.35810. Took 0.04 sec\n",
            "Epoch 101, Loss(train/val) 0.99946/0.35802. Took 0.04 sec\n",
            "Epoch 102, Loss(train/val) 0.99374/0.35804. Took 0.04 sec\n",
            "Epoch 103, Loss(train/val) 1.00367/0.35799. Took 0.04 sec\n",
            "Epoch 104, Loss(train/val) 1.00226/0.35799. Took 0.04 sec\n",
            "Epoch 105, Loss(train/val) 0.99822/0.35786. Took 0.04 sec\n",
            "Epoch 106, Loss(train/val) 1.00208/0.35803. Took 0.04 sec\n",
            "Epoch 107, Loss(train/val) 1.00125/0.35799. Took 0.04 sec\n",
            "Epoch 108, Loss(train/val) 1.00534/0.35804. Took 0.04 sec\n",
            "Epoch 109, Loss(train/val) 0.99864/0.35818. Took 0.04 sec\n",
            "Epoch 110, Loss(train/val) 1.00428/0.35787. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 0.99379/0.35795. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 0.99754/0.35806. Took 0.04 sec\n",
            "Epoch 113, Loss(train/val) 1.00074/0.35793. Took 0.04 sec\n",
            "Epoch 114, Loss(train/val) 0.99846/0.35784. Took 0.04 sec\n",
            "Epoch 115, Loss(train/val) 0.99810/0.35764. Took 0.04 sec\n",
            "Epoch 116, Loss(train/val) 1.00041/0.35765. Took 0.04 sec\n",
            "Epoch 117, Loss(train/val) 0.99695/0.35753. Took 0.04 sec\n",
            "Epoch 118, Loss(train/val) 1.00129/0.35751. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 0.99871/0.35746. Took 0.04 sec\n",
            "Epoch 120, Loss(train/val) 1.00050/0.35774. Took 0.04 sec\n",
            "Epoch 121, Loss(train/val) 0.99363/0.35744. Took 0.04 sec\n",
            "Epoch 122, Loss(train/val) 0.99391/0.35741. Took 0.04 sec\n",
            "Epoch 123, Loss(train/val) 0.99536/0.35743. Took 0.04 sec\n",
            "Epoch 124, Loss(train/val) 0.99760/0.35762. Took 0.03 sec\n",
            "Epoch 125, Loss(train/val) 0.99878/0.35781. Took 0.04 sec\n",
            "Epoch 126, Loss(train/val) 0.99701/0.35791. Took 0.04 sec\n",
            "Epoch 127, Loss(train/val) 0.99549/0.35791. Took 0.04 sec\n",
            "Epoch 128, Loss(train/val) 0.99301/0.35772. Took 0.04 sec\n",
            "Epoch 129, Loss(train/val) 0.98258/0.35782. Took 0.04 sec\n",
            "Epoch 130, Loss(train/val) 0.99344/0.35777. Took 0.05 sec\n",
            "Epoch 131, Loss(train/val) 1.00055/0.35754. Took 0.04 sec\n",
            "Epoch 132, Loss(train/val) 0.99550/0.35745. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 0.99316/0.35725. Took 0.04 sec\n",
            "Epoch 134, Loss(train/val) 0.99096/0.35687. Took 0.04 sec\n",
            "Epoch 135, Loss(train/val) 0.99669/0.35712. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 0.97508/0.35740. Took 0.04 sec\n",
            "Epoch 137, Loss(train/val) 0.98331/0.35773. Took 0.04 sec\n",
            "Epoch 138, Loss(train/val) 0.99719/0.35745. Took 0.04 sec\n",
            "Epoch 139, Loss(train/val) 0.98950/0.35692. Took 0.04 sec\n",
            "Epoch 140, Loss(train/val) 0.98817/0.35714. Took 0.04 sec\n",
            "Epoch 141, Loss(train/val) 0.98793/0.35689. Took 0.04 sec\n",
            "Epoch 142, Loss(train/val) 0.98639/0.35745. Took 0.04 sec\n",
            "Epoch 143, Loss(train/val) 0.98993/0.35662. Took 0.04 sec\n",
            "Epoch 144, Loss(train/val) 0.98323/0.35703. Took 0.04 sec\n",
            "Epoch 145, Loss(train/val) 0.98375/0.35731. Took 0.04 sec\n",
            "Epoch 146, Loss(train/val) 0.99561/0.35711. Took 0.04 sec\n",
            "Epoch 147, Loss(train/val) 0.97748/0.35710. Took 0.04 sec\n",
            "Epoch 148, Loss(train/val) 0.99019/0.35655. Took 0.04 sec\n",
            "Epoch 149, Loss(train/val) 0.98943/0.35652. Took 0.04 sec\n",
            "Epoch 150, Loss(train/val) 0.97635/0.35696. Took 0.04 sec\n",
            "Epoch 151, Loss(train/val) 0.98770/0.35727. Took 0.07 sec\n",
            "Epoch 152, Loss(train/val) 0.98607/0.35687. Took 0.07 sec\n",
            "Epoch 153, Loss(train/val) 0.98732/0.35702. Took 0.06 sec\n",
            "Epoch 154, Loss(train/val) 0.97799/0.35677. Took 0.06 sec\n",
            "Epoch 155, Loss(train/val) 0.98693/0.35689. Took 0.06 sec\n",
            "Epoch 156, Loss(train/val) 0.98075/0.35682. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 0.98729/0.35616. Took 0.04 sec\n",
            "Epoch 158, Loss(train/val) 0.98851/0.35653. Took 0.04 sec\n",
            "Epoch 159, Loss(train/val) 0.99013/0.35645. Took 0.04 sec\n",
            "Epoch 160, Loss(train/val) 0.98459/0.35654. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 0.98063/0.35623. Took 0.04 sec\n",
            "Epoch 162, Loss(train/val) 0.98471/0.35598. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 0.98419/0.35681. Took 0.07 sec\n",
            "Epoch 164, Loss(train/val) 0.97612/0.35583. Took 0.07 sec\n",
            "Epoch 165, Loss(train/val) 0.98624/0.35609. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.97468/0.35661. Took 0.03 sec\n",
            "Epoch 167, Loss(train/val) 0.98308/0.35597. Took 0.04 sec\n",
            "Epoch 168, Loss(train/val) 0.98024/0.35551. Took 0.04 sec\n",
            "Epoch 169, Loss(train/val) 0.97711/0.35483. Took 0.04 sec\n",
            "Epoch 170, Loss(train/val) 0.97677/0.35455. Took 0.04 sec\n",
            "Epoch 171, Loss(train/val) 0.97246/0.35468. Took 0.04 sec\n",
            "Epoch 172, Loss(train/val) 0.98064/0.35465. Took 0.07 sec\n",
            "Epoch 173, Loss(train/val) 0.97537/0.35506. Took 0.06 sec\n",
            "Epoch 174, Loss(train/val) 0.98499/0.35503. Took 0.07 sec\n",
            "Epoch 175, Loss(train/val) 0.97994/0.35482. Took 0.06 sec\n",
            "Epoch 176, Loss(train/val) 0.97358/0.35469. Took 0.07 sec\n",
            "Epoch 177, Loss(train/val) 0.98378/0.35489. Took 0.09 sec\n",
            "Epoch 178, Loss(train/val) 0.98261/0.35516. Took 0.15 sec\n",
            "Epoch 179, Loss(train/val) 0.98021/0.35509. Took 0.12 sec\n",
            "Epoch 180, Loss(train/val) 0.98166/0.35516. Took 0.15 sec\n",
            "Epoch 181, Loss(train/val) 0.97208/0.35525. Took 0.16 sec\n",
            "Epoch 182, Loss(train/val) 0.97676/0.35474. Took 0.15 sec\n",
            "Epoch 183, Loss(train/val) 0.97465/0.35428. Took 0.13 sec\n",
            "Epoch 184, Loss(train/val) 0.98272/0.35422. Took 0.08 sec\n",
            "Epoch 185, Loss(train/val) 0.97920/0.35324. Took 0.07 sec\n",
            "Epoch 186, Loss(train/val) 0.97504/0.35309. Took 0.13 sec\n",
            "Epoch 187, Loss(train/val) 0.97246/0.35307. Took 0.12 sec\n",
            "Epoch 188, Loss(train/val) 0.98056/0.35336. Took 0.06 sec\n",
            "Epoch 189, Loss(train/val) 0.98021/0.35295. Took 0.07 sec\n",
            "Epoch 190, Loss(train/val) 0.97426/0.35288. Took 0.06 sec\n",
            "Epoch 191, Loss(train/val) 0.97151/0.35279. Took 0.06 sec\n",
            "Epoch 192, Loss(train/val) 0.97558/0.35253. Took 0.09 sec\n",
            "Epoch 193, Loss(train/val) 0.96910/0.35182. Took 0.10 sec\n",
            "Epoch 194, Loss(train/val) 0.97120/0.35244. Took 0.08 sec\n",
            "Epoch 195, Loss(train/val) 0.97727/0.35172. Took 0.04 sec\n",
            "Epoch 196, Loss(train/val) 0.97530/0.35157. Took 0.04 sec\n",
            "Epoch 197, Loss(train/val) 0.97579/0.35120. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 0.97350/0.35093. Took 0.04 sec\n",
            "Epoch 199, Loss(train/val) 0.97615/0.35093. Took 0.04 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=200, exp_name='exp1_optim', hid_dim=8, input_dim=1, l2=1e-05, lr=0.0001, n_layers=4, optim='RMSprop', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.08003/0.47097. Took 0.05 sec\n",
            "Epoch 1, Loss(train/val) 1.08836/0.47203. Took 0.07 sec\n",
            "Epoch 2, Loss(train/val) 1.08417/0.47286. Took 0.07 sec\n",
            "Epoch 3, Loss(train/val) 1.07988/0.47362. Took 0.07 sec\n",
            "Epoch 4, Loss(train/val) 1.07811/0.47428. Took 0.04 sec\n",
            "Epoch 5, Loss(train/val) 1.07890/0.47486. Took 0.04 sec\n",
            "Epoch 6, Loss(train/val) 1.07066/0.47536. Took 0.04 sec\n",
            "Epoch 7, Loss(train/val) 1.07480/0.47580. Took 0.04 sec\n",
            "Epoch 8, Loss(train/val) 1.07195/0.47618. Took 0.04 sec\n",
            "Epoch 9, Loss(train/val) 1.06977/0.47647. Took 0.04 sec\n",
            "Epoch 10, Loss(train/val) 1.06620/0.47669. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.06007/0.47688. Took 0.06 sec\n",
            "Epoch 12, Loss(train/val) 1.06871/0.47714. Took 0.07 sec\n",
            "Epoch 13, Loss(train/val) 1.06457/0.47733. Took 0.07 sec\n",
            "Epoch 14, Loss(train/val) 1.05594/0.47752. Took 0.06 sec\n",
            "Epoch 15, Loss(train/val) 1.05066/0.47765. Took 0.06 sec\n",
            "Epoch 16, Loss(train/val) 1.05672/0.47776. Took 0.06 sec\n",
            "Epoch 17, Loss(train/val) 1.05328/0.47788. Took 0.08 sec\n",
            "Epoch 18, Loss(train/val) 1.04255/0.47793. Took 0.06 sec\n",
            "Epoch 19, Loss(train/val) 1.04832/0.47804. Took 0.06 sec\n",
            "Epoch 20, Loss(train/val) 1.05698/0.47811. Took 0.06 sec\n",
            "Epoch 21, Loss(train/val) 1.05182/0.47814. Took 0.08 sec\n",
            "Epoch 22, Loss(train/val) 1.04833/0.47810. Took 0.06 sec\n",
            "Epoch 23, Loss(train/val) 1.04215/0.47788. Took 0.06 sec\n",
            "Epoch 24, Loss(train/val) 1.04892/0.47792. Took 0.08 sec\n",
            "Epoch 25, Loss(train/val) 1.03908/0.47772. Took 0.06 sec\n",
            "Epoch 26, Loss(train/val) 1.03641/0.47704. Took 0.06 sec\n",
            "Epoch 27, Loss(train/val) 1.03257/0.47630. Took 0.06 sec\n",
            "Epoch 28, Loss(train/val) 1.03136/0.47500. Took 0.07 sec\n",
            "Epoch 29, Loss(train/val) 1.04132/0.47413. Took 0.06 sec\n",
            "Epoch 30, Loss(train/val) 1.03831/0.47362. Took 0.04 sec\n",
            "Epoch 31, Loss(train/val) 1.03142/0.47266. Took 0.04 sec\n",
            "Epoch 32, Loss(train/val) 1.02365/0.47064. Took 0.04 sec\n",
            "Epoch 33, Loss(train/val) 1.03341/0.47016. Took 0.04 sec\n",
            "Epoch 34, Loss(train/val) 1.02977/0.46932. Took 0.04 sec\n",
            "Epoch 35, Loss(train/val) 1.02893/0.46674. Took 0.04 sec\n",
            "Epoch 36, Loss(train/val) 1.02201/0.46620. Took 0.04 sec\n",
            "Epoch 37, Loss(train/val) 1.01695/0.46159. Took 0.04 sec\n",
            "Epoch 38, Loss(train/val) 1.02316/0.45362. Took 0.04 sec\n",
            "Epoch 39, Loss(train/val) 1.01475/0.44752. Took 0.04 sec\n",
            "Epoch 40, Loss(train/val) 1.01608/0.44380. Took 0.04 sec\n",
            "Epoch 41, Loss(train/val) 1.01196/0.44152. Took 0.04 sec\n",
            "Epoch 42, Loss(train/val) 1.01602/0.44003. Took 0.04 sec\n",
            "Epoch 43, Loss(train/val) 1.00972/0.43971. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 1.01029/0.43917. Took 0.03 sec\n",
            "Epoch 45, Loss(train/val) 1.00824/0.43884. Took 0.04 sec\n",
            "Epoch 46, Loss(train/val) 1.00782/0.43876. Took 0.04 sec\n",
            "Epoch 47, Loss(train/val) 1.00766/0.43922. Took 0.04 sec\n",
            "Epoch 48, Loss(train/val) 0.99926/0.43951. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 1.00707/0.43958. Took 0.04 sec\n",
            "Epoch 50, Loss(train/val) 1.00157/0.43949. Took 0.04 sec\n",
            "Epoch 51, Loss(train/val) 1.00203/0.43949. Took 0.04 sec\n",
            "Epoch 52, Loss(train/val) 0.99619/0.43956. Took 0.04 sec\n",
            "Epoch 53, Loss(train/val) 0.99809/0.43952. Took 0.04 sec\n",
            "Epoch 54, Loss(train/val) 0.99182/0.43997. Took 0.05 sec\n",
            "Epoch 55, Loss(train/val) 0.99391/0.44014. Took 0.04 sec\n",
            "Epoch 56, Loss(train/val) 0.99225/0.44051. Took 0.04 sec\n",
            "Epoch 57, Loss(train/val) 0.99516/0.43955. Took 0.04 sec\n",
            "Epoch 58, Loss(train/val) 0.98777/0.44005. Took 0.04 sec\n",
            "Epoch 59, Loss(train/val) 0.98717/0.43851. Took 0.04 sec\n",
            "Epoch 60, Loss(train/val) 0.98775/0.43846. Took 0.04 sec\n",
            "Epoch 61, Loss(train/val) 0.98947/0.43917. Took 0.04 sec\n",
            "Epoch 62, Loss(train/val) 0.96924/0.43748. Took 0.04 sec\n",
            "Epoch 63, Loss(train/val) 0.97948/0.43838. Took 0.04 sec\n",
            "Epoch 64, Loss(train/val) 0.98332/0.43742. Took 0.04 sec\n",
            "Epoch 65, Loss(train/val) 0.97409/0.43708. Took 0.04 sec\n",
            "Epoch 66, Loss(train/val) 0.97600/0.43678. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 0.97460/0.43673. Took 0.04 sec\n",
            "Epoch 68, Loss(train/val) 0.97336/0.43714. Took 0.04 sec\n",
            "Epoch 69, Loss(train/val) 0.97428/0.43694. Took 0.04 sec\n",
            "Epoch 70, Loss(train/val) 0.97585/0.43516. Took 0.04 sec\n",
            "Epoch 71, Loss(train/val) 0.97906/0.43383. Took 0.04 sec\n",
            "Epoch 72, Loss(train/val) 0.97031/0.43360. Took 0.04 sec\n",
            "Epoch 73, Loss(train/val) 0.97706/0.43242. Took 0.04 sec\n",
            "Epoch 74, Loss(train/val) 0.96952/0.43257. Took 0.04 sec\n",
            "Epoch 75, Loss(train/val) 0.97032/0.43217. Took 0.04 sec\n",
            "Epoch 76, Loss(train/val) 0.97034/0.43148. Took 0.04 sec\n",
            "Epoch 77, Loss(train/val) 0.96192/0.43147. Took 0.04 sec\n",
            "Epoch 78, Loss(train/val) 0.96260/0.43148. Took 0.04 sec\n",
            "Epoch 79, Loss(train/val) 0.96423/0.43148. Took 0.04 sec\n",
            "Epoch 80, Loss(train/val) 0.94848/0.43013. Took 0.04 sec\n",
            "Epoch 81, Loss(train/val) 0.95977/0.42914. Took 0.05 sec\n",
            "Epoch 82, Loss(train/val) 0.95949/0.42865. Took 0.04 sec\n",
            "Epoch 83, Loss(train/val) 0.95572/0.42807. Took 0.04 sec\n",
            "Epoch 84, Loss(train/val) 0.95814/0.42738. Took 0.04 sec\n",
            "Epoch 85, Loss(train/val) 0.95975/0.42707. Took 0.04 sec\n",
            "Epoch 86, Loss(train/val) 0.94879/0.42746. Took 0.04 sec\n",
            "Epoch 87, Loss(train/val) 0.95610/0.42593. Took 0.04 sec\n",
            "Epoch 88, Loss(train/val) 0.95709/0.42499. Took 0.03 sec\n",
            "Epoch 89, Loss(train/val) 0.95204/0.42437. Took 0.04 sec\n",
            "Epoch 90, Loss(train/val) 0.95253/0.42346. Took 0.04 sec\n",
            "Epoch 91, Loss(train/val) 0.95096/0.42256. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 0.94773/0.42225. Took 0.04 sec\n",
            "Epoch 93, Loss(train/val) 0.94870/0.42141. Took 0.04 sec\n",
            "Epoch 94, Loss(train/val) 0.94989/0.42105. Took 0.04 sec\n",
            "Epoch 95, Loss(train/val) 0.94756/0.42036. Took 0.04 sec\n",
            "Epoch 96, Loss(train/val) 0.94833/0.42007. Took 0.04 sec\n",
            "Epoch 97, Loss(train/val) 0.94161/0.42008. Took 0.04 sec\n",
            "Epoch 98, Loss(train/val) 0.93929/0.41911. Took 0.03 sec\n",
            "Epoch 99, Loss(train/val) 0.94238/0.41872. Took 0.04 sec\n",
            "Epoch 100, Loss(train/val) 0.93808/0.41856. Took 0.04 sec\n",
            "Epoch 101, Loss(train/val) 0.92859/0.41763. Took 0.04 sec\n",
            "Epoch 102, Loss(train/val) 0.93457/0.41768. Took 0.04 sec\n",
            "Epoch 103, Loss(train/val) 0.93328/0.41730. Took 0.04 sec\n",
            "Epoch 104, Loss(train/val) 0.94040/0.41717. Took 0.04 sec\n",
            "Epoch 105, Loss(train/val) 0.92569/0.41533. Took 0.04 sec\n",
            "Epoch 106, Loss(train/val) 0.93568/0.41489. Took 0.04 sec\n",
            "Epoch 107, Loss(train/val) 0.92555/0.41449. Took 0.04 sec\n",
            "Epoch 108, Loss(train/val) 0.93136/0.41393. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 0.92852/0.41414. Took 0.04 sec\n",
            "Epoch 110, Loss(train/val) 0.92710/0.41340. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 0.92532/0.41286. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 0.92843/0.41316. Took 0.04 sec\n",
            "Epoch 113, Loss(train/val) 0.92828/0.41231. Took 0.04 sec\n",
            "Epoch 114, Loss(train/val) 0.92434/0.41181. Took 0.04 sec\n",
            "Epoch 115, Loss(train/val) 0.92574/0.41162. Took 0.04 sec\n",
            "Epoch 116, Loss(train/val) 0.92592/0.41120. Took 0.04 sec\n",
            "Epoch 117, Loss(train/val) 0.91768/0.40968. Took 0.04 sec\n",
            "Epoch 118, Loss(train/val) 0.92283/0.40891. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 0.91998/0.40897. Took 0.04 sec\n",
            "Epoch 120, Loss(train/val) 0.91863/0.40795. Took 0.04 sec\n",
            "Epoch 121, Loss(train/val) 0.91204/0.40829. Took 0.04 sec\n",
            "Epoch 122, Loss(train/val) 0.92321/0.40740. Took 0.04 sec\n",
            "Epoch 123, Loss(train/val) 0.91777/0.40754. Took 0.04 sec\n",
            "Epoch 124, Loss(train/val) 0.91371/0.40705. Took 0.03 sec\n",
            "Epoch 125, Loss(train/val) 0.90781/0.40659. Took 0.04 sec\n",
            "Epoch 126, Loss(train/val) 0.91501/0.40601. Took 0.04 sec\n",
            "Epoch 127, Loss(train/val) 0.91185/0.40526. Took 0.04 sec\n",
            "Epoch 128, Loss(train/val) 0.91330/0.40491. Took 0.04 sec\n",
            "Epoch 129, Loss(train/val) 0.91414/0.40333. Took 0.04 sec\n",
            "Epoch 130, Loss(train/val) 0.91500/0.40314. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 0.91045/0.40265. Took 0.04 sec\n",
            "Epoch 132, Loss(train/val) 0.90148/0.40410. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 0.90409/0.40321. Took 0.04 sec\n",
            "Epoch 134, Loss(train/val) 0.90971/0.40226. Took 0.04 sec\n",
            "Epoch 135, Loss(train/val) 0.90298/0.40175. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 0.88701/0.40218. Took 0.04 sec\n",
            "Epoch 137, Loss(train/val) 0.90012/0.40188. Took 0.04 sec\n",
            "Epoch 138, Loss(train/val) 0.89794/0.40276. Took 0.04 sec\n",
            "Epoch 139, Loss(train/val) 0.89541/0.40311. Took 0.04 sec\n",
            "Epoch 140, Loss(train/val) 0.89726/0.40227. Took 0.04 sec\n",
            "Epoch 141, Loss(train/val) 0.89277/0.40072. Took 0.04 sec\n",
            "Epoch 142, Loss(train/val) 0.90200/0.39940. Took 0.06 sec\n",
            "Epoch 143, Loss(train/val) 0.89855/0.39842. Took 0.06 sec\n",
            "Epoch 144, Loss(train/val) 0.89854/0.39695. Took 0.07 sec\n",
            "Epoch 145, Loss(train/val) 0.88642/0.39712. Took 0.07 sec\n",
            "Epoch 146, Loss(train/val) 0.88962/0.39633. Took 0.04 sec\n",
            "Epoch 147, Loss(train/val) 0.89112/0.39624. Took 0.04 sec\n",
            "Epoch 148, Loss(train/val) 0.88905/0.39645. Took 0.04 sec\n",
            "Epoch 149, Loss(train/val) 0.89380/0.39697. Took 0.04 sec\n",
            "Epoch 150, Loss(train/val) 0.88832/0.39660. Took 0.04 sec\n",
            "Epoch 151, Loss(train/val) 0.88852/0.39550. Took 0.04 sec\n",
            "Epoch 152, Loss(train/val) 0.88840/0.39480. Took 0.06 sec\n",
            "Epoch 153, Loss(train/val) 0.88314/0.39436. Took 0.07 sec\n",
            "Epoch 154, Loss(train/val) 0.88515/0.39309. Took 0.07 sec\n",
            "Epoch 155, Loss(train/val) 0.88474/0.39277. Took 0.06 sec\n",
            "Epoch 156, Loss(train/val) 0.87090/0.39392. Took 0.07 sec\n",
            "Epoch 157, Loss(train/val) 0.87976/0.39323. Took 0.06 sec\n",
            "Epoch 158, Loss(train/val) 0.87906/0.39301. Took 0.09 sec\n",
            "Epoch 159, Loss(train/val) 0.87307/0.39305. Took 0.06 sec\n",
            "Epoch 160, Loss(train/val) 0.87838/0.39253. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 0.87094/0.39415. Took 0.04 sec\n",
            "Epoch 162, Loss(train/val) 0.86812/0.39303. Took 0.04 sec\n",
            "Epoch 163, Loss(train/val) 0.87079/0.39188. Took 0.04 sec\n",
            "Epoch 164, Loss(train/val) 0.86985/0.39124. Took 0.04 sec\n",
            "Epoch 165, Loss(train/val) 0.86976/0.38842. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.87202/0.39010. Took 0.07 sec\n",
            "Epoch 167, Loss(train/val) 0.86950/0.38916. Took 0.06 sec\n",
            "Epoch 168, Loss(train/val) 0.86979/0.38790. Took 0.06 sec\n",
            "Epoch 169, Loss(train/val) 0.87208/0.38745. Took 0.04 sec\n",
            "Epoch 170, Loss(train/val) 0.86701/0.38742. Took 0.04 sec\n",
            "Epoch 171, Loss(train/val) 0.86771/0.38453. Took 0.04 sec\n",
            "Epoch 172, Loss(train/val) 0.86521/0.38590. Took 0.04 sec\n",
            "Epoch 173, Loss(train/val) 0.86906/0.38479. Took 0.04 sec\n",
            "Epoch 174, Loss(train/val) 0.86181/0.38366. Took 0.04 sec\n",
            "Epoch 175, Loss(train/val) 0.84404/0.38376. Took 0.04 sec\n",
            "Epoch 176, Loss(train/val) 0.85846/0.38488. Took 0.09 sec\n",
            "Epoch 177, Loss(train/val) 0.85735/0.38458. Took 0.04 sec\n",
            "Epoch 178, Loss(train/val) 0.85470/0.38425. Took 0.04 sec\n",
            "Epoch 179, Loss(train/val) 0.86196/0.38424. Took 0.04 sec\n",
            "Epoch 180, Loss(train/val) 0.84709/0.38215. Took 0.04 sec\n",
            "Epoch 181, Loss(train/val) 0.85722/0.38210. Took 0.04 sec\n",
            "Epoch 182, Loss(train/val) 0.85257/0.38230. Took 0.04 sec\n",
            "Epoch 183, Loss(train/val) 0.85190/0.38182. Took 0.06 sec\n",
            "Epoch 184, Loss(train/val) 0.84531/0.38180. Took 0.07 sec\n",
            "Epoch 185, Loss(train/val) 0.84732/0.38029. Took 0.06 sec\n",
            "Epoch 186, Loss(train/val) 0.84551/0.38100. Took 0.07 sec\n",
            "Epoch 187, Loss(train/val) 0.84704/0.38075. Took 0.06 sec\n",
            "Epoch 188, Loss(train/val) 0.85162/0.37973. Took 0.07 sec\n",
            "Epoch 189, Loss(train/val) 0.84944/0.37952. Took 0.07 sec\n",
            "Epoch 190, Loss(train/val) 0.85006/0.37814. Took 0.07 sec\n",
            "Epoch 191, Loss(train/val) 0.84679/0.37738. Took 0.07 sec\n",
            "Epoch 192, Loss(train/val) 0.84199/0.37769. Took 0.07 sec\n",
            "Epoch 193, Loss(train/val) 0.84261/0.38016. Took 0.07 sec\n",
            "Epoch 194, Loss(train/val) 0.83467/0.37910. Took 0.07 sec\n",
            "Epoch 195, Loss(train/val) 0.83902/0.37855. Took 0.07 sec\n",
            "Epoch 196, Loss(train/val) 0.84415/0.37727. Took 0.06 sec\n",
            "Epoch 197, Loss(train/val) 0.83982/0.37712. Took 0.06 sec\n",
            "Epoch 198, Loss(train/val) 0.83792/0.37590. Took 0.06 sec\n",
            "Epoch 199, Loss(train/val) 0.83483/0.37543. Took 0.07 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=200, exp_name='exp1_optim', hid_dim=8, input_dim=1, l2=1e-05, lr=0.0001, n_layers=4, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.19810/0.52168. Took 0.07 sec\n",
            "Epoch 1, Loss(train/val) 1.19850/0.52088. Took 0.06 sec\n",
            "Epoch 2, Loss(train/val) 1.19367/0.52017. Took 0.04 sec\n",
            "Epoch 3, Loss(train/val) 1.18993/0.51953. Took 0.04 sec\n",
            "Epoch 4, Loss(train/val) 1.19098/0.51894. Took 0.04 sec\n",
            "Epoch 5, Loss(train/val) 1.18881/0.51841. Took 0.04 sec\n",
            "Epoch 6, Loss(train/val) 1.18932/0.51790. Took 0.05 sec\n",
            "Epoch 7, Loss(train/val) 1.19426/0.51742. Took 0.04 sec\n",
            "Epoch 8, Loss(train/val) 1.18723/0.51696. Took 0.04 sec\n",
            "Epoch 9, Loss(train/val) 1.18221/0.51651. Took 0.04 sec\n",
            "Epoch 10, Loss(train/val) 1.17999/0.51605. Took 0.04 sec\n",
            "Epoch 11, Loss(train/val) 1.18053/0.51560. Took 0.04 sec\n",
            "Epoch 12, Loss(train/val) 1.18630/0.51513. Took 0.04 sec\n",
            "Epoch 13, Loss(train/val) 1.17273/0.51463. Took 0.04 sec\n",
            "Epoch 14, Loss(train/val) 1.17834/0.51410. Took 0.04 sec\n",
            "Epoch 15, Loss(train/val) 1.17374/0.51353. Took 0.06 sec\n",
            "Epoch 16, Loss(train/val) 1.16988/0.51289. Took 0.04 sec\n",
            "Epoch 17, Loss(train/val) 1.16398/0.51216. Took 0.04 sec\n",
            "Epoch 18, Loss(train/val) 1.17231/0.51135. Took 0.04 sec\n",
            "Epoch 19, Loss(train/val) 1.17417/0.51043. Took 0.04 sec\n",
            "Epoch 20, Loss(train/val) 1.15816/0.50935. Took 0.04 sec\n",
            "Epoch 21, Loss(train/val) 1.15431/0.50810. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 1.15664/0.50664. Took 0.04 sec\n",
            "Epoch 23, Loss(train/val) 1.15468/0.50495. Took 0.04 sec\n",
            "Epoch 24, Loss(train/val) 1.15471/0.50295. Took 0.04 sec\n",
            "Epoch 25, Loss(train/val) 1.15062/0.50059. Took 0.04 sec\n",
            "Epoch 26, Loss(train/val) 1.15310/0.49780. Took 0.04 sec\n",
            "Epoch 27, Loss(train/val) 1.14916/0.49462. Took 0.04 sec\n",
            "Epoch 28, Loss(train/val) 1.13883/0.49109. Took 0.04 sec\n",
            "Epoch 29, Loss(train/val) 1.13909/0.48723. Took 0.04 sec\n",
            "Epoch 30, Loss(train/val) 1.13614/0.48317. Took 0.04 sec\n",
            "Epoch 31, Loss(train/val) 1.13736/0.47912. Took 0.04 sec\n",
            "Epoch 32, Loss(train/val) 1.13519/0.47517. Took 0.04 sec\n",
            "Epoch 33, Loss(train/val) 1.12347/0.47168. Took 0.04 sec\n",
            "Epoch 34, Loss(train/val) 1.12007/0.46901. Took 0.04 sec\n",
            "Epoch 35, Loss(train/val) 1.11881/0.46671. Took 0.04 sec\n",
            "Epoch 36, Loss(train/val) 1.12252/0.46461. Took 0.04 sec\n",
            "Epoch 37, Loss(train/val) 1.11777/0.46267. Took 0.04 sec\n",
            "Epoch 38, Loss(train/val) 1.12242/0.46099. Took 0.04 sec\n",
            "Epoch 39, Loss(train/val) 1.12151/0.45960. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 1.11567/0.45843. Took 0.04 sec\n",
            "Epoch 41, Loss(train/val) 1.10588/0.45746. Took 0.04 sec\n",
            "Epoch 42, Loss(train/val) 1.11258/0.45675. Took 0.04 sec\n",
            "Epoch 43, Loss(train/val) 1.11104/0.45618. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 1.10353/0.45570. Took 0.04 sec\n",
            "Epoch 45, Loss(train/val) 1.10449/0.45537. Took 0.04 sec\n",
            "Epoch 46, Loss(train/val) 1.10418/0.45514. Took 0.04 sec\n",
            "Epoch 47, Loss(train/val) 1.10414/0.45492. Took 0.04 sec\n",
            "Epoch 48, Loss(train/val) 1.10357/0.45472. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 1.09878/0.45452. Took 0.04 sec\n",
            "Epoch 50, Loss(train/val) 1.09770/0.45437. Took 0.04 sec\n",
            "Epoch 51, Loss(train/val) 1.09805/0.45413. Took 0.04 sec\n",
            "Epoch 52, Loss(train/val) 1.09047/0.45400. Took 0.04 sec\n",
            "Epoch 53, Loss(train/val) 1.09795/0.45401. Took 0.04 sec\n",
            "Epoch 54, Loss(train/val) 1.08948/0.45398. Took 0.04 sec\n",
            "Epoch 55, Loss(train/val) 1.09297/0.45387. Took 0.04 sec\n",
            "Epoch 56, Loss(train/val) 1.08187/0.45388. Took 0.04 sec\n",
            "Epoch 57, Loss(train/val) 1.08641/0.45378. Took 0.04 sec\n",
            "Epoch 58, Loss(train/val) 1.08613/0.45362. Took 0.04 sec\n",
            "Epoch 59, Loss(train/val) 1.07938/0.45332. Took 0.04 sec\n",
            "Epoch 60, Loss(train/val) 1.08466/0.45323. Took 0.04 sec\n",
            "Epoch 61, Loss(train/val) 1.08202/0.45297. Took 0.04 sec\n",
            "Epoch 62, Loss(train/val) 1.08279/0.45287. Took 0.04 sec\n",
            "Epoch 63, Loss(train/val) 1.07795/0.45253. Took 0.04 sec\n",
            "Epoch 64, Loss(train/val) 1.07805/0.45227. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 1.07552/0.45210. Took 0.04 sec\n",
            "Epoch 66, Loss(train/val) 1.07227/0.45199. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 1.07524/0.45174. Took 0.04 sec\n",
            "Epoch 68, Loss(train/val) 1.06947/0.45151. Took 0.04 sec\n",
            "Epoch 69, Loss(train/val) 1.07326/0.45140. Took 0.04 sec\n",
            "Epoch 70, Loss(train/val) 1.06663/0.45111. Took 0.04 sec\n",
            "Epoch 71, Loss(train/val) 1.06559/0.45067. Took 0.04 sec\n",
            "Epoch 72, Loss(train/val) 1.06897/0.45044. Took 0.04 sec\n",
            "Epoch 73, Loss(train/val) 1.05975/0.45004. Took 0.04 sec\n",
            "Epoch 74, Loss(train/val) 1.05431/0.44958. Took 0.04 sec\n",
            "Epoch 75, Loss(train/val) 1.06142/0.44916. Took 0.04 sec\n",
            "Epoch 76, Loss(train/val) 1.06017/0.44869. Took 0.04 sec\n",
            "Epoch 77, Loss(train/val) 1.06252/0.44840. Took 0.04 sec\n",
            "Epoch 78, Loss(train/val) 1.05976/0.44794. Took 0.04 sec\n",
            "Epoch 79, Loss(train/val) 1.05332/0.44729. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 1.05609/0.44698. Took 0.04 sec\n",
            "Epoch 81, Loss(train/val) 1.05365/0.44677. Took 0.04 sec\n",
            "Epoch 82, Loss(train/val) 1.04399/0.44617. Took 0.04 sec\n",
            "Epoch 83, Loss(train/val) 1.04120/0.44540. Took 0.04 sec\n",
            "Epoch 84, Loss(train/val) 1.05065/0.44533. Took 0.04 sec\n",
            "Epoch 85, Loss(train/val) 1.04363/0.44508. Took 0.04 sec\n",
            "Epoch 86, Loss(train/val) 1.04285/0.44443. Took 0.08 sec\n",
            "Epoch 87, Loss(train/val) 1.04004/0.44381. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 1.03809/0.44358. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 1.03470/0.44305. Took 0.04 sec\n",
            "Epoch 90, Loss(train/val) 1.04343/0.44287. Took 0.04 sec\n",
            "Epoch 91, Loss(train/val) 1.03129/0.44273. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 1.03323/0.44227. Took 0.04 sec\n",
            "Epoch 93, Loss(train/val) 1.02576/0.44203. Took 0.07 sec\n",
            "Epoch 94, Loss(train/val) 1.03193/0.44164. Took 0.06 sec\n",
            "Epoch 95, Loss(train/val) 1.02165/0.44161. Took 0.07 sec\n",
            "Epoch 96, Loss(train/val) 1.02861/0.44133. Took 0.07 sec\n",
            "Epoch 97, Loss(train/val) 1.01601/0.44090. Took 0.07 sec\n",
            "Epoch 98, Loss(train/val) 1.02627/0.44088. Took 0.07 sec\n",
            "Epoch 99, Loss(train/val) 1.02222/0.44067. Took 0.07 sec\n",
            "Epoch 100, Loss(train/val) 1.02486/0.44051. Took 0.07 sec\n",
            "Epoch 101, Loss(train/val) 1.02377/0.44039. Took 0.07 sec\n",
            "Epoch 102, Loss(train/val) 1.01825/0.43991. Took 0.07 sec\n",
            "Epoch 103, Loss(train/val) 1.01122/0.43949. Took 0.07 sec\n",
            "Epoch 104, Loss(train/val) 1.00415/0.43919. Took 0.08 sec\n",
            "Epoch 105, Loss(train/val) 1.01235/0.43891. Took 0.07 sec\n",
            "Epoch 106, Loss(train/val) 1.00271/0.43831. Took 0.07 sec\n",
            "Epoch 107, Loss(train/val) 1.00639/0.43867. Took 0.08 sec\n",
            "Epoch 108, Loss(train/val) 1.00310/0.43852. Took 0.04 sec\n",
            "Epoch 109, Loss(train/val) 1.00585/0.43834. Took 0.04 sec\n",
            "Epoch 110, Loss(train/val) 1.00193/0.43787. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 0.99490/0.43763. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 0.99557/0.43714. Took 0.04 sec\n",
            "Epoch 113, Loss(train/val) 0.99287/0.43694. Took 0.04 sec\n",
            "Epoch 114, Loss(train/val) 0.99518/0.43694. Took 0.07 sec\n",
            "Epoch 115, Loss(train/val) 0.98712/0.43710. Took 0.07 sec\n",
            "Epoch 116, Loss(train/val) 0.99438/0.43738. Took 0.07 sec\n",
            "Epoch 117, Loss(train/val) 0.98696/0.43735. Took 0.07 sec\n",
            "Epoch 118, Loss(train/val) 0.98441/0.43734. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 0.98187/0.43702. Took 0.04 sec\n",
            "Epoch 120, Loss(train/val) 0.98233/0.43694. Took 0.04 sec\n",
            "Epoch 121, Loss(train/val) 0.97852/0.43682. Took 0.04 sec\n",
            "Epoch 122, Loss(train/val) 0.98066/0.43715. Took 0.04 sec\n",
            "Epoch 123, Loss(train/val) 0.98161/0.43728. Took 0.04 sec\n",
            "Epoch 124, Loss(train/val) 0.97512/0.43716. Took 0.07 sec\n",
            "Epoch 125, Loss(train/val) 0.96296/0.43756. Took 0.09 sec\n",
            "Epoch 126, Loss(train/val) 0.96249/0.43739. Took 0.07 sec\n",
            "Epoch 127, Loss(train/val) 0.96799/0.43749. Took 0.07 sec\n",
            "Epoch 128, Loss(train/val) 0.96935/0.43733. Took 0.07 sec\n",
            "Epoch 129, Loss(train/val) 0.96095/0.43681. Took 0.07 sec\n",
            "Epoch 130, Loss(train/val) 0.95898/0.43670. Took 0.07 sec\n",
            "Epoch 131, Loss(train/val) 0.96451/0.43723. Took 0.08 sec\n",
            "Epoch 132, Loss(train/val) 0.96641/0.43761. Took 0.07 sec\n",
            "Epoch 133, Loss(train/val) 0.96086/0.43762. Took 0.07 sec\n",
            "Epoch 134, Loss(train/val) 0.95494/0.43725. Took 0.07 sec\n",
            "Epoch 135, Loss(train/val) 0.95313/0.43740. Took 0.07 sec\n",
            "Epoch 136, Loss(train/val) 0.95563/0.43759. Took 0.07 sec\n",
            "Epoch 137, Loss(train/val) 0.95682/0.43767. Took 0.07 sec\n",
            "Epoch 138, Loss(train/val) 0.95144/0.43736. Took 0.07 sec\n",
            "Epoch 139, Loss(train/val) 0.94088/0.43678. Took 0.07 sec\n",
            "Epoch 140, Loss(train/val) 0.94591/0.43684. Took 0.07 sec\n",
            "Epoch 141, Loss(train/val) 0.93712/0.43681. Took 0.06 sec\n",
            "Epoch 142, Loss(train/val) 0.94264/0.43676. Took 0.06 sec\n",
            "Epoch 143, Loss(train/val) 0.94240/0.43658. Took 0.04 sec\n",
            "Epoch 144, Loss(train/val) 0.93615/0.43619. Took 0.04 sec\n",
            "Epoch 145, Loss(train/val) 0.93387/0.43597. Took 0.04 sec\n",
            "Epoch 146, Loss(train/val) 0.92792/0.43646. Took 0.04 sec\n",
            "Epoch 147, Loss(train/val) 0.93438/0.43599. Took 0.04 sec\n",
            "Epoch 148, Loss(train/val) 0.92963/0.43596. Took 0.04 sec\n",
            "Epoch 149, Loss(train/val) 0.92429/0.43568. Took 0.04 sec\n",
            "Epoch 150, Loss(train/val) 0.92486/0.43538. Took 0.04 sec\n",
            "Epoch 151, Loss(train/val) 0.92086/0.43491. Took 0.04 sec\n",
            "Epoch 152, Loss(train/val) 0.92079/0.43477. Took 0.04 sec\n",
            "Epoch 153, Loss(train/val) 0.92232/0.43453. Took 0.04 sec\n",
            "Epoch 154, Loss(train/val) 0.92754/0.43502. Took 0.04 sec\n",
            "Epoch 155, Loss(train/val) 0.92405/0.43487. Took 0.04 sec\n",
            "Epoch 156, Loss(train/val) 0.91691/0.43496. Took 0.04 sec\n",
            "Epoch 157, Loss(train/val) 0.91399/0.43466. Took 0.04 sec\n",
            "Epoch 158, Loss(train/val) 0.92152/0.43493. Took 0.04 sec\n",
            "Epoch 159, Loss(train/val) 0.90238/0.43421. Took 0.04 sec\n",
            "Epoch 160, Loss(train/val) 0.91521/0.43442. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 0.91157/0.43407. Took 0.04 sec\n",
            "Epoch 162, Loss(train/val) 0.90306/0.43413. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 0.89870/0.43407. Took 0.04 sec\n",
            "Epoch 164, Loss(train/val) 0.90554/0.43400. Took 0.04 sec\n",
            "Epoch 165, Loss(train/val) 0.89272/0.43353. Took 0.04 sec\n",
            "Epoch 166, Loss(train/val) 0.89251/0.43350. Took 0.04 sec\n",
            "Epoch 167, Loss(train/val) 0.90286/0.43401. Took 0.04 sec\n",
            "Epoch 168, Loss(train/val) 0.90089/0.43430. Took 0.04 sec\n",
            "Epoch 169, Loss(train/val) 0.89970/0.43448. Took 0.04 sec\n",
            "Epoch 170, Loss(train/val) 0.89123/0.43390. Took 0.04 sec\n",
            "Epoch 171, Loss(train/val) 0.88594/0.43353. Took 0.04 sec\n",
            "Epoch 172, Loss(train/val) 0.88256/0.43307. Took 0.04 sec\n",
            "Epoch 173, Loss(train/val) 0.88235/0.43252. Took 0.04 sec\n",
            "Epoch 174, Loss(train/val) 0.88109/0.43269. Took 0.04 sec\n",
            "Epoch 175, Loss(train/val) 0.87672/0.43257. Took 0.04 sec\n",
            "Epoch 176, Loss(train/val) 0.88156/0.43262. Took 0.04 sec\n",
            "Epoch 177, Loss(train/val) 0.88581/0.43329. Took 0.04 sec\n",
            "Epoch 178, Loss(train/val) 0.87005/0.43311. Took 0.04 sec\n",
            "Epoch 179, Loss(train/val) 0.87723/0.43279. Took 0.04 sec\n",
            "Epoch 180, Loss(train/val) 0.87805/0.43306. Took 0.04 sec\n",
            "Epoch 181, Loss(train/val) 0.85860/0.43256. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 0.86976/0.43245. Took 0.04 sec\n",
            "Epoch 183, Loss(train/val) 0.86820/0.43241. Took 0.04 sec\n",
            "Epoch 184, Loss(train/val) 0.86839/0.43268. Took 0.04 sec\n",
            "Epoch 185, Loss(train/val) 0.87110/0.43244. Took 0.04 sec\n",
            "Epoch 186, Loss(train/val) 0.86527/0.43233. Took 0.04 sec\n",
            "Epoch 187, Loss(train/val) 0.86483/0.43185. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 0.86271/0.43166. Took 0.04 sec\n",
            "Epoch 189, Loss(train/val) 0.85910/0.43132. Took 0.04 sec\n",
            "Epoch 190, Loss(train/val) 0.85306/0.43098. Took 0.04 sec\n",
            "Epoch 191, Loss(train/val) 0.85313/0.43056. Took 0.04 sec\n",
            "Epoch 192, Loss(train/val) 0.85438/0.43017. Took 0.04 sec\n",
            "Epoch 193, Loss(train/val) 0.85151/0.42956. Took 0.04 sec\n",
            "Epoch 194, Loss(train/val) 0.84719/0.42911. Took 0.04 sec\n",
            "Epoch 195, Loss(train/val) 0.84769/0.42930. Took 0.04 sec\n",
            "Epoch 196, Loss(train/val) 0.84214/0.42921. Took 0.04 sec\n",
            "Epoch 197, Loss(train/val) 0.85064/0.42931. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 0.84117/0.42880. Took 0.04 sec\n",
            "Epoch 199, Loss(train/val) 0.84281/0.42886. Took 0.04 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=200, exp_name='exp1_optim', hid_dim=8, input_dim=1, l2=1e-05, lr=0.0001, n_layers=4, optim='Adagrad', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.06434/0.39193. Took 0.04 sec\n",
            "Epoch 1, Loss(train/val) 1.06047/0.39284. Took 0.04 sec\n",
            "Epoch 2, Loss(train/val) 1.04772/0.39362. Took 0.04 sec\n",
            "Epoch 3, Loss(train/val) 1.05592/0.39430. Took 0.04 sec\n",
            "Epoch 4, Loss(train/val) 1.06201/0.39488. Took 0.04 sec\n",
            "Epoch 5, Loss(train/val) 1.05567/0.39538. Took 0.04 sec\n",
            "Epoch 6, Loss(train/val) 1.05520/0.39581. Took 0.04 sec\n",
            "Epoch 7, Loss(train/val) 1.05706/0.39605. Took 0.04 sec\n",
            "Epoch 8, Loss(train/val) 1.05796/0.39615. Took 0.04 sec\n",
            "Epoch 9, Loss(train/val) 1.05747/0.39622. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.05617/0.39629. Took 0.04 sec\n",
            "Epoch 11, Loss(train/val) 1.05148/0.39633. Took 0.04 sec\n",
            "Epoch 12, Loss(train/val) 1.05964/0.39637. Took 0.04 sec\n",
            "Epoch 13, Loss(train/val) 1.04941/0.39640. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.04710/0.39642. Took 0.04 sec\n",
            "Epoch 15, Loss(train/val) 1.05192/0.39643. Took 0.04 sec\n",
            "Epoch 16, Loss(train/val) 1.05041/0.39643. Took 0.04 sec\n",
            "Epoch 17, Loss(train/val) 1.05285/0.39643. Took 0.04 sec\n",
            "Epoch 18, Loss(train/val) 1.04618/0.39642. Took 0.04 sec\n",
            "Epoch 19, Loss(train/val) 1.04664/0.39640. Took 0.04 sec\n",
            "Epoch 20, Loss(train/val) 1.04659/0.39638. Took 0.04 sec\n",
            "Epoch 21, Loss(train/val) 1.04937/0.39636. Took 0.04 sec\n",
            "Epoch 22, Loss(train/val) 1.05094/0.39642. Took 0.04 sec\n",
            "Epoch 23, Loss(train/val) 1.05156/0.39648. Took 0.04 sec\n",
            "Epoch 24, Loss(train/val) 1.03771/0.39651. Took 0.04 sec\n",
            "Epoch 25, Loss(train/val) 1.04987/0.39651. Took 0.04 sec\n",
            "Epoch 26, Loss(train/val) 1.04662/0.39647. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.04943/0.39639. Took 0.07 sec\n",
            "Epoch 28, Loss(train/val) 1.05027/0.39626. Took 0.06 sec\n",
            "Epoch 29, Loss(train/val) 1.04139/0.39613. Took 0.08 sec\n",
            "Epoch 30, Loss(train/val) 1.04854/0.39599. Took 0.04 sec\n",
            "Epoch 31, Loss(train/val) 1.04246/0.39583. Took 0.04 sec\n",
            "Epoch 32, Loss(train/val) 1.04971/0.39568. Took 0.04 sec\n",
            "Epoch 33, Loss(train/val) 1.04378/0.39553. Took 0.04 sec\n",
            "Epoch 34, Loss(train/val) 1.04936/0.39541. Took 0.04 sec\n",
            "Epoch 35, Loss(train/val) 1.04516/0.39525. Took 0.04 sec\n",
            "Epoch 36, Loss(train/val) 1.05176/0.39515. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.04370/0.39505. Took 0.09 sec\n",
            "Epoch 38, Loss(train/val) 1.04569/0.39491. Took 0.07 sec\n",
            "Epoch 39, Loss(train/val) 1.03921/0.39484. Took 0.06 sec\n",
            "Epoch 40, Loss(train/val) 1.05532/0.39479. Took 0.04 sec\n",
            "Epoch 41, Loss(train/val) 1.04973/0.39475. Took 0.04 sec\n",
            "Epoch 42, Loss(train/val) 1.04504/0.39462. Took 0.04 sec\n",
            "Epoch 43, Loss(train/val) 1.04279/0.39452. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 1.04225/0.39442. Took 0.04 sec\n",
            "Epoch 45, Loss(train/val) 1.04230/0.39438. Took 0.06 sec\n",
            "Epoch 46, Loss(train/val) 1.04548/0.39439. Took 0.07 sec\n",
            "Epoch 47, Loss(train/val) 1.02981/0.39425. Took 0.08 sec\n",
            "Epoch 48, Loss(train/val) 1.04771/0.39420. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 1.04305/0.39410. Took 0.04 sec\n",
            "Epoch 50, Loss(train/val) 1.04460/0.39397. Took 0.04 sec\n",
            "Epoch 51, Loss(train/val) 1.04019/0.39387. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 1.04377/0.39379. Took 0.04 sec\n",
            "Epoch 53, Loss(train/val) 1.04223/0.39374. Took 0.06 sec\n",
            "Epoch 54, Loss(train/val) 1.04682/0.39371. Took 0.07 sec\n",
            "Epoch 55, Loss(train/val) 1.04161/0.39362. Took 0.04 sec\n",
            "Epoch 56, Loss(train/val) 1.04165/0.39358. Took 0.04 sec\n",
            "Epoch 57, Loss(train/val) 1.04306/0.39353. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 1.04079/0.39345. Took 0.04 sec\n",
            "Epoch 59, Loss(train/val) 1.04148/0.39343. Took 0.04 sec\n",
            "Epoch 60, Loss(train/val) 1.03785/0.39334. Took 0.04 sec\n",
            "Epoch 61, Loss(train/val) 1.04297/0.39332. Took 0.07 sec\n",
            "Epoch 62, Loss(train/val) 1.03104/0.39330. Took 0.06 sec\n",
            "Epoch 63, Loss(train/val) 1.03043/0.39327. Took 0.06 sec\n",
            "Epoch 64, Loss(train/val) 1.03689/0.39317. Took 0.04 sec\n",
            "Epoch 65, Loss(train/val) 1.03539/0.39311. Took 0.04 sec\n",
            "Epoch 66, Loss(train/val) 1.04016/0.39308. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 1.03449/0.39300. Took 0.04 sec\n",
            "Epoch 68, Loss(train/val) 1.04209/0.39300. Took 0.04 sec\n",
            "Epoch 69, Loss(train/val) 1.03716/0.39288. Took 0.04 sec\n",
            "Epoch 70, Loss(train/val) 1.04215/0.39283. Took 0.05 sec\n",
            "Epoch 71, Loss(train/val) 1.03836/0.39276. Took 0.07 sec\n",
            "Epoch 72, Loss(train/val) 1.04310/0.39275. Took 0.06 sec\n",
            "Epoch 73, Loss(train/val) 1.04075/0.39273. Took 0.06 sec\n",
            "Epoch 74, Loss(train/val) 1.03471/0.39261. Took 0.06 sec\n",
            "Epoch 75, Loss(train/val) 1.03565/0.39253. Took 0.06 sec\n",
            "Epoch 76, Loss(train/val) 1.03521/0.39249. Took 0.06 sec\n",
            "Epoch 77, Loss(train/val) 1.03820/0.39251. Took 0.07 sec\n",
            "Epoch 78, Loss(train/val) 1.04210/0.39248. Took 0.07 sec\n",
            "Epoch 79, Loss(train/val) 1.03754/0.39240. Took 0.06 sec\n",
            "Epoch 80, Loss(train/val) 1.02007/0.39226. Took 0.06 sec\n",
            "Epoch 81, Loss(train/val) 1.03545/0.39218. Took 0.06 sec\n",
            "Epoch 82, Loss(train/val) 1.03415/0.39216. Took 0.07 sec\n",
            "Epoch 83, Loss(train/val) 1.03737/0.39210. Took 0.06 sec\n",
            "Epoch 84, Loss(train/val) 1.02890/0.39203. Took 0.06 sec\n",
            "Epoch 85, Loss(train/val) 1.03300/0.39202. Took 0.06 sec\n",
            "Epoch 86, Loss(train/val) 1.03916/0.39201. Took 0.06 sec\n",
            "Epoch 87, Loss(train/val) 1.01610/0.39194. Took 0.06 sec\n",
            "Epoch 88, Loss(train/val) 1.03906/0.39194. Took 0.06 sec\n",
            "Epoch 89, Loss(train/val) 1.03222/0.39184. Took 0.06 sec\n",
            "Epoch 90, Loss(train/val) 1.03549/0.39183. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 1.03244/0.39171. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 1.03366/0.39162. Took 0.04 sec\n",
            "Epoch 93, Loss(train/val) 1.02558/0.39157. Took 0.04 sec\n",
            "Epoch 94, Loss(train/val) 1.04016/0.39156. Took 0.04 sec\n",
            "Epoch 95, Loss(train/val) 1.03927/0.39161. Took 0.04 sec\n",
            "Epoch 96, Loss(train/val) 1.03661/0.39164. Took 0.04 sec\n",
            "Epoch 97, Loss(train/val) 1.03423/0.39155. Took 0.04 sec\n",
            "Epoch 98, Loss(train/val) 1.03884/0.39150. Took 0.04 sec\n",
            "Epoch 99, Loss(train/val) 1.02375/0.39137. Took 0.04 sec\n",
            "Epoch 100, Loss(train/val) 1.03598/0.39136. Took 0.04 sec\n",
            "Epoch 101, Loss(train/val) 1.03465/0.39136. Took 0.04 sec\n",
            "Epoch 102, Loss(train/val) 1.03293/0.39129. Took 0.04 sec\n",
            "Epoch 103, Loss(train/val) 1.02612/0.39123. Took 0.04 sec\n",
            "Epoch 104, Loss(train/val) 1.03730/0.39121. Took 0.04 sec\n",
            "Epoch 105, Loss(train/val) 1.02791/0.39114. Took 0.04 sec\n",
            "Epoch 106, Loss(train/val) 1.03360/0.39111. Took 0.04 sec\n",
            "Epoch 107, Loss(train/val) 1.02173/0.39101. Took 0.04 sec\n",
            "Epoch 108, Loss(train/val) 1.03255/0.39095. Took 0.04 sec\n",
            "Epoch 109, Loss(train/val) 1.03287/0.39096. Took 0.04 sec\n",
            "Epoch 110, Loss(train/val) 1.02069/0.39087. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 1.03001/0.39083. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 1.03143/0.39087. Took 0.04 sec\n",
            "Epoch 113, Loss(train/val) 1.02771/0.39076. Took 0.04 sec\n",
            "Epoch 114, Loss(train/val) 1.03164/0.39072. Took 0.04 sec\n",
            "Epoch 115, Loss(train/val) 1.03111/0.39066. Took 0.04 sec\n",
            "Epoch 116, Loss(train/val) 1.01846/0.39063. Took 0.04 sec\n",
            "Epoch 117, Loss(train/val) 1.02609/0.39059. Took 0.04 sec\n",
            "Epoch 118, Loss(train/val) 1.02870/0.39058. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 1.03677/0.39060. Took 0.04 sec\n",
            "Epoch 120, Loss(train/val) 1.02524/0.39047. Took 0.04 sec\n",
            "Epoch 121, Loss(train/val) 1.02672/0.39037. Took 0.04 sec\n",
            "Epoch 122, Loss(train/val) 1.02141/0.39032. Took 0.04 sec\n",
            "Epoch 123, Loss(train/val) 1.03202/0.39030. Took 0.04 sec\n",
            "Epoch 124, Loss(train/val) 1.02578/0.39026. Took 0.04 sec\n",
            "Epoch 125, Loss(train/val) 1.03391/0.39024. Took 0.04 sec\n",
            "Epoch 126, Loss(train/val) 1.03345/0.39030. Took 0.04 sec\n",
            "Epoch 127, Loss(train/val) 1.03007/0.39027. Took 0.04 sec\n",
            "Epoch 128, Loss(train/val) 1.02938/0.39017. Took 0.04 sec\n",
            "Epoch 129, Loss(train/val) 1.02653/0.39004. Took 0.04 sec\n",
            "Epoch 130, Loss(train/val) 1.03048/0.39007. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 1.02150/0.38993. Took 0.04 sec\n",
            "Epoch 132, Loss(train/val) 1.02169/0.38985. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 1.02661/0.38980. Took 0.04 sec\n",
            "Epoch 134, Loss(train/val) 1.02690/0.38987. Took 0.04 sec\n",
            "Epoch 135, Loss(train/val) 1.02467/0.38984. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 1.02723/0.38973. Took 0.04 sec\n",
            "Epoch 137, Loss(train/val) 1.02763/0.38970. Took 0.04 sec\n",
            "Epoch 138, Loss(train/val) 1.02641/0.38964. Took 0.04 sec\n",
            "Epoch 139, Loss(train/val) 1.02253/0.38958. Took 0.04 sec\n",
            "Epoch 140, Loss(train/val) 1.03169/0.38962. Took 0.04 sec\n",
            "Epoch 141, Loss(train/val) 1.01666/0.38953. Took 0.04 sec\n",
            "Epoch 142, Loss(train/val) 1.02246/0.38949. Took 0.04 sec\n",
            "Epoch 143, Loss(train/val) 1.02927/0.38947. Took 0.04 sec\n",
            "Epoch 144, Loss(train/val) 1.02739/0.38949. Took 0.04 sec\n",
            "Epoch 145, Loss(train/val) 1.02701/0.38945. Took 0.04 sec\n",
            "Epoch 146, Loss(train/val) 1.02195/0.38935. Took 0.04 sec\n",
            "Epoch 147, Loss(train/val) 1.02480/0.38933. Took 0.04 sec\n",
            "Epoch 148, Loss(train/val) 1.02046/0.38923. Took 0.04 sec\n",
            "Epoch 149, Loss(train/val) 1.01709/0.38920. Took 0.03 sec\n",
            "Epoch 150, Loss(train/val) 1.01663/0.38917. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 1.02350/0.38918. Took 0.04 sec\n",
            "Epoch 152, Loss(train/val) 1.02025/0.38907. Took 0.03 sec\n",
            "Epoch 153, Loss(train/val) 1.02529/0.38907. Took 0.04 sec\n",
            "Epoch 154, Loss(train/val) 1.02877/0.38907. Took 0.04 sec\n",
            "Epoch 155, Loss(train/val) 1.02021/0.38905. Took 0.04 sec\n",
            "Epoch 156, Loss(train/val) 1.02044/0.38895. Took 0.04 sec\n",
            "Epoch 157, Loss(train/val) 1.02103/0.38901. Took 0.04 sec\n",
            "Epoch 158, Loss(train/val) 1.01727/0.38892. Took 0.05 sec\n",
            "Epoch 159, Loss(train/val) 1.01742/0.38879. Took 0.04 sec\n",
            "Epoch 160, Loss(train/val) 1.02099/0.38881. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 1.01636/0.38876. Took 0.04 sec\n",
            "Epoch 162, Loss(train/val) 1.01949/0.38868. Took 0.04 sec\n",
            "Epoch 163, Loss(train/val) 1.02228/0.38870. Took 0.04 sec\n",
            "Epoch 164, Loss(train/val) 1.02537/0.38867. Took 0.04 sec\n",
            "Epoch 165, Loss(train/val) 1.01075/0.38861. Took 0.04 sec\n",
            "Epoch 166, Loss(train/val) 1.02155/0.38862. Took 0.04 sec\n",
            "Epoch 167, Loss(train/val) 1.02092/0.38854. Took 0.04 sec\n",
            "Epoch 168, Loss(train/val) 1.02287/0.38851. Took 0.04 sec\n",
            "Epoch 169, Loss(train/val) 1.01423/0.38837. Took 0.04 sec\n",
            "Epoch 170, Loss(train/val) 1.02549/0.38838. Took 0.04 sec\n",
            "Epoch 171, Loss(train/val) 1.02515/0.38843. Took 0.04 sec\n",
            "Epoch 172, Loss(train/val) 1.02174/0.38838. Took 0.04 sec\n",
            "Epoch 173, Loss(train/val) 1.02369/0.38834. Took 0.04 sec\n",
            "Epoch 174, Loss(train/val) 1.01047/0.38830. Took 0.04 sec\n",
            "Epoch 175, Loss(train/val) 1.01194/0.38820. Took 0.04 sec\n",
            "Epoch 176, Loss(train/val) 1.02070/0.38815. Took 0.04 sec\n",
            "Epoch 177, Loss(train/val) 1.01221/0.38809. Took 0.04 sec\n",
            "Epoch 178, Loss(train/val) 1.02277/0.38809. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 1.02046/0.38811. Took 0.07 sec\n",
            "Epoch 180, Loss(train/val) 1.01815/0.38799. Took 0.08 sec\n",
            "Epoch 181, Loss(train/val) 1.02547/0.38802. Took 0.08 sec\n",
            "Epoch 182, Loss(train/val) 1.01776/0.38795. Took 0.10 sec\n",
            "Epoch 183, Loss(train/val) 1.01878/0.38793. Took 0.09 sec\n",
            "Epoch 184, Loss(train/val) 1.02256/0.38791. Took 0.06 sec\n",
            "Epoch 185, Loss(train/val) 1.01801/0.38788. Took 0.10 sec\n",
            "Epoch 186, Loss(train/val) 1.00807/0.38778. Took 0.09 sec\n",
            "Epoch 187, Loss(train/val) 1.02621/0.38783. Took 0.10 sec\n",
            "Epoch 188, Loss(train/val) 1.01689/0.38769. Took 0.08 sec\n",
            "Epoch 189, Loss(train/val) 1.01645/0.38766. Took 0.07 sec\n",
            "Epoch 190, Loss(train/val) 1.01913/0.38768. Took 0.06 sec\n",
            "Epoch 191, Loss(train/val) 1.01419/0.38761. Took 0.10 sec\n",
            "Epoch 192, Loss(train/val) 1.01434/0.38749. Took 0.08 sec\n",
            "Epoch 193, Loss(train/val) 1.02201/0.38740. Took 0.09 sec\n",
            "Epoch 194, Loss(train/val) 1.01940/0.38745. Took 0.06 sec\n",
            "Epoch 195, Loss(train/val) 1.01954/0.38744. Took 0.04 sec\n",
            "Epoch 196, Loss(train/val) 1.02017/0.38743. Took 0.04 sec\n",
            "Epoch 197, Loss(train/val) 1.01701/0.38733. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 1.02111/0.38731. Took 0.04 sec\n",
            "Epoch 199, Loss(train/val) 1.01903/0.38723. Took 0.04 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var1 = 'optim'\n",
        "df = load_exp_result('exp1_optim')\n",
        "\n",
        "plot_loss_variation(var1, df, sharey=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "AHoKMe2hvzpt",
        "outputId": "6d066849-d6ab-41c5-9381-272ed93b0d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 923.375x216 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5YAAADXCAYAAABh584qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZxcVZn/8c/T3VlJJ2myELITlgwQliGBEFHBEUZgVFBGIaAjOhJ9jY6ijiNuiDjO4MzoiD8ZmYCICwGUDIssg4qEPUuHLSEhkIR00gnZO0ln7aWe3x/33srt6tq6u6q7qvv7fr3qlbq3bt06Velz733uOec55u6IiIiIiIiIdFZFTxdAREREREREypsCSxEREREREekSBZYiIiIiIiLSJQosRUREREREpEsUWIqIiIiIiEiXKLAUERERERGRLlFgKSIi3cLMHjOzT3TyvevM7PxCl6kUmdnVZvZshtcmm5mbWVV3l0tERCQbBZYiIpKRme2NPRJmdiC2fFVH9uXuF7n7L4tV1lJhZuPMrMXMjk3z2v1m9p89US4REZFiUmApIiIZufuQ6AGsBz4QW3dXtJ1a0A5z943AE8DH4+vN7EjgYqDXB9ciItL3KLAUEZEOM7PzzKzezL5mZpuBX5hZjZk9bGbbzKwhfD4+9p4FZvbp8PnVZvasmf1nuO1bZnZRnp89wMx+bGabwsePzWxA+NrI8HN3mdlOM3vGzCrC175mZhvNrNHMVpnZe9Pse6aZbTazyti6D5nZq+Hzs8ys1sz2mNkWM/tRhmL+kpTAErgCWOHuy8zsOjNbE5ZlhZl9KJ/vnqa8Y83sofC7rjaza2KvpS2rmQ00s9+Y2Y7wd1piZkd15vNFREQiCixFRKSzxgBHApOAOQTnlF+EyxOBA8BPs7x/JrAKGAn8O/BzM7M8PvebwNnA6cBpwFnAt8LXvgLUA6OAo4BvAG5mU4HPA2e6ezXwPmBd6o7dfRGwD/ir2OorgXnh85uBm919KHAs8NsMZbwfGGlm74yt+ziHWyvXAO8ChgHfBX5jZkfn8d1T3UPwfccCfwv8q5lFZc9U1k+EnzsBGAF8luD/SkREpNMUWIqISGclgO+4+yF3P+DuO9x9vrvvd/dG4PvAuVneX+fut7l7K0HAdTRBMJjLVcCN7r7V3bcRBGZR62BzuJ9J7t7s7s+4uwOtwADgJDPr5+7r3H1Nhv3fDcwGMLNqgu6rd8f2f5yZjXT3ve6+MN0O3P0A8Dvg78L9HA9MJwxQ3f137r7J3RPufi/wJkGAnDczmwCcA3zN3Q+6+8vA7dFnZilrM0FAeZy7t7r7Unff05HPFhERSaXAsgSZ2bVmNji2/KiZDe/Gzx9pZs1m9tks29xgZv/UXWUKP/NqM8vW+iEC9GwdCrt7rjKzV8IuhqfHXltnZs+kbP+ymS0Pnw82s7vMbJmZLQ+7ig7pjnJ30jZ3PxgthOX/HzOrM7M9wNPA8Hi30hSboyfuvj98ms/3HQvUxZbrwnUA/wGsBv5gZmvN7Lpw/6uBa4EbgK1mdo+ZjSW9ecCHw+61HwZedPfo8/4eOAF4Pfz/fX+Wcv4S+IiZDSQIfB93960AZvZ34f/9LjPbBUwjaLntiLHAzjCIj9QB43KU9dfA48A9YVfifzezfh387HZ6uN6938xeCuvdCjP7TOy1j5nZq2b2Wvj67VG5YvX1VTN73cx+2p3nWxFd80lvosCyNF0LJA8y7n6xu+/qxs//CLCQ8I59MZkSfkhx9HQdusrdTwP+myDQiasOW5owsxNTXvsisMXdT3H3aQSBQXO+H5olgCsWT1n+CjAVmBl2v3x3uD6f7q0dsYmgu21kYrgOd29096+4+xTgg8CXLRxL6e7z3P2d4Xsd+EG6nbv7CoIA7SLadoPF3d9099nA6PD995nZERnK+SywE7gE+BhhN1gzmwTcRtA1d4S7DweW0/HfaRNwZNiqGv8tNmYra9iS+113Pwl4B/B+DrdydkWP1LswKJ5LkFjqNOAvgQXhaxcCXwIucveTgTOA52nbMn6Vu58KnAocAh4sdplFYnr6fNVt13zS+ymw7AZm9uWw9WG5mV0brpsc3h29y8xWmtl94d3+LxDchX7SzJ4Mt10X3lGK3nOnmb0Rvvd8M3vOzN40sw51o8piNsEF4jhrm3jjm+HnPktw8Ritvya8G/6Kmc2P7ryZ2bFmtjBsffkXM9sbrj/PgoQaDwErwnUPmNnS8I7ynNi+Pxl+5mKCLl/SB5VhHYq8wOHWo8hvgcvD57M53MUSgi6cG6MFd1/l7ocyfdfYd/uBmb1I0Do22w63eCYDJwumB/mvsI49YWajCvxdAaoJxurtsiAD6neK8BkQ/GbfMrNRZjYSuB74DSRbro4zMwN2E3SBTZjZVDP7KwtaIQ+G5Uxk+Yx5BIH+uwm6tBLu/2NmNsrdE0B08Zd2P2EX3F8RBHXDgd+HLx1BENhuC/f5SYIWyw5x9w0EQdK/WZCQ51SCmxHRb5G2rGb2HjM7JbwRsYfg5kW771BG9a4aqAJ2hL/LIXdfFb72TeCfwky9hF1/74i9Hv89m4B/Biaa2WldLJP0UWVUbyKFuua708x+ZsF131oLrvXuCL/vnQUqq5Q6d9ejiA+CMTXLCC4khgCvEdxNnUxwYXFOuN0dBCc/CBJKjIztYx1BF6nJQAtwCsFNgaXh+4zgjvgDaT5/KvByhsfwNNtPAN4Mn/8r8JWU7zEYGErQ1Swq74jY+/8F+Mfw+cPA7PD5Z4G94fPzCJJjHBN735Hhv4MI7tyPILjIXk+QhKM/8Bzw057+P9Wjex9lWIcWADPC59cC/5pSjqnA8+HyS8BJwPJw+XRgK0FA+i/A8eH6XN/1n8PnY2N1pgr4M3Bp+JoTtMxAEIh1uC6Fn3V++Pw8oD7l9bHh998LvAF8Jvzcqthv8+nw+dXAsynvd4Jxf7k+eyDwE+Dt8PETYGD42pfCbfcRJLX5drj+VGAx0EjQivgwMDbLd51IEGw9krL+N+H/0V6Cv8VLc/xmx4T7+VnK+u+H5dgO/Ah4KttvE3tf9LcQ/abjw++ykyAh0GdzlZXgQnJV+BttCX+/qjKvd7eH3/VugjG4FeH6ncCwLP8/Cwjra2zdA8DlxTqm6dF7H2VYbwp5zXcnQTKxqHx7Usp+ek///+jRDXWgpwvQ2x8Ed7xvjC1/D/hCeMBYH1v/V9FBIsdB5s3Y+l9x+EJxCvByAcr7T8D3w+enArXh82tTvsePYgeZc4FnwoPQW8Ct4fodHL74GUrbwPLJlM+9AXglfOwmyPh4KfCr2DZfQIFln3uUYR1aQHDR/hZBi9S4NOV4hGDqiXlhmZbHthlCMK7vvwlamU7M47tOCp9fklJn/h74Ufi8NVYfC/Jd9ei9j3Krd+G+TiG4sfAScGe4LhlYhq+/TBCAXx6uW0D7wPJBFFjq0YlHudUbCnvNd2dK+VLLnvUmnB6946HxbT3Lcyyncyj2PBFbTkD7/08LUuzfm2Ff53n7fvyzgTFmdlW4PNaCbIbZ3ElwwHjFzK4mCBxz2Rcr43nA+cAsd99vZgsIWiREcinFOgRBi8lSgvGV/48gUIy7F7iFoGWqDXffC/wv8L9mliDISDqf7N91Hx2Xz28lkk5J1jt3XwYsM7NfE1zwXk3QYnQGwc3MZcDpFiSBG5Rux2H34FOAlXl8J5GOKMV6U+hrvnj5UsuumKMP0BjL4nsGuDTsS38E8KFwHQTjOGaFz68kSPQAQVetagrAgzFap2d4tDnAmNkJwBB3H+fuk919MvBvBAeep8PvMciCRBEfiL21GnjbggQKV8XWLwQuC59fkaWYw4CGMKj8C4LWSoBFwLlmNiLc90c69SNIuSubOpTyPge+DZwd/l3H3U8wb+Pj8ZVmdo6Z1YTP+xN0k42ykWb6rnGLCerMyPACeTZBF0sIjvd/m+P9IpGyqXdmNiS8QRk5ncP15t+A/4yPHSNzUNkv3H6Du79aiO8hfU451ZtCX/OJKLAsNnd/keDuzmKCQOl2d38pfHkV8DkzWwnUAD8L188F/i8ayN2NZhNc8MbNJxgn+SLBXbBXgMeAJbFtvk3w3Z4DXo+tv5YgI+OrwHEEXVzT+T+gKvwdbiIISHH3twm6yL4Q7lt3kPugMqtDbXgwl+EPga+mrG909x94kCwk7ljgKTNbRtCdr5agDkLm7xrf79vAdcCTBHV1qbtHGS73AWdZMLXJXwE3FuArSi9VZvXOgH+2YNqQlwnmNb0awN0fJRhD+pgF05A8T9AtPH5T567wPLWcYGzcJd1ZeOk9yqzeFPqaTwQLbqpLdzOzycDDHkwp0CuFmcIOuLub2RUEByudsKUg+kIdihTiu5rZXncv5TkxpQz0pXonUiiqN9JXqL+zFNN04KdmZgRJSD7Vw+UREREREZEiUIuliIiIiIiIdInGWIqIiIiIiEiXKLAUERERERGRLim7wPLCCy90grl/9NCjnB4lRfVIjzJ9lBTVIz3K9FFSVI/0KNOHpFF2geX27dt7uggiZU/1SKTrVI9Euk71SKT3KLvAUkREREREREqLAksRERERycrM7jCzrWa2PMPrV5nZq2a2zMyeN7PTuruMItKzFFiKiIiISC53Ahdmef0t4Fx3PwX4HjC3OwolIqWjqqcLUEhL6xpYuHYHZ08ZwfRJNT1dHJGypbokUhiqS9JbuPvTZjY5y+vPxxYXAuO7+plL6xqY/2I9Bnz4jPGqQyIlrtcElkvrGrhi7gs0tzpVFcan33kMRwys4h3HjgRIntjjz3WAEmkvqkstrU5lhXHjJdO4cubEni6WSFl5fvV27l68nkeWvU3Cg+5BMybXMHxwf0ZVD9BFsvR2fw881pUdLK1r4KP/8zytiWD5niUb+F54PtING5HS1GsCyxfWbKe5Ncj+25Jwbn16LQD/xRu4gTsYYAYJh4FVFdx1zdkFPSDpQCe9wc+fWdumLn3j/mUsWLWVz5x7rP6uRfKwtK6Bj/18EYlYQvoEsHhdQ3L57sXr+ZdLT2HqmGqdN6RXMbP3EASW78yyzRxgDsDEielvXC5cuyMZVAK0huejO597ize27gWgf6Vx95xZye1Vj0R6Vq8JLGcdO5KqijdpSbSdWiYBydlmnCDABDjYkuDWBasBqG84wMdnTebKmROZt2g99y5Zz1FDB3Le1NEs37Sb7Y2HGFU9gJPHDmP5pt0YcPLYYTTsb6JmUD8aDjQzbFA/rn9wOe7QLzzQ6eAm5WZpXQOPLd/cbv0fVmzhide3MG3sMC4/c6JaMEWyWLh2R/Jck0nC4Rv3LwOCm579Ko0bPjiNhv1N6l0jZcvMTgVuBy5y9x2ZtnP3uYRjMGfMmJG2tpw9ZQT9K42m1rYvR0ElQFOr85nf1LJzbxMA/asquOvThW00EJH89ZrAcvqkGm68ZFryRJ2PP67cmnz+jfuX8ZM/rWJzY1O4Zjd/WLGlU2VpanVufWoNnz33WBau3cER/St57e3d7N7fAhAEqUcPpeFAMzWD+ycvJPI5EKpVVIpp4dqM1wG0JuCV+t28Ur+MB16q59K/HN+hv12RvuLsKSOorLB2NzozcYLzRnT+quDwDNwVwJTRQ/jUOcfoho6UNDObCPwv8HF3f6Or+5s+qYa758zia/NfZXUsmEy1PXndBoeaE8x/sV7nJJEeYp7rtmqJmTFjhtfW1mZ8/R/nvcjvX327G0uUmZFsLM2p0oKD6JhhA/nEO45h+qSaZBA5fFA/dh1o5o0tjfz+lU0kHKrCsW9Tx1Qzf+kGzKzdmJ3o/emC13mL1vPY8re5aNrR6orVPaynCxCXqR4trWvgqtsX0tScIJHmfen0Vwu9dJ+yqEcQHGOvf3A5rQmnIjy+AyxZ15D3eSHViWOqmXDk4OTyrv1NHGpJMGvKCPYcalGCE8lXp+qRmd0NnAeMBLYA3wH6Abj7rWZ2O3AZUBe+pcXdZ+Tab67ruqV1DVwejvvPq5zAmZNrGDKwiuGD+nPV2ZNUJ6QYSup8VCp6XWB5y5Or+c/HV+EE/+Onjh/GUUMHAsFJuLaugTxvIveYKPnQ3GfWdqisUXKIYYP6YRhPvrE1OVYuvt83tjby59e3JddHNaPC4L0nHsV5U0ezY98hRhwxIK8WqXStqMrk1k5JHYCy1aP4DYnlm3az+K2dWe8WQ3ASP2/qaHXhk2Irm3oEmY+Ntz61hj+t3JKzu2xnVADnn3RUcihHdAwGdEyWSFnVIzh8TbG98RDQseu5/pUV3D1H3WOl4EqqHpWKXhdYRi0uzS0J+qXpax+d1P/8+lYS4RGpvH6B7mVARQWcMLqac08YxZ5DLckDezTu9PoHl9OScPpVGvfMmcWqzY1864FlyQN+hcGMSTUcf1R1xguabK2rmbYts8ClpA5A+ZzII1GdOticuw2zMsyQlUgoo6wURdnWo1RRi2a+3WW7oqoyqJPRR1VWGN+LerykBJuZguEyPOZKZr2iHkXBZj43P6+cOZF//dApnS2iSDolVY9KRa8LLCG/k2B8GyB5J2xU9QAgyNgXpYg/ZfwwVm5upKUl6B5oBCfmT7/zGPYcamH1lkaWrt+Fe9DlKeGkvYsWvdabpHb3HVM9gM1h4JlOVaXxV1NHA+DubGs8xN5DLazdvq/Nb1NhcH7YehpPZnHrU2v488qtJNwZ0C/9IP1sLajR/3G6ADf1fQW+mCqpA1BHT+TxwP/eJet5pX53Xu+rAGbPnKgWEimUsq5HqZbWNfDjP73Bs6u3t2u9NIKWx137m1gSyyZbKBXQrrt7zeB+7D7QnDz3TRk9hPP/YjR3PLeOptZEsovh1y46sc2x9Xe169mxt5nRQ9sfWxWUlqReV4+uun0hh5oTGRsKDBhbM4iTjx7a5rpCf5PSBSVVj0pFrwwsuypdqyeQtUUtXaAav4tWaXD5WRMZN3xQch+NB5p57e09nHz0UNZs38efVmzJ2XpaYXDquGG8nOeFfW9RVREEsK0pV0LHjTqCmVNGJDP2rt7cSO36BtyhsgImjTiCYYP68fL6XW0uovpXVXDDB05m2cZdbNlzEIAFq7bhHvzGZ0yq4eUNu2hudQZWVXD9B04OsgCH3UNXb2ns6NimkjoAdaUe5XMSTzWwX9t6pBO6dFJnx4bdAbwf2Oru09K8bsDNwMXAfuBqd38x134LcT6Kn28qKwzMaG1t2+Mm6mnzxMotQdAX9gJpPNjC61sai9KlNhsDpowczMGWBBt3HWzzWnzM9dK6BmbPfYGWRDC/9EdmTODDZ4xn1eZGHlv+NicfPZTqQVWcPWWkgtHu1WvOR5H4zc8nV21N1pVsqtSrRrqmpOpRqVBgmUEhTmy5uuWmmrdoPd9+YBmtabqQRtObROVJnRYl3vV00pGDqdu5v82+Kw1Sx71HrbH7mlpzdiPp6/JJxNS/qoK7M8+NWlIHoELcIV64dgeNB5q5/dm38urOd9yoI6jbuZ/mVtcJXTqrs4Hlu4G9wK8yBJYXA/9IEFjOBG5295m59luo81HqjclM555s3VRrBvfnhoeWt5uaoSccN+oIjjyiP29u3UvD/uac21fFegCt2LSbVzbsxgmC1Bs+OC057RdAU0sr/Sork62jQLueJvmMJe3jwWuvOh+lE/0d3Lt4fbtrn7iqCuPezyj5nHRKSdWjUlG0wLKU7xB3p46evDqT9OaWJ1fzwz+sIuFBAPnlv57arsti/0rjvLAL6oI3trW5Iw5w1W0LaW5NYFm68nZUZYVxzTuP4bZn36K1t/UBTsOAf3rfVD73nuMyvVwyClmPou58z7y5vUPvi7rVZRt7K5Ki0/XIzCYDD2c4H/0PsMDd7w6XVwHnuXvWFOOldj5KTbz129oNeWfSLEcVALEhJuOHD6Q+1oLav6qCT71jMi+s3cGAqgqGD+6fzA3w3d+/RlPYSvzpdx5D9aB+WQP7XhaI9trzUap5i9bzrfuXZc1yftyoI5gyakjGYTIiGZRUPSoVxZzH8k7gp8CvMrx+EXB8+JgJ/Cz8t1eZPqmmQwepjm4P4STCVRXJltHoxNewv4lX63eHXUid0yYM53PvOS7tCfKua85OO+Z0wRvbkmNLKyw4UV///pNZvmk399XW09KaoCI8Ma/Zvi+ZFCmeuOWCk8cw/8X6Nhc5UVCRrStXR6ZrKQXRb9/XTJ9Uw7Xnn8CSdTtpag7+Ht5/6tE8/OrbWVsyHVi8roHF6xq4Z/EGvnepWjClx4wDNsSW68N1pTF3VZ5Szx9GcGEd1cLKCsMT3uYi24B+lcbpE4azc18TU0YN4bypo7n/pfp2YzuPHNyPhv3NJXNcTkCbk0R9SrfcppYEtz69tt374ueWloQnt+lfabQmwHH6p3RFvvK2hclAVL0tyseVMycydUw1tz61hhWbdvP27oPtbpyv3raP1dv2AfC7pfXZeh6JSA5FCyzd/enwDnEmlxB0S3JgoZkNN7Ojc90hlvamT6rhrk+f3S5YPHvKCAb0axtwRtunHjRT16XrZpU6tvSyM8bnlT0w2vdlZ4xP2xobtdLet7SelpYgMInm6IyPK4Ig+c/lMyZw8thhPLlqa9pxqakBaZiolKoK48Sjh+ZMPJNPQDumegBb9h4KxnKG07R85txj++zJKN3f4MdnTWb+i/XcEybCyqbVnW/ev4wFq7b26d9RSp+ZzQHmAEycWNrBxYfDY250Drj+/YfHisf/TdcKd+XMicnMtQkPAq3bPnEmENx4XL2lsU0gWirdcPORqZTx8h9sTvC1+15hXM0gGvY3c6glCMdbEs63HljG+h37WLN9H1v3HGTWlBFUD+pH44Fmnl29jZrBA7j2ghOUVbdETJ9Uw21/F0ynmauHTXNLgoVrd+j/SaSTijrGMkfXo4eBm9z92XD5CeBr7p61P0SpdT0qdeV0QstU1mzdg+OvReNQawb358aHX2t3MRUF1rPnvtDmAiJK5BOf8+1/X6znrkXrk9ucNn4Ys6aM4LW393DRtKO5cubEjv62JdVlojvr0bxF69tNP5Mt0IzmY1UXWUlDXWE7qKvngHzfn26ewSjwnDLyCG4Ph0RUGBwzaghrtu5NG+CNqxnEuGEDWVLX0K4nS4XB1KOqeX1zY8m0mmZiwHGjh9CaSFC380Aya/zJY4dx+ZkTk+eQ3y5Zz9bGg/SrrGzz/qhbJtCuN1EB5iLts+cjIJlUKt2NkKg31fDB/cFhVJpMxyKhkqpHpaIsAsuUO8TT6+rqilZm6R2yXQzlO/VIRxIv5aHXZbPsiPgNADg8nU8uBlxwUt9uDZY2ihVY/g3weQ4n7/mJu5+Va5/lEFiWitQERfFMuA60tDr9YgnQ4q2l8Yyy8ddaw2EX7z/1aN7avi85jhLgz6u2thlfeuKYagBWbm5MruvJ4RYnjqlm1ebGrGP/KsK/9mj6F+dweaPeO1HweetTa9q0nua4EVBSF8Q9UY/i1wG79jexOMuUPhUEU//oPCQpSqoelYqeDCx77R1i6R1KYR7LUs9m2RmpQfup44ZlPalHjh11BBeceFQ+F03Se3W2Ht0NnAeMBLYA3wH6Abj7reENmp8CFxLcoPlkrt4zoPNRV+STCTfXDcJsx+d0PV1ueXI1//n4KpzgD2n2zIlMGzuMexav59WN5TmFV7qM73B4iqe+kKW8q+J/F9lUmikXgMSVVD0qFcVM3pPLQ8Dnzeweggvi3RpfKaWkM4mUCq03jlVOHY8JMDtMjJHNmm37WLMtSLJRYTDnXVO47uITi15eKX/uPjvH6w58rpuKI2Qe159tm3xfy/R6at6By8KAs2F/U5vAMuoOCbBkXUOHWzW7syU007DWJo0VzNvZU0bQr9JyjhGOcgH8+oV1NLcmmDJqiFoxRVIULbCM3yE2s3pS7hADjxK0sqwmvENcrLKI9GJlmc0y9aLv7jArcTS5dbqkTHEJh1ufXsuidTv5yPQJbcbH6iQvIulkS3Q3sKqCpljyuKhVKt5lMjVLelWFgRmtrUGX3o/EuqbOf7Gel+oaMmY9z6Z6QCWNh1q79F0rzPpklvLOmD6phrvnzEompaqtayDh6W8QOIe7U6/eto8/rtiSHJOp6UpEitwVthh6usuESCcVa2xYrxyrHF3Mrd7S2KEWg0ozPnDa0ezY15RMsiS9Skl1PdL5qPfIZ+hDuizpkL4bb/w9UdbzKAA9fcJwlq7flZya69PvPIY7X1jXJuHc9Q8uI+rEUVkB7sGjsjKoAunmJ40CoarcU6KoHmUR/1tITeSXS3zsqwLMXq+k6lGpUGAp0j2UzbKTltY1cNNjK6lNkyUyl8++e4rGZPYuJXUiL6d6JD0rNXDNZzk+RhTaZoe99ak17eaNnjqmOt+8AKpHeYpyAhxqTnSoe3M0Ddl5U0dnnNJHyl5J1aNSocBSpHsom2UXRRdeb25p5MGXN+V9kjeDASmTnZfLFDzSTkmdyMuxHknv0YVjmepRB8Rbqpdv2s3qLY1s3HWATbsO5n0eUl6AXqmk6lGp6MnkPSKSg8YqHxYfl/nxWZO59ak1PLFyS85pS9yDyc6/cM9LjBs2MDl+pqrC+Nvp4/jIjIkKMEWk7JRCgrm+INPvvLSuIe/zUJQXYMEbW5l45BGMqh6QnHtbNzmlN1GLpUj3KKk7W72lHsW7i1UPqOL2Z9+iJZ8JMmMqgCmjh/Cpc47RmMzSp3ok0nWqRwUUH0ebK7t5KgP6Vbadp1XKRknVo1KhFksRKVupd5IvOHkM81+sZ14Hki0kgNVb9/KNMI38GZNqdIIXEZG8ROehy84Yz/wX6/lt7Ya0yZXScaCp1blr0Xrmv1jPXZ8+G8ieEEqklCmwFJFeY/qkGhau3YEZHU70A0Ea+ZWbG7l78XrOP/Eopow8gtfe3sNF047uSGIMERHpY+IBZjx7cL55AQ42J7juvldYvX0f7nll9xUpOQosRaRXOXvKCAZUHZ4E/epZk3lh7Q4GVFUwfHB/du1vYun6XbRm6TKbcPjDii3J5Wfe3J4MVpWEQUREMkk3JjPfvABvbtuXfN6ScL5x/zLueHYtn3rnFN3clLKgwFJEepVMk6DHxbP83f9SPUvWNeTcb9QCGiVheOqNbRAazE0AACAASURBVFQPrGLnviamjBrCZ849Vid7ERFpZ/qkGm77uxltzj1PrtrKH2M3MDNZvW0f37h/GVUVRsKd/rEs5yKlRsl7RLpHSQ3yVj1qK8rul89JPpOoJVPzZhaV6pFI16kelYh5i9Zz75L1LNu4O2dm2bjjRg/hB5edCmg8Zg8qqXpUKtRiKSJ93vRJNZw+YXibbkodHacZtWQC9K807p4ziz0Hm/njii3JSc514hcRkciVMydy5cyJzFu0nusfXE5LwqkweOdxI3n2ze1kyjG7euteLvvZ80AQ3Qzop1ZMKQ0KLEVECMZm9o+Nzbz+/SezfNNu7ltaT0tLgooK4x3HjuCZN7fnTMLQ1Opc88sl7NzfnFx3b+0GLldKeRERSXHlzIntxlAurWvga/NfZfXWvVnf6wSJf259ag2jqgfoRqb0KHWFFekeJdVlQvUovWj8S7xbUeq6aM6y7Y2H2LW/iY27DrBp18GcwWakApgxuYbhg/sn142qHqALgfyoHol0nepRmVha18DsuS/QlOf0JZGqSuOvpo4GdH4popKqR6VCgaVI9yipA5DqUWHNW7Sebz+wjA6e+9uorIBpY4dx+Znt71xLkuqRSNepHpWR6GamAdUDqvjTyi2s2bYv75uZAJVmfO9STV1SYCVVj0qFusKKiHRR1I3pxt+/xiv1uzu1j9YEvFK/m1fql2EE3ZuqKoxPv/OYZEIgUKIGEZG+JHX6kusuPjGZcC7X9CWRVne+ef8yFqzaqgzmUlQKLEVECmD6pBqu/8DJXHX7Qpqaw5QLYQKgjjZkRtu3JDyZEMhir6UGnLpIEJFiM7M7gPcDW919WprXDbgZuBjYD1zt7i92byn7hvj0Jbc+tYY/rdySM9mcE8zP/OfXt3L5mRM4eewwGvY36RwiBaXAUkSkQFLn0ASSc5Yt37Sb7Y2HANi1v4naugYSTrJ1Mpf4NlHAqWyAItKN7gR+Cvwqw+sXAceHj5nAz8J/pUiiADPKKtua8Jznk5aEc9ei9cnlKIu5ziFSCAosRUQKKLXbUqaTdXyi7Bsffo2DzZkSy2cWzwZ4+oThbRIMqcusiBSSuz9tZpOzbHIJ8CsPkncsNLPhZna0u7/dLQXsw6LhGPNfrOe+pfW0tiaorDA+MmMC1QOqkj1f0mlqdb42/1U+dc4xLN+0W1llpUsUWIqI9IB4ABol62k80Mztz76V113nuD+u2JKcL3PqmCGs2rwXByoM/uXSU5SwQUS6wzhgQ2y5PlzXLrA0sznAHICJE3V8KoTonHLZGePb3Vjcc7CFeYvXZ3zv6q17+cb9y5LLv1taz93XqCeMdJwCSxGRHhYPMi84eUybIDPhTlWFcfqE4RxqSdC/qoIl6xrS7seB1zcfnvMs4fCt+5fxwEv17NjXxPiawXzhvcfrYkFEepS7zwXmQpAVtoeL06uk9poBuGz6eO5buiHvaUuaWhLc+PvXuP4DJ+t8IR2iwFJEpISkCzJT59XsyLxmCWBxGIiu2baPp97YxinjhnL00EFgmuNMRApmIzAhtjw+XCc9bPqkGu6eMys5bcnJY4fx5KqtWbPKvlK/m8t+9jwnjqlmwpGDda6QvCiwFBEpUenuPMcvEN7c3MiSuvStl9ks27iHZRv3JJfvWbKeaWOHMWvKCGWaFZHOegj4vJndQ5C0Z7fGV5aO1PPJlTMn5pVVduXmRlZubgTg3toN3KtEP5KFAksRkTITXSDc8uRqausacILssjWD+7HrQHPyAiHf/mWH59DcrUyzIpKWmd0NnAeMNLN64DtAPwB3vxV4lGCqkdUE0418smdKKvnqaFbZllbn1qfWMKp6QDLJD2h+ZTlMgaWISJk6e8oIBvSroLklQb+qCm77xJkAbbLNRnNqHlndnx2NTTmDzSjT7DW/XMJxo4cwfHB/du1vYue+Jo4ZNYTPnnts8jN0ISHSd7j77ByvO/C5biqOFFCUVTY6d9z/Un3Gsfx/XLEl+fzuRevBgvH8VRXGjZdMU7K4Ps4814yqJWbGjBleW1vb08UQ6SjLvUn3UT3qPbJNLZL6WtTtKdu4mnxEc28O7P6WTdUjka5TPZKc5i1az71L1tPUkuD1zY159YCpAM4/6ai+Mh6zpOpRqVBgKdI9SuoApHrUt0UB56ZdB5i3aH2HpjZJdcFJR3H6hOEMHVhFw/5mzjluZDEvJlSPRLpO9Ug65Jv3L+OuRZmnK0mnssL4Xu9uwSypelQq1BVWRKSPicZoLq1rYP6L9TS3BJNpnzd1NE+8voXWRP77iubQjNz8xBtMGzuMy8+c2JsvKERE+owPnzGe39XmP10JQGvC+dYDwdyYOhf0HQosRUT6qOmTarjr02en7S771ra9HHlE/+QYy427DrBx18Gc+zycCGgZi9/aweABVWzbc4hWT1BVUdFXukiJiPQaqdOVVA+o4vZn36I14VRYMK1Vug6QCYdvP7CMJ1ZuZsywQTr29wHqCivSPUqqy4TqkXRGPOh8a8d+WrswUPOsyTV87aITgQ4lAlI9Euk61SPpsvgY/lWbG5NZZSFzRvIKgznvmtJbprUqqXpUKtRiKSIieYlS0wPc8exabnx4Zaf3tXhdAx/52fNEvW4rDb536SlF6zJlZhcCNwOVwO3uflPK6xOBXwLDw22uc/dHi1IYEZEyF58Xc/qkmmRW2U27DmQcj5lwuPXptUDbLLLZktBJeVFgKSIiHXagOZHMDgtw3KgjqNu5n+YOjMGJD+VsdfhmkcbjmFklcAtwAVAPLDGzh9x9RWyzbwG/dfefmdlJBHPyTS5oQUREeqnUsfuHmhPZ58RMON+8fxnrd+zjjufeoqnVqTTjmncd01taNPskBZYiItJhqXNo/uBvTwOCbq2NB5q5/dm3aIl1la0AJhw5mLqd+zPu0x2uf3A5U8dUF/qC4ixgtbuvBTCze4BLgHhg6cDQ8PkwYFMhCyAi0hdEY/fnv1jPfUuD5HCZAkzncAsmQKs7tz69FgMGdP90VlIACixFRKTD0iX+idYDXHDymORk2w37m5LbzFu0np888Qab9xxKu9+EOwvX7ij0xcQ4YENsuR6YmbLNDcAfzOwfgSOA8wtZABGRviJqvbzsjPHJ88CTq7by59e35jU234GDzQm+89AyThtfo6Q/ZaSogaXGtIiI9F7xMTb5vnblzIlMHVPNVbcvpKk5gRmccFQ1b2zdi7vTv6qCs6eMKHbR05kN3OnuPzSzWcCvzWyau7eZfMXM5gBzACZOVAp9EZFM4ueBaCzl/BfruXfJ+rymtVq+sZHlGxu5t3YDl8+YoACzDBQtsNSYFhERSSfTNCdFTN6wEZgQWx4frov7e+BCAHd/wcwGAiOBrfGN3H0uMBeCbJaFLqiISG8Vb8m89ak1PLFyC1EDZpRiNd1BtaXVuWvReua/WM/VsybzwtodHDV0IJ8591gFmiWmmC2WGtMiUgBq+ZfeKLVFM1vrZwEsAY43s2MIAsorgCtTtlkPvBe408xOBAYC24pVIBGRvirKMB61YBrw4TPGs2pzI99+YBmZcsAdbE7ExmTu5slVW7lnziwFlyWkmIGlxrSIdJFa/kW6zt1bzOzzwOMEN1/ucPfXzOxGoNbdHwK+AtxmZl8iuOl5tZfbRM8iImUk3Q3G1zbtZt6i9VkzykaaW535L9YrsCwhFT38+dGYlvHAxQRjWtqVyczmmFmtmdVu26YbyNKnJFv+3b0JiFr+49TyL5KDuz/q7ie4+7Hu/v1w3fVhUIm7r3D3c9z9NHc/3d3/0LMlFhHpez58xngG9Kug0qAqjyjl7kXrOf9HTzEvnDtzaV0Dtzy5mqV1DUUuqaRTzBZLjWkR6Tq1/IuIiEifkDoGf9XmRuY+vYZ1O9JPVeXA6q17+cb9y/jxn1axtbEJgEqD9554VFmPwzSz4cCV7v7fPV2WfBWzxTI5psXM+hOMaXkoZZtoTAsa0yLSaWr5FxERkV5h+qQaPvee45g+qYYrZ07khx89nf6Vlny9qtKorLB274uCSoBWhz+s2MLs2xaytK6hXFsyhwP/0NOF6IiitVhqTItIQajlX0RERPqs6ZNquHvOrHaJfr51/zJyzVrS1JLgml8uYdeBZhIOFQZz3jWF6y4+sTuK3lU3Acea2cvAm8Bd7v4AgJndBfwWqAE+RDAUahzwG3f/brjNx4AvAP2BRcA/uHtrMQtc1Hksw8yUj6asuz72fAVwTjHLIFLmlM1SRERE+rR0iX6mjqnmpsdWsmRd9lbInfubk88TTjKzbBkEl9cB09z9dDM7F/gS8ICZDQPeAXwC+BhBPo5pwH6CJI+PAPuAy4Fz3L3ZzP4buAr4VTELXNTAUkS6Ri3/IiIiIu1Nn1TD7z77juS0JYvf2snqrXvzem8UXFYP6les+ZMLyt2fMrP/NrNRwGXA/PAaEeCP7r4DwMz+F3gn0AJMJwg0AQaR0pOtGBRYipQ4tfyLiIiIpBe1Zi6ta+Cq2xfS3JKgosJozjQhZigKLivN+N6l05g6pjqZNKhEA81fEbRQXgF8MrY+9Ys6YMAv3f3r3VQ2QIGliIiIiIiUuXhG2U27DnD34vUkwpDruNFDmDZ2KA++vKldFNbqzjfuX0aFBV1l+1caH5kxgQ+fMb6nA8xGoDq2fCewGNicMp/5BWZ2JHAAuBT4FEG32AfN7L/cfWv4erW71xWzwHllhTWzL5rZUAv83MxeNLO/LmbBRERERERE8hVllP3wGePpXxXMhzmwXwU/uOxUfnzFX/L9D51CmoSyAMkgtKnVuWvReq66fWGPZpENu7c+Z2bLzew/3H0LsBL4Rcqmi4H5wKsEXWRrw8DzWwTT0b0K/BE4uthlzrfF8lPufrOZvY8g+9DHgV8DmkBaRERERERKRup8mFHL45UzJwLw7QeWkaOnLE0tCRau3dGjrZbunkzYaGaDgeOBu1M2q3f3S9O8917g3uKWsK18A8sotr8Y+HWYPCRDvC8iIiIiItJzUjPJRq6cOZGpY6qZ/2I9q7c0UlvXkGytjKsw4+wpI7qhpLmZ2fnAz4H/cvfdPV2eTPINLJea2R+AY4Cvm1k15Jw6RkREREREpKTEg86ldQ3c+tQanli5JRlgVlYYN14yrafHWCa5+5+ASWnW30kw9rIk5BtY/j1wOrDW3feHA0A/meM9IiIiIiIiJWv6pBpu+7sZyWlLDEohcU9ZyjewnAW87O77zOxjwBnAzcUrloiIiIiUEjO7kOD6rxK43d1vSnl9IvBLYHi4zXXhlFkiJS9T11nJX15ZYYGfAfvN7DSCydjXEMylIiIiIiK9nJlVArcAFwEnAbPN7KSUzb4F/Nbd/5Jgrr3/7t5SikhPyjewbHF3By4Bfurut9B2XhURERER6b3OAla7+1p3bwLuIbgujHNgaPh8GLCpG8snIj0s38Cy0cy+TjDNyCNmVgH0K16xRERERKSEjAM2xJbrw3VxNwAfM7N64FHgH7unaCISMbPhZvYPnXjfo2Y2vCufnW9geTlwiGA+y83AeOA/uvLBIiIiItKrzAbudPfxhFPUhY0RbZjZHDOrNbPabdu2dXshRXq54UC7wNLMsubWcfeL3X1XVz44r+Q97r7ZzO4CzjSz9wOL3V1jLEVERET6ho3AhNjy+HBd3N8DFwK4+wtmNhAYCWyNb+Tuc4G5ADNmzMgxTb1I7zb5ukdmAecBC9bd9DcvFGCXNwHHmtnLQDNwEGgA/gI4wcweIKjLA4Gbw/qIma0DZgBDgMeAZ4F3ENTzS9z9QK4PziuwNLOPErRQLgAM+H9m9lV3vy//7ygiIiIiZWoJcLyZHUNwoXkFcGXKNuuB9wJ3mtmJBBeuapKUPmnydY/8mGC6xmyGAqcR9CJNTL7ukVeAPVm2f3ndTX9zbY59XgdMc/fTzew84JFw+a3w9U+5+04zGwQsMbP57r4jZR/HA7Pd/Roz+y1wGfCbHJ+b93Qj3wTOdPetAGY2CvgToMBSREREpJdz9xYz+zzwOMFUIne4+2tmdiNQ6+4PEcwccJuZfYkgkc/VYfJHEUlvOIeHJlq4nC2w7IzFsaAS4Atm9qHw+QSCIDI1sHzL3V8Ony8FJufzQfkGlhVRUBnaQf7jM0VERESkzIVzUj6asu762PMVwDndXS6RUpRHy2LUDfYJgqSozcBVBeoOG7cvehK2YJ4PzHL3/Wa2gKBnQapDseetwKB8Pijf4PD/zOxxM7vazK4maFLVhLciIiIiIiKdEAaR7wWuB95boKCykczTQg4DGsKg8i+AswvweUn5Ju/5qpldxuG7UHPd/f5CFkRERERERKQvCYPJgrVSuvsOM3vOzJYDB4AtsZf/D/isma0EVgELC/W5kH9XWNx9PjC/kB8uIiIiIiIihePuqYm1ovWHgIsyvDY5fLodmBZb/5/5fm7WwNLMGgkGX7d7KfgcH5rvB4mIiIiIiEjvlGuizEz9c0VEREREREQAZXYVERERERGRLlJgKSIiIiIiIl2iwFJERERERES6RIGliIiIiIiIdIkCSxER6fXM7EIzW2Vmq83sugzbfNTMVpjZa2Y2r7vLKCIi0t3MbG+h9pX3PJYiIiLlyMwqgVuAC4B6YImZPeTuK2LbHA98HTjH3RvMbHTPlFZERKQ8KbAUEZHe7ixgtbuvBTCze4BLgBWxba4BbnH3BgB339rtpRQRkb7nhmGzgPOABdyw+4Wu7s7MbgI2uPst4fINQAvwHqAG6Ad8y90f7OpnpVJgKSIivd04YENsuR6YmbLNCQBm9hxQCdzg7v+XuiMzmwPMAZg4cWJRCisiIr3ADcN+DJyeY6uhwGkEwxMT3DDsFWBPlu1f5obd1+bY573Ajwl66gB8FHgf8BN332NmI4GFYc8dz/U1OkKBpYiISHA+PJ7grvF44GkzO8Xdd8U3cve5wFyAGTNmFPSELCIifc5wDue8sXA5W2CZk7u/ZGajzWwsMApoADYD/2Vm7wYSBDdcjwrXF0xRA0szuxC4meDu7+3uflOabT4K3AA48Iq7X1nMMomUG9UjkS7bCEyILY8P18XVA4vcvRl4y8zeIAg0l3RPEUVEpFfJ3bIYdYN9gqB7ajNwVSG6wwK/A/4WGEPQgnkVQZA53d2bzWwdMLAAn9NG0bLCxpIlXAScBMw2s5NStoknSzgZyP0fINKHqB6JFMQS4HgzO8bM+gNXAA+lbPMAQWslYTehE4C13VlIERHpY4Ig8r3A9cB7CxRUQhBMXkEQXP4OGAZsDYPK9wCTCvQ5bRSzxVLJEkS6TvVIpIvcvcXMPg88TtDyf4e7v2ZmNwK17v5Q+Npfm9kKoBX4qrvv6LlSi4hInxAEk4UKKAEIz3HVwEZ3f9vM7gJ+b2bLgFrg9UJ+XqSYgWXBkiWI9GGqRyIF4O6PAo+mrLs+9tyBL4cPERGRsubup8SebwdmZdhuSKE+s6eT9+SVLEFZ+ESyUj0SERERkR5VtDGW5J8s4SF3b3b3t4AoWUIb7j7X3We4+4xRo0YVrcAiJUj1SERERERKXjEDSyVLEOk61SMRERERKXlFCyzdvQWIkiWsBH4bJUswsw+Gmz0O7AiTJTyJkiWItKF6JCIiIiLloKhjLJUsQaTrVI9EREREpNQVsyusiIiIiIiI9AEKLEVERERERKRLFFiKiIiIiIhIlyiwFBERERERkS5RYCkiIiIiOZnZhWa2ysxWm9l1Gbb5qJmtMLPXzGxed5dRRHpOUbPCioiIiEj5M7NK4BbgAqAeWGJmD7n7itg2xwNfB85x9wYzG90zpRWRnqAWSxERERHJ5Sxgtbuvdfcm4B7gkpRtrgFucfcGAHff2s1lFJEepMBSRERERHIZB2yILdeH6+JOAE4ws+fMbKGZXdhtpRORHqeusCIiIiJSCFXA8cB5wHjgaTM7xd13xTcysznAHICJEyd2dxlFpEjUYikiIiIiuWwEJsSWx4fr4uqBh9y92d3fAt4gCDTbcPe57j7D3WeMGjWqaAUWke6lwFJEREREclkCHG9mx5hZf+AK4KGUbR4gaK3EzEYSdI1d252FFJGeo8BSRERERLJy9xbg88DjwErgt+7+mpndaGYfDDd7HNhhZiuAJ4GvuvuOnimxiHQ3jbEUERERkZzc/VHg0ZR118eeO/Dl8CEifYxaLEVERERERKRLFFiKiIiIiIhIlyiwFBERERERkS7RGEsR6ZzaO+GlX0HVQBhUAwcaYN92OGJk8Hr0fFBNsDxkNJw2Gyac1WNFFhEREZHiUGApIu1tWAzP/Ri2rz4cHO7bDvu2Qv+hcGgPNGTIIL99VfrnALW/gEnvCPanQFNERESk11BgKSJtLZoLj/0z4MFyanDYJQ51zx1ejALNUVMVZIqIiIiUMY2xFJHDNixuG1QWXRho1t4BP/9r+ON3uulzRUSk5G1YDM/8MPhXREqeWixF5LB1z3T8PWNOgeGTMo+xBKh7ntzBqgfdb2uOgRlXd7wcIiLSe2xYDHe+H1qbAIPh42HYhPZj+tXjRaRkKLAUkcMmvytIxtNyKFgePgGGjW9/Iu/oGMkNi+GVebB3W7CfbIHmS79SYCki0tetewZaw3MRDrvWB4+47avCXi+/gOET25+vRh4P53xRQadIN1FgKSKHTTgLPvFQcEKf/K7CnYwnnNV2X/FAc1cdbF52+LVNLwUZZxVcioj0XZPfBRVVkGjJY2MPziW76tqu3r4KXn8ExkyDyv7wl3+nc4tIESmwFJG2UoPA7viMe66C1x8OnnsCHr4WVv8xvzvNGxYXPhAWEZGeNeEsuPiH8PCXgEQXduSHb15uXArL7oXzv6vzhUgRKLAUkZ53zhfhjf+L3Zn2INB8/RGYcDZUVMC+bTBgOJAIujkNOjIIQt9+KfiXChg3A6r6te2y25UuvCIi0nNmXA1HndR++qvouF7VHzYvp0MJ5+qeh5+/D/7iYnWTFSkwBZYi0vOiO9OPfCkMEiMOG17I8KY1KcsJ2BjLHJg6TUp8ufaOzEmHWg6qu1QvZGYXAjcDlcDt7n5Thu0uA+4DznT32m4sooikM+EsuGJe5tfTzbt8oAF2bYDdG0gfdCaCm5dvPA6ffFTBpUiBKLAUkdIQBXIPX0u3THeyeVnbsZ3xwHPjUqi9HQYMVWtnL2BmlcAtwAVAPbDEzB5y9xUp21UDXwQWdX8pRaRTsgWe0Xj++tq2x/tIohke+jx88Kc6rosUgAJLESkdyeCyq2NqCiBT0JmptVMp70vZWcBqd18LYGb3AJcAK1K2+x7wA+Cr3Vs8ESmK+Hj+qGXz9Udpc/Ny2yq440L4mx+pp4pIFymwFJHSEo2pibLGQuY5MgfVtH9/6pjK+PKhPenvWndUutbOKOX9sPFB2nvNtVZKxgEbYsv1wMz4BmZ2BjDB3R8xs4yBpZnNAeYATJw4sQhFFZGiiFo2a+9s3zPGW4N1DW/BBd/tqRKKlD0FliJSeoqZmbb2zmCuzKqBRQg8PRjTs3tD29XJwPMOGH0y1EwOPnf/DnWzLQFmVgH8CLg617buPheYCzBjxoxu6LMtIgUVtUo+8uUgoEzyoEWzfrGyxop0kgJLkRKnpCMFNuPq7N2d4nNsFqO1c+trwSOS2s121IlB4HloT/uW2Sj4BE2x0jEbgQmx5fHhukg1MA1YYGYAY4CHzOyDqksivVA82+zrj7R9re55+PkFwZCH8Wfqhp9IByiwFClhSjrSA7K1lmZq7exMyvtMtq0MHpHU7La1d4BVhNlzLZj4e/ikoPVz3/Yg+FQLaKolwPFmdgxBQHkFcGX0orvvBkZGy2a2APinLgWVaxbAigdg+xvZp7/JZ1mZikUKL+oa+8fvBAFmqmjIQ+0vguENw8bnrrMa8iB9XFEDS7W0iHSZko6UkmytnZlS3u/bHmQe3Lm2cOVITsni7cd77njz8PPaX8CkdwTPMwUrR53UtvVzw+Je1xrq7i1m9nngcYLz0R3u/pqZ3QjUuvtDBf3ADYvh15fS5kZDtulv8lmOMhWrBUWksKIxlc/dTPqbgw676oJHXLo6G421rx4bBKKDj4SDuw8Pe4D8biyNPF5zbEpZKlpgqZYWkYJQ0pFykWuutaKO7czEgwudSLpgBSN5MTVsEuzZ0L41FMq+BdTdHwUeTVl3fYZtz+vSh617pktvzyjegjLpHZlbR6Lu3Ns60FoKuqCVvuuC78Jf/A386TtBV9gucWjcGDzi4sfffG4svf4IjD4JEi3q0SBlo5gtlj3b0pJpnBR0vVtSrhOzukNIN1HSkTKSa2xntsATwoudYvy3xfa5u67t+tTW0NpfwFHToLVJ3cCymfwuqOwX/E5FEd4wiBJCRdPf7N8JDeug8W063VoaXdCOOjFIbFLIc6UugKWUTTgLPvnY4WNxa1Phhjh0ih8ej59Pj4ZnfhQMyzhiJAwe0bE6OmoqjDkNNr98OBt7fEx/am+c6PXoPak3seLv7+vngz7G3ItTYczsb4EL3f3T4fLHgZnu/vnYNmcA33T3y7KNaUlpaZleV1eXukkgCibrFrYdo9SThhwVPAYMhUN74dBuGDQcBtbAwT1wsAEGjYDBw+HA7mB58AgYdCQc3JVfEFvMALm79917D0TWqTeZzQJucPf3hctfB3D3fwuXhwFrgL3hW8YAO4GsSUdmzJjhtbXqdV5W4jfLhowOjinrninueM9OMRg6LuwGNqL9cSx+MRJ9l/zrfKfqUbFkrUedaTVMt7yrrkit2T3oiNFQUQUDhsHAodC0N2i1HzQ8WHdoLxzaBQOHB68f3BP8HQ2ILR8KlwcNhYONQXfDwUcCFvyGg4YHywd2w4GdwTl1cA0c2HV4eVBNsN/9O+GIEcFn798ZrBt8ZFiWPUE3xsFHBmObo/+bgcPbZ3Yu1rkxumEDheqmXj71qKdlG+KQutzjx94SNeb04C/u4J6wXoV1/uDuvmggQgAACmpJREFUnrsGLUzPjJKqR6WixwLLsKXlz8DV7r4u32QJGQ9AGxbDz99Hj0+qLoVRMwWGHh2c1A/uCS8wwpP8wd3B8qDhwWsHGsILCsILiligfmBnGKjHLyCKcBGQ++K4s4FlFfAG8F6CpCNLgCvd/bUM2y+gK/VIeod4EArlEazkl4GxpE7k3VaPMk3sLn1M2G3dKmDE8ZCItSjHs0jn7jnQN+tRd8gViELu64xd0ZRVquvFZUGAmUjAkFHtW3nLrB6VimIGlt3b0vLMD+GJGwv5FUQ6pnIAXP1wpoNQpw9AZnYx8GMOJx35fqakIwosJW9R8Im1bf0cNbXtMhSxG24aRapHxdDt9Sh+wyDnzYFwjGxLhm7L6S54dUHbe6gelbd8ezyU4k3C3qSM6lGpKOYYy+5N7z75XVDZv/2YljGnBBdJ3dnEru4QfVNrU3AxXuBuvN2adET6jmzTqqTKNGa9GN3AilSPeoXU/7NM43K70s2rUF14dQHcs1SPyltnjs+drbNV/WHzaxzu8Wdw5DHtM5kPnxQMcYCUm40VMObkwzex2r1exlSPOqxogWW3p3efcBZc/cjhytXTSQI60i+/3MZBFmvfRcuM2U0q+wc3OER6m3wvcnId91LrePVYaNzUdh+qR/nLlRCqMzpyQdtRxQpaS+H8Vax9d/aGTZHqkaaRK0GFqLPxHixR98/oxlX10e1vVKXbPt3+Mg3H6Ml61pGeGTofdVjRusIWi7pM9HLZMmP25MGoh8ZYFovqkfSY2jth5YNw4iVBUBSv82U2pkX1SLpFdMOmcXNwkXtod/bgvEj1KJxG7g1i08gBszNMI/cI0B/4vIZmSMnJ5yZXmZ2PSkUxu8KKdFwx7sCLSOlIreOq8yLZ5Zojt/v07DRyIoVSzJ4ZfVxFTxdAREREREreOGBDbLk+XJcUTiM3wd0f6c6CiUhpUGApIiIiIl0STiP3I+AreWw7x8xqzax227ZtxS+ciHQLBZYiIiIikstGYEJseXy4LlINTAMWmNk64GzgITObkbojd5/r7jPcfcaoUaOKWGQR6U5ll7zHzLYBdVk2GQls76bidEYpl6+UywblXb7t7n5hdxYmG9WjoirlskF5l0/1qLBKuXylXDYo7/J1qh6ZWRVB8p73EgSUS4Ar3f21DNsvII9p5FSPiqqUywblXb6SOh+VirJL3uPuWW9tmVmtu7e7O1YqSrl8pVw2UPkKSfWoeEq5bKDyFZLqUfGUctmgb5avWNPIqR4VTymXDVS+3qjsAksRERER6X7u/ijwaMq66zNse153lElESofGWIqIiIiIiEiX9MbAcm5PFyCHUi5fKZcNVL7uVOrfpZTLV8plA5WvO5X6dynl8pVy2UDl606l/l1KuXylXDZQ+XqdskveIyIiIiIiIqWlN7ZYioiIiIiISDfqNYGlmV1oZqvMbLWZXdfT5QEws3VmtszMXjaz2nDdkWb2RzN7M/y3phvLc4eZbTWz5bF1actjgZ+Ev+erZnZGD5XvBjPbGP6GL5vZxbHXvh6Wb5WZva/IZZtgZk+a2Qoze83MvhiuL5nfr1BKrS6pHhWkfKpH3Uz1KGd5VI86XzbVo54rj+pR18unetSbuXvZPwjSXq8BpgD9gVeAk0qgXOuAkSnr/h24Lnx+HfCDbizPu4EzgP/f3v2FXFbVYRz/PlkN5QyZkjJMkaN50Qg5akikRRAEejMGRlLZEEE3duFdhEXRfXUlKlEw1pBROSRBIM7FhBc6lozjv/6IXTQyOReGaZDJ+Otir7HTwJvOWec9e5/j9wObd5919nve315nPQfWuxf7PP569QDXAb8BwvAlxw+NVN+3Gb4H6/Rjd7X3eQuws73/Z21ibduBK9r+Nobv8to1pf5b0HlOLkvmaCH1maMlbuZo7nE6mXFgjsbfzNHc43Qy48Acvfm2dblieRXwdFU9U1X/Bu4G9oxc00b2APva/j7g+mX94ar6LfD8G6xnD3BXDR4EzkmyfYT6NrIHuLuqXq6qvwBPM4yDzarteFU90vZfBJ4CdjCh/luQVcmSOTqz+jZijjaHOXod5qirNnM0LebozOrbiDlaA+sysdwB/HXm8bHWNrYC7kvy+yRfaW0XVNXxtv834IJxSnvNRvVMqU+/2pYd/Ghmiclo9SW5ELgceIjV6L8zMcW6zdFimKPlmWLd5mgxzNHyTLFuc7QY5mhNrcvEcqquqaorgGuBm5N8fPbJqiqGD6lJmFo9ze3AxcBu4Djw3TGLSbIV+CVwS1X9Y/a5ifbfOjBH/cyRzFE/cyRz1M8crbF1mVg+C7xv5vF7W9uoqurZ9vMEcIDhkv5zpy6dt58nxqsQ/k89k+jTqnquqk5W1avAD/jvsoil15fkbQwfPvur6p7WPOn+m8Pk6jZH/czR0k2ubnPUzxwt3eTqNkf9zNF6W5eJ5cPAJUl2Jnk7cCNw75gFJTk7ybZT+8CngMdbXXvbYXuBX41T4Ws2qude4IvtLlgfAV6YWRqwNKetX/80Qx+equ/GJFuS7AQuAQ5vYh0Bfgg8VVXfm3lq0v03h0llyRwthjlaOnM0n0mPA3O0dOZoPpMeB+ZozdUE7iC0iI3hbk1/YriL1K0TqOcihrtbPQo8caom4DzgIPBn4H7g3CXW9FOGZQevMKwN//JG9TDc9eq21p+PAR8eqb4ft79/lCHU22eOv7XV90fg2k2u7RqG5RBHgSNtu25K/bfAc51MlszRwuozR8sfu+bozMfpZMaBOZrGZo7mGqeTGQfm6M23pXWWJEmSJElzWZelsJIkSZKkkTixlCRJkiR1cWIpSZIkSerixFKSJEmS1MWJpSRJkiSpixNLvWFJPpHk12PXIa0ycyT1M0dSP3OkRXNiKUmSJEnq4sRyDSX5QpLDSY4kuTPJWUleSvL9JE8kOZjkPe3Y3UkeTHI0yYEk727tH0hyf5JHkzyS5OL28luT/CLJH5LsT5LRTlTaROZI6meOpH7mSKvCieWaSfJB4LPA1VW1GzgJfB44G/hdVV0KHAK+1X7lLuBrVfUh4LGZ9v3AbVV1GfBR4Hhrvxy4BdgFXARcveknJS2ZOZL6mSOpnznSKnnr2AVo4T4JXAk83P7p9A7gBPAq8LN2zE+Ae5K8Czinqg619n3Az5NsA3ZU1QGAqvoXQHu9w1V1rD0+AlwIPLD5pyUtlTmS+pkjqZ850spwYrl+Auyrqq//T2PyzdOOqzlf/+WZ/ZM4hrSezJHUzxxJ/cyRVoZLYdfPQeCGJOcDJDk3yfsZ3usb2jGfAx6oqheAvyf5WGu/CThUVS8Cx5Jc315jS5J3LvUspHGZI6mfOZL6mSOtDP8rsWaq6skk3wDuS/IW4BXgZuCfwFXtuRMM6/UB9gJ3tA+YZ4AvtfabgDuTfKe9xmeWeBrSqMyR1M8cSf3MkVZJqua9cq5VkuSlqto6dh3SKjNHUj9zJPUzR5oil8JKkiRJkrp4xVKSJEmS1MUrlpIkSZKkLk4sJUmSJEldnFhKkiRJkro4sZQkSZIkdXFiKUmSJEnq4sRSkiRJktTlP1cFOdjToPdDAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result\n",
        "### optim = Adam"
      ],
      "metadata": {
        "id": "GjJpN43cksrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Capacity"
      ],
      "metadata": {
        "id": "MWPqCacquWM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 666\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp2_hidn\"\n",
        "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "args.batch_size = 193\n",
        "args.x_frames = 4\n",
        "args.y_frames = 4\n",
        "trainset = MyDataset(args.x_frames, args.y_frames, mydata, 0, int(0.6*len(mydata)))\n",
        "valset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.6*len(mydata)), int(0.8*len(mydata)))\n",
        "testset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.8*len(mydata)), int(len(mydata)))\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
        "\n",
        "args.input_dim = 1\n",
        "args.hid_dim = 8\n",
        "args.n_layers = 4\n",
        "\n",
        "args.l2 = 0.00001\n",
        "args.dropout = 0.0\n",
        "args.use_bn = True\n",
        "\n",
        "args.optim = 'Adam'\n",
        "args.lr = 0.0001\n",
        "args.epoch = 500\n",
        "\n",
        "name_var1 = 'hid_dim'\n",
        "name_var2 = 'n_layers'\n",
        "list_var1 = [2, 4, 8, 16, 32, 64]\n",
        "list_var2 = [2, 4, 6, 8]\n",
        "\n",
        "\n",
        "for var1 in list_var1:\n",
        "    for var2 in list_var2:\n",
        "        setattr(args, name_var1, var1)\n",
        "        setattr(args, name_var2, var2)\n",
        "        print(args)\n",
        "                \n",
        "        setting, result = experiment(partition, deepcopy(args))\n",
        "        save_exp_result(setting, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6kcm1S1r1I2",
        "outputId": "9edeaed8-c20e-4a52-e858-1c032e78a946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Epoch 9, Loss(train/val) 1.05139/0.41763. Took 0.04 sec\n",
            "Epoch 10, Loss(train/val) 1.04982/0.41738. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.05698/0.41718. Took 0.05 sec\n",
            "Epoch 12, Loss(train/val) 1.04872/0.41704. Took 0.04 sec\n",
            "Epoch 13, Loss(train/val) 1.05299/0.41697. Took 0.04 sec\n",
            "Epoch 14, Loss(train/val) 1.04889/0.41700. Took 0.04 sec\n",
            "Epoch 15, Loss(train/val) 1.05041/0.41711. Took 0.04 sec\n",
            "Epoch 16, Loss(train/val) 1.05114/0.41734. Took 0.06 sec\n",
            "Epoch 17, Loss(train/val) 1.04668/0.41769. Took 0.04 sec\n",
            "Epoch 18, Loss(train/val) 1.04293/0.41819. Took 0.04 sec\n",
            "Epoch 19, Loss(train/val) 1.04417/0.41887. Took 0.04 sec\n",
            "Epoch 20, Loss(train/val) 1.03695/0.41978. Took 0.04 sec\n",
            "Epoch 21, Loss(train/val) 1.03223/0.42018. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 1.04689/0.42000. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 1.04750/0.41983. Took 0.04 sec\n",
            "Epoch 24, Loss(train/val) 1.03906/0.41967. Took 0.04 sec\n",
            "Epoch 25, Loss(train/val) 1.03758/0.41951. Took 0.04 sec\n",
            "Epoch 26, Loss(train/val) 1.03636/0.41939. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.04227/0.41933. Took 0.04 sec\n",
            "Epoch 28, Loss(train/val) 1.03607/0.41982. Took 0.04 sec\n",
            "Epoch 29, Loss(train/val) 1.04059/0.42035. Took 0.04 sec\n",
            "Epoch 30, Loss(train/val) 1.04125/0.42097. Took 0.04 sec\n",
            "Epoch 31, Loss(train/val) 1.03679/0.42166. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 1.02225/0.42238. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 1.02057/0.42317. Took 0.04 sec\n",
            "Epoch 34, Loss(train/val) 1.02958/0.42393. Took 0.04 sec\n",
            "Epoch 35, Loss(train/val) 1.02452/0.42478. Took 0.04 sec\n",
            "Epoch 36, Loss(train/val) 1.03524/0.42611. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.02548/0.42748. Took 0.04 sec\n",
            "Epoch 38, Loss(train/val) 1.02462/0.42869. Took 0.04 sec\n",
            "Epoch 39, Loss(train/val) 1.02706/0.42971. Took 0.04 sec\n",
            "Epoch 40, Loss(train/val) 1.02763/0.43052. Took 0.04 sec\n",
            "Epoch 41, Loss(train/val) 1.01738/0.43110. Took 0.05 sec\n",
            "Epoch 42, Loss(train/val) 1.02353/0.43146. Took 0.04 sec\n",
            "Epoch 43, Loss(train/val) 1.01768/0.43161. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 1.01183/0.43167. Took 0.04 sec\n",
            "Epoch 45, Loss(train/val) 1.01456/0.43172. Took 0.04 sec\n",
            "Epoch 46, Loss(train/val) 1.00794/0.43180. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 0.99405/0.43189. Took 0.04 sec\n",
            "Epoch 48, Loss(train/val) 1.00426/0.43202. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 1.00456/0.43212. Took 0.04 sec\n",
            "Epoch 50, Loss(train/val) 0.98710/0.43216. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 0.99525/0.43207. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 0.98901/0.43210. Took 0.04 sec\n",
            "Epoch 53, Loss(train/val) 0.98493/0.43275. Took 0.04 sec\n",
            "Epoch 54, Loss(train/val) 0.98000/0.43389. Took 0.04 sec\n",
            "Epoch 55, Loss(train/val) 0.96893/0.43495. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 0.96990/0.43592. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 0.95997/0.43666. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 0.95255/0.43725. Took 0.04 sec\n",
            "Epoch 59, Loss(train/val) 0.95191/0.43757. Took 0.04 sec\n",
            "Epoch 60, Loss(train/val) 0.94520/0.43768. Took 0.04 sec\n",
            "Epoch 61, Loss(train/val) 0.93851/0.43768. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 0.91589/0.43720. Took 0.04 sec\n",
            "Epoch 63, Loss(train/val) 0.91269/0.43644. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 0.91626/0.43558. Took 0.04 sec\n",
            "Epoch 65, Loss(train/val) 0.90824/0.43445. Took 0.04 sec\n",
            "Epoch 66, Loss(train/val) 0.88696/0.43276. Took 0.05 sec\n",
            "Epoch 67, Loss(train/val) 0.89417/0.43025. Took 0.04 sec\n",
            "Epoch 68, Loss(train/val) 0.87903/0.42733. Took 0.04 sec\n",
            "Epoch 69, Loss(train/val) 0.87608/0.42423. Took 0.04 sec\n",
            "Epoch 70, Loss(train/val) 0.87107/0.42114. Took 0.04 sec\n",
            "Epoch 71, Loss(train/val) 0.86675/0.41804. Took 0.05 sec\n",
            "Epoch 72, Loss(train/val) 0.86040/0.41499. Took 0.04 sec\n",
            "Epoch 73, Loss(train/val) 0.85847/0.41215. Took 0.04 sec\n",
            "Epoch 74, Loss(train/val) 0.84755/0.40928. Took 0.04 sec\n",
            "Epoch 75, Loss(train/val) 0.84821/0.40648. Took 0.04 sec\n",
            "Epoch 76, Loss(train/val) 0.85013/0.40308. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 0.83900/0.39791. Took 0.04 sec\n",
            "Epoch 78, Loss(train/val) 0.83356/0.39147. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 0.82923/0.38549. Took 0.04 sec\n",
            "Epoch 80, Loss(train/val) 0.82594/0.37873. Took 0.04 sec\n",
            "Epoch 81, Loss(train/val) 0.82869/0.37031. Took 0.05 sec\n",
            "Epoch 82, Loss(train/val) 0.82440/0.36230. Took 0.04 sec\n",
            "Epoch 83, Loss(train/val) 0.82682/0.35149. Took 0.04 sec\n",
            "Epoch 84, Loss(train/val) 0.82391/0.34328. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 0.82301/0.33471. Took 0.04 sec\n",
            "Epoch 86, Loss(train/val) 0.82184/0.32744. Took 0.05 sec\n",
            "Epoch 87, Loss(train/val) 0.81804/0.32111. Took 0.04 sec\n",
            "Epoch 88, Loss(train/val) 0.82035/0.31542. Took 0.04 sec\n",
            "Epoch 89, Loss(train/val) 0.81726/0.31126. Took 0.04 sec\n",
            "Epoch 90, Loss(train/val) 0.82057/0.30702. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 0.82025/0.30134. Took 0.05 sec\n",
            "Epoch 92, Loss(train/val) 0.82289/0.29747. Took 0.04 sec\n",
            "Epoch 93, Loss(train/val) 0.82116/0.29518. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 0.81622/0.29295. Took 0.04 sec\n",
            "Epoch 95, Loss(train/val) 0.81362/0.29202. Took 0.04 sec\n",
            "Epoch 96, Loss(train/val) 0.81821/0.29073. Took 0.05 sec\n",
            "Epoch 97, Loss(train/val) 0.81588/0.28960. Took 0.04 sec\n",
            "Epoch 98, Loss(train/val) 0.81904/0.28884. Took 0.04 sec\n",
            "Epoch 99, Loss(train/val) 0.81646/0.28800. Took 0.04 sec\n",
            "Epoch 100, Loss(train/val) 0.81341/0.28748. Took 0.04 sec\n",
            "Epoch 101, Loss(train/val) 0.81706/0.28745. Took 0.06 sec\n",
            "Epoch 102, Loss(train/val) 0.80798/0.28705. Took 0.04 sec\n",
            "Epoch 103, Loss(train/val) 0.81940/0.28721. Took 0.04 sec\n",
            "Epoch 104, Loss(train/val) 0.81969/0.28728. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 0.81334/0.28752. Took 0.04 sec\n",
            "Epoch 106, Loss(train/val) 0.81565/0.28769. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 0.81488/0.28810. Took 0.04 sec\n",
            "Epoch 108, Loss(train/val) 0.82009/0.28984. Took 0.04 sec\n",
            "Epoch 109, Loss(train/val) 0.80873/0.29117. Took 0.04 sec\n",
            "Epoch 110, Loss(train/val) 0.81407/0.29329. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 0.80549/0.29480. Took 0.05 sec\n",
            "Epoch 112, Loss(train/val) 0.81099/0.29547. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 0.80991/0.29408. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 0.80287/0.29302. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 0.80528/0.29281. Took 0.05 sec\n",
            "Epoch 116, Loss(train/val) 0.80640/0.29058. Took 0.06 sec\n",
            "Epoch 117, Loss(train/val) 0.81359/0.28904. Took 0.05 sec\n",
            "Epoch 118, Loss(train/val) 0.81112/0.28943. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 0.81382/0.28849. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 0.80573/0.28788. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 0.81710/0.28757. Took 0.05 sec\n",
            "Epoch 122, Loss(train/val) 0.80657/0.28798. Took 0.04 sec\n",
            "Epoch 123, Loss(train/val) 0.80677/0.28730. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 0.80836/0.28725. Took 0.04 sec\n",
            "Epoch 125, Loss(train/val) 0.79975/0.28681. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 0.81108/0.28622. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 0.81069/0.28648. Took 0.05 sec\n",
            "Epoch 128, Loss(train/val) 0.79845/0.28494. Took 0.04 sec\n",
            "Epoch 129, Loss(train/val) 0.80855/0.28454. Took 0.04 sec\n",
            "Epoch 130, Loss(train/val) 0.80503/0.28454. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 0.80637/0.28530. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 0.80749/0.28564. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 0.81188/0.28633. Took 0.04 sec\n",
            "Epoch 134, Loss(train/val) 0.80319/0.28701. Took 0.04 sec\n",
            "Epoch 135, Loss(train/val) 0.80570/0.28740. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 0.79836/0.28580. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 0.80592/0.28677. Took 0.04 sec\n",
            "Epoch 138, Loss(train/val) 0.80595/0.28604. Took 0.04 sec\n",
            "Epoch 139, Loss(train/val) 0.80307/0.28448. Took 0.04 sec\n",
            "Epoch 140, Loss(train/val) 0.79927/0.28382. Took 0.04 sec\n",
            "Epoch 141, Loss(train/val) 0.80838/0.28405. Took 0.04 sec\n",
            "Epoch 142, Loss(train/val) 0.80381/0.28445. Took 0.06 sec\n",
            "Epoch 143, Loss(train/val) 0.80848/0.28464. Took 0.04 sec\n",
            "Epoch 144, Loss(train/val) 0.80801/0.28483. Took 0.04 sec\n",
            "Epoch 145, Loss(train/val) 0.81180/0.28474. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 0.80811/0.28535. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 0.80707/0.28527. Took 0.04 sec\n",
            "Epoch 148, Loss(train/val) 0.80286/0.28605. Took 0.04 sec\n",
            "Epoch 149, Loss(train/val) 0.80830/0.28598. Took 0.04 sec\n",
            "Epoch 150, Loss(train/val) 0.79582/0.28484. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 0.79977/0.28621. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 0.80053/0.28477. Took 0.04 sec\n",
            "Epoch 153, Loss(train/val) 0.80103/0.28284. Took 0.04 sec\n",
            "Epoch 154, Loss(train/val) 0.80580/0.28222. Took 0.04 sec\n",
            "Epoch 155, Loss(train/val) 0.80449/0.28310. Took 0.04 sec\n",
            "Epoch 156, Loss(train/val) 0.79716/0.28294. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 0.80028/0.28393. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 0.79953/0.28369. Took 0.04 sec\n",
            "Epoch 159, Loss(train/val) 0.80679/0.28417. Took 0.04 sec\n",
            "Epoch 160, Loss(train/val) 0.80622/0.28399. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 0.79385/0.28431. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 0.80304/0.28430. Took 0.04 sec\n",
            "Epoch 163, Loss(train/val) 0.80476/0.28432. Took 0.04 sec\n",
            "Epoch 164, Loss(train/val) 0.79901/0.28339. Took 0.04 sec\n",
            "Epoch 165, Loss(train/val) 0.80914/0.28240. Took 0.04 sec\n",
            "Epoch 166, Loss(train/val) 0.80319/0.28366. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 0.79657/0.28133. Took 0.04 sec\n",
            "Epoch 168, Loss(train/val) 0.79741/0.28123. Took 0.04 sec\n",
            "Epoch 169, Loss(train/val) 0.79237/0.28357. Took 0.05 sec\n",
            "Epoch 170, Loss(train/val) 0.80253/0.28434. Took 0.04 sec\n",
            "Epoch 171, Loss(train/val) 0.80347/0.28254. Took 0.06 sec\n",
            "Epoch 172, Loss(train/val) 0.79452/0.28239. Took 0.04 sec\n",
            "Epoch 173, Loss(train/val) 0.79869/0.28168. Took 0.04 sec\n",
            "Epoch 174, Loss(train/val) 0.79755/0.28174. Took 0.04 sec\n",
            "Epoch 175, Loss(train/val) 0.79989/0.28119. Took 0.04 sec\n",
            "Epoch 176, Loss(train/val) 0.80077/0.28041. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 0.79175/0.27919. Took 0.04 sec\n",
            "Epoch 178, Loss(train/val) 0.79825/0.27920. Took 0.04 sec\n",
            "Epoch 179, Loss(train/val) 0.79605/0.27952. Took 0.04 sec\n",
            "Epoch 180, Loss(train/val) 0.80005/0.27929. Took 0.04 sec\n",
            "Epoch 181, Loss(train/val) 0.79841/0.27920. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 0.78850/0.28033. Took 0.04 sec\n",
            "Epoch 183, Loss(train/val) 0.80080/0.28158. Took 0.04 sec\n",
            "Epoch 184, Loss(train/val) 0.79830/0.28070. Took 0.04 sec\n",
            "Epoch 185, Loss(train/val) 0.78539/0.27992. Took 0.04 sec\n",
            "Epoch 186, Loss(train/val) 0.80304/0.27889. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 0.79166/0.27890. Took 0.04 sec\n",
            "Epoch 188, Loss(train/val) 0.81015/0.27922. Took 0.04 sec\n",
            "Epoch 189, Loss(train/val) 0.80081/0.27868. Took 0.04 sec\n",
            "Epoch 190, Loss(train/val) 0.78841/0.27795. Took 0.04 sec\n",
            "Epoch 191, Loss(train/val) 0.79446/0.27757. Took 0.04 sec\n",
            "Epoch 192, Loss(train/val) 0.79040/0.27756. Took 0.05 sec\n",
            "Epoch 193, Loss(train/val) 0.79943/0.27763. Took 0.04 sec\n",
            "Epoch 194, Loss(train/val) 0.79861/0.27767. Took 0.04 sec\n",
            "Epoch 195, Loss(train/val) 0.79882/0.27758. Took 0.04 sec\n",
            "Epoch 196, Loss(train/val) 0.79835/0.27818. Took 0.05 sec\n",
            "Epoch 197, Loss(train/val) 0.79927/0.27734. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 0.79147/0.27731. Took 0.04 sec\n",
            "Epoch 199, Loss(train/val) 0.79586/0.27779. Took 0.04 sec\n",
            "Epoch 200, Loss(train/val) 0.79247/0.27711. Took 0.04 sec\n",
            "Epoch 201, Loss(train/val) 0.79618/0.27702. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 0.79754/0.27812. Took 0.04 sec\n",
            "Epoch 203, Loss(train/val) 0.79176/0.27929. Took 0.04 sec\n",
            "Epoch 204, Loss(train/val) 0.79183/0.28055. Took 0.04 sec\n",
            "Epoch 205, Loss(train/val) 0.79000/0.28079. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 0.79203/0.27938. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 0.78773/0.27886. Took 0.04 sec\n",
            "Epoch 208, Loss(train/val) 0.78856/0.27788. Took 0.04 sec\n",
            "Epoch 209, Loss(train/val) 0.78596/0.27732. Took 0.04 sec\n",
            "Epoch 210, Loss(train/val) 0.78718/0.27606. Took 0.04 sec\n",
            "Epoch 211, Loss(train/val) 0.78833/0.27669. Took 0.04 sec\n",
            "Epoch 212, Loss(train/val) 0.78428/0.27717. Took 0.04 sec\n",
            "Epoch 213, Loss(train/val) 0.78847/0.27792. Took 0.04 sec\n",
            "Epoch 214, Loss(train/val) 0.78747/0.27747. Took 0.04 sec\n",
            "Epoch 215, Loss(train/val) 0.79421/0.27688. Took 0.05 sec\n",
            "Epoch 216, Loss(train/val) 0.78682/0.27834. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 0.78751/0.27764. Took 0.04 sec\n",
            "Epoch 218, Loss(train/val) 0.78721/0.27711. Took 0.04 sec\n",
            "Epoch 219, Loss(train/val) 0.79328/0.27660. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 0.79350/0.27717. Took 0.04 sec\n",
            "Epoch 221, Loss(train/val) 0.78820/0.27634. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 0.79435/0.27605. Took 0.04 sec\n",
            "Epoch 223, Loss(train/val) 0.79025/0.27536. Took 0.04 sec\n",
            "Epoch 224, Loss(train/val) 0.79328/0.27630. Took 0.04 sec\n",
            "Epoch 225, Loss(train/val) 0.78894/0.27622. Took 0.04 sec\n",
            "Epoch 226, Loss(train/val) 0.78737/0.27745. Took 0.05 sec\n",
            "Epoch 227, Loss(train/val) 0.79318/0.27667. Took 0.04 sec\n",
            "Epoch 228, Loss(train/val) 0.79167/0.27588. Took 0.04 sec\n",
            "Epoch 229, Loss(train/val) 0.78564/0.27530. Took 0.04 sec\n",
            "Epoch 230, Loss(train/val) 0.79079/0.27496. Took 0.04 sec\n",
            "Epoch 231, Loss(train/val) 0.78596/0.27503. Took 0.05 sec\n",
            "Epoch 232, Loss(train/val) 0.78130/0.27401. Took 0.04 sec\n",
            "Epoch 233, Loss(train/val) 0.78128/0.27416. Took 0.04 sec\n",
            "Epoch 234, Loss(train/val) 0.78179/0.27384. Took 0.04 sec\n",
            "Epoch 235, Loss(train/val) 0.79110/0.27345. Took 0.04 sec\n",
            "Epoch 236, Loss(train/val) 0.77891/0.27334. Took 0.04 sec\n",
            "Epoch 237, Loss(train/val) 0.78264/0.27325. Took 0.05 sec\n",
            "Epoch 238, Loss(train/val) 0.78775/0.27353. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 0.78007/0.27316. Took 0.04 sec\n",
            "Epoch 240, Loss(train/val) 0.79398/0.27337. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 0.78577/0.27255. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 0.78522/0.27240. Took 0.04 sec\n",
            "Epoch 243, Loss(train/val) 0.78032/0.27234. Took 0.04 sec\n",
            "Epoch 244, Loss(train/val) 0.78064/0.27212. Took 0.04 sec\n",
            "Epoch 245, Loss(train/val) 0.77625/0.27200. Took 0.04 sec\n",
            "Epoch 246, Loss(train/val) 0.78730/0.27275. Took 0.05 sec\n",
            "Epoch 247, Loss(train/val) 0.78745/0.27320. Took 0.04 sec\n",
            "Epoch 248, Loss(train/val) 0.78511/0.27299. Took 0.04 sec\n",
            "Epoch 249, Loss(train/val) 0.78906/0.27227. Took 0.04 sec\n",
            "Epoch 250, Loss(train/val) 0.77574/0.27216. Took 0.04 sec\n",
            "Epoch 251, Loss(train/val) 0.78345/0.27210. Took 0.05 sec\n",
            "Epoch 252, Loss(train/val) 0.77782/0.27179. Took 0.04 sec\n",
            "Epoch 253, Loss(train/val) 0.78578/0.27261. Took 0.04 sec\n",
            "Epoch 254, Loss(train/val) 0.78230/0.27408. Took 0.04 sec\n",
            "Epoch 255, Loss(train/val) 0.78530/0.27513. Took 0.04 sec\n",
            "Epoch 256, Loss(train/val) 0.78198/0.27651. Took 0.05 sec\n",
            "Epoch 257, Loss(train/val) 0.77887/0.27531. Took 0.04 sec\n",
            "Epoch 258, Loss(train/val) 0.78139/0.27400. Took 0.04 sec\n",
            "Epoch 259, Loss(train/val) 0.78423/0.27583. Took 0.04 sec\n",
            "Epoch 260, Loss(train/val) 0.78270/0.27361. Took 0.04 sec\n",
            "Epoch 261, Loss(train/val) 0.78463/0.27289. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.78318/0.27161. Took 0.05 sec\n",
            "Epoch 263, Loss(train/val) 0.78331/0.27104. Took 0.04 sec\n",
            "Epoch 264, Loss(train/val) 0.77767/0.27062. Took 0.04 sec\n",
            "Epoch 265, Loss(train/val) 0.77559/0.27021. Took 0.04 sec\n",
            "Epoch 266, Loss(train/val) 0.77777/0.26981. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 0.78195/0.27020. Took 0.04 sec\n",
            "Epoch 268, Loss(train/val) 0.77673/0.27053. Took 0.04 sec\n",
            "Epoch 269, Loss(train/val) 0.78080/0.26987. Took 0.04 sec\n",
            "Epoch 270, Loss(train/val) 0.78066/0.26963. Took 0.04 sec\n",
            "Epoch 271, Loss(train/val) 0.77780/0.26905. Took 0.04 sec\n",
            "Epoch 272, Loss(train/val) 0.78115/0.26918. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 0.77597/0.27044. Took 0.04 sec\n",
            "Epoch 274, Loss(train/val) 0.78362/0.27342. Took 0.04 sec\n",
            "Epoch 275, Loss(train/val) 0.77957/0.27322. Took 0.04 sec\n",
            "Epoch 276, Loss(train/val) 0.77960/0.27149. Took 0.04 sec\n",
            "Epoch 277, Loss(train/val) 0.77700/0.26940. Took 0.05 sec\n",
            "Epoch 278, Loss(train/val) 0.77634/0.26935. Took 0.04 sec\n",
            "Epoch 279, Loss(train/val) 0.77494/0.26911. Took 0.04 sec\n",
            "Epoch 280, Loss(train/val) 0.77704/0.26935. Took 0.04 sec\n",
            "Epoch 281, Loss(train/val) 0.78283/0.26930. Took 0.04 sec\n",
            "Epoch 282, Loss(train/val) 0.78021/0.26888. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 0.77831/0.26903. Took 0.04 sec\n",
            "Epoch 284, Loss(train/val) 0.77908/0.26849. Took 0.04 sec\n",
            "Epoch 285, Loss(train/val) 0.77626/0.26967. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.77466/0.27142. Took 0.04 sec\n",
            "Epoch 287, Loss(train/val) 0.77134/0.27023. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.77430/0.26858. Took 0.04 sec\n",
            "Epoch 289, Loss(train/val) 0.77985/0.26854. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.78075/0.26786. Took 0.04 sec\n",
            "Epoch 291, Loss(train/val) 0.78311/0.26739. Took 0.04 sec\n",
            "Epoch 292, Loss(train/val) 0.77280/0.26732. Took 0.05 sec\n",
            "Epoch 293, Loss(train/val) 0.77367/0.26750. Took 0.04 sec\n",
            "Epoch 294, Loss(train/val) 0.77181/0.26725. Took 0.04 sec\n",
            "Epoch 295, Loss(train/val) 0.76679/0.26727. Took 0.04 sec\n",
            "Epoch 296, Loss(train/val) 0.77460/0.26695. Took 0.04 sec\n",
            "Epoch 297, Loss(train/val) 0.77127/0.26729. Took 0.04 sec\n",
            "Epoch 298, Loss(train/val) 0.77747/0.26718. Took 0.04 sec\n",
            "Epoch 299, Loss(train/val) 0.77593/0.26681. Took 0.04 sec\n",
            "Epoch 300, Loss(train/val) 0.77554/0.26596. Took 0.04 sec\n",
            "Epoch 301, Loss(train/val) 0.76924/0.26564. Took 0.04 sec\n",
            "Epoch 302, Loss(train/val) 0.77707/0.26592. Took 0.05 sec\n",
            "Epoch 303, Loss(train/val) 0.77879/0.26596. Took 0.04 sec\n",
            "Epoch 304, Loss(train/val) 0.77389/0.26665. Took 0.04 sec\n",
            "Epoch 305, Loss(train/val) 0.77351/0.26634. Took 0.04 sec\n",
            "Epoch 306, Loss(train/val) 0.77494/0.26587. Took 0.04 sec\n",
            "Epoch 307, Loss(train/val) 0.77324/0.26592. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.76800/0.26618. Took 0.04 sec\n",
            "Epoch 309, Loss(train/val) 0.77502/0.26626. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.76862/0.26628. Took 0.04 sec\n",
            "Epoch 311, Loss(train/val) 0.77602/0.26615. Took 0.04 sec\n",
            "Epoch 312, Loss(train/val) 0.77835/0.26618. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 0.76577/0.26600. Took 0.04 sec\n",
            "Epoch 314, Loss(train/val) 0.77141/0.26563. Took 0.04 sec\n",
            "Epoch 315, Loss(train/val) 0.77257/0.26564. Took 0.04 sec\n",
            "Epoch 316, Loss(train/val) 0.76866/0.26555. Took 0.04 sec\n",
            "Epoch 317, Loss(train/val) 0.77335/0.26590. Took 0.04 sec\n",
            "Epoch 318, Loss(train/val) 0.77445/0.26539. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.77285/0.26543. Took 0.04 sec\n",
            "Epoch 320, Loss(train/val) 0.77107/0.26521. Took 0.04 sec\n",
            "Epoch 321, Loss(train/val) 0.76801/0.26475. Took 0.04 sec\n",
            "Epoch 322, Loss(train/val) 0.76433/0.26472. Took 0.05 sec\n",
            "Epoch 323, Loss(train/val) 0.77038/0.26494. Took 0.04 sec\n",
            "Epoch 324, Loss(train/val) 0.77134/0.26507. Took 0.04 sec\n",
            "Epoch 325, Loss(train/val) 0.76852/0.26436. Took 0.06 sec\n",
            "Epoch 326, Loss(train/val) 0.77040/0.26471. Took 0.04 sec\n",
            "Epoch 327, Loss(train/val) 0.76630/0.26427. Took 0.05 sec\n",
            "Epoch 328, Loss(train/val) 0.76750/0.26403. Took 0.04 sec\n",
            "Epoch 329, Loss(train/val) 0.76489/0.26420. Took 0.04 sec\n",
            "Epoch 330, Loss(train/val) 0.76870/0.26401. Took 0.04 sec\n",
            "Epoch 331, Loss(train/val) 0.76745/0.26504. Took 0.04 sec\n",
            "Epoch 332, Loss(train/val) 0.76801/0.26602. Took 0.06 sec\n",
            "Epoch 333, Loss(train/val) 0.76535/0.26712. Took 0.04 sec\n",
            "Epoch 334, Loss(train/val) 0.76798/0.26792. Took 0.04 sec\n",
            "Epoch 335, Loss(train/val) 0.76471/0.26951. Took 0.04 sec\n",
            "Epoch 336, Loss(train/val) 0.76584/0.27036. Took 0.04 sec\n",
            "Epoch 337, Loss(train/val) 0.76368/0.26636. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.76474/0.26385. Took 0.04 sec\n",
            "Epoch 339, Loss(train/val) 0.76585/0.26285. Took 0.04 sec\n",
            "Epoch 340, Loss(train/val) 0.76872/0.26273. Took 0.04 sec\n",
            "Epoch 341, Loss(train/val) 0.76167/0.26363. Took 0.04 sec\n",
            "Epoch 342, Loss(train/val) 0.76930/0.26696. Took 0.04 sec\n",
            "Epoch 343, Loss(train/val) 0.76225/0.26944. Took 0.04 sec\n",
            "Epoch 344, Loss(train/val) 0.76973/0.26916. Took 0.04 sec\n",
            "Epoch 345, Loss(train/val) 0.77047/0.26785. Took 0.04 sec\n",
            "Epoch 346, Loss(train/val) 0.76208/0.26947. Took 0.04 sec\n",
            "Epoch 347, Loss(train/val) 0.76355/0.26704. Took 0.05 sec\n",
            "Epoch 348, Loss(train/val) 0.76826/0.26407. Took 0.04 sec\n",
            "Epoch 349, Loss(train/val) 0.76738/0.26170. Took 0.04 sec\n",
            "Epoch 350, Loss(train/val) 0.76738/0.26138. Took 0.04 sec\n",
            "Epoch 351, Loss(train/val) 0.77063/0.26106. Took 0.04 sec\n",
            "Epoch 352, Loss(train/val) 0.76585/0.26219. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 0.76949/0.26218. Took 0.04 sec\n",
            "Epoch 354, Loss(train/val) 0.76484/0.26309. Took 0.04 sec\n",
            "Epoch 355, Loss(train/val) 0.76349/0.26365. Took 0.06 sec\n",
            "Epoch 356, Loss(train/val) 0.76441/0.26382. Took 0.04 sec\n",
            "Epoch 357, Loss(train/val) 0.76505/0.26336. Took 0.06 sec\n",
            "Epoch 358, Loss(train/val) 0.75751/0.26263. Took 0.04 sec\n",
            "Epoch 359, Loss(train/val) 0.76518/0.26153. Took 0.04 sec\n",
            "Epoch 360, Loss(train/val) 0.76138/0.26121. Took 0.04 sec\n",
            "Epoch 361, Loss(train/val) 0.76208/0.26147. Took 0.04 sec\n",
            "Epoch 362, Loss(train/val) 0.76762/0.26169. Took 0.06 sec\n",
            "Epoch 363, Loss(train/val) 0.76506/0.26081. Took 0.04 sec\n",
            "Epoch 364, Loss(train/val) 0.75888/0.26079. Took 0.04 sec\n",
            "Epoch 365, Loss(train/val) 0.75694/0.26090. Took 0.04 sec\n",
            "Epoch 366, Loss(train/val) 0.75649/0.26205. Took 0.04 sec\n",
            "Epoch 367, Loss(train/val) 0.76670/0.26435. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.75693/0.26087. Took 0.04 sec\n",
            "Epoch 369, Loss(train/val) 0.75726/0.26035. Took 0.04 sec\n",
            "Epoch 370, Loss(train/val) 0.76234/0.26067. Took 0.04 sec\n",
            "Epoch 371, Loss(train/val) 0.76097/0.25946. Took 0.04 sec\n",
            "Epoch 372, Loss(train/val) 0.75651/0.25917. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 0.75997/0.25938. Took 0.04 sec\n",
            "Epoch 374, Loss(train/val) 0.75952/0.25926. Took 0.04 sec\n",
            "Epoch 375, Loss(train/val) 0.75666/0.25919. Took 0.04 sec\n",
            "Epoch 376, Loss(train/val) 0.76293/0.25915. Took 0.04 sec\n",
            "Epoch 377, Loss(train/val) 0.75342/0.25911. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.76411/0.25964. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 0.75616/0.25981. Took 0.04 sec\n",
            "Epoch 380, Loss(train/val) 0.75988/0.25872. Took 0.04 sec\n",
            "Epoch 381, Loss(train/val) 0.75404/0.25997. Took 0.04 sec\n",
            "Epoch 382, Loss(train/val) 0.75472/0.26035. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.75601/0.26046. Took 0.04 sec\n",
            "Epoch 384, Loss(train/val) 0.75885/0.25921. Took 0.04 sec\n",
            "Epoch 385, Loss(train/val) 0.75798/0.25839. Took 0.04 sec\n",
            "Epoch 386, Loss(train/val) 0.76371/0.25827. Took 0.04 sec\n",
            "Epoch 387, Loss(train/val) 0.75714/0.25849. Took 0.04 sec\n",
            "Epoch 388, Loss(train/val) 0.76206/0.25910. Took 0.05 sec\n",
            "Epoch 389, Loss(train/val) 0.74868/0.25950. Took 0.04 sec\n",
            "Epoch 390, Loss(train/val) 0.75621/0.26089. Took 0.04 sec\n",
            "Epoch 391, Loss(train/val) 0.75206/0.25986. Took 0.04 sec\n",
            "Epoch 392, Loss(train/val) 0.75895/0.25940. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.75720/0.25847. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 0.75640/0.25711. Took 0.04 sec\n",
            "Epoch 395, Loss(train/val) 0.75964/0.25806. Took 0.04 sec\n",
            "Epoch 396, Loss(train/val) 0.75537/0.25780. Took 0.04 sec\n",
            "Epoch 397, Loss(train/val) 0.75742/0.25760. Took 0.06 sec\n",
            "Epoch 398, Loss(train/val) 0.75830/0.25719. Took 0.04 sec\n",
            "Epoch 399, Loss(train/val) 0.74974/0.25722. Took 0.04 sec\n",
            "Epoch 400, Loss(train/val) 0.75735/0.25767. Took 0.04 sec\n",
            "Epoch 401, Loss(train/val) 0.75432/0.25821. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.75512/0.25833. Took 0.05 sec\n",
            "Epoch 403, Loss(train/val) 0.75684/0.25772. Took 0.04 sec\n",
            "Epoch 404, Loss(train/val) 0.75019/0.25732. Took 0.04 sec\n",
            "Epoch 405, Loss(train/val) 0.74813/0.25643. Took 0.04 sec\n",
            "Epoch 406, Loss(train/val) 0.75894/0.25622. Took 0.04 sec\n",
            "Epoch 407, Loss(train/val) 0.75641/0.25621. Took 0.05 sec\n",
            "Epoch 408, Loss(train/val) 0.74964/0.25612. Took 0.04 sec\n",
            "Epoch 409, Loss(train/val) 0.74915/0.25621. Took 0.04 sec\n",
            "Epoch 410, Loss(train/val) 0.75525/0.25740. Took 0.04 sec\n",
            "Epoch 411, Loss(train/val) 0.75426/0.25887. Took 0.04 sec\n",
            "Epoch 412, Loss(train/val) 0.75410/0.26043. Took 0.05 sec\n",
            "Epoch 413, Loss(train/val) 0.75511/0.25997. Took 0.04 sec\n",
            "Epoch 414, Loss(train/val) 0.74983/0.25821. Took 0.04 sec\n",
            "Epoch 415, Loss(train/val) 0.75076/0.25606. Took 0.04 sec\n",
            "Epoch 416, Loss(train/val) 0.74892/0.25542. Took 0.04 sec\n",
            "Epoch 417, Loss(train/val) 0.75257/0.25418. Took 0.04 sec\n",
            "Epoch 418, Loss(train/val) 0.75154/0.25415. Took 0.05 sec\n",
            "Epoch 419, Loss(train/val) 0.75180/0.25701. Took 0.04 sec\n",
            "Epoch 420, Loss(train/val) 0.74960/0.25831. Took 0.04 sec\n",
            "Epoch 421, Loss(train/val) 0.74926/0.25779. Took 0.04 sec\n",
            "Epoch 422, Loss(train/val) 0.75071/0.25773. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.74315/0.25734. Took 0.04 sec\n",
            "Epoch 424, Loss(train/val) 0.74844/0.25857. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.74291/0.25915. Took 0.04 sec\n",
            "Epoch 426, Loss(train/val) 0.74846/0.25899. Took 0.04 sec\n",
            "Epoch 427, Loss(train/val) 0.74991/0.25914. Took 0.05 sec\n",
            "Epoch 428, Loss(train/val) 0.75215/0.25661. Took 0.04 sec\n",
            "Epoch 429, Loss(train/val) 0.74913/0.25546. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.74823/0.25474. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.74754/0.25521. Took 0.05 sec\n",
            "Epoch 432, Loss(train/val) 0.74832/0.25637. Took 0.05 sec\n",
            "Epoch 433, Loss(train/val) 0.74452/0.25501. Took 0.04 sec\n",
            "Epoch 434, Loss(train/val) 0.74941/0.25491. Took 0.04 sec\n",
            "Epoch 435, Loss(train/val) 0.74247/0.25642. Took 0.04 sec\n",
            "Epoch 436, Loss(train/val) 0.74942/0.26147. Took 0.04 sec\n",
            "Epoch 437, Loss(train/val) 0.75135/0.26038. Took 0.05 sec\n",
            "Epoch 438, Loss(train/val) 0.74657/0.25599. Took 0.04 sec\n",
            "Epoch 439, Loss(train/val) 0.74822/0.25351. Took 0.04 sec\n",
            "Epoch 440, Loss(train/val) 0.74375/0.25306. Took 0.04 sec\n",
            "Epoch 441, Loss(train/val) 0.74520/0.25264. Took 0.04 sec\n",
            "Epoch 442, Loss(train/val) 0.74534/0.25325. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 0.74387/0.25413. Took 0.04 sec\n",
            "Epoch 444, Loss(train/val) 0.74274/0.25368. Took 0.04 sec\n",
            "Epoch 445, Loss(train/val) 0.74135/0.25295. Took 0.04 sec\n",
            "Epoch 446, Loss(train/val) 0.74019/0.25263. Took 0.04 sec\n",
            "Epoch 447, Loss(train/val) 0.74079/0.25301. Took 0.07 sec\n",
            "Epoch 448, Loss(train/val) 0.74403/0.25308. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 0.74430/0.25357. Took 0.04 sec\n",
            "Epoch 450, Loss(train/val) 0.75049/0.25257. Took 0.04 sec\n",
            "Epoch 451, Loss(train/val) 0.74513/0.25291. Took 0.04 sec\n",
            "Epoch 452, Loss(train/val) 0.74147/0.25170. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.74569/0.25235. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.74322/0.25482. Took 0.04 sec\n",
            "Epoch 455, Loss(train/val) 0.74144/0.25235. Took 0.04 sec\n",
            "Epoch 456, Loss(train/val) 0.74614/0.25174. Took 0.04 sec\n",
            "Epoch 457, Loss(train/val) 0.74171/0.25132. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.74365/0.25081. Took 0.04 sec\n",
            "Epoch 459, Loss(train/val) 0.74834/0.25152. Took 0.04 sec\n",
            "Epoch 460, Loss(train/val) 0.74692/0.25045. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.74476/0.25030. Took 0.04 sec\n",
            "Epoch 462, Loss(train/val) 0.73982/0.25033. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.73862/0.25009. Took 0.04 sec\n",
            "Epoch 464, Loss(train/val) 0.73333/0.24993. Took 0.04 sec\n",
            "Epoch 465, Loss(train/val) 0.74088/0.25062. Took 0.04 sec\n",
            "Epoch 466, Loss(train/val) 0.74018/0.24997. Took 0.04 sec\n",
            "Epoch 467, Loss(train/val) 0.74174/0.24999. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.74086/0.25019. Took 0.04 sec\n",
            "Epoch 469, Loss(train/val) 0.74145/0.25022. Took 0.05 sec\n",
            "Epoch 470, Loss(train/val) 0.74203/0.24945. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 0.74005/0.24908. Took 0.04 sec\n",
            "Epoch 472, Loss(train/val) 0.73869/0.24869. Took 0.05 sec\n",
            "Epoch 473, Loss(train/val) 0.74065/0.24898. Took 0.04 sec\n",
            "Epoch 474, Loss(train/val) 0.74123/0.24959. Took 0.04 sec\n",
            "Epoch 475, Loss(train/val) 0.73906/0.25024. Took 0.04 sec\n",
            "Epoch 476, Loss(train/val) 0.73267/0.25105. Took 0.04 sec\n",
            "Epoch 477, Loss(train/val) 0.73690/0.25143. Took 0.05 sec\n",
            "Epoch 478, Loss(train/val) 0.73912/0.25056. Took 0.04 sec\n",
            "Epoch 479, Loss(train/val) 0.74191/0.25080. Took 0.04 sec\n",
            "Epoch 480, Loss(train/val) 0.74541/0.24955. Took 0.04 sec\n",
            "Epoch 481, Loss(train/val) 0.74013/0.24836. Took 0.04 sec\n",
            "Epoch 482, Loss(train/val) 0.73475/0.24824. Took 0.05 sec\n",
            "Epoch 483, Loss(train/val) 0.73690/0.24907. Took 0.04 sec\n",
            "Epoch 484, Loss(train/val) 0.74143/0.24922. Took 0.04 sec\n",
            "Epoch 485, Loss(train/val) 0.73852/0.24905. Took 0.04 sec\n",
            "Epoch 486, Loss(train/val) 0.74140/0.24908. Took 0.04 sec\n",
            "Epoch 487, Loss(train/val) 0.73486/0.24827. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.73753/0.24756. Took 0.04 sec\n",
            "Epoch 489, Loss(train/val) 0.73240/0.24761. Took 0.04 sec\n",
            "Epoch 490, Loss(train/val) 0.73370/0.24841. Took 0.04 sec\n",
            "Epoch 491, Loss(train/val) 0.74178/0.24750. Took 0.04 sec\n",
            "Epoch 492, Loss(train/val) 0.73265/0.24764. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.73204/0.24747. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.73832/0.24941. Took 0.04 sec\n",
            "Epoch 495, Loss(train/val) 0.73641/0.24941. Took 0.04 sec\n",
            "Epoch 496, Loss(train/val) 0.72838/0.24713. Took 0.04 sec\n",
            "Epoch 497, Loss(train/val) 0.73987/0.24702. Took 0.05 sec\n",
            "Epoch 498, Loss(train/val) 0.73251/0.24642. Took 0.04 sec\n",
            "Epoch 499, Loss(train/val) 0.73699/0.24927. Took 0.04 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=500, exp_name='exp2_hidn', hid_dim=16, input_dim=1, l2=1e-05, lr=0.0001, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.05574/0.38775. Took 0.04 sec\n",
            "Epoch 1, Loss(train/val) 1.04595/0.38747. Took 0.05 sec\n",
            "Epoch 2, Loss(train/val) 1.03976/0.38724. Took 0.05 sec\n",
            "Epoch 3, Loss(train/val) 1.05184/0.38702. Took 0.04 sec\n",
            "Epoch 4, Loss(train/val) 1.05050/0.38683. Took 0.04 sec\n",
            "Epoch 5, Loss(train/val) 1.04522/0.38667. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.05188/0.38655. Took 0.04 sec\n",
            "Epoch 7, Loss(train/val) 1.04931/0.38647. Took 0.05 sec\n",
            "Epoch 8, Loss(train/val) 1.04746/0.38640. Took 0.04 sec\n",
            "Epoch 9, Loss(train/val) 1.04486/0.38634. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.05257/0.38629. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.04921/0.38625. Took 0.04 sec\n",
            "Epoch 12, Loss(train/val) 1.04363/0.38621. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 1.04817/0.38617. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.04587/0.38613. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.03472/0.38612. Took 0.05 sec\n",
            "Epoch 16, Loss(train/val) 1.04610/0.38610. Took 0.05 sec\n",
            "Epoch 17, Loss(train/val) 1.03698/0.38609. Took 0.05 sec\n",
            "Epoch 18, Loss(train/val) 1.04568/0.38611. Took 0.05 sec\n",
            "Epoch 19, Loss(train/val) 1.04591/0.38613. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 1.04902/0.38616. Took 0.05 sec\n",
            "Epoch 21, Loss(train/val) 1.04260/0.38620. Took 0.04 sec\n",
            "Epoch 22, Loss(train/val) 1.04151/0.38625. Took 0.06 sec\n",
            "Epoch 23, Loss(train/val) 1.05054/0.38632. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 1.03884/0.38639. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 1.04931/0.38648. Took 0.05 sec\n",
            "Epoch 26, Loss(train/val) 1.03982/0.38667. Took 0.04 sec\n",
            "Epoch 27, Loss(train/val) 1.03965/0.38690. Took 0.06 sec\n",
            "Epoch 28, Loss(train/val) 1.04995/0.38717. Took 0.05 sec\n",
            "Epoch 29, Loss(train/val) 1.04852/0.38749. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 1.04738/0.38786. Took 0.04 sec\n",
            "Epoch 31, Loss(train/val) 1.05134/0.38827. Took 0.04 sec\n",
            "Epoch 32, Loss(train/val) 1.03216/0.38870. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 1.04319/0.38914. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 1.04201/0.38958. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 1.04132/0.38998. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 1.04731/0.39034. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.03687/0.39064. Took 0.06 sec\n",
            "Epoch 38, Loss(train/val) 1.04859/0.39084. Took 0.04 sec\n",
            "Epoch 39, Loss(train/val) 1.04197/0.39097. Took 0.04 sec\n",
            "Epoch 40, Loss(train/val) 1.04827/0.39196. Took 0.05 sec\n",
            "Epoch 41, Loss(train/val) 1.04877/0.39278. Took 0.04 sec\n",
            "Epoch 42, Loss(train/val) 1.04304/0.39172. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 1.03067/0.39073. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 1.03073/0.38983. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 1.04825/0.38915. Took 0.04 sec\n",
            "Epoch 46, Loss(train/val) 1.04006/0.38875. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 1.04467/0.38872. Took 0.05 sec\n",
            "Epoch 48, Loss(train/val) 1.03649/0.38880. Took 0.05 sec\n",
            "Epoch 49, Loss(train/val) 1.03551/0.38904. Took 0.04 sec\n",
            "Epoch 50, Loss(train/val) 1.04539/0.38950. Took 0.04 sec\n",
            "Epoch 51, Loss(train/val) 1.03833/0.39013. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 1.04649/0.39088. Took 0.06 sec\n",
            "Epoch 53, Loss(train/val) 1.03582/0.39180. Took 0.04 sec\n",
            "Epoch 54, Loss(train/val) 1.03919/0.39279. Took 0.04 sec\n",
            "Epoch 55, Loss(train/val) 1.04537/0.39426. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 1.04532/0.39603. Took 0.04 sec\n",
            "Epoch 57, Loss(train/val) 1.03856/0.39800. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 1.03683/0.40024. Took 0.06 sec\n",
            "Epoch 59, Loss(train/val) 1.03933/0.40262. Took 0.06 sec\n",
            "Epoch 60, Loss(train/val) 1.03960/0.40377. Took 0.05 sec\n",
            "Epoch 61, Loss(train/val) 1.03680/0.40516. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 1.04228/0.40671. Took 0.05 sec\n",
            "Epoch 63, Loss(train/val) 1.03541/0.40830. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 1.04291/0.40955. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 1.03294/0.40993. Took 0.05 sec\n",
            "Epoch 66, Loss(train/val) 1.02912/0.41015. Took 0.06 sec\n",
            "Epoch 67, Loss(train/val) 1.03177/0.41026. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 1.02375/0.41023. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 1.02967/0.40957. Took 0.05 sec\n",
            "Epoch 70, Loss(train/val) 1.02879/0.40835. Took 0.05 sec\n",
            "Epoch 71, Loss(train/val) 1.03062/0.40773. Took 0.05 sec\n",
            "Epoch 72, Loss(train/val) 1.03026/0.40718. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 1.02490/0.40785. Took 0.05 sec\n",
            "Epoch 74, Loss(train/val) 1.01847/0.40954. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 1.02152/0.41090. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 1.02106/0.41126. Took 0.06 sec\n",
            "Epoch 77, Loss(train/val) 1.01526/0.41064. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 1.01690/0.41046. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 1.00110/0.41122. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 1.00491/0.41328. Took 0.05 sec\n",
            "Epoch 81, Loss(train/val) 0.99893/0.41653. Took 0.06 sec\n",
            "Epoch 82, Loss(train/val) 0.98472/0.41988. Took 0.05 sec\n",
            "Epoch 83, Loss(train/val) 0.98862/0.42544. Took 0.04 sec\n",
            "Epoch 84, Loss(train/val) 0.97148/0.43094. Took 0.04 sec\n",
            "Epoch 85, Loss(train/val) 0.97272/0.43720. Took 0.05 sec\n",
            "Epoch 86, Loss(train/val) 0.96692/0.44332. Took 0.05 sec\n",
            "Epoch 87, Loss(train/val) 0.96451/0.44778. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 0.96651/0.45064. Took 0.04 sec\n",
            "Epoch 89, Loss(train/val) 0.95422/0.44918. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 0.94472/0.43273. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 0.94290/0.40475. Took 0.05 sec\n",
            "Epoch 92, Loss(train/val) 0.93853/0.37590. Took 0.04 sec\n",
            "Epoch 93, Loss(train/val) 0.93218/0.35690. Took 0.04 sec\n",
            "Epoch 94, Loss(train/val) 0.92060/0.34703. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 0.92522/0.34159. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 0.91603/0.33939. Took 0.05 sec\n",
            "Epoch 97, Loss(train/val) 0.91143/0.33892. Took 0.04 sec\n",
            "Epoch 98, Loss(train/val) 0.89833/0.34100. Took 0.04 sec\n",
            "Epoch 99, Loss(train/val) 0.89983/0.34237. Took 0.05 sec\n",
            "Epoch 100, Loss(train/val) 0.89614/0.34477. Took 0.04 sec\n",
            "Epoch 101, Loss(train/val) 0.88105/0.34517. Took 0.05 sec\n",
            "Epoch 102, Loss(train/val) 0.88864/0.34317. Took 0.05 sec\n",
            "Epoch 103, Loss(train/val) 0.87574/0.34028. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 0.87536/0.33968. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 0.87653/0.33814. Took 0.04 sec\n",
            "Epoch 106, Loss(train/val) 0.87532/0.33795. Took 0.06 sec\n",
            "Epoch 107, Loss(train/val) 0.87210/0.33845. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 0.86630/0.33853. Took 0.04 sec\n",
            "Epoch 109, Loss(train/val) 0.86884/0.33755. Took 0.04 sec\n",
            "Epoch 110, Loss(train/val) 0.86454/0.33791. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 0.86462/0.33581. Took 0.05 sec\n",
            "Epoch 112, Loss(train/val) 0.85521/0.33468. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 0.85620/0.33453. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 0.85006/0.33423. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 0.85108/0.33551. Took 0.05 sec\n",
            "Epoch 116, Loss(train/val) 0.84638/0.33831. Took 0.05 sec\n",
            "Epoch 117, Loss(train/val) 0.84155/0.34201. Took 0.05 sec\n",
            "Epoch 118, Loss(train/val) 0.84158/0.34184. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 0.83307/0.33898. Took 0.04 sec\n",
            "Epoch 120, Loss(train/val) 0.83786/0.33432. Took 0.04 sec\n",
            "Epoch 121, Loss(train/val) 0.83300/0.33244. Took 0.06 sec\n",
            "Epoch 122, Loss(train/val) 0.82919/0.33541. Took 0.04 sec\n",
            "Epoch 123, Loss(train/val) 0.83254/0.33434. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 0.82990/0.33407. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 0.81983/0.33417. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 0.82648/0.33162. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 0.82409/0.32862. Took 0.04 sec\n",
            "Epoch 128, Loss(train/val) 0.81729/0.32656. Took 0.04 sec\n",
            "Epoch 129, Loss(train/val) 0.80843/0.32877. Took 0.04 sec\n",
            "Epoch 130, Loss(train/val) 0.81123/0.32927. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 0.81156/0.32858. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 0.79891/0.32928. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 0.80417/0.32544. Took 0.04 sec\n",
            "Epoch 134, Loss(train/val) 0.79723/0.32450. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 0.80304/0.32126. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 0.79864/0.31880. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 0.79497/0.31939. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 0.79338/0.31742. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 0.79437/0.31735. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 0.78884/0.31862. Took 0.05 sec\n",
            "Epoch 141, Loss(train/val) 0.78829/0.31903. Took 0.05 sec\n",
            "Epoch 142, Loss(train/val) 0.78384/0.32042. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 0.77867/0.32142. Took 0.04 sec\n",
            "Epoch 144, Loss(train/val) 0.78294/0.31872. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 0.77801/0.31606. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 0.76820/0.31329. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 0.77919/0.31227. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 0.77649/0.31175. Took 0.05 sec\n",
            "Epoch 149, Loss(train/val) 0.77624/0.30970. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 0.76595/0.30829. Took 0.04 sec\n",
            "Epoch 151, Loss(train/val) 0.76612/0.30774. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 0.76227/0.30782. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 0.77125/0.30830. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 0.76075/0.30570. Took 0.04 sec\n",
            "Epoch 155, Loss(train/val) 0.75087/0.30446. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 0.75448/0.29958. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 0.76097/0.29695. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 0.75016/0.29655. Took 0.05 sec\n",
            "Epoch 159, Loss(train/val) 0.74906/0.29533. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 0.74872/0.29460. Took 0.05 sec\n",
            "Epoch 161, Loss(train/val) 0.74018/0.29419. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 0.75076/0.29591. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 0.74305/0.29802. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 0.73703/0.29545. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 0.73360/0.29589. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.73678/0.29880. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 0.72866/0.29537. Took 0.05 sec\n",
            "Epoch 168, Loss(train/val) 0.73190/0.29173. Took 0.04 sec\n",
            "Epoch 169, Loss(train/val) 0.73053/0.28852. Took 0.05 sec\n",
            "Epoch 170, Loss(train/val) 0.72822/0.28746. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 0.72203/0.28682. Took 0.06 sec\n",
            "Epoch 172, Loss(train/val) 0.72251/0.28607. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 0.72498/0.28533. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 0.70517/0.28472. Took 0.04 sec\n",
            "Epoch 175, Loss(train/val) 0.71981/0.28427. Took 0.05 sec\n",
            "Epoch 176, Loss(train/val) 0.71387/0.28341. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 0.70843/0.28314. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 0.71150/0.28332. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 0.70322/0.28227. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 0.70420/0.28277. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 0.69775/0.28405. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 0.70285/0.28217. Took 0.05 sec\n",
            "Epoch 183, Loss(train/val) 0.69454/0.28174. Took 0.05 sec\n",
            "Epoch 184, Loss(train/val) 0.69528/0.28104. Took 0.05 sec\n",
            "Epoch 185, Loss(train/val) 0.69015/0.27981. Took 0.05 sec\n",
            "Epoch 186, Loss(train/val) 0.68105/0.27893. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 0.69101/0.27701. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 0.68852/0.27660. Took 0.04 sec\n",
            "Epoch 189, Loss(train/val) 0.67703/0.27840. Took 0.05 sec\n",
            "Epoch 190, Loss(train/val) 0.68094/0.27652. Took 0.05 sec\n",
            "Epoch 191, Loss(train/val) 0.67601/0.27469. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 0.67501/0.27447. Took 0.04 sec\n",
            "Epoch 193, Loss(train/val) 0.67246/0.27390. Took 0.05 sec\n",
            "Epoch 194, Loss(train/val) 0.67134/0.27393. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 0.66966/0.27445. Took 0.05 sec\n",
            "Epoch 196, Loss(train/val) 0.66790/0.27346. Took 0.05 sec\n",
            "Epoch 197, Loss(train/val) 0.65703/0.27159. Took 0.05 sec\n",
            "Epoch 198, Loss(train/val) 0.66404/0.27143. Took 0.05 sec\n",
            "Epoch 199, Loss(train/val) 0.65789/0.27096. Took 0.04 sec\n",
            "Epoch 200, Loss(train/val) 0.65859/0.27329. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 0.65228/0.27329. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 0.64891/0.27506. Took 0.05 sec\n",
            "Epoch 203, Loss(train/val) 0.64715/0.27661. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 0.64864/0.27511. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.64060/0.27613. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 0.63752/0.27123. Took 0.06 sec\n",
            "Epoch 207, Loss(train/val) 0.64118/0.26827. Took 0.05 sec\n",
            "Epoch 208, Loss(train/val) 0.63578/0.26671. Took 0.04 sec\n",
            "Epoch 209, Loss(train/val) 0.63115/0.26685. Took 0.04 sec\n",
            "Epoch 210, Loss(train/val) 0.63194/0.26539. Took 0.05 sec\n",
            "Epoch 211, Loss(train/val) 0.63076/0.26432. Took 0.05 sec\n",
            "Epoch 212, Loss(train/val) 0.62817/0.26343. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 0.62592/0.26288. Took 0.04 sec\n",
            "Epoch 214, Loss(train/val) 0.62026/0.26276. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.61992/0.26194. Took 0.05 sec\n",
            "Epoch 216, Loss(train/val) 0.61653/0.26124. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 0.62084/0.26065. Took 0.05 sec\n",
            "Epoch 218, Loss(train/val) 0.61314/0.26044. Took 0.05 sec\n",
            "Epoch 219, Loss(train/val) 0.59901/0.26333. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 0.59931/0.26412. Took 0.05 sec\n",
            "Epoch 221, Loss(train/val) 0.60424/0.26452. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 0.60429/0.26655. Took 0.05 sec\n",
            "Epoch 223, Loss(train/val) 0.60039/0.26755. Took 0.05 sec\n",
            "Epoch 224, Loss(train/val) 0.59644/0.26605. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 0.59177/0.26378. Took 0.05 sec\n",
            "Epoch 226, Loss(train/val) 0.59043/0.26109. Took 0.05 sec\n",
            "Epoch 227, Loss(train/val) 0.59439/0.25557. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 0.59163/0.25495. Took 0.04 sec\n",
            "Epoch 229, Loss(train/val) 0.58929/0.25436. Took 0.04 sec\n",
            "Epoch 230, Loss(train/val) 0.57979/0.25392. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 0.58128/0.25428. Took 0.06 sec\n",
            "Epoch 232, Loss(train/val) 0.58361/0.25740. Took 0.05 sec\n",
            "Epoch 233, Loss(train/val) 0.57648/0.25904. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 0.57184/0.25490. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 0.56558/0.25423. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 0.56592/0.25166. Took 0.06 sec\n",
            "Epoch 237, Loss(train/val) 0.57118/0.25169. Took 0.05 sec\n",
            "Epoch 238, Loss(train/val) 0.57315/0.25106. Took 0.04 sec\n",
            "Epoch 239, Loss(train/val) 0.56175/0.25111. Took 0.04 sec\n",
            "Epoch 240, Loss(train/val) 0.56638/0.25136. Took 0.04 sec\n",
            "Epoch 241, Loss(train/val) 0.55914/0.25102. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 0.55954/0.25300. Took 0.05 sec\n",
            "Epoch 243, Loss(train/val) 0.55681/0.25443. Took 0.05 sec\n",
            "Epoch 244, Loss(train/val) 0.55055/0.25477. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.55349/0.25448. Took 0.05 sec\n",
            "Epoch 246, Loss(train/val) 0.55208/0.25527. Took 0.05 sec\n",
            "Epoch 247, Loss(train/val) 0.54454/0.25525. Took 0.05 sec\n",
            "Epoch 248, Loss(train/val) 0.54890/0.25478. Took 0.06 sec\n",
            "Epoch 249, Loss(train/val) 0.53305/0.25560. Took 0.05 sec\n",
            "Epoch 250, Loss(train/val) 0.54191/0.25716. Took 0.04 sec\n",
            "Epoch 251, Loss(train/val) 0.54288/0.25881. Took 0.05 sec\n",
            "Epoch 252, Loss(train/val) 0.53805/0.26277. Took 0.05 sec\n",
            "Epoch 253, Loss(train/val) 0.53791/0.25955. Took 0.04 sec\n",
            "Epoch 254, Loss(train/val) 0.53737/0.25546. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.53633/0.25230. Took 0.04 sec\n",
            "Epoch 256, Loss(train/val) 0.52995/0.25029. Took 0.05 sec\n",
            "Epoch 257, Loss(train/val) 0.52970/0.25005. Took 0.04 sec\n",
            "Epoch 258, Loss(train/val) 0.53001/0.24968. Took 0.05 sec\n",
            "Epoch 259, Loss(train/val) 0.51836/0.25117. Took 0.05 sec\n",
            "Epoch 260, Loss(train/val) 0.51558/0.25225. Took 0.05 sec\n",
            "Epoch 261, Loss(train/val) 0.51955/0.24909. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.50538/0.24832. Took 0.05 sec\n",
            "Epoch 263, Loss(train/val) 0.51013/0.24742. Took 0.05 sec\n",
            "Epoch 264, Loss(train/val) 0.51729/0.25099. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.50607/0.24882. Took 0.05 sec\n",
            "Epoch 266, Loss(train/val) 0.50397/0.24800. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 0.50559/0.24657. Took 0.04 sec\n",
            "Epoch 268, Loss(train/val) 0.50320/0.24506. Took 0.04 sec\n",
            "Epoch 269, Loss(train/val) 0.49668/0.24488. Took 0.05 sec\n",
            "Epoch 270, Loss(train/val) 0.49754/0.24561. Took 0.06 sec\n",
            "Epoch 271, Loss(train/val) 0.50050/0.24387. Took 0.05 sec\n",
            "Epoch 272, Loss(train/val) 0.49887/0.24308. Took 0.04 sec\n",
            "Epoch 273, Loss(train/val) 0.49516/0.24334. Took 0.04 sec\n",
            "Epoch 274, Loss(train/val) 0.49509/0.24584. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.48586/0.24435. Took 0.04 sec\n",
            "Epoch 276, Loss(train/val) 0.48662/0.24197. Took 0.05 sec\n",
            "Epoch 277, Loss(train/val) 0.48837/0.24179. Took 0.05 sec\n",
            "Epoch 278, Loss(train/val) 0.48782/0.24158. Took 0.05 sec\n",
            "Epoch 279, Loss(train/val) 0.48043/0.24369. Took 0.05 sec\n",
            "Epoch 280, Loss(train/val) 0.48492/0.24460. Took 0.04 sec\n",
            "Epoch 281, Loss(train/val) 0.48071/0.24706. Took 0.05 sec\n",
            "Epoch 282, Loss(train/val) 0.47625/0.24851. Took 0.04 sec\n",
            "Epoch 283, Loss(train/val) 0.47699/0.24146. Took 0.04 sec\n",
            "Epoch 284, Loss(train/val) 0.47381/0.23957. Took 0.06 sec\n",
            "Epoch 285, Loss(train/val) 0.47569/0.23943. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.47157/0.23955. Took 0.06 sec\n",
            "Epoch 287, Loss(train/val) 0.47108/0.23924. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.46639/0.23928. Took 0.05 sec\n",
            "Epoch 289, Loss(train/val) 0.46952/0.23972. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.46244/0.23957. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.46482/0.24099. Took 0.06 sec\n",
            "Epoch 292, Loss(train/val) 0.46988/0.23939. Took 0.05 sec\n",
            "Epoch 293, Loss(train/val) 0.46481/0.24152. Took 0.04 sec\n",
            "Epoch 294, Loss(train/val) 0.45741/0.24063. Took 0.05 sec\n",
            "Epoch 295, Loss(train/val) 0.45842/0.23901. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 0.45982/0.23750. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.45774/0.23870. Took 0.04 sec\n",
            "Epoch 298, Loss(train/val) 0.45897/0.23782. Took 0.05 sec\n",
            "Epoch 299, Loss(train/val) 0.45618/0.23709. Took 0.04 sec\n",
            "Epoch 300, Loss(train/val) 0.45326/0.23825. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.45531/0.24093. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.45060/0.23945. Took 0.05 sec\n",
            "Epoch 303, Loss(train/val) 0.45842/0.23664. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 0.43905/0.23675. Took 0.05 sec\n",
            "Epoch 305, Loss(train/val) 0.44642/0.23602. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 0.43607/0.23984. Took 0.05 sec\n",
            "Epoch 307, Loss(train/val) 0.43822/0.24152. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.43374/0.23540. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.43421/0.23516. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.43903/0.23589. Took 0.05 sec\n",
            "Epoch 311, Loss(train/val) 0.43154/0.24213. Took 0.06 sec\n",
            "Epoch 312, Loss(train/val) 0.43456/0.24888. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 0.43085/0.23708. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.43060/0.23375. Took 0.04 sec\n",
            "Epoch 315, Loss(train/val) 0.42708/0.23465. Took 0.04 sec\n",
            "Epoch 316, Loss(train/val) 0.43318/0.23534. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.43029/0.23435. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.42473/0.24091. Took 0.04 sec\n",
            "Epoch 319, Loss(train/val) 0.41884/0.23620. Took 0.04 sec\n",
            "Epoch 320, Loss(train/val) 0.42802/0.23410. Took 0.04 sec\n",
            "Epoch 321, Loss(train/val) 0.41641/0.23418. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.41464/0.23532. Took 0.05 sec\n",
            "Epoch 323, Loss(train/val) 0.41928/0.23583. Took 0.05 sec\n",
            "Epoch 324, Loss(train/val) 0.42707/0.23338. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.41458/0.23531. Took 0.04 sec\n",
            "Epoch 326, Loss(train/val) 0.41479/0.23408. Took 0.06 sec\n",
            "Epoch 327, Loss(train/val) 0.40848/0.23403. Took 0.04 sec\n",
            "Epoch 328, Loss(train/val) 0.41496/0.23415. Took 0.04 sec\n",
            "Epoch 329, Loss(train/val) 0.40726/0.23256. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.40854/0.23261. Took 0.05 sec\n",
            "Epoch 331, Loss(train/val) 0.40411/0.23532. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 0.41589/0.23421. Took 0.05 sec\n",
            "Epoch 333, Loss(train/val) 0.40389/0.23409. Took 0.05 sec\n",
            "Epoch 334, Loss(train/val) 0.40635/0.23480. Took 0.05 sec\n",
            "Epoch 335, Loss(train/val) 0.39696/0.23270. Took 0.05 sec\n",
            "Epoch 336, Loss(train/val) 0.40053/0.23282. Took 0.05 sec\n",
            "Epoch 337, Loss(train/val) 0.39354/0.23160. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.39805/0.23083. Took 0.04 sec\n",
            "Epoch 339, Loss(train/val) 0.39983/0.23283. Took 0.05 sec\n",
            "Epoch 340, Loss(train/val) 0.40062/0.23580. Took 0.05 sec\n",
            "Epoch 341, Loss(train/val) 0.39400/0.23396. Took 0.06 sec\n",
            "Epoch 342, Loss(train/val) 0.39492/0.23208. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.39419/0.23068. Took 0.04 sec\n",
            "Epoch 344, Loss(train/val) 0.38426/0.23131. Took 0.05 sec\n",
            "Epoch 345, Loss(train/val) 0.38569/0.23185. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.38132/0.23284. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 0.38636/0.23020. Took 0.05 sec\n",
            "Epoch 348, Loss(train/val) 0.38573/0.22973. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 0.38166/0.22923. Took 0.05 sec\n",
            "Epoch 350, Loss(train/val) 0.37848/0.22927. Took 0.05 sec\n",
            "Epoch 351, Loss(train/val) 0.38850/0.23125. Took 0.06 sec\n",
            "Epoch 352, Loss(train/val) 0.37497/0.23234. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 0.37757/0.22888. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 0.38198/0.22917. Took 0.04 sec\n",
            "Epoch 355, Loss(train/val) 0.37880/0.22961. Took 0.05 sec\n",
            "Epoch 356, Loss(train/val) 0.38166/0.22801. Took 0.05 sec\n",
            "Epoch 357, Loss(train/val) 0.37877/0.22907. Took 0.05 sec\n",
            "Epoch 358, Loss(train/val) 0.35938/0.23296. Took 0.04 sec\n",
            "Epoch 359, Loss(train/val) 0.37285/0.23673. Took 0.04 sec\n",
            "Epoch 360, Loss(train/val) 0.37503/0.23458. Took 0.05 sec\n",
            "Epoch 361, Loss(train/val) 0.37126/0.22815. Took 0.05 sec\n",
            "Epoch 362, Loss(train/val) 0.37006/0.22845. Took 0.04 sec\n",
            "Epoch 363, Loss(train/val) 0.37086/0.22950. Took 0.04 sec\n",
            "Epoch 364, Loss(train/val) 0.37396/0.23009. Took 0.04 sec\n",
            "Epoch 365, Loss(train/val) 0.37154/0.22741. Took 0.04 sec\n",
            "Epoch 366, Loss(train/val) 0.36155/0.22866. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 0.36567/0.22917. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.35796/0.22864. Took 0.05 sec\n",
            "Epoch 369, Loss(train/val) 0.36043/0.22896. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.36322/0.22708. Took 0.05 sec\n",
            "Epoch 371, Loss(train/val) 0.36941/0.22629. Took 0.06 sec\n",
            "Epoch 372, Loss(train/val) 0.37169/0.22736. Took 0.04 sec\n",
            "Epoch 373, Loss(train/val) 0.35633/0.22625. Took 0.05 sec\n",
            "Epoch 374, Loss(train/val) 0.36095/0.22675. Took 0.05 sec\n",
            "Epoch 375, Loss(train/val) 0.36060/0.22822. Took 0.04 sec\n",
            "Epoch 376, Loss(train/val) 0.35497/0.22655. Took 0.06 sec\n",
            "Epoch 377, Loss(train/val) 0.36550/0.22650. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.35420/0.22596. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 0.35657/0.22619. Took 0.05 sec\n",
            "Epoch 380, Loss(train/val) 0.34964/0.22623. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 0.36199/0.22526. Took 0.06 sec\n",
            "Epoch 382, Loss(train/val) 0.35337/0.22505. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.35963/0.22509. Took 0.05 sec\n",
            "Epoch 384, Loss(train/val) 0.35212/0.22492. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 0.34860/0.22592. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.34797/0.22909. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.35184/0.22904. Took 0.05 sec\n",
            "Epoch 388, Loss(train/val) 0.35781/0.22477. Took 0.05 sec\n",
            "Epoch 389, Loss(train/val) 0.35077/0.22832. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.34065/0.23328. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.36902/0.23684. Took 0.06 sec\n",
            "Epoch 392, Loss(train/val) 0.35007/0.22849. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.34408/0.22417. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 0.33885/0.22534. Took 0.05 sec\n",
            "Epoch 395, Loss(train/val) 0.36053/0.22471. Took 0.06 sec\n",
            "Epoch 396, Loss(train/val) 0.33772/0.22371. Took 0.06 sec\n",
            "Epoch 397, Loss(train/val) 0.33890/0.22324. Took 0.05 sec\n",
            "Epoch 398, Loss(train/val) 0.34249/0.22310. Took 0.06 sec\n",
            "Epoch 399, Loss(train/val) 0.34827/0.22276. Took 0.06 sec\n",
            "Epoch 400, Loss(train/val) 0.34022/0.22252. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 0.33979/0.22272. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.33606/0.22563. Took 0.05 sec\n",
            "Epoch 403, Loss(train/val) 0.34080/0.22173. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 0.35573/0.22297. Took 0.05 sec\n",
            "Epoch 405, Loss(train/val) 0.34266/0.22287. Took 0.04 sec\n",
            "Epoch 406, Loss(train/val) 0.34869/0.22418. Took 0.04 sec\n",
            "Epoch 407, Loss(train/val) 0.33623/0.22142. Took 0.06 sec\n",
            "Epoch 408, Loss(train/val) 0.34460/0.22406. Took 0.04 sec\n",
            "Epoch 409, Loss(train/val) 0.33919/0.22468. Took 0.05 sec\n",
            "Epoch 410, Loss(train/val) 0.33725/0.22787. Took 0.04 sec\n",
            "Epoch 411, Loss(train/val) 0.33898/0.22769. Took 0.04 sec\n",
            "Epoch 412, Loss(train/val) 0.33724/0.22087. Took 0.04 sec\n",
            "Epoch 413, Loss(train/val) 0.33559/0.22198. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.33883/0.22041. Took 0.05 sec\n",
            "Epoch 415, Loss(train/val) 0.33907/0.22046. Took 0.05 sec\n",
            "Epoch 416, Loss(train/val) 0.34608/0.22077. Took 0.04 sec\n",
            "Epoch 417, Loss(train/val) 0.33935/0.21977. Took 0.05 sec\n",
            "Epoch 418, Loss(train/val) 0.34222/0.21992. Took 0.04 sec\n",
            "Epoch 419, Loss(train/val) 0.34310/0.22010. Took 0.05 sec\n",
            "Epoch 420, Loss(train/val) 0.33602/0.21903. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.33609/0.21895. Took 0.04 sec\n",
            "Epoch 422, Loss(train/val) 0.33475/0.21871. Took 0.04 sec\n",
            "Epoch 423, Loss(train/val) 0.33481/0.21884. Took 0.04 sec\n",
            "Epoch 424, Loss(train/val) 0.33493/0.22041. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.32623/0.22008. Took 0.04 sec\n",
            "Epoch 426, Loss(train/val) 0.32681/0.21971. Took 0.04 sec\n",
            "Epoch 427, Loss(train/val) 0.32773/0.21990. Took 0.04 sec\n",
            "Epoch 428, Loss(train/val) 0.33253/0.21812. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.32180/0.21903. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.32798/0.22606. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.32328/0.22747. Took 0.04 sec\n",
            "Epoch 432, Loss(train/val) 0.34608/0.22198. Took 0.04 sec\n",
            "Epoch 433, Loss(train/val) 0.32512/0.21809. Took 0.04 sec\n",
            "Epoch 434, Loss(train/val) 0.32467/0.21715. Took 0.05 sec\n",
            "Epoch 435, Loss(train/val) 0.32907/0.21655. Took 0.05 sec\n",
            "Epoch 436, Loss(train/val) 0.33059/0.21619. Took 0.04 sec\n",
            "Epoch 437, Loss(train/val) 0.32939/0.21652. Took 0.04 sec\n",
            "Epoch 438, Loss(train/val) 0.33248/0.21706. Took 0.04 sec\n",
            "Epoch 439, Loss(train/val) 0.34387/0.21691. Took 0.06 sec\n",
            "Epoch 440, Loss(train/val) 0.33900/0.21417. Took 0.05 sec\n",
            "Epoch 441, Loss(train/val) 0.31934/0.21512. Took 0.05 sec\n",
            "Epoch 442, Loss(train/val) 0.32255/0.21626. Took 0.04 sec\n",
            "Epoch 443, Loss(train/val) 0.34042/0.21414. Took 0.04 sec\n",
            "Epoch 444, Loss(train/val) 0.31967/0.21394. Took 0.05 sec\n",
            "Epoch 445, Loss(train/val) 0.32736/0.21378. Took 0.05 sec\n",
            "Epoch 446, Loss(train/val) 0.33027/0.21572. Took 0.04 sec\n",
            "Epoch 447, Loss(train/val) 0.32864/0.22531. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 0.34721/0.22032. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 0.32856/0.21322. Took 0.05 sec\n",
            "Epoch 450, Loss(train/val) 0.33245/0.21362. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 0.32500/0.21311. Took 0.05 sec\n",
            "Epoch 452, Loss(train/val) 0.32106/0.21280. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.31535/0.21280. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.32189/0.21269. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 0.33117/0.21192. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.31364/0.21179. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.32107/0.21344. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.32298/0.21159. Took 0.04 sec\n",
            "Epoch 459, Loss(train/val) 0.31889/0.20962. Took 0.05 sec\n",
            "Epoch 460, Loss(train/val) 0.30859/0.20937. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.32490/0.20940. Took 0.05 sec\n",
            "Epoch 462, Loss(train/val) 0.31778/0.21137. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.33914/0.21498. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.31528/0.21548. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.31904/0.21389. Took 0.05 sec\n",
            "Epoch 466, Loss(train/val) 0.32974/0.21024. Took 0.05 sec\n",
            "Epoch 467, Loss(train/val) 0.31441/0.20982. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.30884/0.20989. Took 0.04 sec\n",
            "Epoch 469, Loss(train/val) 0.31082/0.20845. Took 0.05 sec\n",
            "Epoch 470, Loss(train/val) 0.31164/0.21311. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 0.32860/0.21322. Took 0.05 sec\n",
            "Epoch 472, Loss(train/val) 0.31896/0.20866. Took 0.05 sec\n",
            "Epoch 473, Loss(train/val) 0.30783/0.20437. Took 0.04 sec\n",
            "Epoch 474, Loss(train/val) 0.31558/0.20776. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 0.30966/0.21257. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.31145/0.22097. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.30699/0.22639. Took 0.04 sec\n",
            "Epoch 478, Loss(train/val) 0.30721/0.21399. Took 0.05 sec\n",
            "Epoch 479, Loss(train/val) 0.30370/0.20890. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.30991/0.20360. Took 0.05 sec\n",
            "Epoch 481, Loss(train/val) 0.30459/0.20091. Took 0.05 sec\n",
            "Epoch 482, Loss(train/val) 0.31369/0.20054. Took 0.04 sec\n",
            "Epoch 483, Loss(train/val) 0.30507/0.19849. Took 0.04 sec\n",
            "Epoch 484, Loss(train/val) 0.30365/0.19750. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.29982/0.19746. Took 0.04 sec\n",
            "Epoch 486, Loss(train/val) 0.31200/0.20142. Took 0.04 sec\n",
            "Epoch 487, Loss(train/val) 0.32667/0.21780. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.29525/0.24462. Took 0.04 sec\n",
            "Epoch 489, Loss(train/val) 0.29927/0.21993. Took 0.05 sec\n",
            "Epoch 490, Loss(train/val) 0.31324/0.21110. Took 0.04 sec\n",
            "Epoch 491, Loss(train/val) 0.29283/0.19505. Took 0.04 sec\n",
            "Epoch 492, Loss(train/val) 0.30282/0.19433. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.30333/0.20497. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.29672/0.21420. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.29975/0.22910. Took 0.05 sec\n",
            "Epoch 496, Loss(train/val) 0.30328/0.22531. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 0.29193/0.21060. Took 0.04 sec\n",
            "Epoch 498, Loss(train/val) 0.29355/0.19230. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.29878/0.19158. Took 0.05 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=500, exp_name='exp2_hidn', hid_dim=32, input_dim=1, l2=1e-05, lr=0.0001, n_layers=2, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.03236/0.39438. Took 0.04 sec\n",
            "Epoch 1, Loss(train/val) 1.00705/0.39408. Took 0.03 sec\n",
            "Epoch 2, Loss(train/val) 0.99491/0.39379. Took 0.04 sec\n",
            "Epoch 3, Loss(train/val) 0.98111/0.39354. Took 0.04 sec\n",
            "Epoch 4, Loss(train/val) 0.96583/0.39326. Took 0.04 sec\n",
            "Epoch 5, Loss(train/val) 0.95552/0.39297. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 0.93803/0.39271. Took 0.04 sec\n",
            "Epoch 7, Loss(train/val) 0.91184/0.39241. Took 0.03 sec\n",
            "Epoch 8, Loss(train/val) 0.90284/0.39208. Took 0.03 sec\n",
            "Epoch 9, Loss(train/val) 0.89256/0.39165. Took 0.03 sec\n",
            "Epoch 10, Loss(train/val) 0.89375/0.39112. Took 0.04 sec\n",
            "Epoch 11, Loss(train/val) 0.87584/0.39042. Took 0.05 sec\n",
            "Epoch 12, Loss(train/val) 0.86097/0.38959. Took 0.04 sec\n",
            "Epoch 13, Loss(train/val) 0.85803/0.38867. Took 0.04 sec\n",
            "Epoch 14, Loss(train/val) 0.84993/0.38764. Took 0.04 sec\n",
            "Epoch 15, Loss(train/val) 0.84390/0.38651. Took 0.04 sec\n",
            "Epoch 16, Loss(train/val) 0.83480/0.38525. Took 0.04 sec\n",
            "Epoch 17, Loss(train/val) 0.82233/0.38385. Took 0.04 sec\n",
            "Epoch 18, Loss(train/val) 0.81520/0.38223. Took 0.04 sec\n",
            "Epoch 19, Loss(train/val) 0.79804/0.38036. Took 0.04 sec\n",
            "Epoch 20, Loss(train/val) 0.79611/0.37784. Took 0.03 sec\n",
            "Epoch 21, Loss(train/val) 0.78598/0.37436. Took 0.04 sec\n",
            "Epoch 22, Loss(train/val) 0.76903/0.36997. Took 0.03 sec\n",
            "Epoch 23, Loss(train/val) 0.76362/0.36466. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 0.76787/0.35850. Took 0.04 sec\n",
            "Epoch 25, Loss(train/val) 0.75892/0.35158. Took 0.03 sec\n",
            "Epoch 26, Loss(train/val) 0.75502/0.34372. Took 0.04 sec\n",
            "Epoch 27, Loss(train/val) 0.74903/0.33535. Took 0.03 sec\n",
            "Epoch 28, Loss(train/val) 0.74307/0.32716. Took 0.04 sec\n",
            "Epoch 29, Loss(train/val) 0.72489/0.31922. Took 0.04 sec\n",
            "Epoch 30, Loss(train/val) 0.72120/0.31186. Took 0.04 sec\n",
            "Epoch 31, Loss(train/val) 0.71619/0.30557. Took 0.04 sec\n",
            "Epoch 32, Loss(train/val) 0.71303/0.30015. Took 0.04 sec\n",
            "Epoch 33, Loss(train/val) 0.70926/0.29576. Took 0.04 sec\n",
            "Epoch 34, Loss(train/val) 0.70339/0.29242. Took 0.04 sec\n",
            "Epoch 35, Loss(train/val) 0.68856/0.28946. Took 0.04 sec\n",
            "Epoch 36, Loss(train/val) 0.69399/0.28729. Took 0.04 sec\n",
            "Epoch 37, Loss(train/val) 0.67936/0.28520. Took 0.04 sec\n",
            "Epoch 38, Loss(train/val) 0.68059/0.28367. Took 0.04 sec\n",
            "Epoch 39, Loss(train/val) 0.67665/0.28225. Took 0.04 sec\n",
            "Epoch 40, Loss(train/val) 0.66717/0.28072. Took 0.04 sec\n",
            "Epoch 41, Loss(train/val) 0.66112/0.27973. Took 0.04 sec\n",
            "Epoch 42, Loss(train/val) 0.64907/0.27853. Took 0.04 sec\n",
            "Epoch 43, Loss(train/val) 0.64051/0.27713. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 0.64300/0.27618. Took 0.04 sec\n",
            "Epoch 45, Loss(train/val) 0.64015/0.27531. Took 0.04 sec\n",
            "Epoch 46, Loss(train/val) 0.62909/0.27438. Took 0.04 sec\n",
            "Epoch 47, Loss(train/val) 0.62898/0.27331. Took 0.04 sec\n",
            "Epoch 48, Loss(train/val) 0.61920/0.27202. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 0.61138/0.27099. Took 0.04 sec\n",
            "Epoch 50, Loss(train/val) 0.60870/0.27000. Took 0.04 sec\n",
            "Epoch 51, Loss(train/val) 0.59472/0.26861. Took 0.04 sec\n",
            "Epoch 52, Loss(train/val) 0.59476/0.26775. Took 0.04 sec\n",
            "Epoch 53, Loss(train/val) 0.58914/0.26660. Took 0.04 sec\n",
            "Epoch 54, Loss(train/val) 0.58077/0.26560. Took 0.04 sec\n",
            "Epoch 55, Loss(train/val) 0.57641/0.26473. Took 0.04 sec\n",
            "Epoch 56, Loss(train/val) 0.58036/0.26383. Took 0.04 sec\n",
            "Epoch 57, Loss(train/val) 0.56226/0.26276. Took 0.04 sec\n",
            "Epoch 58, Loss(train/val) 0.56772/0.26188. Took 0.04 sec\n",
            "Epoch 59, Loss(train/val) 0.54847/0.26095. Took 0.04 sec\n",
            "Epoch 60, Loss(train/val) 0.54357/0.25999. Took 0.04 sec\n",
            "Epoch 61, Loss(train/val) 0.53281/0.25909. Took 0.04 sec\n",
            "Epoch 62, Loss(train/val) 0.52555/0.25807. Took 0.04 sec\n",
            "Epoch 63, Loss(train/val) 0.53015/0.25718. Took 0.04 sec\n",
            "Epoch 64, Loss(train/val) 0.51893/0.25638. Took 0.04 sec\n",
            "Epoch 65, Loss(train/val) 0.51714/0.25567. Took 0.04 sec\n",
            "Epoch 66, Loss(train/val) 0.51163/0.25493. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 0.50658/0.25433. Took 0.04 sec\n",
            "Epoch 68, Loss(train/val) 0.49368/0.25366. Took 0.04 sec\n",
            "Epoch 69, Loss(train/val) 0.48625/0.25287. Took 0.04 sec\n",
            "Epoch 70, Loss(train/val) 0.48848/0.25233. Took 0.04 sec\n",
            "Epoch 71, Loss(train/val) 0.49445/0.25155. Took 0.04 sec\n",
            "Epoch 72, Loss(train/val) 0.47998/0.25098. Took 0.04 sec\n",
            "Epoch 73, Loss(train/val) 0.46430/0.25021. Took 0.04 sec\n",
            "Epoch 74, Loss(train/val) 0.45789/0.24956. Took 0.04 sec\n",
            "Epoch 75, Loss(train/val) 0.45843/0.24901. Took 0.04 sec\n",
            "Epoch 76, Loss(train/val) 0.45080/0.24816. Took 0.04 sec\n",
            "Epoch 77, Loss(train/val) 0.44389/0.24728. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 0.43188/0.24653. Took 0.04 sec\n",
            "Epoch 79, Loss(train/val) 0.41750/0.24577. Took 0.04 sec\n",
            "Epoch 80, Loss(train/val) 0.42549/0.24492. Took 0.04 sec\n",
            "Epoch 81, Loss(train/val) 0.41786/0.24365. Took 0.04 sec\n",
            "Epoch 82, Loss(train/val) 0.41620/0.24268. Took 0.04 sec\n",
            "Epoch 83, Loss(train/val) 0.41258/0.24209. Took 0.04 sec\n",
            "Epoch 84, Loss(train/val) 0.40383/0.24098. Took 0.04 sec\n",
            "Epoch 85, Loss(train/val) 0.41298/0.24033. Took 0.04 sec\n",
            "Epoch 86, Loss(train/val) 0.39325/0.23895. Took 0.04 sec\n",
            "Epoch 87, Loss(train/val) 0.39010/0.23801. Took 0.04 sec\n",
            "Epoch 88, Loss(train/val) 0.38740/0.23725. Took 0.04 sec\n",
            "Epoch 89, Loss(train/val) 0.38948/0.23661. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 0.37632/0.23546. Took 0.04 sec\n",
            "Epoch 91, Loss(train/val) 0.36734/0.23472. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 0.36127/0.23371. Took 0.04 sec\n",
            "Epoch 93, Loss(train/val) 0.37463/0.23285. Took 0.04 sec\n",
            "Epoch 94, Loss(train/val) 0.35900/0.23140. Took 0.04 sec\n",
            "Epoch 95, Loss(train/val) 0.36092/0.23032. Took 0.04 sec\n",
            "Epoch 96, Loss(train/val) 0.34761/0.22912. Took 0.04 sec\n",
            "Epoch 97, Loss(train/val) 0.34708/0.22793. Took 0.04 sec\n",
            "Epoch 98, Loss(train/val) 0.34133/0.22635. Took 0.04 sec\n",
            "Epoch 99, Loss(train/val) 0.33176/0.22512. Took 0.03 sec\n",
            "Epoch 100, Loss(train/val) 0.34360/0.22420. Took 0.03 sec\n",
            "Epoch 101, Loss(train/val) 0.33758/0.22309. Took 0.04 sec\n",
            "Epoch 102, Loss(train/val) 0.33613/0.22228. Took 0.04 sec\n",
            "Epoch 103, Loss(train/val) 0.33749/0.22161. Took 0.04 sec\n",
            "Epoch 104, Loss(train/val) 0.32194/0.22104. Took 0.03 sec\n",
            "Epoch 105, Loss(train/val) 0.33041/0.22081. Took 0.04 sec\n",
            "Epoch 106, Loss(train/val) 0.31755/0.22045. Took 0.04 sec\n",
            "Epoch 107, Loss(train/val) 0.32374/0.21949. Took 0.04 sec\n",
            "Epoch 108, Loss(train/val) 0.32214/0.21904. Took 0.04 sec\n",
            "Epoch 109, Loss(train/val) 0.32581/0.21794. Took 0.04 sec\n",
            "Epoch 110, Loss(train/val) 0.31548/0.21743. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 0.31786/0.21643. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 0.30676/0.21581. Took 0.04 sec\n",
            "Epoch 113, Loss(train/val) 0.31056/0.21502. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 0.30858/0.21451. Took 0.04 sec\n",
            "Epoch 115, Loss(train/val) 0.31630/0.21372. Took 0.04 sec\n",
            "Epoch 116, Loss(train/val) 0.31003/0.21315. Took 0.04 sec\n",
            "Epoch 117, Loss(train/val) 0.30481/0.21243. Took 0.04 sec\n",
            "Epoch 118, Loss(train/val) 0.32527/0.21153. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 0.30422/0.21084. Took 0.04 sec\n",
            "Epoch 120, Loss(train/val) 0.30456/0.21004. Took 0.04 sec\n",
            "Epoch 121, Loss(train/val) 0.30934/0.20942. Took 0.04 sec\n",
            "Epoch 122, Loss(train/val) 0.31439/0.20864. Took 0.04 sec\n",
            "Epoch 123, Loss(train/val) 0.30289/0.20793. Took 0.04 sec\n",
            "Epoch 124, Loss(train/val) 0.29751/0.20748. Took 0.04 sec\n",
            "Epoch 125, Loss(train/val) 0.29764/0.20689. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 0.30424/0.20630. Took 0.04 sec\n",
            "Epoch 127, Loss(train/val) 0.30538/0.20566. Took 0.04 sec\n",
            "Epoch 128, Loss(train/val) 0.30020/0.20510. Took 0.04 sec\n",
            "Epoch 129, Loss(train/val) 0.29512/0.20465. Took 0.04 sec\n",
            "Epoch 130, Loss(train/val) 0.30803/0.20397. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 0.29855/0.20378. Took 0.04 sec\n",
            "Epoch 132, Loss(train/val) 0.29259/0.20238. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 0.29016/0.20144. Took 0.04 sec\n",
            "Epoch 134, Loss(train/val) 0.29338/0.20049. Took 0.04 sec\n",
            "Epoch 135, Loss(train/val) 0.29756/0.20041. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 0.28994/0.20023. Took 0.03 sec\n",
            "Epoch 137, Loss(train/val) 0.30034/0.19998. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 0.29012/0.19953. Took 0.04 sec\n",
            "Epoch 139, Loss(train/val) 0.29023/0.19948. Took 0.04 sec\n",
            "Epoch 140, Loss(train/val) 0.29726/0.19990. Took 0.04 sec\n",
            "Epoch 141, Loss(train/val) 0.28999/0.19943. Took 0.04 sec\n",
            "Epoch 142, Loss(train/val) 0.28797/0.19855. Took 0.04 sec\n",
            "Epoch 143, Loss(train/val) 0.28685/0.19843. Took 0.04 sec\n",
            "Epoch 144, Loss(train/val) 0.28369/0.19721. Took 0.04 sec\n",
            "Epoch 145, Loss(train/val) 0.28318/0.19648. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 0.29596/0.19729. Took 0.04 sec\n",
            "Epoch 147, Loss(train/val) 0.31091/0.19775. Took 0.04 sec\n",
            "Epoch 148, Loss(train/val) 0.28615/0.19650. Took 0.04 sec\n",
            "Epoch 149, Loss(train/val) 0.29702/0.19548. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 0.28368/0.19394. Took 0.04 sec\n",
            "Epoch 151, Loss(train/val) 0.31380/0.19147. Took 0.04 sec\n",
            "Epoch 152, Loss(train/val) 0.29179/0.19228. Took 0.04 sec\n",
            "Epoch 153, Loss(train/val) 0.27940/0.19399. Took 0.04 sec\n",
            "Epoch 154, Loss(train/val) 0.28804/0.19484. Took 0.04 sec\n",
            "Epoch 155, Loss(train/val) 0.27580/0.19563. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 0.28439/0.19537. Took 0.04 sec\n",
            "Epoch 157, Loss(train/val) 0.27547/0.19489. Took 0.04 sec\n",
            "Epoch 158, Loss(train/val) 0.27852/0.19512. Took 0.04 sec\n",
            "Epoch 159, Loss(train/val) 0.28448/0.19560. Took 0.04 sec\n",
            "Epoch 160, Loss(train/val) 0.28526/0.19477. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 0.27358/0.19298. Took 0.04 sec\n",
            "Epoch 162, Loss(train/val) 0.29288/0.19361. Took 0.04 sec\n",
            "Epoch 163, Loss(train/val) 0.27623/0.19194. Took 0.04 sec\n",
            "Epoch 164, Loss(train/val) 0.28850/0.19241. Took 0.04 sec\n",
            "Epoch 165, Loss(train/val) 0.27143/0.19098. Took 0.04 sec\n",
            "Epoch 166, Loss(train/val) 0.28209/0.19012. Took 0.03 sec\n",
            "Epoch 167, Loss(train/val) 0.27844/0.18972. Took 0.04 sec\n",
            "Epoch 168, Loss(train/val) 0.28121/0.19041. Took 0.03 sec\n",
            "Epoch 169, Loss(train/val) 0.27724/0.19039. Took 0.03 sec\n",
            "Epoch 170, Loss(train/val) 0.28427/0.19070. Took 0.04 sec\n",
            "Epoch 171, Loss(train/val) 0.27299/0.18949. Took 0.04 sec\n",
            "Epoch 172, Loss(train/val) 0.27721/0.18872. Took 0.04 sec\n",
            "Epoch 173, Loss(train/val) 0.27163/0.18884. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 0.28011/0.18936. Took 0.04 sec\n",
            "Epoch 175, Loss(train/val) 0.26572/0.18865. Took 0.04 sec\n",
            "Epoch 176, Loss(train/val) 0.27257/0.18748. Took 0.04 sec\n",
            "Epoch 177, Loss(train/val) 0.27557/0.18629. Took 0.04 sec\n",
            "Epoch 178, Loss(train/val) 0.26680/0.18622. Took 0.04 sec\n",
            "Epoch 179, Loss(train/val) 0.26461/0.18676. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 0.27743/0.18577. Took 0.04 sec\n",
            "Epoch 181, Loss(train/val) 0.27149/0.18550. Took 0.04 sec\n",
            "Epoch 182, Loss(train/val) 0.27363/0.18520. Took 0.04 sec\n",
            "Epoch 183, Loss(train/val) 0.27362/0.18762. Took 0.04 sec\n",
            "Epoch 184, Loss(train/val) 0.26828/0.18811. Took 0.03 sec\n",
            "Epoch 185, Loss(train/val) 0.27003/0.18803. Took 0.04 sec\n",
            "Epoch 186, Loss(train/val) 0.26980/0.18662. Took 0.04 sec\n",
            "Epoch 187, Loss(train/val) 0.26672/0.18517. Took 0.04 sec\n",
            "Epoch 188, Loss(train/val) 0.26631/0.18735. Took 0.04 sec\n",
            "Epoch 189, Loss(train/val) 0.27048/0.18620. Took 0.04 sec\n",
            "Epoch 190, Loss(train/val) 0.27823/0.18543. Took 0.04 sec\n",
            "Epoch 191, Loss(train/val) 0.26591/0.18411. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 0.28270/0.18401. Took 0.04 sec\n",
            "Epoch 193, Loss(train/val) 0.27326/0.18381. Took 0.04 sec\n",
            "Epoch 194, Loss(train/val) 0.26627/0.18546. Took 0.04 sec\n",
            "Epoch 195, Loss(train/val) 0.27184/0.18569. Took 0.04 sec\n",
            "Epoch 196, Loss(train/val) 0.28081/0.18430. Took 0.04 sec\n",
            "Epoch 197, Loss(train/val) 0.26846/0.18394. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 0.27199/0.18317. Took 0.04 sec\n",
            "Epoch 199, Loss(train/val) 0.26595/0.18362. Took 0.03 sec\n",
            "Epoch 200, Loss(train/val) 0.27138/0.18263. Took 0.04 sec\n",
            "Epoch 201, Loss(train/val) 0.26537/0.18272. Took 0.04 sec\n",
            "Epoch 202, Loss(train/val) 0.26221/0.18458. Took 0.04 sec\n",
            "Epoch 203, Loss(train/val) 0.26574/0.18418. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 0.26768/0.18492. Took 0.04 sec\n",
            "Epoch 205, Loss(train/val) 0.25857/0.18414. Took 0.04 sec\n",
            "Epoch 206, Loss(train/val) 0.26904/0.18215. Took 0.04 sec\n",
            "Epoch 207, Loss(train/val) 0.25941/0.18171. Took 0.04 sec\n",
            "Epoch 208, Loss(train/val) 0.26747/0.18073. Took 0.04 sec\n",
            "Epoch 209, Loss(train/val) 0.26571/0.18186. Took 0.04 sec\n",
            "Epoch 210, Loss(train/val) 0.28153/0.18214. Took 0.04 sec\n",
            "Epoch 211, Loss(train/val) 0.26979/0.18067. Took 0.04 sec\n",
            "Epoch 212, Loss(train/val) 0.26632/0.18284. Took 0.04 sec\n",
            "Epoch 213, Loss(train/val) 0.26942/0.18264. Took 0.04 sec\n",
            "Epoch 214, Loss(train/val) 0.26103/0.18117. Took 0.04 sec\n",
            "Epoch 215, Loss(train/val) 0.26647/0.17879. Took 0.04 sec\n",
            "Epoch 216, Loss(train/val) 0.26184/0.17890. Took 0.04 sec\n",
            "Epoch 217, Loss(train/val) 0.28558/0.17827. Took 0.04 sec\n",
            "Epoch 218, Loss(train/val) 0.26249/0.17963. Took 0.04 sec\n",
            "Epoch 219, Loss(train/val) 0.26013/0.18042. Took 0.03 sec\n",
            "Epoch 220, Loss(train/val) 0.26808/0.17957. Took 0.04 sec\n",
            "Epoch 221, Loss(train/val) 0.26313/0.17977. Took 0.04 sec\n",
            "Epoch 222, Loss(train/val) 0.26797/0.17832. Took 0.04 sec\n",
            "Epoch 223, Loss(train/val) 0.26658/0.17749. Took 0.04 sec\n",
            "Epoch 224, Loss(train/val) 0.26507/0.17849. Took 0.03 sec\n",
            "Epoch 225, Loss(train/val) 0.26378/0.17931. Took 0.03 sec\n",
            "Epoch 226, Loss(train/val) 0.26561/0.17932. Took 0.04 sec\n",
            "Epoch 227, Loss(train/val) 0.26404/0.17989. Took 0.04 sec\n",
            "Epoch 228, Loss(train/val) 0.26219/0.17872. Took 0.04 sec\n",
            "Epoch 229, Loss(train/val) 0.25634/0.17669. Took 0.04 sec\n",
            "Epoch 230, Loss(train/val) 0.26868/0.17535. Took 0.04 sec\n",
            "Epoch 231, Loss(train/val) 0.27697/0.17485. Took 0.04 sec\n",
            "Epoch 232, Loss(train/val) 0.26793/0.17548. Took 0.04 sec\n",
            "Epoch 233, Loss(train/val) 0.25717/0.17744. Took 0.04 sec\n",
            "Epoch 234, Loss(train/val) 0.25998/0.17918. Took 0.04 sec\n",
            "Epoch 235, Loss(train/val) 0.25811/0.17918. Took 0.03 sec\n",
            "Epoch 236, Loss(train/val) 0.26757/0.17711. Took 0.03 sec\n",
            "Epoch 237, Loss(train/val) 0.26186/0.17662. Took 0.03 sec\n",
            "Epoch 238, Loss(train/val) 0.27333/0.17597. Took 0.03 sec\n",
            "Epoch 239, Loss(train/val) 0.26613/0.17598. Took 0.04 sec\n",
            "Epoch 240, Loss(train/val) 0.26910/0.17518. Took 0.04 sec\n",
            "Epoch 241, Loss(train/val) 0.28299/0.17664. Took 0.03 sec\n",
            "Epoch 242, Loss(train/val) 0.24940/0.17581. Took 0.04 sec\n",
            "Epoch 243, Loss(train/val) 0.25659/0.17521. Took 0.04 sec\n",
            "Epoch 244, Loss(train/val) 0.26362/0.17303. Took 0.04 sec\n",
            "Epoch 245, Loss(train/val) 0.27225/0.17414. Took 0.06 sec\n",
            "Epoch 246, Loss(train/val) 0.25875/0.17460. Took 0.04 sec\n",
            "Epoch 247, Loss(train/val) 0.26070/0.17413. Took 0.04 sec\n",
            "Epoch 248, Loss(train/val) 0.27259/0.17441. Took 0.04 sec\n",
            "Epoch 249, Loss(train/val) 0.25621/0.17411. Took 0.04 sec\n",
            "Epoch 250, Loss(train/val) 0.25646/0.17338. Took 0.04 sec\n",
            "Epoch 251, Loss(train/val) 0.26143/0.17309. Took 0.05 sec\n",
            "Epoch 252, Loss(train/val) 0.25719/0.17390. Took 0.04 sec\n",
            "Epoch 253, Loss(train/val) 0.25385/0.17522. Took 0.04 sec\n",
            "Epoch 254, Loss(train/val) 0.26183/0.17598. Took 0.04 sec\n",
            "Epoch 255, Loss(train/val) 0.25951/0.17555. Took 0.04 sec\n",
            "Epoch 256, Loss(train/val) 0.26919/0.17484. Took 0.04 sec\n",
            "Epoch 257, Loss(train/val) 0.26123/0.17401. Took 0.04 sec\n",
            "Epoch 258, Loss(train/val) 0.25401/0.17333. Took 0.04 sec\n",
            "Epoch 259, Loss(train/val) 0.25510/0.17249. Took 0.04 sec\n",
            "Epoch 260, Loss(train/val) 0.25258/0.17257. Took 0.04 sec\n",
            "Epoch 261, Loss(train/val) 0.26046/0.17390. Took 0.04 sec\n",
            "Epoch 262, Loss(train/val) 0.25674/0.17494. Took 0.04 sec\n",
            "Epoch 263, Loss(train/val) 0.26452/0.17386. Took 0.04 sec\n",
            "Epoch 264, Loss(train/val) 0.25888/0.17296. Took 0.03 sec\n",
            "Epoch 265, Loss(train/val) 0.26037/0.17240. Took 0.03 sec\n",
            "Epoch 266, Loss(train/val) 0.27426/0.17163. Took 0.04 sec\n",
            "Epoch 267, Loss(train/val) 0.25356/0.17267. Took 0.03 sec\n",
            "Epoch 268, Loss(train/val) 0.25615/0.17277. Took 0.03 sec\n",
            "Epoch 269, Loss(train/val) 0.25123/0.17288. Took 0.04 sec\n",
            "Epoch 270, Loss(train/val) 0.25465/0.17248. Took 0.03 sec\n",
            "Epoch 271, Loss(train/val) 0.25348/0.17188. Took 0.04 sec\n",
            "Epoch 272, Loss(train/val) 0.24946/0.17161. Took 0.04 sec\n",
            "Epoch 273, Loss(train/val) 0.25423/0.17178. Took 0.04 sec\n",
            "Epoch 274, Loss(train/val) 0.25146/0.17175. Took 0.03 sec\n",
            "Epoch 275, Loss(train/val) 0.25690/0.17229. Took 0.04 sec\n",
            "Epoch 276, Loss(train/val) 0.25258/0.17289. Took 0.04 sec\n",
            "Epoch 277, Loss(train/val) 0.26196/0.17189. Took 0.03 sec\n",
            "Epoch 278, Loss(train/val) 0.26015/0.17245. Took 0.04 sec\n",
            "Epoch 279, Loss(train/val) 0.26757/0.17129. Took 0.03 sec\n",
            "Epoch 280, Loss(train/val) 0.25856/0.17162. Took 0.04 sec\n",
            "Epoch 281, Loss(train/val) 0.26034/0.17091. Took 0.04 sec\n",
            "Epoch 282, Loss(train/val) 0.25407/0.17125. Took 0.04 sec\n",
            "Epoch 283, Loss(train/val) 0.25949/0.17041. Took 0.04 sec\n",
            "Epoch 284, Loss(train/val) 0.27628/0.16970. Took 0.03 sec\n",
            "Epoch 285, Loss(train/val) 0.25198/0.17031. Took 0.03 sec\n",
            "Epoch 286, Loss(train/val) 0.25197/0.17058. Took 0.04 sec\n",
            "Epoch 287, Loss(train/val) 0.25017/0.17079. Took 0.04 sec\n",
            "Epoch 288, Loss(train/val) 0.24760/0.17050. Took 0.04 sec\n",
            "Epoch 289, Loss(train/val) 0.25740/0.16991. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.25534/0.17073. Took 0.03 sec\n",
            "Epoch 291, Loss(train/val) 0.26579/0.17068. Took 0.03 sec\n",
            "Epoch 292, Loss(train/val) 0.26379/0.17090. Took 0.03 sec\n",
            "Epoch 293, Loss(train/val) 0.25599/0.17060. Took 0.04 sec\n",
            "Epoch 294, Loss(train/val) 0.25614/0.17116. Took 0.04 sec\n",
            "Epoch 295, Loss(train/val) 0.25108/0.16974. Took 0.03 sec\n",
            "Epoch 296, Loss(train/val) 0.24908/0.16956. Took 0.03 sec\n",
            "Epoch 297, Loss(train/val) 0.25291/0.16997. Took 0.04 sec\n",
            "Epoch 298, Loss(train/val) 0.25219/0.17042. Took 0.04 sec\n",
            "Epoch 299, Loss(train/val) 0.26940/0.16933. Took 0.04 sec\n",
            "Epoch 300, Loss(train/val) 0.25141/0.16991. Took 0.04 sec\n",
            "Epoch 301, Loss(train/val) 0.25979/0.17004. Took 0.04 sec\n",
            "Epoch 302, Loss(train/val) 0.25055/0.17091. Took 0.04 sec\n",
            "Epoch 303, Loss(train/val) 0.25225/0.17060. Took 0.04 sec\n",
            "Epoch 304, Loss(train/val) 0.24792/0.17020. Took 0.03 sec\n",
            "Epoch 305, Loss(train/val) 0.27055/0.17014. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 0.25944/0.16968. Took 0.04 sec\n",
            "Epoch 307, Loss(train/val) 0.25167/0.16886. Took 0.04 sec\n",
            "Epoch 308, Loss(train/val) 0.24916/0.16861. Took 0.04 sec\n",
            "Epoch 309, Loss(train/val) 0.27236/0.17036. Took 0.03 sec\n",
            "Epoch 310, Loss(train/val) 0.25833/0.17034. Took 0.03 sec\n",
            "Epoch 311, Loss(train/val) 0.25461/0.16964. Took 0.04 sec\n",
            "Epoch 312, Loss(train/val) 0.25338/0.16938. Took 0.03 sec\n",
            "Epoch 313, Loss(train/val) 0.25271/0.16910. Took 0.04 sec\n",
            "Epoch 314, Loss(train/val) 0.25376/0.16959. Took 0.04 sec\n",
            "Epoch 315, Loss(train/val) 0.25400/0.16992. Took 0.04 sec\n",
            "Epoch 316, Loss(train/val) 0.24087/0.16984. Took 0.04 sec\n",
            "Epoch 317, Loss(train/val) 0.24957/0.16982. Took 0.04 sec\n",
            "Epoch 318, Loss(train/val) 0.24999/0.16941. Took 0.04 sec\n",
            "Epoch 319, Loss(train/val) 0.25166/0.16966. Took 0.04 sec\n",
            "Epoch 320, Loss(train/val) 0.25722/0.16861. Took 0.03 sec\n",
            "Epoch 321, Loss(train/val) 0.25335/0.16835. Took 0.03 sec\n",
            "Epoch 322, Loss(train/val) 0.26293/0.16799. Took 0.03 sec\n",
            "Epoch 323, Loss(train/val) 0.24958/0.16799. Took 0.04 sec\n",
            "Epoch 324, Loss(train/val) 0.24980/0.16875. Took 0.04 sec\n",
            "Epoch 325, Loss(train/val) 0.26086/0.16993. Took 0.04 sec\n",
            "Epoch 326, Loss(train/val) 0.25409/0.17002. Took 0.04 sec\n",
            "Epoch 327, Loss(train/val) 0.24846/0.16932. Took 0.03 sec\n",
            "Epoch 328, Loss(train/val) 0.25190/0.16878. Took 0.04 sec\n",
            "Epoch 329, Loss(train/val) 0.25283/0.16932. Took 0.04 sec\n",
            "Epoch 330, Loss(train/val) 0.27575/0.16952. Took 0.04 sec\n",
            "Epoch 331, Loss(train/val) 0.25475/0.16812. Took 0.03 sec\n",
            "Epoch 332, Loss(train/val) 0.26052/0.16750. Took 0.04 sec\n",
            "Epoch 333, Loss(train/val) 0.24828/0.16743. Took 0.04 sec\n",
            "Epoch 334, Loss(train/val) 0.25820/0.16869. Took 0.04 sec\n",
            "Epoch 335, Loss(train/val) 0.24976/0.16936. Took 0.04 sec\n",
            "Epoch 336, Loss(train/val) 0.25120/0.16959. Took 0.04 sec\n",
            "Epoch 337, Loss(train/val) 0.25614/0.16918. Took 0.03 sec\n",
            "Epoch 338, Loss(train/val) 0.26290/0.16778. Took 0.04 sec\n",
            "Epoch 339, Loss(train/val) 0.25193/0.16710. Took 0.04 sec\n",
            "Epoch 340, Loss(train/val) 0.25032/0.16729. Took 0.03 sec\n",
            "Epoch 341, Loss(train/val) 0.25128/0.16858. Took 0.04 sec\n",
            "Epoch 342, Loss(train/val) 0.25666/0.16990. Took 0.04 sec\n",
            "Epoch 343, Loss(train/val) 0.24970/0.16963. Took 0.03 sec\n",
            "Epoch 344, Loss(train/val) 0.25017/0.16917. Took 0.03 sec\n",
            "Epoch 345, Loss(train/val) 0.25881/0.16843. Took 0.04 sec\n",
            "Epoch 346, Loss(train/val) 0.26835/0.16710. Took 0.04 sec\n",
            "Epoch 347, Loss(train/val) 0.23888/0.16635. Took 0.04 sec\n",
            "Epoch 348, Loss(train/val) 0.24872/0.16710. Took 0.04 sec\n",
            "Epoch 349, Loss(train/val) 0.25199/0.16842. Took 0.03 sec\n",
            "Epoch 350, Loss(train/val) 0.25693/0.16904. Took 0.03 sec\n",
            "Epoch 351, Loss(train/val) 0.27271/0.16919. Took 0.04 sec\n",
            "Epoch 352, Loss(train/val) 0.25856/0.16826. Took 0.04 sec\n",
            "Epoch 353, Loss(train/val) 0.24670/0.16796. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 0.26076/0.16806. Took 0.04 sec\n",
            "Epoch 355, Loss(train/val) 0.25949/0.16762. Took 0.03 sec\n",
            "Epoch 356, Loss(train/val) 0.25031/0.16655. Took 0.04 sec\n",
            "Epoch 357, Loss(train/val) 0.25370/0.16654. Took 0.04 sec\n",
            "Epoch 358, Loss(train/val) 0.24523/0.16626. Took 0.04 sec\n",
            "Epoch 359, Loss(train/val) 0.24761/0.16707. Took 0.04 sec\n",
            "Epoch 360, Loss(train/val) 0.24583/0.16785. Took 0.04 sec\n",
            "Epoch 361, Loss(train/val) 0.24584/0.16828. Took 0.04 sec\n",
            "Epoch 362, Loss(train/val) 0.25635/0.16772. Took 0.03 sec\n",
            "Epoch 363, Loss(train/val) 0.24466/0.16804. Took 0.04 sec\n",
            "Epoch 364, Loss(train/val) 0.26474/0.16750. Took 0.04 sec\n",
            "Epoch 365, Loss(train/val) 0.24757/0.16727. Took 0.04 sec\n",
            "Epoch 366, Loss(train/val) 0.25950/0.16733. Took 0.04 sec\n",
            "Epoch 367, Loss(train/val) 0.24997/0.16696. Took 0.03 sec\n",
            "Epoch 368, Loss(train/val) 0.24170/0.16785. Took 0.04 sec\n",
            "Epoch 369, Loss(train/val) 0.26043/0.16788. Took 0.03 sec\n",
            "Epoch 370, Loss(train/val) 0.24888/0.16769. Took 0.03 sec\n",
            "Epoch 371, Loss(train/val) 0.24962/0.16711. Took 0.04 sec\n",
            "Epoch 372, Loss(train/val) 0.25633/0.16743. Took 0.04 sec\n",
            "Epoch 373, Loss(train/val) 0.25461/0.16647. Took 0.04 sec\n",
            "Epoch 374, Loss(train/val) 0.24414/0.16689. Took 0.04 sec\n",
            "Epoch 375, Loss(train/val) 0.24614/0.16676. Took 0.03 sec\n",
            "Epoch 376, Loss(train/val) 0.26192/0.16681. Took 0.04 sec\n",
            "Epoch 377, Loss(train/val) 0.24675/0.16710. Took 0.04 sec\n",
            "Epoch 378, Loss(train/val) 0.25457/0.16794. Took 0.04 sec\n",
            "Epoch 379, Loss(train/val) 0.25061/0.16832. Took 0.03 sec\n",
            "Epoch 380, Loss(train/val) 0.25299/0.16809. Took 0.04 sec\n",
            "Epoch 381, Loss(train/val) 0.25699/0.16750. Took 0.04 sec\n",
            "Epoch 382, Loss(train/val) 0.24500/0.16699. Took 0.04 sec\n",
            "Epoch 383, Loss(train/val) 0.25142/0.16659. Took 0.04 sec\n",
            "Epoch 384, Loss(train/val) 0.24126/0.16672. Took 0.03 sec\n",
            "Epoch 385, Loss(train/val) 0.25446/0.16724. Took 0.04 sec\n",
            "Epoch 386, Loss(train/val) 0.24781/0.16614. Took 0.03 sec\n",
            "Epoch 387, Loss(train/val) 0.27184/0.16575. Took 0.04 sec\n",
            "Epoch 388, Loss(train/val) 0.25099/0.16637. Took 0.04 sec\n",
            "Epoch 389, Loss(train/val) 0.24672/0.16757. Took 0.04 sec\n",
            "Epoch 390, Loss(train/val) 0.26500/0.16761. Took 0.04 sec\n",
            "Epoch 391, Loss(train/val) 0.23765/0.16747. Took 0.04 sec\n",
            "Epoch 392, Loss(train/val) 0.24590/0.16726. Took 0.04 sec\n",
            "Epoch 393, Loss(train/val) 0.24193/0.16686. Took 0.04 sec\n",
            "Epoch 394, Loss(train/val) 0.24504/0.16630. Took 0.04 sec\n",
            "Epoch 395, Loss(train/val) 0.23841/0.16612. Took 0.04 sec\n",
            "Epoch 396, Loss(train/val) 0.25601/0.16583. Took 0.04 sec\n",
            "Epoch 397, Loss(train/val) 0.24755/0.16662. Took 0.03 sec\n",
            "Epoch 398, Loss(train/val) 0.25817/0.16709. Took 0.04 sec\n",
            "Epoch 399, Loss(train/val) 0.24582/0.16710. Took 0.04 sec\n",
            "Epoch 400, Loss(train/val) 0.24949/0.16701. Took 0.04 sec\n",
            "Epoch 401, Loss(train/val) 0.24533/0.16615. Took 0.04 sec\n",
            "Epoch 402, Loss(train/val) 0.25554/0.16527. Took 0.04 sec\n",
            "Epoch 403, Loss(train/val) 0.24408/0.16556. Took 0.04 sec\n",
            "Epoch 404, Loss(train/val) 0.24761/0.16586. Took 0.04 sec\n",
            "Epoch 405, Loss(train/val) 0.25542/0.16677. Took 0.04 sec\n",
            "Epoch 406, Loss(train/val) 0.24610/0.16748. Took 0.04 sec\n",
            "Epoch 407, Loss(train/val) 0.25110/0.16726. Took 0.04 sec\n",
            "Epoch 408, Loss(train/val) 0.24565/0.16631. Took 0.04 sec\n",
            "Epoch 409, Loss(train/val) 0.24157/0.16616. Took 0.04 sec\n",
            "Epoch 410, Loss(train/val) 0.24887/0.16636. Took 0.04 sec\n",
            "Epoch 411, Loss(train/val) 0.24958/0.16703. Took 0.04 sec\n",
            "Epoch 412, Loss(train/val) 0.26054/0.16733. Took 0.04 sec\n",
            "Epoch 413, Loss(train/val) 0.24464/0.16637. Took 0.04 sec\n",
            "Epoch 414, Loss(train/val) 0.24754/0.16532. Took 0.04 sec\n",
            "Epoch 415, Loss(train/val) 0.24376/0.16575. Took 0.04 sec\n",
            "Epoch 416, Loss(train/val) 0.27616/0.16567. Took 0.03 sec\n",
            "Epoch 417, Loss(train/val) 0.26085/0.16644. Took 0.04 sec\n",
            "Epoch 418, Loss(train/val) 0.24032/0.16698. Took 0.04 sec\n",
            "Epoch 419, Loss(train/val) 0.25460/0.16641. Took 0.04 sec\n",
            "Epoch 420, Loss(train/val) 0.25189/0.16673. Took 0.04 sec\n",
            "Epoch 421, Loss(train/val) 0.24652/0.16674. Took 0.04 sec\n",
            "Epoch 422, Loss(train/val) 0.25444/0.16583. Took 0.03 sec\n",
            "Epoch 423, Loss(train/val) 0.25002/0.16531. Took 0.04 sec\n",
            "Epoch 424, Loss(train/val) 0.24510/0.16527. Took 0.04 sec\n",
            "Epoch 425, Loss(train/val) 0.25427/0.16597. Took 0.04 sec\n",
            "Epoch 426, Loss(train/val) 0.24715/0.16677. Took 0.04 sec\n",
            "Epoch 427, Loss(train/val) 0.25535/0.16652. Took 0.03 sec\n",
            "Epoch 428, Loss(train/val) 0.24408/0.16620. Took 0.03 sec\n",
            "Epoch 429, Loss(train/val) 0.26530/0.16564. Took 0.04 sec\n",
            "Epoch 430, Loss(train/val) 0.24803/0.16503. Took 0.04 sec\n",
            "Epoch 431, Loss(train/val) 0.24909/0.16528. Took 0.04 sec\n",
            "Epoch 432, Loss(train/val) 0.25488/0.16656. Took 0.04 sec\n",
            "Epoch 433, Loss(train/val) 0.24656/0.16659. Took 0.04 sec\n",
            "Epoch 434, Loss(train/val) 0.25505/0.16634. Took 0.04 sec\n",
            "Epoch 435, Loss(train/val) 0.25255/0.16570. Took 0.04 sec\n",
            "Epoch 436, Loss(train/val) 0.24695/0.16655. Took 0.04 sec\n",
            "Epoch 437, Loss(train/val) 0.25704/0.16575. Took 0.04 sec\n",
            "Epoch 438, Loss(train/val) 0.24697/0.16638. Took 0.04 sec\n",
            "Epoch 439, Loss(train/val) 0.25101/0.16638. Took 0.04 sec\n",
            "Epoch 440, Loss(train/val) 0.24369/0.16528. Took 0.04 sec\n",
            "Epoch 441, Loss(train/val) 0.25245/0.16406. Took 0.04 sec\n",
            "Epoch 442, Loss(train/val) 0.24742/0.16468. Took 0.04 sec\n",
            "Epoch 443, Loss(train/val) 0.25229/0.16605. Took 0.04 sec\n",
            "Epoch 444, Loss(train/val) 0.25141/0.16593. Took 0.04 sec\n",
            "Epoch 445, Loss(train/val) 0.24056/0.16601. Took 0.04 sec\n",
            "Epoch 446, Loss(train/val) 0.24262/0.16550. Took 0.04 sec\n",
            "Epoch 447, Loss(train/val) 0.24043/0.16580. Took 0.03 sec\n",
            "Epoch 448, Loss(train/val) 0.25314/0.16540. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 0.24120/0.16538. Took 0.04 sec\n",
            "Epoch 450, Loss(train/val) 0.24402/0.16594. Took 0.04 sec\n",
            "Epoch 451, Loss(train/val) 0.25233/0.16604. Took 0.04 sec\n",
            "Epoch 452, Loss(train/val) 0.24208/0.16600. Took 0.04 sec\n",
            "Epoch 453, Loss(train/val) 0.24486/0.16586. Took 0.04 sec\n",
            "Epoch 454, Loss(train/val) 0.24510/0.16562. Took 0.04 sec\n",
            "Epoch 455, Loss(train/val) 0.24455/0.16513. Took 0.04 sec\n",
            "Epoch 456, Loss(train/val) 0.24131/0.16478. Took 0.04 sec\n",
            "Epoch 457, Loss(train/val) 0.25147/0.16600. Took 0.04 sec\n",
            "Epoch 458, Loss(train/val) 0.25868/0.16510. Took 0.04 sec\n",
            "Epoch 459, Loss(train/val) 0.24785/0.16551. Took 0.04 sec\n",
            "Epoch 460, Loss(train/val) 0.25863/0.16513. Took 0.03 sec\n",
            "Epoch 461, Loss(train/val) 0.25534/0.16486. Took 0.04 sec\n",
            "Epoch 462, Loss(train/val) 0.25448/0.16530. Took 0.04 sec\n",
            "Epoch 463, Loss(train/val) 0.24187/0.16513. Took 0.04 sec\n",
            "Epoch 464, Loss(train/val) 0.24751/0.16505. Took 0.03 sec\n",
            "Epoch 465, Loss(train/val) 0.24118/0.16510. Took 0.03 sec\n",
            "Epoch 466, Loss(train/val) 0.24584/0.16553. Took 0.04 sec\n",
            "Epoch 467, Loss(train/val) 0.24111/0.16592. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.23607/0.16615. Took 0.04 sec\n",
            "Epoch 469, Loss(train/val) 0.24304/0.16571. Took 0.03 sec\n",
            "Epoch 470, Loss(train/val) 0.24558/0.16525. Took 0.04 sec\n",
            "Epoch 471, Loss(train/val) 0.23783/0.16454. Took 0.04 sec\n",
            "Epoch 472, Loss(train/val) 0.25132/0.16386. Took 0.04 sec\n",
            "Epoch 473, Loss(train/val) 0.24746/0.16452. Took 0.04 sec\n",
            "Epoch 474, Loss(train/val) 0.23788/0.16535. Took 0.03 sec\n",
            "Epoch 475, Loss(train/val) 0.23633/0.16610. Took 0.03 sec\n",
            "Epoch 476, Loss(train/val) 0.25470/0.16560. Took 0.04 sec\n",
            "Epoch 477, Loss(train/val) 0.24799/0.16521. Took 0.04 sec\n",
            "Epoch 478, Loss(train/val) 0.24717/0.16409. Took 0.04 sec\n",
            "Epoch 479, Loss(train/val) 0.24607/0.16355. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.24398/0.16412. Took 0.04 sec\n",
            "Epoch 481, Loss(train/val) 0.24527/0.16543. Took 0.03 sec\n",
            "Epoch 482, Loss(train/val) 0.24104/0.16585. Took 0.03 sec\n",
            "Epoch 483, Loss(train/val) 0.23729/0.16570. Took 0.04 sec\n",
            "Epoch 484, Loss(train/val) 0.24083/0.16481. Took 0.04 sec\n",
            "Epoch 485, Loss(train/val) 0.25076/0.16504. Took 0.04 sec\n",
            "Epoch 486, Loss(train/val) 0.24155/0.16521. Took 0.04 sec\n",
            "Epoch 487, Loss(train/val) 0.25179/0.16523. Took 0.04 sec\n",
            "Epoch 488, Loss(train/val) 0.23560/0.16435. Took 0.04 sec\n",
            "Epoch 489, Loss(train/val) 0.24447/0.16346. Took 0.05 sec\n",
            "Epoch 490, Loss(train/val) 0.24138/0.16355. Took 0.03 sec\n",
            "Epoch 491, Loss(train/val) 0.24328/0.16420. Took 0.04 sec\n",
            "Epoch 492, Loss(train/val) 0.24721/0.16431. Took 0.04 sec\n",
            "Epoch 493, Loss(train/val) 0.24597/0.16578. Took 0.04 sec\n",
            "Epoch 494, Loss(train/val) 0.25638/0.16663. Took 0.04 sec\n",
            "Epoch 495, Loss(train/val) 0.24441/0.16555. Took 0.04 sec\n",
            "Epoch 496, Loss(train/val) 0.24672/0.16446. Took 0.04 sec\n",
            "Epoch 497, Loss(train/val) 0.23795/0.16395. Took 0.04 sec\n",
            "Epoch 498, Loss(train/val) 0.24333/0.16427. Took 0.04 sec\n",
            "Epoch 499, Loss(train/val) 0.24300/0.16465. Took 0.04 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=500, exp_name='exp2_hidn', hid_dim=32, input_dim=1, l2=1e-05, lr=0.0001, n_layers=4, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.04482/0.38508. Took 0.04 sec\n",
            "Epoch 1, Loss(train/val) 1.03882/0.38541. Took 0.04 sec\n",
            "Epoch 2, Loss(train/val) 1.03928/0.38572. Took 0.04 sec\n",
            "Epoch 3, Loss(train/val) 1.02913/0.38599. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.02389/0.38621. Took 0.04 sec\n",
            "Epoch 5, Loss(train/val) 1.01871/0.38642. Took 0.04 sec\n",
            "Epoch 6, Loss(train/val) 1.01252/0.38660. Took 0.04 sec\n",
            "Epoch 7, Loss(train/val) 1.00468/0.38677. Took 0.04 sec\n",
            "Epoch 8, Loss(train/val) 1.00473/0.38690. Took 0.04 sec\n",
            "Epoch 9, Loss(train/val) 1.00173/0.38700. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.00080/0.38708. Took 0.04 sec\n",
            "Epoch 11, Loss(train/val) 0.99571/0.38713. Took 0.04 sec\n",
            "Epoch 12, Loss(train/val) 0.97905/0.38714. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 0.96383/0.38712. Took 0.04 sec\n",
            "Epoch 14, Loss(train/val) 0.98508/0.38707. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 0.97616/0.38697. Took 0.04 sec\n",
            "Epoch 16, Loss(train/val) 0.97826/0.38682. Took 0.04 sec\n",
            "Epoch 17, Loss(train/val) 0.96601/0.38659. Took 0.04 sec\n",
            "Epoch 18, Loss(train/val) 0.96669/0.38630. Took 0.04 sec\n",
            "Epoch 19, Loss(train/val) 0.95750/0.38593. Took 0.04 sec\n",
            "Epoch 20, Loss(train/val) 0.96030/0.38540. Took 0.04 sec\n",
            "Epoch 21, Loss(train/val) 0.94486/0.38473. Took 0.04 sec\n",
            "Epoch 22, Loss(train/val) 0.94073/0.38389. Took 0.04 sec\n",
            "Epoch 23, Loss(train/val) 0.93879/0.38274. Took 0.04 sec\n",
            "Epoch 24, Loss(train/val) 0.92442/0.38131. Took 0.04 sec\n",
            "Epoch 25, Loss(train/val) 0.91436/0.37944. Took 0.04 sec\n",
            "Epoch 26, Loss(train/val) 0.91472/0.37707. Took 0.04 sec\n",
            "Epoch 27, Loss(train/val) 0.90723/0.37411. Took 0.04 sec\n",
            "Epoch 28, Loss(train/val) 0.90051/0.37056. Took 0.04 sec\n",
            "Epoch 29, Loss(train/val) 0.87821/0.36625. Took 0.04 sec\n",
            "Epoch 30, Loss(train/val) 0.88252/0.36126. Took 0.04 sec\n",
            "Epoch 31, Loss(train/val) 0.87035/0.35591. Took 0.04 sec\n",
            "Epoch 32, Loss(train/val) 0.85630/0.34998. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 0.84425/0.34405. Took 0.04 sec\n",
            "Epoch 34, Loss(train/val) 0.83199/0.33830. Took 0.04 sec\n",
            "Epoch 35, Loss(train/val) 0.82080/0.33297. Took 0.04 sec\n",
            "Epoch 36, Loss(train/val) 0.80624/0.32893. Took 0.04 sec\n",
            "Epoch 37, Loss(train/val) 0.79125/0.32520. Took 0.04 sec\n",
            "Epoch 38, Loss(train/val) 0.78035/0.32200. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 0.76365/0.31989. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 0.74345/0.31760. Took 0.04 sec\n",
            "Epoch 41, Loss(train/val) 0.73974/0.31576. Took 0.04 sec\n",
            "Epoch 42, Loss(train/val) 0.72433/0.31505. Took 0.04 sec\n",
            "Epoch 43, Loss(train/val) 0.71164/0.31407. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 0.69150/0.31368. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 0.68028/0.31278. Took 0.04 sec\n",
            "Epoch 46, Loss(train/val) 0.67110/0.31116. Took 0.04 sec\n",
            "Epoch 47, Loss(train/val) 0.65447/0.30977. Took 0.04 sec\n",
            "Epoch 48, Loss(train/val) 0.64742/0.30795. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 0.63813/0.30459. Took 0.05 sec\n",
            "Epoch 50, Loss(train/val) 0.62454/0.30287. Took 0.04 sec\n",
            "Epoch 51, Loss(train/val) 0.61867/0.29889. Took 0.04 sec\n",
            "Epoch 52, Loss(train/val) 0.60748/0.29388. Took 0.04 sec\n",
            "Epoch 53, Loss(train/val) 0.60171/0.28842. Took 0.04 sec\n",
            "Epoch 54, Loss(train/val) 0.59313/0.28216. Took 0.04 sec\n",
            "Epoch 55, Loss(train/val) 0.57563/0.27784. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 0.57595/0.27189. Took 0.04 sec\n",
            "Epoch 57, Loss(train/val) 0.56246/0.26867. Took 0.04 sec\n",
            "Epoch 58, Loss(train/val) 0.55977/0.26396. Took 0.04 sec\n",
            "Epoch 59, Loss(train/val) 0.55452/0.25831. Took 0.04 sec\n",
            "Epoch 60, Loss(train/val) 0.54901/0.25329. Took 0.04 sec\n",
            "Epoch 61, Loss(train/val) 0.54268/0.24763. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 0.53522/0.24454. Took 0.04 sec\n",
            "Epoch 63, Loss(train/val) 0.53137/0.24159. Took 0.04 sec\n",
            "Epoch 64, Loss(train/val) 0.52679/0.23878. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 0.52489/0.23831. Took 0.04 sec\n",
            "Epoch 66, Loss(train/val) 0.51404/0.23705. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 0.50705/0.23661. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 0.50135/0.23620. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 0.49584/0.23681. Took 0.04 sec\n",
            "Epoch 70, Loss(train/val) 0.49058/0.23594. Took 0.04 sec\n",
            "Epoch 71, Loss(train/val) 0.48630/0.23537. Took 0.04 sec\n",
            "Epoch 72, Loss(train/val) 0.48969/0.23552. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 0.49282/0.23638. Took 0.04 sec\n",
            "Epoch 74, Loss(train/val) 0.47471/0.23738. Took 0.04 sec\n",
            "Epoch 75, Loss(train/val) 0.47443/0.23696. Took 0.04 sec\n",
            "Epoch 76, Loss(train/val) 0.46697/0.23714. Took 0.04 sec\n",
            "Epoch 77, Loss(train/val) 0.47020/0.23543. Took 0.04 sec\n",
            "Epoch 78, Loss(train/val) 0.46057/0.23463. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 0.45841/0.23451. Took 0.04 sec\n",
            "Epoch 80, Loss(train/val) 0.45473/0.23545. Took 0.04 sec\n",
            "Epoch 81, Loss(train/val) 0.45306/0.23546. Took 0.04 sec\n",
            "Epoch 82, Loss(train/val) 0.44720/0.23438. Took 0.04 sec\n",
            "Epoch 83, Loss(train/val) 0.44432/0.23500. Took 0.04 sec\n",
            "Epoch 84, Loss(train/val) 0.44333/0.23365. Took 0.04 sec\n",
            "Epoch 85, Loss(train/val) 0.43711/0.23273. Took 0.04 sec\n",
            "Epoch 86, Loss(train/val) 0.43144/0.23235. Took 0.04 sec\n",
            "Epoch 87, Loss(train/val) 0.43250/0.23298. Took 0.04 sec\n",
            "Epoch 88, Loss(train/val) 0.43007/0.23394. Took 0.04 sec\n",
            "Epoch 89, Loss(train/val) 0.42432/0.23439. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 0.41894/0.23315. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 0.42792/0.23203. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 0.42868/0.23156. Took 0.04 sec\n",
            "Epoch 93, Loss(train/val) 0.41815/0.23147. Took 0.04 sec\n",
            "Epoch 94, Loss(train/val) 0.42368/0.23151. Took 0.04 sec\n",
            "Epoch 95, Loss(train/val) 0.41063/0.23124. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 0.40466/0.23035. Took 0.04 sec\n",
            "Epoch 97, Loss(train/val) 0.41571/0.22880. Took 0.04 sec\n",
            "Epoch 98, Loss(train/val) 0.39977/0.22657. Took 0.04 sec\n",
            "Epoch 99, Loss(train/val) 0.40676/0.22625. Took 0.04 sec\n",
            "Epoch 100, Loss(train/val) 0.40375/0.22633. Took 0.04 sec\n",
            "Epoch 101, Loss(train/val) 0.40586/0.22592. Took 0.05 sec\n",
            "Epoch 102, Loss(train/val) 0.39833/0.22595. Took 0.04 sec\n",
            "Epoch 103, Loss(train/val) 0.39749/0.22639. Took 0.04 sec\n",
            "Epoch 104, Loss(train/val) 0.39574/0.22706. Took 0.04 sec\n",
            "Epoch 105, Loss(train/val) 0.41414/0.22726. Took 0.04 sec\n",
            "Epoch 106, Loss(train/val) 0.39868/0.22747. Took 0.04 sec\n",
            "Epoch 107, Loss(train/val) 0.39725/0.22756. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 0.39407/0.22662. Took 0.04 sec\n",
            "Epoch 109, Loss(train/val) 0.37901/0.22562. Took 0.04 sec\n",
            "Epoch 110, Loss(train/val) 0.39228/0.22522. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 0.39492/0.22481. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 0.39075/0.22550. Took 0.04 sec\n",
            "Epoch 113, Loss(train/val) 0.37879/0.22494. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 0.38450/0.22369. Took 0.04 sec\n",
            "Epoch 115, Loss(train/val) 0.38476/0.22331. Took 0.04 sec\n",
            "Epoch 116, Loss(train/val) 0.38479/0.22309. Took 0.04 sec\n",
            "Epoch 117, Loss(train/val) 0.38114/0.22328. Took 0.04 sec\n",
            "Epoch 118, Loss(train/val) 0.38724/0.22324. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 0.38156/0.22331. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 0.37991/0.22353. Took 0.04 sec\n",
            "Epoch 121, Loss(train/val) 0.37590/0.22307. Took 0.04 sec\n",
            "Epoch 122, Loss(train/val) 0.37500/0.22251. Took 0.04 sec\n",
            "Epoch 123, Loss(train/val) 0.37758/0.22215. Took 0.04 sec\n",
            "Epoch 124, Loss(train/val) 0.37446/0.22185. Took 0.04 sec\n",
            "Epoch 125, Loss(train/val) 0.37596/0.22136. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 0.37362/0.22115. Took 0.04 sec\n",
            "Epoch 127, Loss(train/val) 0.36701/0.22156. Took 0.04 sec\n",
            "Epoch 128, Loss(train/val) 0.38764/0.22134. Took 0.04 sec\n",
            "Epoch 129, Loss(train/val) 0.36989/0.22135. Took 0.04 sec\n",
            "Epoch 130, Loss(train/val) 0.36711/0.22136. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 0.36676/0.22114. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 0.36939/0.22086. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 0.36753/0.22100. Took 0.04 sec\n",
            "Epoch 134, Loss(train/val) 0.36563/0.22106. Took 0.04 sec\n",
            "Epoch 135, Loss(train/val) 0.37350/0.22060. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 0.37037/0.22011. Took 0.04 sec\n",
            "Epoch 137, Loss(train/val) 0.36035/0.22003. Took 0.04 sec\n",
            "Epoch 138, Loss(train/val) 0.37344/0.22018. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 0.36371/0.22033. Took 0.04 sec\n",
            "Epoch 140, Loss(train/val) 0.36145/0.22008. Took 0.04 sec\n",
            "Epoch 141, Loss(train/val) 0.35900/0.21971. Took 0.04 sec\n",
            "Epoch 142, Loss(train/val) 0.36847/0.21933. Took 0.04 sec\n",
            "Epoch 143, Loss(train/val) 0.35096/0.21932. Took 0.04 sec\n",
            "Epoch 144, Loss(train/val) 0.35512/0.21917. Took 0.04 sec\n",
            "Epoch 145, Loss(train/val) 0.36306/0.21913. Took 0.04 sec\n",
            "Epoch 146, Loss(train/val) 0.37039/0.21885. Took 0.04 sec\n",
            "Epoch 147, Loss(train/val) 0.35608/0.21829. Took 0.04 sec\n",
            "Epoch 148, Loss(train/val) 0.35873/0.21807. Took 0.04 sec\n",
            "Epoch 149, Loss(train/val) 0.35494/0.21781. Took 0.04 sec\n",
            "Epoch 150, Loss(train/val) 0.35223/0.21741. Took 0.04 sec\n",
            "Epoch 151, Loss(train/val) 0.35328/0.21719. Took 0.04 sec\n",
            "Epoch 152, Loss(train/val) 0.35383/0.21705. Took 0.04 sec\n",
            "Epoch 153, Loss(train/val) 0.34648/0.21717. Took 0.04 sec\n",
            "Epoch 154, Loss(train/val) 0.34577/0.21721. Took 0.05 sec\n",
            "Epoch 155, Loss(train/val) 0.35116/0.21699. Took 0.04 sec\n",
            "Epoch 156, Loss(train/val) 0.34586/0.21699. Took 0.04 sec\n",
            "Epoch 157, Loss(train/val) 0.34707/0.21661. Took 0.04 sec\n",
            "Epoch 158, Loss(train/val) 0.34621/0.21650. Took 0.04 sec\n",
            "Epoch 159, Loss(train/val) 0.33622/0.21634. Took 0.04 sec\n",
            "Epoch 160, Loss(train/val) 0.34280/0.21604. Took 0.05 sec\n",
            "Epoch 161, Loss(train/val) 0.34690/0.21584. Took 0.04 sec\n",
            "Epoch 162, Loss(train/val) 0.34432/0.21600. Took 0.04 sec\n",
            "Epoch 163, Loss(train/val) 0.34214/0.21577. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 0.36557/0.21582. Took 0.04 sec\n",
            "Epoch 165, Loss(train/val) 0.33485/0.21519. Took 0.04 sec\n",
            "Epoch 166, Loss(train/val) 0.33962/0.21461. Took 0.04 sec\n",
            "Epoch 167, Loss(train/val) 0.32689/0.21426. Took 0.04 sec\n",
            "Epoch 168, Loss(train/val) 0.33485/0.21396. Took 0.04 sec\n",
            "Epoch 169, Loss(train/val) 0.33185/0.21373. Took 0.04 sec\n",
            "Epoch 170, Loss(train/val) 0.32410/0.21425. Took 0.04 sec\n",
            "Epoch 171, Loss(train/val) 0.33502/0.21447. Took 0.05 sec\n",
            "Epoch 172, Loss(train/val) 0.32969/0.21482. Took 0.04 sec\n",
            "Epoch 173, Loss(train/val) 0.31965/0.21457. Took 0.04 sec\n",
            "Epoch 174, Loss(train/val) 0.32358/0.21383. Took 0.04 sec\n",
            "Epoch 175, Loss(train/val) 0.32541/0.21428. Took 0.04 sec\n",
            "Epoch 176, Loss(train/val) 0.33091/0.21381. Took 0.04 sec\n",
            "Epoch 177, Loss(train/val) 0.32979/0.21295. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 0.33439/0.21237. Took 0.04 sec\n",
            "Epoch 179, Loss(train/val) 0.32318/0.21193. Took 0.04 sec\n",
            "Epoch 180, Loss(train/val) 0.32621/0.21169. Took 0.04 sec\n",
            "Epoch 181, Loss(train/val) 0.32130/0.21189. Took 0.04 sec\n",
            "Epoch 182, Loss(train/val) 0.31923/0.21194. Took 0.04 sec\n",
            "Epoch 183, Loss(train/val) 0.32683/0.21171. Took 0.04 sec\n",
            "Epoch 184, Loss(train/val) 0.32348/0.21212. Took 0.04 sec\n",
            "Epoch 185, Loss(train/val) 0.32233/0.21225. Took 0.04 sec\n",
            "Epoch 186, Loss(train/val) 0.32562/0.21181. Took 0.04 sec\n",
            "Epoch 187, Loss(train/val) 0.31890/0.21142. Took 0.04 sec\n",
            "Epoch 188, Loss(train/val) 0.31960/0.21195. Took 0.05 sec\n",
            "Epoch 189, Loss(train/val) 0.31683/0.21256. Took 0.04 sec\n",
            "Epoch 190, Loss(train/val) 0.31876/0.21331. Took 0.04 sec\n",
            "Epoch 191, Loss(train/val) 0.32022/0.21424. Took 0.04 sec\n",
            "Epoch 192, Loss(train/val) 0.31869/0.21351. Took 0.04 sec\n",
            "Epoch 193, Loss(train/val) 0.31686/0.21401. Took 0.05 sec\n",
            "Epoch 194, Loss(train/val) 0.31849/0.21473. Took 0.04 sec\n",
            "Epoch 195, Loss(train/val) 0.32025/0.21393. Took 0.04 sec\n",
            "Epoch 196, Loss(train/val) 0.29557/0.21275. Took 0.04 sec\n",
            "Epoch 197, Loss(train/val) 0.30614/0.21325. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 0.31306/0.21379. Took 0.04 sec\n",
            "Epoch 199, Loss(train/val) 0.32030/0.21526. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 0.30687/0.22063. Took 0.04 sec\n",
            "Epoch 201, Loss(train/val) 0.30377/0.21972. Took 0.04 sec\n",
            "Epoch 202, Loss(train/val) 0.31800/0.22139. Took 0.04 sec\n",
            "Epoch 203, Loss(train/val) 0.31259/0.21572. Took 0.04 sec\n",
            "Epoch 204, Loss(train/val) 0.31247/0.21007. Took 0.04 sec\n",
            "Epoch 205, Loss(train/val) 0.32123/0.20834. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 0.30195/0.20948. Took 0.04 sec\n",
            "Epoch 207, Loss(train/val) 0.30832/0.21148. Took 0.04 sec\n",
            "Epoch 208, Loss(train/val) 0.30746/0.21198. Took 0.04 sec\n",
            "Epoch 209, Loss(train/val) 0.30899/0.21545. Took 0.04 sec\n",
            "Epoch 210, Loss(train/val) 0.30937/0.22078. Took 0.04 sec\n",
            "Epoch 211, Loss(train/val) 0.30457/0.22905. Took 0.04 sec\n",
            "Epoch 212, Loss(train/val) 0.30444/0.23864. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 0.29954/0.24794. Took 0.04 sec\n",
            "Epoch 214, Loss(train/val) 0.30467/0.24150. Took 0.04 sec\n",
            "Epoch 215, Loss(train/val) 0.30050/0.23830. Took 0.04 sec\n",
            "Epoch 216, Loss(train/val) 0.32620/0.23802. Took 0.04 sec\n",
            "Epoch 217, Loss(train/val) 0.29354/0.24064. Took 0.04 sec\n",
            "Epoch 218, Loss(train/val) 0.30152/0.24182. Took 0.04 sec\n",
            "Epoch 219, Loss(train/val) 0.29700/0.25170. Took 0.04 sec\n",
            "Epoch 220, Loss(train/val) 0.29651/0.24869. Took 0.04 sec\n",
            "Epoch 221, Loss(train/val) 0.30262/0.23673. Took 0.04 sec\n",
            "Epoch 222, Loss(train/val) 0.30187/0.23016. Took 0.04 sec\n",
            "Epoch 223, Loss(train/val) 0.30268/0.23729. Took 0.04 sec\n",
            "Epoch 224, Loss(train/val) 0.29283/0.24080. Took 0.04 sec\n",
            "Epoch 225, Loss(train/val) 0.30458/0.24402. Took 0.04 sec\n",
            "Epoch 226, Loss(train/val) 0.28910/0.23759. Took 0.04 sec\n",
            "Epoch 227, Loss(train/val) 0.29685/0.24179. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 0.30093/0.23951. Took 0.04 sec\n",
            "Epoch 229, Loss(train/val) 0.28966/0.23361. Took 0.04 sec\n",
            "Epoch 230, Loss(train/val) 0.29008/0.23797. Took 0.04 sec\n",
            "Epoch 231, Loss(train/val) 0.29027/0.25802. Took 0.04 sec\n",
            "Epoch 232, Loss(train/val) 0.28558/0.28345. Took 0.04 sec\n",
            "Epoch 233, Loss(train/val) 0.30367/0.25664. Took 0.04 sec\n",
            "Epoch 234, Loss(train/val) 0.29075/0.23670. Took 0.04 sec\n",
            "Epoch 235, Loss(train/val) 0.28273/0.24336. Took 0.04 sec\n",
            "Epoch 236, Loss(train/val) 0.28092/0.24425. Took 0.04 sec\n",
            "Epoch 237, Loss(train/val) 0.28229/0.25181. Took 0.05 sec\n",
            "Epoch 238, Loss(train/val) 0.28037/0.24887. Took 0.04 sec\n",
            "Epoch 239, Loss(train/val) 0.27618/0.23171. Took 0.04 sec\n",
            "Epoch 240, Loss(train/val) 0.27187/0.23486. Took 0.04 sec\n",
            "Epoch 241, Loss(train/val) 0.28615/0.23712. Took 0.04 sec\n",
            "Epoch 242, Loss(train/val) 0.27425/0.23338. Took 0.04 sec\n",
            "Epoch 243, Loss(train/val) 0.27842/0.23463. Took 0.04 sec\n",
            "Epoch 244, Loss(train/val) 0.27190/0.22866. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.28153/0.23569. Took 0.04 sec\n",
            "Epoch 246, Loss(train/val) 0.27908/0.23212. Took 0.04 sec\n",
            "Epoch 247, Loss(train/val) 0.27578/0.24249. Took 0.04 sec\n",
            "Epoch 248, Loss(train/val) 0.28802/0.22606. Took 0.04 sec\n",
            "Epoch 249, Loss(train/val) 0.27683/0.24353. Took 0.04 sec\n",
            "Epoch 250, Loss(train/val) 0.27511/0.23034. Took 0.05 sec\n",
            "Epoch 251, Loss(train/val) 0.28122/0.23038. Took 0.04 sec\n",
            "Epoch 252, Loss(train/val) 0.27440/0.22554. Took 0.04 sec\n",
            "Epoch 253, Loss(train/val) 0.27783/0.23910. Took 0.04 sec\n",
            "Epoch 254, Loss(train/val) 0.27469/0.24312. Took 0.04 sec\n",
            "Epoch 255, Loss(train/val) 0.27078/0.20989. Took 0.04 sec\n",
            "Epoch 256, Loss(train/val) 0.26807/0.20399. Took 0.04 sec\n",
            "Epoch 257, Loss(train/val) 0.27928/0.22635. Took 0.04 sec\n",
            "Epoch 258, Loss(train/val) 0.28755/0.20728. Took 0.04 sec\n",
            "Epoch 259, Loss(train/val) 0.27064/0.21192. Took 0.04 sec\n",
            "Epoch 260, Loss(train/val) 0.27308/0.18775. Took 0.04 sec\n",
            "Epoch 261, Loss(train/val) 0.27278/0.18750. Took 0.04 sec\n",
            "Epoch 262, Loss(train/val) 0.26656/0.18883. Took 0.06 sec\n",
            "Epoch 263, Loss(train/val) 0.26389/0.19276. Took 0.04 sec\n",
            "Epoch 264, Loss(train/val) 0.27304/0.19371. Took 0.04 sec\n",
            "Epoch 265, Loss(train/val) 0.26481/0.18478. Took 0.04 sec\n",
            "Epoch 266, Loss(train/val) 0.27327/0.18839. Took 0.04 sec\n",
            "Epoch 267, Loss(train/val) 0.26920/0.19746. Took 0.05 sec\n",
            "Epoch 268, Loss(train/val) 0.26201/0.19298. Took 0.05 sec\n",
            "Epoch 269, Loss(train/val) 0.26970/0.18383. Took 0.04 sec\n",
            "Epoch 270, Loss(train/val) 0.27683/0.18026. Took 0.04 sec\n",
            "Epoch 271, Loss(train/val) 0.25945/0.18147. Took 0.04 sec\n",
            "Epoch 272, Loss(train/val) 0.26932/0.18233. Took 0.04 sec\n",
            "Epoch 273, Loss(train/val) 0.26592/0.19019. Took 0.04 sec\n",
            "Epoch 274, Loss(train/val) 0.26702/0.17964. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.26783/0.17816. Took 0.04 sec\n",
            "Epoch 276, Loss(train/val) 0.28665/0.18053. Took 0.04 sec\n",
            "Epoch 277, Loss(train/val) 0.26115/0.19316. Took 0.04 sec\n",
            "Epoch 278, Loss(train/val) 0.27157/0.18383. Took 0.04 sec\n",
            "Epoch 279, Loss(train/val) 0.28525/0.17795. Took 0.04 sec\n",
            "Epoch 280, Loss(train/val) 0.27034/0.17679. Took 0.05 sec\n",
            "Epoch 281, Loss(train/val) 0.29775/0.17820. Took 0.04 sec\n",
            "Epoch 282, Loss(train/val) 0.26701/0.18215. Took 0.04 sec\n",
            "Epoch 283, Loss(train/val) 0.26622/0.17751. Took 0.04 sec\n",
            "Epoch 284, Loss(train/val) 0.25848/0.17664. Took 0.04 sec\n",
            "Epoch 285, Loss(train/val) 0.26656/0.17752. Took 0.04 sec\n",
            "Epoch 286, Loss(train/val) 0.27667/0.17640. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.26682/0.17732. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.26352/0.18340. Took 0.04 sec\n",
            "Epoch 289, Loss(train/val) 0.26774/0.18884. Took 0.04 sec\n",
            "Epoch 290, Loss(train/val) 0.27257/0.17742. Took 0.04 sec\n",
            "Epoch 291, Loss(train/val) 0.26100/0.17845. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.26857/0.17979. Took 0.04 sec\n",
            "Epoch 293, Loss(train/val) 0.26987/0.17748. Took 0.04 sec\n",
            "Epoch 294, Loss(train/val) 0.26163/0.17963. Took 0.04 sec\n",
            "Epoch 295, Loss(train/val) 0.26617/0.18579. Took 0.04 sec\n",
            "Epoch 296, Loss(train/val) 0.26234/0.17664. Took 0.04 sec\n",
            "Epoch 297, Loss(train/val) 0.26248/0.17729. Took 0.04 sec\n",
            "Epoch 298, Loss(train/val) 0.26461/0.17730. Took 0.04 sec\n",
            "Epoch 299, Loss(train/val) 0.25730/0.17673. Took 0.04 sec\n",
            "Epoch 300, Loss(train/val) 0.26696/0.17615. Took 0.04 sec\n",
            "Epoch 301, Loss(train/val) 0.27351/0.17594. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.26583/0.17586. Took 0.04 sec\n",
            "Epoch 303, Loss(train/val) 0.25893/0.17605. Took 0.04 sec\n",
            "Epoch 304, Loss(train/val) 0.27143/0.17849. Took 0.04 sec\n",
            "Epoch 305, Loss(train/val) 0.27353/0.18036. Took 0.04 sec\n",
            "Epoch 306, Loss(train/val) 0.26030/0.17897. Took 0.04 sec\n",
            "Epoch 307, Loss(train/val) 0.26168/0.17662. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.28513/0.18289. Took 0.04 sec\n",
            "Epoch 309, Loss(train/val) 0.25958/0.17483. Took 0.04 sec\n",
            "Epoch 310, Loss(train/val) 0.26272/0.18419. Took 0.04 sec\n",
            "Epoch 311, Loss(train/val) 0.25649/0.18252. Took 0.05 sec\n",
            "Epoch 312, Loss(train/val) 0.26930/0.17682. Took 0.04 sec\n",
            "Epoch 313, Loss(train/val) 0.26904/0.18302. Took 0.04 sec\n",
            "Epoch 314, Loss(train/val) 0.26423/0.18018. Took 0.04 sec\n",
            "Epoch 315, Loss(train/val) 0.25373/0.17818. Took 0.04 sec\n",
            "Epoch 316, Loss(train/val) 0.26039/0.18023. Took 0.04 sec\n",
            "Epoch 317, Loss(train/val) 0.25806/0.17758. Took 0.04 sec\n",
            "Epoch 318, Loss(train/val) 0.25975/0.17549. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.26751/0.17593. Took 0.04 sec\n",
            "Epoch 320, Loss(train/val) 0.26468/0.17502. Took 0.04 sec\n",
            "Epoch 321, Loss(train/val) 0.28803/0.17476. Took 0.04 sec\n",
            "Epoch 322, Loss(train/val) 0.26173/0.17501. Took 0.04 sec\n",
            "Epoch 323, Loss(train/val) 0.26012/0.17498. Took 0.04 sec\n",
            "Epoch 324, Loss(train/val) 0.26390/0.17512. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.26673/0.17508. Took 0.04 sec\n",
            "Epoch 326, Loss(train/val) 0.26626/0.17562. Took 0.06 sec\n",
            "Epoch 327, Loss(train/val) 0.26097/0.17525. Took 0.08 sec\n",
            "Epoch 328, Loss(train/val) 0.26642/0.17572. Took 0.07 sec\n",
            "Epoch 329, Loss(train/val) 0.26503/0.17673. Took 0.06 sec\n",
            "Epoch 330, Loss(train/val) 0.25775/0.17636. Took 0.06 sec\n",
            "Epoch 331, Loss(train/val) 0.25942/0.17547. Took 0.06 sec\n",
            "Epoch 332, Loss(train/val) 0.25939/0.17518. Took 0.08 sec\n",
            "Epoch 333, Loss(train/val) 0.26767/0.17467. Took 0.06 sec\n",
            "Epoch 334, Loss(train/val) 0.26665/0.17615. Took 0.06 sec\n",
            "Epoch 335, Loss(train/val) 0.26536/0.17450. Took 0.06 sec\n",
            "Epoch 336, Loss(train/val) 0.25519/0.17409. Took 0.07 sec\n",
            "Epoch 337, Loss(train/val) 0.27001/0.17461. Took 0.06 sec\n",
            "Epoch 338, Loss(train/val) 0.26328/0.17400. Took 0.06 sec\n",
            "Epoch 339, Loss(train/val) 0.26527/0.17889. Took 0.06 sec\n",
            "Epoch 340, Loss(train/val) 0.26045/0.17590. Took 0.06 sec\n",
            "Epoch 341, Loss(train/val) 0.25787/0.17614. Took 0.06 sec\n",
            "Epoch 342, Loss(train/val) 0.26847/0.17710. Took 0.06 sec\n",
            "Epoch 343, Loss(train/val) 0.25422/0.17502. Took 0.06 sec\n",
            "Epoch 344, Loss(train/val) 0.25956/0.17759. Took 0.08 sec\n",
            "Epoch 345, Loss(train/val) 0.26016/0.17780. Took 0.06 sec\n",
            "Epoch 346, Loss(train/val) 0.27083/0.17532. Took 0.06 sec\n",
            "Epoch 347, Loss(train/val) 0.26131/0.18271. Took 0.06 sec\n",
            "Epoch 348, Loss(train/val) 0.26132/0.18356. Took 0.07 sec\n",
            "Epoch 349, Loss(train/val) 0.25412/0.18260. Took 0.06 sec\n",
            "Epoch 350, Loss(train/val) 0.25662/0.17456. Took 0.06 sec\n",
            "Epoch 351, Loss(train/val) 0.25900/0.17915. Took 0.06 sec\n",
            "Epoch 352, Loss(train/val) 0.29350/0.17576. Took 0.06 sec\n",
            "Epoch 353, Loss(train/val) 0.26059/0.17443. Took 0.06 sec\n",
            "Epoch 354, Loss(train/val) 0.26233/0.17832. Took 0.06 sec\n",
            "Epoch 355, Loss(train/val) 0.25952/0.17551. Took 0.06 sec\n",
            "Epoch 356, Loss(train/val) 0.26350/0.17831. Took 0.07 sec\n",
            "Epoch 357, Loss(train/val) 0.25704/0.17997. Took 0.06 sec\n",
            "Epoch 358, Loss(train/val) 0.27070/0.17658. Took 0.06 sec\n",
            "Epoch 359, Loss(train/val) 0.25434/0.17488. Took 0.06 sec\n",
            "Epoch 360, Loss(train/val) 0.26162/0.17402. Took 0.07 sec\n",
            "Epoch 361, Loss(train/val) 0.25329/0.17379. Took 0.06 sec\n",
            "Epoch 362, Loss(train/val) 0.26947/0.17412. Took 0.04 sec\n",
            "Epoch 363, Loss(train/val) 0.25024/0.17379. Took 0.04 sec\n",
            "Epoch 364, Loss(train/val) 0.26063/0.17499. Took 0.04 sec\n",
            "Epoch 365, Loss(train/val) 0.25405/0.17503. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.26497/0.17597. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 0.26807/0.17398. Took 0.04 sec\n",
            "Epoch 368, Loss(train/val) 0.26525/0.17593. Took 0.04 sec\n",
            "Epoch 369, Loss(train/val) 0.27080/0.17418. Took 0.04 sec\n",
            "Epoch 370, Loss(train/val) 0.27177/0.17424. Took 0.05 sec\n",
            "Epoch 371, Loss(train/val) 0.26160/0.17646. Took 0.04 sec\n",
            "Epoch 372, Loss(train/val) 0.26106/0.17548. Took 0.04 sec\n",
            "Epoch 373, Loss(train/val) 0.25940/0.17464. Took 0.04 sec\n",
            "Epoch 374, Loss(train/val) 0.26674/0.17528. Took 0.04 sec\n",
            "Epoch 375, Loss(train/val) 0.25534/0.18026. Took 0.04 sec\n",
            "Epoch 376, Loss(train/val) 0.25895/0.17592. Took 0.05 sec\n",
            "Epoch 377, Loss(train/val) 0.25752/0.17625. Took 0.04 sec\n",
            "Epoch 378, Loss(train/val) 0.26180/0.17443. Took 0.04 sec\n",
            "Epoch 379, Loss(train/val) 0.25339/0.17469. Took 0.04 sec\n",
            "Epoch 380, Loss(train/val) 0.26721/0.17516. Took 0.04 sec\n",
            "Epoch 381, Loss(train/val) 0.25485/0.17609. Took 0.04 sec\n",
            "Epoch 382, Loss(train/val) 0.24832/0.17887. Took 0.04 sec\n",
            "Epoch 383, Loss(train/val) 0.25599/0.17412. Took 0.04 sec\n",
            "Epoch 384, Loss(train/val) 0.27451/0.17414. Took 0.04 sec\n",
            "Epoch 385, Loss(train/val) 0.26193/0.18028. Took 0.04 sec\n",
            "Epoch 386, Loss(train/val) 0.26628/0.17502. Took 0.04 sec\n",
            "Epoch 387, Loss(train/val) 0.25049/0.17311. Took 0.04 sec\n",
            "Epoch 388, Loss(train/val) 0.26166/0.17338. Took 0.04 sec\n",
            "Epoch 389, Loss(train/val) 0.25560/0.17329. Took 0.04 sec\n",
            "Epoch 390, Loss(train/val) 0.25559/0.17498. Took 0.04 sec\n",
            "Epoch 391, Loss(train/val) 0.25283/0.17337. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.25509/0.17695. Took 0.04 sec\n",
            "Epoch 393, Loss(train/val) 0.27990/0.17536. Took 0.04 sec\n",
            "Epoch 394, Loss(train/val) 0.26019/0.17291. Took 0.04 sec\n",
            "Epoch 395, Loss(train/val) 0.25197/0.17294. Took 0.04 sec\n",
            "Epoch 396, Loss(train/val) 0.25944/0.17320. Took 0.04 sec\n",
            "Epoch 397, Loss(train/val) 0.25242/0.17371. Took 0.04 sec\n",
            "Epoch 398, Loss(train/val) 0.25309/0.17421. Took 0.05 sec\n",
            "Epoch 399, Loss(train/val) 0.25474/0.17559. Took 0.04 sec\n",
            "Epoch 400, Loss(train/val) 0.26118/0.17422. Took 0.04 sec\n",
            "Epoch 401, Loss(train/val) 0.26091/0.17382. Took 0.04 sec\n",
            "Epoch 402, Loss(train/val) 0.27257/0.17342. Took 0.04 sec\n",
            "Epoch 403, Loss(train/val) 0.25643/0.17401. Took 0.04 sec\n",
            "Epoch 404, Loss(train/val) 0.26007/0.17373. Took 0.05 sec\n",
            "Epoch 405, Loss(train/val) 0.26608/0.17305. Took 0.04 sec\n",
            "Epoch 406, Loss(train/val) 0.27618/0.17342. Took 0.04 sec\n",
            "Epoch 407, Loss(train/val) 0.26927/0.17261. Took 0.04 sec\n",
            "Epoch 408, Loss(train/val) 0.26386/0.17258. Took 0.04 sec\n",
            "Epoch 409, Loss(train/val) 0.24897/0.17307. Took 0.04 sec\n",
            "Epoch 410, Loss(train/val) 0.25397/0.17467. Took 0.05 sec\n",
            "Epoch 411, Loss(train/val) 0.25398/0.17389. Took 0.04 sec\n",
            "Epoch 412, Loss(train/val) 0.26595/0.17352. Took 0.04 sec\n",
            "Epoch 413, Loss(train/val) 0.25434/0.17332. Took 0.04 sec\n",
            "Epoch 414, Loss(train/val) 0.25170/0.17273. Took 0.04 sec\n",
            "Epoch 415, Loss(train/val) 0.25383/0.17649. Took 0.06 sec\n",
            "Epoch 416, Loss(train/val) 0.25182/0.17650. Took 0.04 sec\n",
            "Epoch 417, Loss(train/val) 0.26576/0.17233. Took 0.04 sec\n",
            "Epoch 418, Loss(train/val) 0.25239/0.17251. Took 0.04 sec\n",
            "Epoch 419, Loss(train/val) 0.25733/0.17372. Took 0.04 sec\n",
            "Epoch 420, Loss(train/val) 0.24982/0.17651. Took 0.04 sec\n",
            "Epoch 421, Loss(train/val) 0.26345/0.17252. Took 0.05 sec\n",
            "Epoch 422, Loss(train/val) 0.25487/0.17349. Took 0.04 sec\n",
            "Epoch 423, Loss(train/val) 0.25651/0.17320. Took 0.04 sec\n",
            "Epoch 424, Loss(train/val) 0.25597/0.17259. Took 0.04 sec\n",
            "Epoch 425, Loss(train/val) 0.26322/0.17552. Took 0.05 sec\n",
            "Epoch 426, Loss(train/val) 0.26702/0.17473. Took 0.04 sec\n",
            "Epoch 427, Loss(train/val) 0.25249/0.17392. Took 0.04 sec\n",
            "Epoch 428, Loss(train/val) 0.26518/0.17343. Took 0.04 sec\n",
            "Epoch 429, Loss(train/val) 0.25538/0.17512. Took 0.04 sec\n",
            "Epoch 430, Loss(train/val) 0.25918/0.17963. Took 0.04 sec\n",
            "Epoch 431, Loss(train/val) 0.24896/0.17259. Took 0.04 sec\n",
            "Epoch 432, Loss(train/val) 0.25785/0.17253. Took 0.04 sec\n",
            "Epoch 433, Loss(train/val) 0.26216/0.17375. Took 0.04 sec\n",
            "Epoch 434, Loss(train/val) 0.25434/0.17415. Took 0.04 sec\n",
            "Epoch 435, Loss(train/val) 0.26775/0.17314. Took 0.05 sec\n",
            "Epoch 436, Loss(train/val) 0.26677/0.17470. Took 0.04 sec\n",
            "Epoch 437, Loss(train/val) 0.25710/0.17197. Took 0.05 sec\n",
            "Epoch 438, Loss(train/val) 0.25092/0.17185. Took 0.04 sec\n",
            "Epoch 439, Loss(train/val) 0.25390/0.17202. Took 0.04 sec\n",
            "Epoch 440, Loss(train/val) 0.25025/0.17412. Took 0.05 sec\n",
            "Epoch 441, Loss(train/val) 0.24402/0.17370. Took 0.05 sec\n",
            "Epoch 442, Loss(train/val) 0.25427/0.17250. Took 0.04 sec\n",
            "Epoch 443, Loss(train/val) 0.25372/0.17275. Took 0.04 sec\n",
            "Epoch 444, Loss(train/val) 0.25227/0.17295. Took 0.04 sec\n",
            "Epoch 445, Loss(train/val) 0.25537/0.17242. Took 0.04 sec\n",
            "Epoch 446, Loss(train/val) 0.25000/0.17286. Took 0.04 sec\n",
            "Epoch 447, Loss(train/val) 0.25413/0.17336. Took 0.04 sec\n",
            "Epoch 448, Loss(train/val) 0.25072/0.17642. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 0.26129/0.17280. Took 0.04 sec\n",
            "Epoch 450, Loss(train/val) 0.25138/0.17212. Took 0.04 sec\n",
            "Epoch 451, Loss(train/val) 0.25276/0.17225. Took 0.04 sec\n",
            "Epoch 452, Loss(train/val) 0.26527/0.17139. Took 0.04 sec\n",
            "Epoch 453, Loss(train/val) 0.25075/0.17313. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.25699/0.17440. Took 0.04 sec\n",
            "Epoch 455, Loss(train/val) 0.25232/0.17435. Took 0.04 sec\n",
            "Epoch 456, Loss(train/val) 0.24671/0.17539. Took 0.04 sec\n",
            "Epoch 457, Loss(train/val) 0.25129/0.17227. Took 0.04 sec\n",
            "Epoch 458, Loss(train/val) 0.25146/0.17232. Took 0.05 sec\n",
            "Epoch 459, Loss(train/val) 0.24681/0.17191. Took 0.04 sec\n",
            "Epoch 460, Loss(train/val) 0.25012/0.17195. Took 0.04 sec\n",
            "Epoch 461, Loss(train/val) 0.25531/0.17206. Took 0.04 sec\n",
            "Epoch 462, Loss(train/val) 0.24965/0.17243. Took 0.04 sec\n",
            "Epoch 463, Loss(train/val) 0.26802/0.17295. Took 0.04 sec\n",
            "Epoch 464, Loss(train/val) 0.25909/0.17297. Took 0.06 sec\n",
            "Epoch 465, Loss(train/val) 0.25838/0.17213. Took 0.04 sec\n",
            "Epoch 466, Loss(train/val) 0.24995/0.17079. Took 0.04 sec\n",
            "Epoch 467, Loss(train/val) 0.28272/0.17188. Took 0.04 sec\n",
            "Epoch 468, Loss(train/val) 0.24917/0.17449. Took 0.04 sec\n",
            "Epoch 469, Loss(train/val) 0.25956/0.17288. Took 0.05 sec\n",
            "Epoch 470, Loss(train/val) 0.25918/0.17134. Took 0.04 sec\n",
            "Epoch 471, Loss(train/val) 0.25285/0.17144. Took 0.04 sec\n",
            "Epoch 472, Loss(train/val) 0.25197/0.17450. Took 0.04 sec\n",
            "Epoch 473, Loss(train/val) 0.25583/0.17465. Took 0.04 sec\n",
            "Epoch 474, Loss(train/val) 0.24838/0.17243. Took 0.04 sec\n",
            "Epoch 475, Loss(train/val) 0.25202/0.17553. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.25159/0.17179. Took 0.04 sec\n",
            "Epoch 477, Loss(train/val) 0.24817/0.17322. Took 0.04 sec\n",
            "Epoch 478, Loss(train/val) 0.24469/0.17365. Took 0.05 sec\n",
            "Epoch 479, Loss(train/val) 0.26189/0.17197. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.25333/0.17124. Took 0.04 sec\n",
            "Epoch 481, Loss(train/val) 0.25701/0.17361. Took 0.04 sec\n",
            "Epoch 482, Loss(train/val) 0.25022/0.17526. Took 0.04 sec\n",
            "Epoch 483, Loss(train/val) 0.24987/0.17408. Took 0.04 sec\n",
            "Epoch 484, Loss(train/val) 0.25381/0.17145. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.25690/0.17189. Took 0.05 sec\n",
            "Epoch 486, Loss(train/val) 0.25301/0.17109. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.25533/0.17207. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.25951/0.17284. Took 0.04 sec\n",
            "Epoch 489, Loss(train/val) 0.24937/0.17186. Took 0.05 sec\n",
            "Epoch 490, Loss(train/val) 0.26409/0.17390. Took 0.04 sec\n",
            "Epoch 491, Loss(train/val) 0.25477/0.17567. Took 0.04 sec\n",
            "Epoch 492, Loss(train/val) 0.26691/0.17333. Took 0.04 sec\n",
            "Epoch 493, Loss(train/val) 0.25977/0.17161. Took 0.04 sec\n",
            "Epoch 494, Loss(train/val) 0.25304/0.17148. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.25961/0.17157. Took 0.04 sec\n",
            "Epoch 496, Loss(train/val) 0.26703/0.17192. Took 0.04 sec\n",
            "Epoch 497, Loss(train/val) 0.25363/0.17137. Took 0.04 sec\n",
            "Epoch 498, Loss(train/val) 0.25495/0.17144. Took 0.04 sec\n",
            "Epoch 499, Loss(train/val) 0.24968/0.17199. Took 0.04 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=500, exp_name='exp2_hidn', hid_dim=32, input_dim=1, l2=1e-05, lr=0.0001, n_layers=6, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.05829/0.37376. Took 0.04 sec\n",
            "Epoch 1, Loss(train/val) 1.05853/0.37288. Took 0.04 sec\n",
            "Epoch 2, Loss(train/val) 1.05998/0.37211. Took 0.04 sec\n",
            "Epoch 3, Loss(train/val) 1.05830/0.37145. Took 0.04 sec\n",
            "Epoch 4, Loss(train/val) 1.05591/0.37087. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.06766/0.37035. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.06058/0.36988. Took 0.04 sec\n",
            "Epoch 7, Loss(train/val) 1.05064/0.36945. Took 0.04 sec\n",
            "Epoch 8, Loss(train/val) 1.06573/0.36906. Took 0.04 sec\n",
            "Epoch 9, Loss(train/val) 1.06173/0.36869. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.04983/0.36834. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.06381/0.36802. Took 0.04 sec\n",
            "Epoch 12, Loss(train/val) 1.05289/0.36775. Took 0.04 sec\n",
            "Epoch 13, Loss(train/val) 1.04698/0.36749. Took 0.04 sec\n",
            "Epoch 14, Loss(train/val) 1.05851/0.36724. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.05681/0.36707. Took 0.05 sec\n",
            "Epoch 16, Loss(train/val) 1.05181/0.36692. Took 0.04 sec\n",
            "Epoch 17, Loss(train/val) 1.05244/0.36677. Took 0.04 sec\n",
            "Epoch 18, Loss(train/val) 1.04500/0.36660. Took 0.04 sec\n",
            "Epoch 19, Loss(train/val) 1.04752/0.36641. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 1.03639/0.36617. Took 0.04 sec\n",
            "Epoch 21, Loss(train/val) 1.01846/0.36599. Took 0.04 sec\n",
            "Epoch 22, Loss(train/val) 1.03967/0.36611. Took 0.04 sec\n",
            "Epoch 23, Loss(train/val) 1.04209/0.36627. Took 0.04 sec\n",
            "Epoch 24, Loss(train/val) 1.03630/0.36654. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 1.02570/0.36705. Took 0.04 sec\n",
            "Epoch 26, Loss(train/val) 1.03267/0.36799. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.02096/0.36965. Took 0.04 sec\n",
            "Epoch 28, Loss(train/val) 1.01813/0.37212. Took 0.04 sec\n",
            "Epoch 29, Loss(train/val) 1.01346/0.37586. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 0.99996/0.38137. Took 0.04 sec\n",
            "Epoch 31, Loss(train/val) 1.00074/0.38882. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 0.99236/0.39919. Took 0.04 sec\n",
            "Epoch 33, Loss(train/val) 0.99186/0.41259. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 0.96899/0.42810. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 0.97584/0.44645. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 0.96581/0.46484. Took 0.04 sec\n",
            "Epoch 37, Loss(train/val) 0.95131/0.48206. Took 0.04 sec\n",
            "Epoch 38, Loss(train/val) 0.93866/0.49775. Took 0.04 sec\n",
            "Epoch 39, Loss(train/val) 0.92957/0.50862. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 0.90866/0.51535. Took 0.04 sec\n",
            "Epoch 41, Loss(train/val) 0.89960/0.51896. Took 0.04 sec\n",
            "Epoch 42, Loss(train/val) 0.89436/0.52115. Took 0.04 sec\n",
            "Epoch 43, Loss(train/val) 0.88634/0.51973. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 0.88056/0.51653. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 0.86886/0.50805. Took 0.04 sec\n",
            "Epoch 46, Loss(train/val) 0.85499/0.49743. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 0.85373/0.49064. Took 0.04 sec\n",
            "Epoch 48, Loss(train/val) 0.84428/0.48992. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 0.83721/0.48792. Took 0.05 sec\n",
            "Epoch 50, Loss(train/val) 0.82199/0.48788. Took 0.04 sec\n",
            "Epoch 51, Loss(train/val) 0.82324/0.48939. Took 0.04 sec\n",
            "Epoch 52, Loss(train/val) 0.81190/0.48235. Took 0.04 sec\n",
            "Epoch 53, Loss(train/val) 0.79685/0.47788. Took 0.04 sec\n",
            "Epoch 54, Loss(train/val) 0.79917/0.47963. Took 0.05 sec\n",
            "Epoch 55, Loss(train/val) 0.78733/0.48211. Took 0.06 sec\n",
            "Epoch 56, Loss(train/val) 0.77409/0.48694. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 0.77493/0.49888. Took 0.04 sec\n",
            "Epoch 58, Loss(train/val) 0.76200/0.51104. Took 0.04 sec\n",
            "Epoch 59, Loss(train/val) 0.75068/0.52156. Took 0.05 sec\n",
            "Epoch 60, Loss(train/val) 0.74284/0.52663. Took 0.04 sec\n",
            "Epoch 61, Loss(train/val) 0.74258/0.52971. Took 0.04 sec\n",
            "Epoch 62, Loss(train/val) 0.73507/0.53562. Took 0.04 sec\n",
            "Epoch 63, Loss(train/val) 0.72314/0.54591. Took 0.04 sec\n",
            "Epoch 64, Loss(train/val) 0.71230/0.55245. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 0.69432/0.55477. Took 0.04 sec\n",
            "Epoch 66, Loss(train/val) 0.69454/0.55993. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 0.68084/0.55041. Took 0.04 sec\n",
            "Epoch 68, Loss(train/val) 0.67048/0.54220. Took 0.04 sec\n",
            "Epoch 69, Loss(train/val) 0.66241/0.53434. Took 0.05 sec\n",
            "Epoch 70, Loss(train/val) 0.65302/0.53554. Took 0.04 sec\n",
            "Epoch 71, Loss(train/val) 0.65296/0.53588. Took 0.04 sec\n",
            "Epoch 72, Loss(train/val) 0.64059/0.53408. Took 0.04 sec\n",
            "Epoch 73, Loss(train/val) 0.63044/0.52368. Took 0.04 sec\n",
            "Epoch 74, Loss(train/val) 0.62401/0.51383. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 0.60545/0.49796. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 0.60027/0.49085. Took 0.04 sec\n",
            "Epoch 77, Loss(train/val) 0.59914/0.48026. Took 0.04 sec\n",
            "Epoch 78, Loss(train/val) 0.57647/0.46325. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 0.58817/0.44415. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 0.57118/0.42661. Took 0.04 sec\n",
            "Epoch 81, Loss(train/val) 0.56332/0.40803. Took 0.04 sec\n",
            "Epoch 82, Loss(train/val) 0.56326/0.39346. Took 0.04 sec\n",
            "Epoch 83, Loss(train/val) 0.55524/0.38271. Took 0.04 sec\n",
            "Epoch 84, Loss(train/val) 0.54930/0.36775. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 0.54414/0.35600. Took 0.04 sec\n",
            "Epoch 86, Loss(train/val) 0.53989/0.34672. Took 0.04 sec\n",
            "Epoch 87, Loss(train/val) 0.53021/0.33860. Took 0.04 sec\n",
            "Epoch 88, Loss(train/val) 0.53580/0.32749. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 0.51492/0.31670. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 0.49813/0.30600. Took 0.04 sec\n",
            "Epoch 91, Loss(train/val) 0.51201/0.29662. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 0.49878/0.29113. Took 0.04 sec\n",
            "Epoch 93, Loss(train/val) 0.49979/0.28643. Took 0.04 sec\n",
            "Epoch 94, Loss(train/val) 0.48441/0.28307. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 0.48258/0.28072. Took 0.04 sec\n",
            "Epoch 96, Loss(train/val) 0.49518/0.27817. Took 0.05 sec\n",
            "Epoch 97, Loss(train/val) 0.47560/0.27673. Took 0.05 sec\n",
            "Epoch 98, Loss(train/val) 0.47210/0.27526. Took 0.04 sec\n",
            "Epoch 99, Loss(train/val) 0.47023/0.27440. Took 0.05 sec\n",
            "Epoch 100, Loss(train/val) 0.46815/0.27307. Took 0.05 sec\n",
            "Epoch 101, Loss(train/val) 0.45833/0.27221. Took 0.04 sec\n",
            "Epoch 102, Loss(train/val) 0.46047/0.27130. Took 0.04 sec\n",
            "Epoch 103, Loss(train/val) 0.45216/0.27061. Took 0.04 sec\n",
            "Epoch 104, Loss(train/val) 0.45233/0.26994. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 0.44720/0.26893. Took 0.04 sec\n",
            "Epoch 106, Loss(train/val) 0.45923/0.26858. Took 0.04 sec\n",
            "Epoch 107, Loss(train/val) 0.44284/0.26795. Took 0.04 sec\n",
            "Epoch 108, Loss(train/val) 0.44824/0.26752. Took 0.04 sec\n",
            "Epoch 109, Loss(train/val) 0.44690/0.26770. Took 0.05 sec\n",
            "Epoch 110, Loss(train/val) 0.43578/0.26626. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 0.43124/0.26558. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 0.42411/0.26497. Took 0.04 sec\n",
            "Epoch 113, Loss(train/val) 0.42494/0.26427. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 0.42925/0.26378. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 0.42050/0.26346. Took 0.05 sec\n",
            "Epoch 116, Loss(train/val) 0.41896/0.26280. Took 0.04 sec\n",
            "Epoch 117, Loss(train/val) 0.42103/0.26232. Took 0.04 sec\n",
            "Epoch 118, Loss(train/val) 0.42231/0.26221. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 0.40938/0.26191. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 0.41748/0.26164. Took 0.04 sec\n",
            "Epoch 121, Loss(train/val) 0.43414/0.26209. Took 0.04 sec\n",
            "Epoch 122, Loss(train/val) 0.42074/0.26251. Took 0.05 sec\n",
            "Epoch 123, Loss(train/val) 0.41196/0.26212. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 0.39393/0.26347. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 0.41977/0.26556. Took 0.04 sec\n",
            "Epoch 126, Loss(train/val) 0.40232/0.26335. Took 0.04 sec\n",
            "Epoch 127, Loss(train/val) 0.39624/0.26047. Took 0.04 sec\n",
            "Epoch 128, Loss(train/val) 0.40572/0.25947. Took 0.04 sec\n",
            "Epoch 129, Loss(train/val) 0.40050/0.25940. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 0.41164/0.25970. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 0.39218/0.25881. Took 0.04 sec\n",
            "Epoch 132, Loss(train/val) 0.39794/0.25932. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 0.39839/0.25796. Took 0.05 sec\n",
            "Epoch 134, Loss(train/val) 0.40562/0.25741. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 0.38722/0.25662. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 0.38906/0.25644. Took 0.04 sec\n",
            "Epoch 137, Loss(train/val) 0.39356/0.25610. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 0.38469/0.25623. Took 0.04 sec\n",
            "Epoch 139, Loss(train/val) 0.38677/0.25767. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 0.38548/0.26009. Took 0.04 sec\n",
            "Epoch 141, Loss(train/val) 0.37981/0.25856. Took 0.05 sec\n",
            "Epoch 142, Loss(train/val) 0.37923/0.25608. Took 0.04 sec\n",
            "Epoch 143, Loss(train/val) 0.38481/0.25321. Took 0.04 sec\n",
            "Epoch 144, Loss(train/val) 0.38049/0.25308. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 0.36451/0.25233. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 0.37627/0.25208. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 0.38725/0.25181. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 0.37348/0.25089. Took 0.05 sec\n",
            "Epoch 149, Loss(train/val) 0.36580/0.25033. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 0.37249/0.24997. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 0.37149/0.24916. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 0.36883/0.24883. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 0.36353/0.24904. Took 0.04 sec\n",
            "Epoch 154, Loss(train/val) 0.36369/0.24869. Took 0.05 sec\n",
            "Epoch 155, Loss(train/val) 0.36868/0.24802. Took 0.04 sec\n",
            "Epoch 156, Loss(train/val) 0.36472/0.24716. Took 0.04 sec\n",
            "Epoch 157, Loss(train/val) 0.36538/0.24796. Took 0.04 sec\n",
            "Epoch 158, Loss(train/val) 0.38853/0.24667. Took 0.04 sec\n",
            "Epoch 159, Loss(train/val) 0.36326/0.24611. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 0.36545/0.24546. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 0.35716/0.24593. Took 0.04 sec\n",
            "Epoch 162, Loss(train/val) 0.36675/0.24545. Took 0.04 sec\n",
            "Epoch 163, Loss(train/val) 0.35442/0.24398. Took 0.04 sec\n",
            "Epoch 164, Loss(train/val) 0.36336/0.24395. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 0.36340/0.24360. Took 0.04 sec\n",
            "Epoch 166, Loss(train/val) 0.35784/0.24236. Took 0.04 sec\n",
            "Epoch 167, Loss(train/val) 0.36265/0.24227. Took 0.05 sec\n",
            "Epoch 168, Loss(train/val) 0.35157/0.24354. Took 0.04 sec\n",
            "Epoch 169, Loss(train/val) 0.35522/0.24529. Took 0.05 sec\n",
            "Epoch 170, Loss(train/val) 0.35827/0.24613. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 0.34571/0.25067. Took 0.04 sec\n",
            "Epoch 172, Loss(train/val) 0.36225/0.24588. Took 0.04 sec\n",
            "Epoch 173, Loss(train/val) 0.36035/0.24226. Took 0.04 sec\n",
            "Epoch 174, Loss(train/val) 0.35688/0.24098. Took 0.05 sec\n",
            "Epoch 175, Loss(train/val) 0.35917/0.23923. Took 0.04 sec\n",
            "Epoch 176, Loss(train/val) 0.36754/0.23842. Took 0.04 sec\n",
            "Epoch 177, Loss(train/val) 0.35366/0.23934. Took 0.04 sec\n",
            "Epoch 178, Loss(train/val) 0.34784/0.24124. Took 0.04 sec\n",
            "Epoch 179, Loss(train/val) 0.34961/0.23955. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 0.35847/0.23841. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 0.35636/0.23602. Took 0.04 sec\n",
            "Epoch 182, Loss(train/val) 0.34875/0.23677. Took 0.04 sec\n",
            "Epoch 183, Loss(train/val) 0.34316/0.23739. Took 0.05 sec\n",
            "Epoch 184, Loss(train/val) 0.35387/0.23552. Took 0.05 sec\n",
            "Epoch 185, Loss(train/val) 0.34174/0.23499. Took 0.04 sec\n",
            "Epoch 186, Loss(train/val) 0.34949/0.23577. Took 0.04 sec\n",
            "Epoch 187, Loss(train/val) 0.35366/0.23729. Took 0.04 sec\n",
            "Epoch 188, Loss(train/val) 0.33437/0.23773. Took 0.04 sec\n",
            "Epoch 189, Loss(train/val) 0.35443/0.23733. Took 0.06 sec\n",
            "Epoch 190, Loss(train/val) 0.34373/0.23651. Took 0.04 sec\n",
            "Epoch 191, Loss(train/val) 0.34257/0.23383. Took 0.04 sec\n",
            "Epoch 192, Loss(train/val) 0.34771/0.23368. Took 0.04 sec\n",
            "Epoch 193, Loss(train/val) 0.35659/0.23258. Took 0.04 sec\n",
            "Epoch 194, Loss(train/val) 0.35238/0.23212. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 0.35148/0.23097. Took 0.04 sec\n",
            "Epoch 196, Loss(train/val) 0.33965/0.22997. Took 0.04 sec\n",
            "Epoch 197, Loss(train/val) 0.34325/0.22941. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 0.34872/0.23162. Took 0.04 sec\n",
            "Epoch 199, Loss(train/val) 0.34104/0.23355. Took 0.06 sec\n",
            "Epoch 200, Loss(train/val) 0.33819/0.23110. Took 0.04 sec\n",
            "Epoch 201, Loss(train/val) 0.33705/0.22961. Took 0.04 sec\n",
            "Epoch 202, Loss(train/val) 0.34166/0.22842. Took 0.04 sec\n",
            "Epoch 203, Loss(train/val) 0.33631/0.22834. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 0.33554/0.22813. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.33040/0.22777. Took 0.04 sec\n",
            "Epoch 206, Loss(train/val) 0.34054/0.22783. Took 0.04 sec\n",
            "Epoch 207, Loss(train/val) 0.32688/0.22727. Took 0.04 sec\n",
            "Epoch 208, Loss(train/val) 0.35441/0.22694. Took 0.04 sec\n",
            "Epoch 209, Loss(train/val) 0.33414/0.22692. Took 0.05 sec\n",
            "Epoch 210, Loss(train/val) 0.34044/0.22651. Took 0.04 sec\n",
            "Epoch 211, Loss(train/val) 0.33307/0.22636. Took 0.04 sec\n",
            "Epoch 212, Loss(train/val) 0.34001/0.22748. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 0.33300/0.22678. Took 0.05 sec\n",
            "Epoch 214, Loss(train/val) 0.32856/0.22587. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.32284/0.22561. Took 0.04 sec\n",
            "Epoch 216, Loss(train/val) 0.33683/0.22546. Took 0.04 sec\n",
            "Epoch 217, Loss(train/val) 0.33588/0.22519. Took 0.04 sec\n",
            "Epoch 218, Loss(train/val) 0.33192/0.22526. Took 0.04 sec\n",
            "Epoch 219, Loss(train/val) 0.33635/0.22515. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 0.33696/0.22513. Took 0.04 sec\n",
            "Epoch 221, Loss(train/val) 0.32942/0.22489. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 0.32864/0.22482. Took 0.04 sec\n",
            "Epoch 223, Loss(train/val) 0.33295/0.22483. Took 0.04 sec\n",
            "Epoch 224, Loss(train/val) 0.34232/0.22563. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 0.33131/0.22378. Took 0.04 sec\n",
            "Epoch 226, Loss(train/val) 0.32611/0.22323. Took 0.04 sec\n",
            "Epoch 227, Loss(train/val) 0.32470/0.22429. Took 0.04 sec\n",
            "Epoch 228, Loss(train/val) 0.33497/0.22263. Took 0.05 sec\n",
            "Epoch 229, Loss(train/val) 0.32791/0.22092. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 0.32557/0.21943. Took 0.04 sec\n",
            "Epoch 231, Loss(train/val) 0.32454/0.21956. Took 0.04 sec\n",
            "Epoch 232, Loss(train/val) 0.34702/0.21885. Took 0.05 sec\n",
            "Epoch 233, Loss(train/val) 0.31622/0.21872. Took 0.04 sec\n",
            "Epoch 234, Loss(train/val) 0.31768/0.21887. Took 0.06 sec\n",
            "Epoch 235, Loss(train/val) 0.33113/0.22059. Took 0.04 sec\n",
            "Epoch 236, Loss(train/val) 0.32733/0.21990. Took 0.04 sec\n",
            "Epoch 237, Loss(train/val) 0.32303/0.21813. Took 0.04 sec\n",
            "Epoch 238, Loss(train/val) 0.33514/0.21982. Took 0.04 sec\n",
            "Epoch 239, Loss(train/val) 0.32102/0.21756. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 0.31295/0.21789. Took 0.04 sec\n",
            "Epoch 241, Loss(train/val) 0.32873/0.21655. Took 0.04 sec\n",
            "Epoch 242, Loss(train/val) 0.30905/0.21688. Took 0.05 sec\n",
            "Epoch 243, Loss(train/val) 0.31198/0.21610. Took 0.04 sec\n",
            "Epoch 244, Loss(train/val) 0.32369/0.22049. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.31299/0.21815. Took 0.05 sec\n",
            "Epoch 246, Loss(train/val) 0.31033/0.21542. Took 0.04 sec\n",
            "Epoch 247, Loss(train/val) 0.30964/0.21473. Took 0.04 sec\n",
            "Epoch 248, Loss(train/val) 0.31284/0.21500. Took 0.04 sec\n",
            "Epoch 249, Loss(train/val) 0.31569/0.21602. Took 0.05 sec\n",
            "Epoch 250, Loss(train/val) 0.31717/0.21298. Took 0.05 sec\n",
            "Epoch 251, Loss(train/val) 0.30312/0.21382. Took 0.04 sec\n",
            "Epoch 252, Loss(train/val) 0.31700/0.22610. Took 0.04 sec\n",
            "Epoch 253, Loss(train/val) 0.31466/0.21918. Took 0.04 sec\n",
            "Epoch 254, Loss(train/val) 0.30503/0.21125. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.29670/0.21251. Took 0.04 sec\n",
            "Epoch 256, Loss(train/val) 0.30318/0.20990. Took 0.05 sec\n",
            "Epoch 257, Loss(train/val) 0.30059/0.22462. Took 0.05 sec\n",
            "Epoch 258, Loss(train/val) 0.30138/0.22983. Took 0.04 sec\n",
            "Epoch 259, Loss(train/val) 0.30819/0.21315. Took 0.05 sec\n",
            "Epoch 260, Loss(train/val) 0.30560/0.21102. Took 0.05 sec\n",
            "Epoch 261, Loss(train/val) 0.30761/0.20993. Took 0.04 sec\n",
            "Epoch 262, Loss(train/val) 0.30487/0.21020. Took 0.04 sec\n",
            "Epoch 263, Loss(train/val) 0.29635/0.20875. Took 0.04 sec\n",
            "Epoch 264, Loss(train/val) 0.29507/0.20820. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.29531/0.21218. Took 0.05 sec\n",
            "Epoch 266, Loss(train/val) 0.29222/0.21101. Took 0.04 sec\n",
            "Epoch 267, Loss(train/val) 0.29175/0.20609. Took 0.04 sec\n",
            "Epoch 268, Loss(train/val) 0.28753/0.20814. Took 0.05 sec\n",
            "Epoch 269, Loss(train/val) 0.29385/0.20852. Took 0.05 sec\n",
            "Epoch 270, Loss(train/val) 0.29644/0.20445. Took 0.04 sec\n",
            "Epoch 271, Loss(train/val) 0.29651/0.20424. Took 0.04 sec\n",
            "Epoch 272, Loss(train/val) 0.29713/0.20619. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 0.28827/0.20553. Took 0.04 sec\n",
            "Epoch 274, Loss(train/val) 0.30085/0.20702. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.28290/0.20309. Took 0.04 sec\n",
            "Epoch 276, Loss(train/val) 0.28335/0.20450. Took 0.05 sec\n",
            "Epoch 277, Loss(train/val) 0.28083/0.20202. Took 0.04 sec\n",
            "Epoch 278, Loss(train/val) 0.29599/0.20111. Took 0.05 sec\n",
            "Epoch 279, Loss(train/val) 0.28907/0.20125. Took 0.06 sec\n",
            "Epoch 280, Loss(train/val) 0.28208/0.20316. Took 0.05 sec\n",
            "Epoch 281, Loss(train/val) 0.28814/0.20070. Took 0.05 sec\n",
            "Epoch 282, Loss(train/val) 0.27829/0.19809. Took 0.06 sec\n",
            "Epoch 283, Loss(train/val) 0.28308/0.19992. Took 0.05 sec\n",
            "Epoch 284, Loss(train/val) 0.28220/0.20699. Took 0.06 sec\n",
            "Epoch 285, Loss(train/val) 0.27492/0.20737. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.28008/0.19817. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.26781/0.19527. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.29192/0.19573. Took 0.05 sec\n",
            "Epoch 289, Loss(train/val) 0.27205/0.19578. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.27086/0.19731. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.27364/0.19570. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.28763/0.19369. Took 0.05 sec\n",
            "Epoch 293, Loss(train/val) 0.28545/0.19518. Took 0.06 sec\n",
            "Epoch 294, Loss(train/val) 0.29105/0.19841. Took 0.05 sec\n",
            "Epoch 295, Loss(train/val) 0.27012/0.19218. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 0.29426/0.20057. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.27400/0.19144. Took 0.05 sec\n",
            "Epoch 298, Loss(train/val) 0.27583/0.20681. Took 0.07 sec\n",
            "Epoch 299, Loss(train/val) 0.28003/0.19503. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.27726/0.19039. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.27574/0.18967. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.27284/0.18780. Took 0.05 sec\n",
            "Epoch 303, Loss(train/val) 0.27153/0.19593. Took 0.06 sec\n",
            "Epoch 304, Loss(train/val) 0.28842/0.19502. Took 0.04 sec\n",
            "Epoch 305, Loss(train/val) 0.27366/0.19629. Took 0.04 sec\n",
            "Epoch 306, Loss(train/val) 0.26580/0.18895. Took 0.04 sec\n",
            "Epoch 307, Loss(train/val) 0.27050/0.19459. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.26608/0.19875. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.27798/0.19235. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.27187/0.18762. Took 0.05 sec\n",
            "Epoch 311, Loss(train/val) 0.27930/0.18684. Took 0.04 sec\n",
            "Epoch 312, Loss(train/val) 0.26451/0.18609. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 0.26860/0.18482. Took 0.04 sec\n",
            "Epoch 314, Loss(train/val) 0.26732/0.18666. Took 0.04 sec\n",
            "Epoch 315, Loss(train/val) 0.26692/0.19205. Took 0.04 sec\n",
            "Epoch 316, Loss(train/val) 0.27169/0.19400. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.26531/0.18729. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.26337/0.18462. Took 0.04 sec\n",
            "Epoch 319, Loss(train/val) 0.27379/0.18339. Took 0.04 sec\n",
            "Epoch 320, Loss(train/val) 0.28106/0.18945. Took 0.05 sec\n",
            "Epoch 321, Loss(train/val) 0.26235/0.18663. Took 0.04 sec\n",
            "Epoch 322, Loss(train/val) 0.28689/0.18245. Took 0.06 sec\n",
            "Epoch 323, Loss(train/val) 0.27746/0.18225. Took 0.04 sec\n",
            "Epoch 324, Loss(train/val) 0.27038/0.19348. Took 0.04 sec\n",
            "Epoch 325, Loss(train/val) 0.27866/0.19553. Took 0.05 sec\n",
            "Epoch 326, Loss(train/val) 0.26406/0.18442. Took 0.04 sec\n",
            "Epoch 327, Loss(train/val) 0.26500/0.18142. Took 0.05 sec\n",
            "Epoch 328, Loss(train/val) 0.26496/0.18142. Took 0.04 sec\n",
            "Epoch 329, Loss(train/val) 0.25988/0.18134. Took 0.04 sec\n",
            "Epoch 330, Loss(train/val) 0.26341/0.18128. Took 0.04 sec\n",
            "Epoch 331, Loss(train/val) 0.25654/0.18206. Took 0.04 sec\n",
            "Epoch 332, Loss(train/val) 0.26225/0.18409. Took 0.05 sec\n",
            "Epoch 333, Loss(train/val) 0.25928/0.18152. Took 0.05 sec\n",
            "Epoch 334, Loss(train/val) 0.27391/0.17969. Took 0.05 sec\n",
            "Epoch 335, Loss(train/val) 0.27357/0.18080. Took 0.04 sec\n",
            "Epoch 336, Loss(train/val) 0.25930/0.18790. Took 0.04 sec\n",
            "Epoch 337, Loss(train/val) 0.27270/0.18715. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.26354/0.18222. Took 0.04 sec\n",
            "Epoch 339, Loss(train/val) 0.26252/0.17885. Took 0.04 sec\n",
            "Epoch 340, Loss(train/val) 0.26770/0.17863. Took 0.04 sec\n",
            "Epoch 341, Loss(train/val) 0.25822/0.18136. Took 0.04 sec\n",
            "Epoch 342, Loss(train/val) 0.25609/0.18205. Took 0.06 sec\n",
            "Epoch 343, Loss(train/val) 0.27201/0.17967. Took 0.04 sec\n",
            "Epoch 344, Loss(train/val) 0.26229/0.17995. Took 0.04 sec\n",
            "Epoch 345, Loss(train/val) 0.27334/0.17887. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.27373/0.18219. Took 0.04 sec\n",
            "Epoch 347, Loss(train/val) 0.26663/0.18030. Took 0.05 sec\n",
            "Epoch 348, Loss(train/val) 0.27699/0.18460. Took 0.04 sec\n",
            "Epoch 349, Loss(train/val) 0.25763/0.18965. Took 0.04 sec\n",
            "Epoch 350, Loss(train/val) 0.26224/0.18214. Took 0.04 sec\n",
            "Epoch 351, Loss(train/val) 0.27559/0.18145. Took 0.04 sec\n",
            "Epoch 352, Loss(train/val) 0.26005/0.17854. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 0.26249/0.17804. Took 0.04 sec\n",
            "Epoch 354, Loss(train/val) 0.27181/0.17750. Took 0.04 sec\n",
            "Epoch 355, Loss(train/val) 0.25489/0.18038. Took 0.04 sec\n",
            "Epoch 356, Loss(train/val) 0.26057/0.17986. Took 0.04 sec\n",
            "Epoch 357, Loss(train/val) 0.25092/0.18404. Took 0.05 sec\n",
            "Epoch 358, Loss(train/val) 0.26754/0.18097. Took 0.04 sec\n",
            "Epoch 359, Loss(train/val) 0.26925/0.18069. Took 0.04 sec\n",
            "Epoch 360, Loss(train/val) 0.26689/0.17863. Took 0.04 sec\n",
            "Epoch 361, Loss(train/val) 0.25813/0.17743. Took 0.04 sec\n",
            "Epoch 362, Loss(train/val) 0.25723/0.17711. Took 0.05 sec\n",
            "Epoch 363, Loss(train/val) 0.27042/0.17754. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 0.25335/0.17768. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.24491/0.18059. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.24972/0.17890. Took 0.04 sec\n",
            "Epoch 367, Loss(train/val) 0.26107/0.17916. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.25730/0.18111. Took 0.04 sec\n",
            "Epoch 369, Loss(train/val) 0.25294/0.18717. Took 0.04 sec\n",
            "Epoch 370, Loss(train/val) 0.25175/0.18058. Took 0.04 sec\n",
            "Epoch 371, Loss(train/val) 0.25785/0.17593. Took 0.04 sec\n",
            "Epoch 372, Loss(train/val) 0.25344/0.17502. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 0.25910/0.17463. Took 0.04 sec\n",
            "Epoch 374, Loss(train/val) 0.25263/0.18025. Took 0.04 sec\n",
            "Epoch 375, Loss(train/val) 0.25500/0.18475. Took 0.04 sec\n",
            "Epoch 376, Loss(train/val) 0.26603/0.18867. Took 0.04 sec\n",
            "Epoch 377, Loss(train/val) 0.26195/0.17679. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.25631/0.17476. Took 0.04 sec\n",
            "Epoch 379, Loss(train/val) 0.25652/0.17408. Took 0.04 sec\n",
            "Epoch 380, Loss(train/val) 0.25498/0.17450. Took 0.04 sec\n",
            "Epoch 381, Loss(train/val) 0.26365/0.17469. Took 0.04 sec\n",
            "Epoch 382, Loss(train/val) 0.25260/0.17562. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.25114/0.18379. Took 0.04 sec\n",
            "Epoch 384, Loss(train/val) 0.26140/0.18535. Took 0.04 sec\n",
            "Epoch 385, Loss(train/val) 0.25083/0.18157. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.27088/0.17561. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.24309/0.17498. Took 0.05 sec\n",
            "Epoch 388, Loss(train/val) 0.25881/0.17501. Took 0.04 sec\n",
            "Epoch 389, Loss(train/val) 0.25830/0.17534. Took 0.04 sec\n",
            "Epoch 390, Loss(train/val) 0.25458/0.17748. Took 0.04 sec\n",
            "Epoch 391, Loss(train/val) 0.27719/0.17409. Took 0.04 sec\n",
            "Epoch 392, Loss(train/val) 0.25678/0.17624. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.26636/0.17770. Took 0.04 sec\n",
            "Epoch 394, Loss(train/val) 0.25389/0.17467. Took 0.05 sec\n",
            "Epoch 395, Loss(train/val) 0.26058/0.17377. Took 0.04 sec\n",
            "Epoch 396, Loss(train/val) 0.25946/0.17730. Took 0.05 sec\n",
            "Epoch 397, Loss(train/val) 0.25160/0.17415. Took 0.05 sec\n",
            "Epoch 398, Loss(train/val) 0.25043/0.17347. Took 0.04 sec\n",
            "Epoch 399, Loss(train/val) 0.26946/0.17465. Took 0.05 sec\n",
            "Epoch 400, Loss(train/val) 0.25248/0.17367. Took 0.04 sec\n",
            "Epoch 401, Loss(train/val) 0.27735/0.17516. Took 0.04 sec\n",
            "Epoch 402, Loss(train/val) 0.25646/0.17630. Took 0.05 sec\n",
            "Epoch 403, Loss(train/val) 0.25430/0.17786. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 0.24918/0.17632. Took 0.04 sec\n",
            "Epoch 405, Loss(train/val) 0.25402/0.17711. Took 0.04 sec\n",
            "Epoch 406, Loss(train/val) 0.24430/0.17687. Took 0.04 sec\n",
            "Epoch 407, Loss(train/val) 0.27306/0.17740. Took 0.05 sec\n",
            "Epoch 408, Loss(train/val) 0.25001/0.17880. Took 0.05 sec\n",
            "Epoch 409, Loss(train/val) 0.25577/0.18168. Took 0.05 sec\n",
            "Epoch 410, Loss(train/val) 0.25635/0.17397. Took 0.04 sec\n",
            "Epoch 411, Loss(train/val) 0.24610/0.17217. Took 0.04 sec\n",
            "Epoch 412, Loss(train/val) 0.25213/0.17214. Took 0.05 sec\n",
            "Epoch 413, Loss(train/val) 0.24871/0.17322. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.24659/0.17357. Took 0.04 sec\n",
            "Epoch 415, Loss(train/val) 0.25159/0.17284. Took 0.04 sec\n",
            "Epoch 416, Loss(train/val) 0.24834/0.17191. Took 0.05 sec\n",
            "Epoch 417, Loss(train/val) 0.25788/0.17211. Took 0.05 sec\n",
            "Epoch 418, Loss(train/val) 0.25036/0.17390. Took 0.04 sec\n",
            "Epoch 419, Loss(train/val) 0.25556/0.17345. Took 0.04 sec\n",
            "Epoch 420, Loss(train/val) 0.26070/0.17174. Took 0.04 sec\n",
            "Epoch 421, Loss(train/val) 0.25627/0.17257. Took 0.04 sec\n",
            "Epoch 422, Loss(train/val) 0.24596/0.17172. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.24543/0.17205. Took 0.04 sec\n",
            "Epoch 424, Loss(train/val) 0.24371/0.17342. Took 0.04 sec\n",
            "Epoch 425, Loss(train/val) 0.26133/0.17454. Took 0.05 sec\n",
            "Epoch 426, Loss(train/val) 0.25309/0.17810. Took 0.04 sec\n",
            "Epoch 427, Loss(train/val) 0.24634/0.17635. Took 0.05 sec\n",
            "Epoch 428, Loss(train/val) 0.25297/0.17458. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.24059/0.17599. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.24779/0.17201. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.24191/0.17262. Took 0.05 sec\n",
            "Epoch 432, Loss(train/val) 0.25215/0.17205. Took 0.06 sec\n",
            "Epoch 433, Loss(train/val) 0.25008/0.17181. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.25176/0.17134. Took 0.04 sec\n",
            "Epoch 435, Loss(train/val) 0.25642/0.17067. Took 0.04 sec\n",
            "Epoch 436, Loss(train/val) 0.24596/0.17180. Took 0.04 sec\n",
            "Epoch 437, Loss(train/val) 0.25543/0.17414. Took 0.05 sec\n",
            "Epoch 438, Loss(train/val) 0.24129/0.17420. Took 0.04 sec\n",
            "Epoch 439, Loss(train/val) 0.25913/0.17686. Took 0.04 sec\n",
            "Epoch 440, Loss(train/val) 0.25249/0.17455. Took 0.04 sec\n",
            "Epoch 441, Loss(train/val) 0.24962/0.17355. Took 0.04 sec\n",
            "Epoch 442, Loss(train/val) 0.25328/0.17101. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 0.24345/0.17137. Took 0.04 sec\n",
            "Epoch 444, Loss(train/val) 0.24810/0.17309. Took 0.04 sec\n",
            "Epoch 445, Loss(train/val) 0.24838/0.17315. Took 0.04 sec\n",
            "Epoch 446, Loss(train/val) 0.24658/0.17372. Took 0.04 sec\n",
            "Epoch 447, Loss(train/val) 0.24872/0.17267. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 0.25643/0.17059. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 0.25332/0.17038. Took 0.04 sec\n",
            "Epoch 450, Loss(train/val) 0.24794/0.16942. Took 0.04 sec\n",
            "Epoch 451, Loss(train/val) 0.26091/0.17025. Took 0.04 sec\n",
            "Epoch 452, Loss(train/val) 0.26554/0.17220. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.24233/0.17088. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.24903/0.16977. Took 0.04 sec\n",
            "Epoch 455, Loss(train/val) 0.25295/0.17003. Took 0.04 sec\n",
            "Epoch 456, Loss(train/val) 0.26058/0.17322. Took 0.04 sec\n",
            "Epoch 457, Loss(train/val) 0.24340/0.17074. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.24665/0.16915. Took 0.05 sec\n",
            "Epoch 459, Loss(train/val) 0.25150/0.16982. Took 0.05 sec\n",
            "Epoch 460, Loss(train/val) 0.24756/0.17145. Took 0.04 sec\n",
            "Epoch 461, Loss(train/val) 0.24537/0.17721. Took 0.04 sec\n",
            "Epoch 462, Loss(train/val) 0.25138/0.18044. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.23687/0.18063. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.24260/0.17338. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.24148/0.16916. Took 0.04 sec\n",
            "Epoch 466, Loss(train/val) 0.26502/0.16926. Took 0.04 sec\n",
            "Epoch 467, Loss(train/val) 0.24869/0.16901. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.26786/0.16914. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 0.26870/0.16937. Took 0.04 sec\n",
            "Epoch 470, Loss(train/val) 0.24371/0.16961. Took 0.04 sec\n",
            "Epoch 471, Loss(train/val) 0.24774/0.17043. Took 0.04 sec\n",
            "Epoch 472, Loss(train/val) 0.24138/0.16868. Took 0.05 sec\n",
            "Epoch 473, Loss(train/val) 0.27076/0.16917. Took 0.04 sec\n",
            "Epoch 474, Loss(train/val) 0.25776/0.17309. Took 0.04 sec\n",
            "Epoch 475, Loss(train/val) 0.25371/0.17614. Took 0.06 sec\n",
            "Epoch 476, Loss(train/val) 0.24890/0.17433. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.25038/0.16990. Took 0.05 sec\n",
            "Epoch 478, Loss(train/val) 0.24277/0.17062. Took 0.05 sec\n",
            "Epoch 479, Loss(train/val) 0.25839/0.17008. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.26347/0.16867. Took 0.04 sec\n",
            "Epoch 481, Loss(train/val) 0.24506/0.16804. Took 0.04 sec\n",
            "Epoch 482, Loss(train/val) 0.23958/0.16838. Took 0.05 sec\n",
            "Epoch 483, Loss(train/val) 0.25097/0.16856. Took 0.05 sec\n",
            "Epoch 484, Loss(train/val) 0.25296/0.16868. Took 0.04 sec\n",
            "Epoch 485, Loss(train/val) 0.23873/0.16974. Took 0.04 sec\n",
            "Epoch 486, Loss(train/val) 0.24718/0.17158. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.24984/0.17166. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.25186/0.16863. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.25289/0.16782. Took 0.04 sec\n",
            "Epoch 490, Loss(train/val) 0.25585/0.16752. Took 0.04 sec\n",
            "Epoch 491, Loss(train/val) 0.24215/0.16996. Took 0.04 sec\n",
            "Epoch 492, Loss(train/val) 0.25227/0.17127. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.24195/0.17427. Took 0.04 sec\n",
            "Epoch 494, Loss(train/val) 0.24813/0.17186. Took 0.04 sec\n",
            "Epoch 495, Loss(train/val) 0.23942/0.16944. Took 0.04 sec\n",
            "Epoch 496, Loss(train/val) 0.26385/0.16694. Took 0.04 sec\n",
            "Epoch 497, Loss(train/val) 0.25112/0.16707. Took 0.06 sec\n",
            "Epoch 498, Loss(train/val) 0.24202/0.16810. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.25170/0.17227. Took 0.05 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=500, exp_name='exp2_hidn', hid_dim=32, input_dim=1, l2=1e-05, lr=0.0001, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.05795/0.37535. Took 0.05 sec\n",
            "Epoch 1, Loss(train/val) 1.05747/0.37438. Took 0.05 sec\n",
            "Epoch 2, Loss(train/val) 1.05864/0.37357. Took 0.05 sec\n",
            "Epoch 3, Loss(train/val) 1.05575/0.37287. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.05883/0.37227. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.05514/0.37174. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.06090/0.37128. Took 0.06 sec\n",
            "Epoch 7, Loss(train/val) 1.05899/0.37086. Took 0.05 sec\n",
            "Epoch 8, Loss(train/val) 1.05187/0.37048. Took 0.05 sec\n",
            "Epoch 9, Loss(train/val) 1.05884/0.37011. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.04937/0.36975. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.05435/0.36940. Took 0.06 sec\n",
            "Epoch 12, Loss(train/val) 1.05188/0.36907. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 1.04926/0.36875. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.05782/0.36844. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.04956/0.36811. Took 0.05 sec\n",
            "Epoch 16, Loss(train/val) 1.05584/0.36778. Took 0.05 sec\n",
            "Epoch 17, Loss(train/val) 1.05764/0.36763. Took 0.06 sec\n",
            "Epoch 18, Loss(train/val) 1.05105/0.36766. Took 0.05 sec\n",
            "Epoch 19, Loss(train/val) 1.05553/0.36774. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 1.04528/0.36790. Took 0.05 sec\n",
            "Epoch 21, Loss(train/val) 1.03997/0.36817. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 1.05264/0.36874. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 1.05136/0.36953. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 1.05399/0.37056. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 1.03849/0.37186. Took 0.05 sec\n",
            "Epoch 26, Loss(train/val) 1.04622/0.37349. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.05274/0.37562. Took 0.05 sec\n",
            "Epoch 28, Loss(train/val) 1.04360/0.37840. Took 0.05 sec\n",
            "Epoch 29, Loss(train/val) 1.04683/0.38087. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 1.03832/0.38350. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 1.04324/0.38709. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 1.03150/0.39178. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 1.02947/0.39807. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 1.02642/0.40595. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 1.02034/0.41539. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 1.01338/0.42491. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.01079/0.43561. Took 0.05 sec\n",
            "Epoch 38, Loss(train/val) 1.00738/0.44700. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 0.99971/0.45895. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 0.98700/0.46982. Took 0.06 sec\n",
            "Epoch 41, Loss(train/val) 0.97823/0.47872. Took 0.06 sec\n",
            "Epoch 42, Loss(train/val) 0.96119/0.48359. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 0.95060/0.48530. Took 0.05 sec\n",
            "Epoch 44, Loss(train/val) 0.92809/0.48565. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 0.91652/0.48544. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 0.88899/0.48634. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 0.86551/0.48761. Took 0.05 sec\n",
            "Epoch 48, Loss(train/val) 0.84289/0.48781. Took 0.05 sec\n",
            "Epoch 49, Loss(train/val) 0.81168/0.48478. Took 0.05 sec\n",
            "Epoch 50, Loss(train/val) 0.79968/0.47490. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 0.77899/0.45455. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 0.76280/0.42667. Took 0.05 sec\n",
            "Epoch 53, Loss(train/val) 0.76107/0.39658. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 0.74963/0.36441. Took 0.05 sec\n",
            "Epoch 55, Loss(train/val) 0.74252/0.34497. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 0.72921/0.33681. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 0.72202/0.34260. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 0.71521/0.35694. Took 0.05 sec\n",
            "Epoch 59, Loss(train/val) 0.70597/0.38475. Took 0.05 sec\n",
            "Epoch 60, Loss(train/val) 0.69678/0.42564. Took 0.05 sec\n",
            "Epoch 61, Loss(train/val) 0.68326/0.46032. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 0.67668/0.49877. Took 0.05 sec\n",
            "Epoch 63, Loss(train/val) 0.66852/0.53441. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 0.66685/0.56152. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 0.64821/0.58377. Took 0.05 sec\n",
            "Epoch 66, Loss(train/val) 0.64968/0.59838. Took 0.05 sec\n",
            "Epoch 67, Loss(train/val) 0.64244/0.61170. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 0.62970/0.61680. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 0.61950/0.60664. Took 0.05 sec\n",
            "Epoch 70, Loss(train/val) 0.61693/0.60231. Took 0.05 sec\n",
            "Epoch 71, Loss(train/val) 0.61189/0.58368. Took 0.05 sec\n",
            "Epoch 72, Loss(train/val) 0.59902/0.56862. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 0.61123/0.54824. Took 0.05 sec\n",
            "Epoch 74, Loss(train/val) 0.58203/0.52228. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 0.58568/0.49697. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 0.56724/0.47513. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 0.57173/0.45188. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 0.56954/0.43705. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 0.56079/0.41310. Took 0.06 sec\n",
            "Epoch 80, Loss(train/val) 0.55612/0.38660. Took 0.05 sec\n",
            "Epoch 81, Loss(train/val) 0.55722/0.37519. Took 0.05 sec\n",
            "Epoch 82, Loss(train/val) 0.54183/0.37717. Took 0.05 sec\n",
            "Epoch 83, Loss(train/val) 0.53717/0.38508. Took 0.05 sec\n",
            "Epoch 84, Loss(train/val) 0.54019/0.38072. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 0.52174/0.37760. Took 0.05 sec\n",
            "Epoch 86, Loss(train/val) 0.52005/0.34922. Took 0.05 sec\n",
            "Epoch 87, Loss(train/val) 0.52016/0.33339. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 0.51527/0.32016. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 0.51202/0.31637. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 0.51226/0.31127. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 0.50065/0.29734. Took 0.05 sec\n",
            "Epoch 92, Loss(train/val) 0.49344/0.28687. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 0.49020/0.27623. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 0.47611/0.26649. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 0.48303/0.26671. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 0.46744/0.27074. Took 0.05 sec\n",
            "Epoch 97, Loss(train/val) 0.47454/0.27426. Took 0.05 sec\n",
            "Epoch 98, Loss(train/val) 0.46607/0.27232. Took 0.05 sec\n",
            "Epoch 99, Loss(train/val) 0.46340/0.26240. Took 0.05 sec\n",
            "Epoch 100, Loss(train/val) 0.45253/0.25813. Took 0.06 sec\n",
            "Epoch 101, Loss(train/val) 0.45071/0.25519. Took 0.05 sec\n",
            "Epoch 102, Loss(train/val) 0.44731/0.25104. Took 0.05 sec\n",
            "Epoch 103, Loss(train/val) 0.45282/0.25047. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 0.44610/0.24991. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 0.43960/0.24855. Took 0.05 sec\n",
            "Epoch 106, Loss(train/val) 0.43972/0.24758. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 0.43000/0.24886. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 0.42696/0.24910. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 0.42844/0.25162. Took 0.05 sec\n",
            "Epoch 110, Loss(train/val) 0.42754/0.25318. Took 0.05 sec\n",
            "Epoch 111, Loss(train/val) 0.41780/0.25352. Took 0.05 sec\n",
            "Epoch 112, Loss(train/val) 0.41002/0.25368. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 0.41596/0.24892. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 0.41169/0.24062. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 0.40824/0.24038. Took 0.06 sec\n",
            "Epoch 116, Loss(train/val) 0.40847/0.23936. Took 0.05 sec\n",
            "Epoch 117, Loss(train/val) 0.40610/0.24363. Took 0.05 sec\n",
            "Epoch 118, Loss(train/val) 0.40273/0.24819. Took 0.05 sec\n",
            "Epoch 119, Loss(train/val) 0.40548/0.25532. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 0.39772/0.24768. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 0.39871/0.24088. Took 0.05 sec\n",
            "Epoch 122, Loss(train/val) 0.39829/0.23890. Took 0.05 sec\n",
            "Epoch 123, Loss(train/val) 0.38992/0.24290. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 0.39135/0.24176. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 0.38128/0.23998. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 0.38896/0.23735. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 0.38273/0.23554. Took 0.05 sec\n",
            "Epoch 128, Loss(train/val) 0.38317/0.23548. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 0.39377/0.23437. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 0.38465/0.23509. Took 0.06 sec\n",
            "Epoch 131, Loss(train/val) 0.37869/0.23418. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 0.38070/0.23584. Took 0.05 sec\n",
            "Epoch 133, Loss(train/val) 0.37485/0.23945. Took 0.05 sec\n",
            "Epoch 134, Loss(train/val) 0.37459/0.24332. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 0.39014/0.24189. Took 0.05 sec\n",
            "Epoch 136, Loss(train/val) 0.38188/0.23723. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 0.37927/0.23358. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 0.37040/0.23303. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 0.37057/0.23622. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 0.38510/0.24236. Took 0.06 sec\n",
            "Epoch 141, Loss(train/val) 0.37506/0.23843. Took 0.06 sec\n",
            "Epoch 142, Loss(train/val) 0.37073/0.23931. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 0.36647/0.23490. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 0.35888/0.23285. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 0.36777/0.23423. Took 0.06 sec\n",
            "Epoch 146, Loss(train/val) 0.35128/0.23439. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 0.36457/0.23723. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 0.36366/0.23496. Took 0.05 sec\n",
            "Epoch 149, Loss(train/val) 0.36894/0.23240. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 0.35945/0.23201. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 0.36820/0.23163. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 0.35944/0.23334. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 0.35542/0.23392. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 0.36032/0.23170. Took 0.05 sec\n",
            "Epoch 155, Loss(train/val) 0.35631/0.23115. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 0.36265/0.23119. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 0.36059/0.23164. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 0.36390/0.23218. Took 0.05 sec\n",
            "Epoch 159, Loss(train/val) 0.35420/0.23381. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 0.37728/0.24199. Took 0.05 sec\n",
            "Epoch 161, Loss(train/val) 0.36313/0.23882. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 0.36035/0.23472. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 0.35424/0.23288. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 0.37004/0.23186. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 0.35370/0.23190. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.36428/0.23223. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 0.36136/0.23306. Took 0.05 sec\n",
            "Epoch 168, Loss(train/val) 0.35333/0.23259. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 0.35087/0.23183. Took 0.06 sec\n",
            "Epoch 170, Loss(train/val) 0.34853/0.23200. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 0.35242/0.23220. Took 0.05 sec\n",
            "Epoch 172, Loss(train/val) 0.35224/0.23768. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 0.35912/0.23990. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 0.35710/0.23045. Took 0.05 sec\n",
            "Epoch 175, Loss(train/val) 0.34829/0.23102. Took 0.05 sec\n",
            "Epoch 176, Loss(train/val) 0.35056/0.23345. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 0.35134/0.23442. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 0.35848/0.24137. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 0.35731/0.24151. Took 0.06 sec\n",
            "Epoch 180, Loss(train/val) 0.34704/0.24159. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 0.35021/0.23185. Took 0.06 sec\n",
            "Epoch 182, Loss(train/val) 0.36097/0.22950. Took 0.06 sec\n",
            "Epoch 183, Loss(train/val) 0.35177/0.22847. Took 0.06 sec\n",
            "Epoch 184, Loss(train/val) 0.35904/0.23024. Took 0.05 sec\n",
            "Epoch 185, Loss(train/val) 0.36638/0.22884. Took 0.05 sec\n",
            "Epoch 186, Loss(train/val) 0.36328/0.23250. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 0.35911/0.24020. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 0.34871/0.23774. Took 0.06 sec\n",
            "Epoch 189, Loss(train/val) 0.35653/0.23097. Took 0.05 sec\n",
            "Epoch 190, Loss(train/val) 0.34321/0.22895. Took 0.05 sec\n",
            "Epoch 191, Loss(train/val) 0.35250/0.22867. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 0.34525/0.23142. Took 0.05 sec\n",
            "Epoch 193, Loss(train/val) 0.34906/0.23990. Took 0.06 sec\n",
            "Epoch 194, Loss(train/val) 0.35090/0.23832. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 0.35903/0.23150. Took 0.05 sec\n",
            "Epoch 196, Loss(train/val) 0.35336/0.22801. Took 0.05 sec\n",
            "Epoch 197, Loss(train/val) 0.35165/0.22923. Took 0.05 sec\n",
            "Epoch 198, Loss(train/val) 0.34477/0.23046. Took 0.05 sec\n",
            "Epoch 199, Loss(train/val) 0.34886/0.23326. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 0.34338/0.23294. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 0.35274/0.23039. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 0.33289/0.23013. Took 0.06 sec\n",
            "Epoch 203, Loss(train/val) 0.34978/0.23259. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 0.34899/0.22685. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.34928/0.22662. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 0.35300/0.22603. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 0.35094/0.22665. Took 0.05 sec\n",
            "Epoch 208, Loss(train/val) 0.35456/0.22876. Took 0.05 sec\n",
            "Epoch 209, Loss(train/val) 0.34779/0.23237. Took 0.05 sec\n",
            "Epoch 210, Loss(train/val) 0.34552/0.23029. Took 0.05 sec\n",
            "Epoch 211, Loss(train/val) 0.35362/0.23055. Took 0.05 sec\n",
            "Epoch 212, Loss(train/val) 0.34946/0.22989. Took 0.06 sec\n",
            "Epoch 213, Loss(train/val) 0.34229/0.22715. Took 0.05 sec\n",
            "Epoch 214, Loss(train/val) 0.33903/0.22702. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.34713/0.22673. Took 0.05 sec\n",
            "Epoch 216, Loss(train/val) 0.35116/0.22673. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 0.34861/0.22533. Took 0.05 sec\n",
            "Epoch 218, Loss(train/val) 0.34295/0.22894. Took 0.05 sec\n",
            "Epoch 219, Loss(train/val) 0.35241/0.22628. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 0.33720/0.22603. Took 0.05 sec\n",
            "Epoch 221, Loss(train/val) 0.34959/0.22891. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 0.34418/0.22574. Took 0.06 sec\n",
            "Epoch 223, Loss(train/val) 0.34406/0.22492. Took 0.05 sec\n",
            "Epoch 224, Loss(train/val) 0.34628/0.22501. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 0.35009/0.22496. Took 0.05 sec\n",
            "Epoch 226, Loss(train/val) 0.35294/0.22509. Took 0.05 sec\n",
            "Epoch 227, Loss(train/val) 0.34596/0.22523. Took 0.06 sec\n",
            "Epoch 228, Loss(train/val) 0.33656/0.22636. Took 0.05 sec\n",
            "Epoch 229, Loss(train/val) 0.33773/0.22545. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 0.34177/0.22528. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 0.34434/0.22399. Took 0.05 sec\n",
            "Epoch 232, Loss(train/val) 0.34862/0.22595. Took 0.06 sec\n",
            "Epoch 233, Loss(train/val) 0.33434/0.22621. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 0.34235/0.22721. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 0.34316/0.22526. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 0.36293/0.22502. Took 0.05 sec\n",
            "Epoch 237, Loss(train/val) 0.33893/0.22417. Took 0.06 sec\n",
            "Epoch 238, Loss(train/val) 0.33786/0.22397. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 0.34236/0.22416. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 0.34451/0.22415. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 0.33562/0.22428. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 0.34302/0.22314. Took 0.06 sec\n",
            "Epoch 243, Loss(train/val) 0.33582/0.22214. Took 0.06 sec\n",
            "Epoch 244, Loss(train/val) 0.34157/0.22200. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.34504/0.22245. Took 0.05 sec\n",
            "Epoch 246, Loss(train/val) 0.33693/0.22406. Took 0.05 sec\n",
            "Epoch 247, Loss(train/val) 0.33804/0.22215. Took 0.06 sec\n",
            "Epoch 248, Loss(train/val) 0.36243/0.22189. Took 0.05 sec\n",
            "Epoch 249, Loss(train/val) 0.34278/0.22297. Took 0.05 sec\n",
            "Epoch 250, Loss(train/val) 0.33174/0.22392. Took 0.05 sec\n",
            "Epoch 251, Loss(train/val) 0.33817/0.22460. Took 0.06 sec\n",
            "Epoch 252, Loss(train/val) 0.34226/0.22479. Took 0.05 sec\n",
            "Epoch 253, Loss(train/val) 0.33662/0.22879. Took 0.05 sec\n",
            "Epoch 254, Loss(train/val) 0.32653/0.22559. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.33964/0.22126. Took 0.06 sec\n",
            "Epoch 256, Loss(train/val) 0.33587/0.22059. Took 0.05 sec\n",
            "Epoch 257, Loss(train/val) 0.34486/0.22003. Took 0.05 sec\n",
            "Epoch 258, Loss(train/val) 0.33408/0.22136. Took 0.05 sec\n",
            "Epoch 259, Loss(train/val) 0.33189/0.22148. Took 0.05 sec\n",
            "Epoch 260, Loss(train/val) 0.32808/0.21967. Took 0.05 sec\n",
            "Epoch 261, Loss(train/val) 0.33453/0.22484. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.33983/0.22590. Took 0.05 sec\n",
            "Epoch 263, Loss(train/val) 0.33196/0.22648. Took 0.06 sec\n",
            "Epoch 264, Loss(train/val) 0.32704/0.22276. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.33354/0.22058. Took 0.05 sec\n",
            "Epoch 266, Loss(train/val) 0.34571/0.21960. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 0.33683/0.21962. Took 0.05 sec\n",
            "Epoch 268, Loss(train/val) 0.32620/0.21842. Took 0.05 sec\n",
            "Epoch 269, Loss(train/val) 0.33959/0.21867. Took 0.05 sec\n",
            "Epoch 270, Loss(train/val) 0.33290/0.21781. Took 0.06 sec\n",
            "Epoch 271, Loss(train/val) 0.33178/0.22100. Took 0.05 sec\n",
            "Epoch 272, Loss(train/val) 0.32457/0.22368. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 0.32828/0.21892. Took 0.05 sec\n",
            "Epoch 274, Loss(train/val) 0.33330/0.21927. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.32744/0.21946. Took 0.06 sec\n",
            "Epoch 276, Loss(train/val) 0.32391/0.21951. Took 0.05 sec\n",
            "Epoch 277, Loss(train/val) 0.32535/0.21698. Took 0.05 sec\n",
            "Epoch 278, Loss(train/val) 0.34361/0.21636. Took 0.05 sec\n",
            "Epoch 279, Loss(train/val) 0.33239/0.21604. Took 0.05 sec\n",
            "Epoch 280, Loss(train/val) 0.32605/0.21717. Took 0.05 sec\n",
            "Epoch 281, Loss(train/val) 0.32434/0.21771. Took 0.05 sec\n",
            "Epoch 282, Loss(train/val) 0.33095/0.22275. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 0.33235/0.22482. Took 0.05 sec\n",
            "Epoch 284, Loss(train/val) 0.32123/0.22023. Took 0.06 sec\n",
            "Epoch 285, Loss(train/val) 0.32272/0.21740. Took 0.06 sec\n",
            "Epoch 286, Loss(train/val) 0.33752/0.21904. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.32688/0.21822. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.32615/0.21532. Took 0.05 sec\n",
            "Epoch 289, Loss(train/val) 0.32083/0.21582. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.32567/0.21602. Took 0.06 sec\n",
            "Epoch 291, Loss(train/val) 0.32436/0.21602. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.32605/0.21597. Took 0.05 sec\n",
            "Epoch 293, Loss(train/val) 0.33207/0.21640. Took 0.05 sec\n",
            "Epoch 294, Loss(train/val) 0.31590/0.21686. Took 0.05 sec\n",
            "Epoch 295, Loss(train/val) 0.32696/0.21714. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 0.31816/0.21726. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.31869/0.21490. Took 0.05 sec\n",
            "Epoch 298, Loss(train/val) 0.33897/0.21293. Took 0.05 sec\n",
            "Epoch 299, Loss(train/val) 0.33208/0.21139. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.33121/0.21241. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.32167/0.21318. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.32621/0.21392. Took 0.05 sec\n",
            "Epoch 303, Loss(train/val) 0.32635/0.21480. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 0.31303/0.21439. Took 0.06 sec\n",
            "Epoch 305, Loss(train/val) 0.31225/0.21336. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 0.31809/0.21190. Took 0.05 sec\n",
            "Epoch 307, Loss(train/val) 0.31700/0.21225. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.33200/0.21250. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.31351/0.21870. Took 0.06 sec\n",
            "Epoch 310, Loss(train/val) 0.32092/0.21451. Took 0.05 sec\n",
            "Epoch 311, Loss(train/val) 0.31476/0.21483. Took 0.05 sec\n",
            "Epoch 312, Loss(train/val) 0.32891/0.20864. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 0.31266/0.20896. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.32068/0.20889. Took 0.05 sec\n",
            "Epoch 315, Loss(train/val) 0.31230/0.20692. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.32345/0.20618. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.31332/0.20608. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.31000/0.20632. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.31009/0.20958. Took 0.05 sec\n",
            "Epoch 320, Loss(train/val) 0.30763/0.21019. Took 0.05 sec\n",
            "Epoch 321, Loss(train/val) 0.30613/0.21145. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.30752/0.20622. Took 0.05 sec\n",
            "Epoch 323, Loss(train/val) 0.31346/0.20581. Took 0.05 sec\n",
            "Epoch 324, Loss(train/val) 0.30811/0.20477. Took 0.06 sec\n",
            "Epoch 325, Loss(train/val) 0.30604/0.20549. Took 0.05 sec\n",
            "Epoch 326, Loss(train/val) 0.31237/0.20296. Took 0.05 sec\n",
            "Epoch 327, Loss(train/val) 0.30655/0.20602. Took 0.05 sec\n",
            "Epoch 328, Loss(train/val) 0.29102/0.21033. Took 0.05 sec\n",
            "Epoch 329, Loss(train/val) 0.32259/0.20815. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.31262/0.21100. Took 0.05 sec\n",
            "Epoch 331, Loss(train/val) 0.29972/0.20583. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 0.30490/0.20280. Took 0.05 sec\n",
            "Epoch 333, Loss(train/val) 0.30371/0.20348. Took 0.05 sec\n",
            "Epoch 334, Loss(train/val) 0.30092/0.19890. Took 0.05 sec\n",
            "Epoch 335, Loss(train/val) 0.28848/0.19829. Took 0.05 sec\n",
            "Epoch 336, Loss(train/val) 0.29930/0.20727. Took 0.05 sec\n",
            "Epoch 337, Loss(train/val) 0.31366/0.21505. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.29273/0.20052. Took 0.05 sec\n",
            "Epoch 339, Loss(train/val) 0.30245/0.19787. Took 0.06 sec\n",
            "Epoch 340, Loss(train/val) 0.29182/0.20112. Took 0.05 sec\n",
            "Epoch 341, Loss(train/val) 0.28780/0.19862. Took 0.05 sec\n",
            "Epoch 342, Loss(train/val) 0.28601/0.20005. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.29460/0.21498. Took 0.05 sec\n",
            "Epoch 344, Loss(train/val) 0.29807/0.21981. Took 0.05 sec\n",
            "Epoch 345, Loss(train/val) 0.29028/0.20100. Took 0.06 sec\n",
            "Epoch 346, Loss(train/val) 0.29040/0.19939. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 0.30752/0.19226. Took 0.05 sec\n",
            "Epoch 348, Loss(train/val) 0.28121/0.19118. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 0.28331/0.19189. Took 0.05 sec\n",
            "Epoch 350, Loss(train/val) 0.28200/0.20066. Took 0.05 sec\n",
            "Epoch 351, Loss(train/val) 0.28453/0.20468. Took 0.05 sec\n",
            "Epoch 352, Loss(train/val) 0.28134/0.20033. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 0.28094/0.19405. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 0.27514/0.18912. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 0.26988/0.18963. Took 0.05 sec\n",
            "Epoch 356, Loss(train/val) 0.27204/0.19070. Took 0.05 sec\n",
            "Epoch 357, Loss(train/val) 0.28006/0.19370. Took 0.05 sec\n",
            "Epoch 358, Loss(train/val) 0.28428/0.18933. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 0.28781/0.18638. Took 0.05 sec\n",
            "Epoch 360, Loss(train/val) 0.28520/0.19123. Took 0.05 sec\n",
            "Epoch 361, Loss(train/val) 0.27201/0.19052. Took 0.05 sec\n",
            "Epoch 362, Loss(train/val) 0.27205/0.18231. Took 0.05 sec\n",
            "Epoch 363, Loss(train/val) 0.26490/0.17709. Took 0.06 sec\n",
            "Epoch 364, Loss(train/val) 0.27846/0.18168. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.27499/0.19661. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.26474/0.18514. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 0.28686/0.18141. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.27230/0.17456. Took 0.05 sec\n",
            "Epoch 369, Loss(train/val) 0.27394/0.17746. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.27551/0.18852. Took 0.05 sec\n",
            "Epoch 371, Loss(train/val) 0.26209/0.19445. Took 0.04 sec\n",
            "Epoch 372, Loss(train/val) 0.26890/0.18399. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 0.28019/0.17260. Took 0.05 sec\n",
            "Epoch 374, Loss(train/val) 0.26289/0.17287. Took 0.05 sec\n",
            "Epoch 375, Loss(train/val) 0.28495/0.17633. Took 0.05 sec\n",
            "Epoch 376, Loss(train/val) 0.27103/0.17561. Took 0.05 sec\n",
            "Epoch 377, Loss(train/val) 0.26149/0.17700. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.27396/0.17598. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 0.26459/0.17799. Took 0.05 sec\n",
            "Epoch 380, Loss(train/val) 0.25728/0.17999. Took 0.04 sec\n",
            "Epoch 381, Loss(train/val) 0.27202/0.17667. Took 0.05 sec\n",
            "Epoch 382, Loss(train/val) 0.25777/0.17702. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.26210/0.17444. Took 0.06 sec\n",
            "Epoch 384, Loss(train/val) 0.27247/0.18421. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 0.26019/0.17310. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.25669/0.17302. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.27089/0.17726. Took 0.06 sec\n",
            "Epoch 388, Loss(train/val) 0.26275/0.17549. Took 0.05 sec\n",
            "Epoch 389, Loss(train/val) 0.26792/0.17172. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.26205/0.17088. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.26092/0.17067. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.26305/0.17456. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.25897/0.17387. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 0.25958/0.17496. Took 0.05 sec\n",
            "Epoch 395, Loss(train/val) 0.26021/0.17202. Took 0.05 sec\n",
            "Epoch 396, Loss(train/val) 0.27017/0.17368. Took 0.05 sec\n",
            "Epoch 397, Loss(train/val) 0.25355/0.17434. Took 0.05 sec\n",
            "Epoch 398, Loss(train/val) 0.26196/0.17195. Took 0.05 sec\n",
            "Epoch 399, Loss(train/val) 0.24976/0.17114. Took 0.05 sec\n",
            "Epoch 400, Loss(train/val) 0.25177/0.17135. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 0.24822/0.17327. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.26403/0.18209. Took 0.05 sec\n",
            "Epoch 403, Loss(train/val) 0.24750/0.17710. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 0.25215/0.16979. Took 0.05 sec\n",
            "Epoch 405, Loss(train/val) 0.25140/0.17579. Took 0.05 sec\n",
            "Epoch 406, Loss(train/val) 0.25075/0.16900. Took 0.05 sec\n",
            "Epoch 407, Loss(train/val) 0.25845/0.16942. Took 0.05 sec\n",
            "Epoch 408, Loss(train/val) 0.24915/0.17196. Took 0.06 sec\n",
            "Epoch 409, Loss(train/val) 0.27213/0.17134. Took 0.05 sec\n",
            "Epoch 410, Loss(train/val) 0.24985/0.17176. Took 0.05 sec\n",
            "Epoch 411, Loss(train/val) 0.26291/0.17014. Took 0.05 sec\n",
            "Epoch 412, Loss(train/val) 0.25915/0.16913. Took 0.05 sec\n",
            "Epoch 413, Loss(train/val) 0.25464/0.16935. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.24675/0.17028. Took 0.05 sec\n",
            "Epoch 415, Loss(train/val) 0.24653/0.17334. Took 0.05 sec\n",
            "Epoch 416, Loss(train/val) 0.26125/0.17284. Took 0.04 sec\n",
            "Epoch 417, Loss(train/val) 0.25644/0.17212. Took 0.05 sec\n",
            "Epoch 418, Loss(train/val) 0.24946/0.17260. Took 0.05 sec\n",
            "Epoch 419, Loss(train/val) 0.24841/0.17514. Took 0.05 sec\n",
            "Epoch 420, Loss(train/val) 0.24524/0.18813. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.24945/0.16988. Took 0.05 sec\n",
            "Epoch 422, Loss(train/val) 0.24352/0.16879. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.26830/0.16748. Took 0.05 sec\n",
            "Epoch 424, Loss(train/val) 0.25771/0.16845. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.25078/0.16786. Took 0.05 sec\n",
            "Epoch 426, Loss(train/val) 0.25700/0.16947. Took 0.05 sec\n",
            "Epoch 427, Loss(train/val) 0.24957/0.16854. Took 0.05 sec\n",
            "Epoch 428, Loss(train/val) 0.25509/0.16788. Took 0.06 sec\n",
            "Epoch 429, Loss(train/val) 0.25625/0.17017. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.27153/0.16906. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.24469/0.17024. Took 0.05 sec\n",
            "Epoch 432, Loss(train/val) 0.24594/0.17453. Took 0.05 sec\n",
            "Epoch 433, Loss(train/val) 0.25460/0.16905. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.26692/0.16744. Took 0.05 sec\n",
            "Epoch 435, Loss(train/val) 0.24794/0.16917. Took 0.05 sec\n",
            "Epoch 436, Loss(train/val) 0.24335/0.17240. Took 0.05 sec\n",
            "Epoch 437, Loss(train/val) 0.24865/0.17133. Took 0.05 sec\n",
            "Epoch 438, Loss(train/val) 0.26192/0.17227. Took 0.05 sec\n",
            "Epoch 439, Loss(train/val) 0.24803/0.16950. Took 0.05 sec\n",
            "Epoch 440, Loss(train/val) 0.26322/0.16663. Took 0.05 sec\n",
            "Epoch 441, Loss(train/val) 0.24723/0.16620. Took 0.04 sec\n",
            "Epoch 442, Loss(train/val) 0.24880/0.16645. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 0.24358/0.16794. Took 0.05 sec\n",
            "Epoch 444, Loss(train/val) 0.24692/0.17591. Took 0.06 sec\n",
            "Epoch 445, Loss(train/val) 0.26511/0.16917. Took 0.05 sec\n",
            "Epoch 446, Loss(train/val) 0.24498/0.16918. Took 0.05 sec\n",
            "Epoch 447, Loss(train/val) 0.25438/0.17019. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 0.25305/0.16518. Took 0.05 sec\n",
            "Epoch 449, Loss(train/val) 0.24049/0.16921. Took 0.05 sec\n",
            "Epoch 450, Loss(train/val) 0.24773/0.17576. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 0.24672/0.17590. Took 0.05 sec\n",
            "Epoch 452, Loss(train/val) 0.25521/0.17061. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.24607/0.16731. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.26266/0.16606. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 0.24985/0.16845. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.23511/0.16513. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.23893/0.16861. Took 0.06 sec\n",
            "Epoch 458, Loss(train/val) 0.23528/0.16421. Took 0.05 sec\n",
            "Epoch 459, Loss(train/val) 0.24982/0.16387. Took 0.05 sec\n",
            "Epoch 460, Loss(train/val) 0.25326/0.16482. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.27636/0.17334. Took 0.05 sec\n",
            "Epoch 462, Loss(train/val) 0.24054/0.16709. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.24933/0.16362. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.25836/0.16636. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.24245/0.17450. Took 0.05 sec\n",
            "Epoch 466, Loss(train/val) 0.24919/0.16786. Took 0.05 sec\n",
            "Epoch 467, Loss(train/val) 0.25372/0.16427. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.25479/0.16371. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 0.23970/0.16780. Took 0.05 sec\n",
            "Epoch 470, Loss(train/val) 0.25025/0.16660. Took 0.06 sec\n",
            "Epoch 471, Loss(train/val) 0.24680/0.16398. Took 0.05 sec\n",
            "Epoch 472, Loss(train/val) 0.25673/0.17022. Took 0.06 sec\n",
            "Epoch 473, Loss(train/val) 0.24338/0.16456. Took 0.05 sec\n",
            "Epoch 474, Loss(train/val) 0.23570/0.16363. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 0.24086/0.16558. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.23365/0.16350. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.24027/0.16368. Took 0.06 sec\n",
            "Epoch 478, Loss(train/val) 0.23623/0.16306. Took 0.05 sec\n",
            "Epoch 479, Loss(train/val) 0.23771/0.16327. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.24298/0.16349. Took 0.05 sec\n",
            "Epoch 481, Loss(train/val) 0.24111/0.16447. Took 0.05 sec\n",
            "Epoch 482, Loss(train/val) 0.24749/0.16484. Took 0.05 sec\n",
            "Epoch 483, Loss(train/val) 0.24215/0.16308. Took 0.05 sec\n",
            "Epoch 484, Loss(train/val) 0.23487/0.16308. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.24785/0.16510. Took 0.05 sec\n",
            "Epoch 486, Loss(train/val) 0.23410/0.16538. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.25348/0.16322. Took 0.06 sec\n",
            "Epoch 488, Loss(train/val) 0.23369/0.16291. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.24446/0.16627. Took 0.05 sec\n",
            "Epoch 490, Loss(train/val) 0.23839/0.17500. Took 0.05 sec\n",
            "Epoch 491, Loss(train/val) 0.25585/0.16281. Took 0.06 sec\n",
            "Epoch 492, Loss(train/val) 0.23829/0.16258. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.25943/0.17067. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.24089/0.17363. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.24287/0.16503. Took 0.05 sec\n",
            "Epoch 496, Loss(train/val) 0.25021/0.16127. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 0.24628/0.17113. Took 0.06 sec\n",
            "Epoch 498, Loss(train/val) 0.24765/0.16124. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.23515/0.16356. Took 0.05 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=500, exp_name='exp2_hidn', hid_dim=64, input_dim=1, l2=1e-05, lr=0.0001, n_layers=2, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.03514/0.35129. Took 0.04 sec\n",
            "Epoch 1, Loss(train/val) 1.01343/0.35134. Took 0.04 sec\n",
            "Epoch 2, Loss(train/val) 0.98865/0.35135. Took 0.04 sec\n",
            "Epoch 3, Loss(train/val) 0.95546/0.35131. Took 0.04 sec\n",
            "Epoch 4, Loss(train/val) 0.93865/0.35123. Took 0.04 sec\n",
            "Epoch 5, Loss(train/val) 0.90949/0.35110. Took 0.04 sec\n",
            "Epoch 6, Loss(train/val) 0.87726/0.35089. Took 0.04 sec\n",
            "Epoch 7, Loss(train/val) 0.85453/0.35061. Took 0.04 sec\n",
            "Epoch 8, Loss(train/val) 0.82521/0.35026. Took 0.05 sec\n",
            "Epoch 9, Loss(train/val) 0.80255/0.34984. Took 0.04 sec\n",
            "Epoch 10, Loss(train/val) 0.77791/0.34931. Took 0.04 sec\n",
            "Epoch 11, Loss(train/val) 0.75866/0.34863. Took 0.04 sec\n",
            "Epoch 12, Loss(train/val) 0.73423/0.34775. Took 0.04 sec\n",
            "Epoch 13, Loss(train/val) 0.70369/0.34658. Took 0.04 sec\n",
            "Epoch 14, Loss(train/val) 0.68349/0.34502. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 0.65283/0.34289. Took 0.04 sec\n",
            "Epoch 16, Loss(train/val) 0.63486/0.34015. Took 0.04 sec\n",
            "Epoch 17, Loss(train/val) 0.62444/0.33687. Took 0.04 sec\n",
            "Epoch 18, Loss(train/val) 0.60677/0.33281. Took 0.04 sec\n",
            "Epoch 19, Loss(train/val) 0.58857/0.32808. Took 0.04 sec\n",
            "Epoch 20, Loss(train/val) 0.56079/0.32256. Took 0.04 sec\n",
            "Epoch 21, Loss(train/val) 0.54795/0.31599. Took 0.04 sec\n",
            "Epoch 22, Loss(train/val) 0.53777/0.30839. Took 0.04 sec\n",
            "Epoch 23, Loss(train/val) 0.52579/0.29995. Took 0.04 sec\n",
            "Epoch 24, Loss(train/val) 0.50981/0.29090. Took 0.04 sec\n",
            "Epoch 25, Loss(train/val) 0.49199/0.28145. Took 0.04 sec\n",
            "Epoch 26, Loss(train/val) 0.47186/0.27203. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 0.45228/0.26331. Took 0.04 sec\n",
            "Epoch 28, Loss(train/val) 0.43927/0.25560. Took 0.04 sec\n",
            "Epoch 29, Loss(train/val) 0.44792/0.24973. Took 0.04 sec\n",
            "Epoch 30, Loss(train/val) 0.40914/0.24563. Took 0.04 sec\n",
            "Epoch 31, Loss(train/val) 0.40445/0.24237. Took 0.04 sec\n",
            "Epoch 32, Loss(train/val) 0.40255/0.24073. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 0.38796/0.23967. Took 0.04 sec\n",
            "Epoch 34, Loss(train/val) 0.38341/0.23858. Took 0.04 sec\n",
            "Epoch 35, Loss(train/val) 0.36367/0.23781. Took 0.04 sec\n",
            "Epoch 36, Loss(train/val) 0.36765/0.23633. Took 0.04 sec\n",
            "Epoch 37, Loss(train/val) 0.36928/0.23480. Took 0.04 sec\n",
            "Epoch 38, Loss(train/val) 0.35545/0.23211. Took 0.04 sec\n",
            "Epoch 39, Loss(train/val) 0.35522/0.23005. Took 0.04 sec\n",
            "Epoch 40, Loss(train/val) 0.34653/0.22680. Took 0.05 sec\n",
            "Epoch 41, Loss(train/val) 0.34286/0.22479. Took 0.04 sec\n",
            "Epoch 42, Loss(train/val) 0.33599/0.22247. Took 0.04 sec\n",
            "Epoch 43, Loss(train/val) 0.34473/0.22049. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 0.33777/0.21914. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 0.32727/0.21767. Took 0.04 sec\n",
            "Epoch 46, Loss(train/val) 0.33875/0.21798. Took 0.04 sec\n",
            "Epoch 47, Loss(train/val) 0.33543/0.21750. Took 0.04 sec\n",
            "Epoch 48, Loss(train/val) 0.32652/0.21644. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 0.33325/0.21626. Took 0.04 sec\n",
            "Epoch 50, Loss(train/val) 0.33673/0.21534. Took 0.04 sec\n",
            "Epoch 51, Loss(train/val) 0.32374/0.21463. Took 0.04 sec\n",
            "Epoch 52, Loss(train/val) 0.32242/0.21442. Took 0.04 sec\n",
            "Epoch 53, Loss(train/val) 0.31946/0.21296. Took 0.04 sec\n",
            "Epoch 54, Loss(train/val) 0.32000/0.21215. Took 0.04 sec\n",
            "Epoch 55, Loss(train/val) 0.32347/0.21349. Took 0.04 sec\n",
            "Epoch 56, Loss(train/val) 0.31778/0.21183. Took 0.04 sec\n",
            "Epoch 57, Loss(train/val) 0.31270/0.21069. Took 0.04 sec\n",
            "Epoch 58, Loss(train/val) 0.33375/0.21178. Took 0.04 sec\n",
            "Epoch 59, Loss(train/val) 0.32166/0.20987. Took 0.04 sec\n",
            "Epoch 60, Loss(train/val) 0.30532/0.20872. Took 0.04 sec\n",
            "Epoch 61, Loss(train/val) 0.31379/0.20893. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 0.30832/0.20885. Took 0.04 sec\n",
            "Epoch 63, Loss(train/val) 0.30284/0.20867. Took 0.04 sec\n",
            "Epoch 64, Loss(train/val) 0.29976/0.20863. Took 0.04 sec\n",
            "Epoch 65, Loss(train/val) 0.30740/0.20780. Took 0.04 sec\n",
            "Epoch 66, Loss(train/val) 0.30859/0.20612. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 0.31755/0.20667. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 0.30842/0.20675. Took 0.04 sec\n",
            "Epoch 69, Loss(train/val) 0.29982/0.20717. Took 0.04 sec\n",
            "Epoch 70, Loss(train/val) 0.30669/0.20768. Took 0.04 sec\n",
            "Epoch 71, Loss(train/val) 0.32404/0.20801. Took 0.04 sec\n",
            "Epoch 72, Loss(train/val) 0.32149/0.20482. Took 0.04 sec\n",
            "Epoch 73, Loss(train/val) 0.29620/0.20447. Took 0.05 sec\n",
            "Epoch 74, Loss(train/val) 0.30047/0.20408. Took 0.04 sec\n",
            "Epoch 75, Loss(train/val) 0.31376/0.20354. Took 0.04 sec\n",
            "Epoch 76, Loss(train/val) 0.30312/0.20243. Took 0.04 sec\n",
            "Epoch 77, Loss(train/val) 0.30251/0.20355. Took 0.04 sec\n",
            "Epoch 78, Loss(train/val) 0.30693/0.20329. Took 0.04 sec\n",
            "Epoch 79, Loss(train/val) 0.30309/0.20257. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 0.29513/0.20232. Took 0.04 sec\n",
            "Epoch 81, Loss(train/val) 0.32433/0.20148. Took 0.04 sec\n",
            "Epoch 82, Loss(train/val) 0.29695/0.20306. Took 0.04 sec\n",
            "Epoch 83, Loss(train/val) 0.29821/0.20207. Took 0.04 sec\n",
            "Epoch 84, Loss(train/val) 0.30793/0.19987. Took 0.04 sec\n",
            "Epoch 85, Loss(train/val) 0.29192/0.19945. Took 0.04 sec\n",
            "Epoch 86, Loss(train/val) 0.28598/0.19912. Took 0.04 sec\n",
            "Epoch 87, Loss(train/val) 0.30144/0.19854. Took 0.04 sec\n",
            "Epoch 88, Loss(train/val) 0.29697/0.19839. Took 0.04 sec\n",
            "Epoch 89, Loss(train/val) 0.29730/0.19834. Took 0.04 sec\n",
            "Epoch 90, Loss(train/val) 0.29001/0.19794. Took 0.04 sec\n",
            "Epoch 91, Loss(train/val) 0.29487/0.19715. Took 0.05 sec\n",
            "Epoch 92, Loss(train/val) 0.30465/0.19787. Took 0.04 sec\n",
            "Epoch 93, Loss(train/val) 0.29517/0.19714. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 0.28750/0.19660. Took 0.04 sec\n",
            "Epoch 95, Loss(train/val) 0.29258/0.19470. Took 0.04 sec\n",
            "Epoch 96, Loss(train/val) 0.28879/0.19441. Took 0.04 sec\n",
            "Epoch 97, Loss(train/val) 0.29129/0.19346. Took 0.04 sec\n",
            "Epoch 98, Loss(train/val) 0.28068/0.19373. Took 0.04 sec\n",
            "Epoch 99, Loss(train/val) 0.29605/0.19143. Took 0.04 sec\n",
            "Epoch 100, Loss(train/val) 0.28775/0.19178. Took 0.04 sec\n",
            "Epoch 101, Loss(train/val) 0.28124/0.19207. Took 0.04 sec\n",
            "Epoch 102, Loss(train/val) 0.27990/0.19166. Took 0.04 sec\n",
            "Epoch 103, Loss(train/val) 0.28771/0.19185. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 0.29743/0.19004. Took 0.04 sec\n",
            "Epoch 105, Loss(train/val) 0.28275/0.19093. Took 0.04 sec\n",
            "Epoch 106, Loss(train/val) 0.28581/0.18863. Took 0.04 sec\n",
            "Epoch 107, Loss(train/val) 0.29187/0.18786. Took 0.04 sec\n",
            "Epoch 108, Loss(train/val) 0.29230/0.18976. Took 0.04 sec\n",
            "Epoch 109, Loss(train/val) 0.28316/0.18995. Took 0.05 sec\n",
            "Epoch 110, Loss(train/val) 0.28213/0.18917. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 0.27872/0.18789. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 0.27730/0.18685. Took 0.04 sec\n",
            "Epoch 113, Loss(train/val) 0.27247/0.18715. Took 0.04 sec\n",
            "Epoch 114, Loss(train/val) 0.28843/0.18702. Took 0.04 sec\n",
            "Epoch 115, Loss(train/val) 0.27456/0.18649. Took 0.04 sec\n",
            "Epoch 116, Loss(train/val) 0.29255/0.18660. Took 0.04 sec\n",
            "Epoch 117, Loss(train/val) 0.27914/0.18619. Took 0.04 sec\n",
            "Epoch 118, Loss(train/val) 0.29113/0.18530. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 0.27838/0.18677. Took 0.04 sec\n",
            "Epoch 120, Loss(train/val) 0.27373/0.18728. Took 0.04 sec\n",
            "Epoch 121, Loss(train/val) 0.27664/0.18762. Took 0.05 sec\n",
            "Epoch 122, Loss(train/val) 0.28014/0.18418. Took 0.04 sec\n",
            "Epoch 123, Loss(train/val) 0.27584/0.18499. Took 0.04 sec\n",
            "Epoch 124, Loss(train/val) 0.27830/0.18340. Took 0.04 sec\n",
            "Epoch 125, Loss(train/val) 0.28455/0.18322. Took 0.04 sec\n",
            "Epoch 126, Loss(train/val) 0.28130/0.18292. Took 0.04 sec\n",
            "Epoch 127, Loss(train/val) 0.29469/0.18417. Took 0.04 sec\n",
            "Epoch 128, Loss(train/val) 0.26829/0.18305. Took 0.04 sec\n",
            "Epoch 129, Loss(train/val) 0.27535/0.18431. Took 0.04 sec\n",
            "Epoch 130, Loss(train/val) 0.27306/0.18397. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 0.27235/0.18447. Took 0.04 sec\n",
            "Epoch 132, Loss(train/val) 0.28085/0.18191. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 0.27689/0.18317. Took 0.04 sec\n",
            "Epoch 134, Loss(train/val) 0.27923/0.18255. Took 0.04 sec\n",
            "Epoch 135, Loss(train/val) 0.26951/0.18243. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 0.27882/0.18322. Took 0.04 sec\n",
            "Epoch 137, Loss(train/val) 0.28393/0.18284. Took 0.04 sec\n",
            "Epoch 138, Loss(train/val) 0.27463/0.18202. Took 0.04 sec\n",
            "Epoch 139, Loss(train/val) 0.27054/0.18048. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 0.27010/0.18145. Took 0.04 sec\n",
            "Epoch 141, Loss(train/val) 0.28344/0.18073. Took 0.04 sec\n",
            "Epoch 142, Loss(train/val) 0.27164/0.18012. Took 0.04 sec\n",
            "Epoch 143, Loss(train/val) 0.27224/0.17989. Took 0.04 sec\n",
            "Epoch 144, Loss(train/val) 0.26856/0.17950. Took 0.04 sec\n",
            "Epoch 145, Loss(train/val) 0.27247/0.17910. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 0.29449/0.17972. Took 0.04 sec\n",
            "Epoch 147, Loss(train/val) 0.27821/0.17741. Took 0.04 sec\n",
            "Epoch 148, Loss(train/val) 0.28272/0.17827. Took 0.04 sec\n",
            "Epoch 149, Loss(train/val) 0.28670/0.17711. Took 0.04 sec\n",
            "Epoch 150, Loss(train/val) 0.27699/0.17780. Took 0.04 sec\n",
            "Epoch 151, Loss(train/val) 0.26764/0.18086. Took 0.04 sec\n",
            "Epoch 152, Loss(train/val) 0.27310/0.17977. Took 0.04 sec\n",
            "Epoch 153, Loss(train/val) 0.28910/0.17921. Took 0.04 sec\n",
            "Epoch 154, Loss(train/val) 0.27389/0.17826. Took 0.04 sec\n",
            "Epoch 155, Loss(train/val) 0.26367/0.17943. Took 0.04 sec\n",
            "Epoch 156, Loss(train/val) 0.26750/0.17974. Took 0.04 sec\n",
            "Epoch 157, Loss(train/val) 0.26467/0.17811. Took 0.04 sec\n",
            "Epoch 158, Loss(train/val) 0.26575/0.17698. Took 0.04 sec\n",
            "Epoch 159, Loss(train/val) 0.26853/0.17739. Took 0.04 sec\n",
            "Epoch 160, Loss(train/val) 0.28122/0.17946. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 0.26405/0.17820. Took 0.04 sec\n",
            "Epoch 162, Loss(train/val) 0.26237/0.17814. Took 0.04 sec\n",
            "Epoch 163, Loss(train/val) 0.27016/0.17822. Took 0.04 sec\n",
            "Epoch 164, Loss(train/val) 0.27692/0.17636. Took 0.04 sec\n",
            "Epoch 165, Loss(train/val) 0.26530/0.17418. Took 0.04 sec\n",
            "Epoch 166, Loss(train/val) 0.27216/0.17482. Took 0.04 sec\n",
            "Epoch 167, Loss(train/val) 0.25814/0.17544. Took 0.04 sec\n",
            "Epoch 168, Loss(train/val) 0.26970/0.17687. Took 0.04 sec\n",
            "Epoch 169, Loss(train/val) 0.27017/0.17779. Took 0.04 sec\n",
            "Epoch 170, Loss(train/val) 0.27215/0.17593. Took 0.04 sec\n",
            "Epoch 171, Loss(train/val) 0.26691/0.17375. Took 0.04 sec\n",
            "Epoch 172, Loss(train/val) 0.25195/0.17407. Took 0.04 sec\n",
            "Epoch 173, Loss(train/val) 0.27909/0.17506. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 0.26314/0.17573. Took 0.05 sec\n",
            "Epoch 175, Loss(train/val) 0.25902/0.17709. Took 0.04 sec\n",
            "Epoch 176, Loss(train/val) 0.26150/0.17585. Took 0.04 sec\n",
            "Epoch 177, Loss(train/val) 0.27540/0.17644. Took 0.04 sec\n",
            "Epoch 178, Loss(train/val) 0.27977/0.17443. Took 0.04 sec\n",
            "Epoch 179, Loss(train/val) 0.26459/0.17239. Took 0.04 sec\n",
            "Epoch 180, Loss(train/val) 0.26521/0.17425. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 0.26381/0.17602. Took 0.04 sec\n",
            "Epoch 182, Loss(train/val) 0.27144/0.17566. Took 0.04 sec\n",
            "Epoch 183, Loss(train/val) 0.28240/0.17322. Took 0.04 sec\n",
            "Epoch 184, Loss(train/val) 0.25542/0.17185. Took 0.04 sec\n",
            "Epoch 185, Loss(train/val) 0.25765/0.17202. Took 0.04 sec\n",
            "Epoch 186, Loss(train/val) 0.26336/0.17228. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 0.26909/0.17264. Took 0.04 sec\n",
            "Epoch 188, Loss(train/val) 0.26138/0.17206. Took 0.04 sec\n",
            "Epoch 189, Loss(train/val) 0.25844/0.17225. Took 0.04 sec\n",
            "Epoch 190, Loss(train/val) 0.25909/0.17283. Took 0.04 sec\n",
            "Epoch 191, Loss(train/val) 0.25521/0.17360. Took 0.04 sec\n",
            "Epoch 192, Loss(train/val) 0.25851/0.17345. Took 0.05 sec\n",
            "Epoch 193, Loss(train/val) 0.27210/0.17026. Took 0.04 sec\n",
            "Epoch 194, Loss(train/val) 0.25607/0.17127. Took 0.04 sec\n",
            "Epoch 195, Loss(train/val) 0.26534/0.17299. Took 0.04 sec\n",
            "Epoch 196, Loss(train/val) 0.26266/0.17331. Took 0.04 sec\n",
            "Epoch 197, Loss(train/val) 0.25906/0.17203. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 0.25212/0.17148. Took 0.04 sec\n",
            "Epoch 199, Loss(train/val) 0.25584/0.17167. Took 0.04 sec\n",
            "Epoch 200, Loss(train/val) 0.26061/0.17360. Took 0.04 sec\n",
            "Epoch 201, Loss(train/val) 0.27154/0.17235. Took 0.04 sec\n",
            "Epoch 202, Loss(train/val) 0.25895/0.17109. Took 0.04 sec\n",
            "Epoch 203, Loss(train/val) 0.25188/0.16990. Took 0.04 sec\n",
            "Epoch 204, Loss(train/val) 0.25560/0.17060. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.25801/0.17247. Took 0.04 sec\n",
            "Epoch 206, Loss(train/val) 0.25683/0.17361. Took 0.04 sec\n",
            "Epoch 207, Loss(train/val) 0.27657/0.17333. Took 0.04 sec\n",
            "Epoch 208, Loss(train/val) 0.26220/0.17255. Took 0.04 sec\n",
            "Epoch 209, Loss(train/val) 0.28150/0.17176. Took 0.04 sec\n",
            "Epoch 210, Loss(train/val) 0.24731/0.16961. Took 0.04 sec\n",
            "Epoch 211, Loss(train/val) 0.25699/0.17026. Took 0.04 sec\n",
            "Epoch 212, Loss(train/val) 0.26309/0.16831. Took 0.04 sec\n",
            "Epoch 213, Loss(train/val) 0.26041/0.16945. Took 0.04 sec\n",
            "Epoch 214, Loss(train/val) 0.26015/0.16959. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.25232/0.16812. Took 0.04 sec\n",
            "Epoch 216, Loss(train/val) 0.26548/0.16887. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 0.25954/0.17027. Took 0.04 sec\n",
            "Epoch 218, Loss(train/val) 0.25694/0.16956. Took 0.04 sec\n",
            "Epoch 219, Loss(train/val) 0.26213/0.16841. Took 0.04 sec\n",
            "Epoch 220, Loss(train/val) 0.26038/0.16651. Took 0.04 sec\n",
            "Epoch 221, Loss(train/val) 0.25443/0.16621. Took 0.04 sec\n",
            "Epoch 222, Loss(train/val) 0.27463/0.16774. Took 0.06 sec\n",
            "Epoch 223, Loss(train/val) 0.24759/0.16838. Took 0.04 sec\n",
            "Epoch 224, Loss(train/val) 0.25244/0.16863. Took 0.04 sec\n",
            "Epoch 225, Loss(train/val) 0.25271/0.16968. Took 0.04 sec\n",
            "Epoch 226, Loss(train/val) 0.24988/0.16940. Took 0.04 sec\n",
            "Epoch 227, Loss(train/val) 0.25727/0.16993. Took 0.04 sec\n",
            "Epoch 228, Loss(train/val) 0.25589/0.17012. Took 0.04 sec\n",
            "Epoch 229, Loss(train/val) 0.25183/0.17027. Took 0.04 sec\n",
            "Epoch 230, Loss(train/val) 0.25423/0.16867. Took 0.04 sec\n",
            "Epoch 231, Loss(train/val) 0.25412/0.16829. Took 0.04 sec\n",
            "Epoch 232, Loss(train/val) 0.25680/0.17053. Took 0.04 sec\n",
            "Epoch 233, Loss(train/val) 0.25914/0.16877. Took 0.04 sec\n",
            "Epoch 234, Loss(train/val) 0.26325/0.16768. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 0.25517/0.16647. Took 0.04 sec\n",
            "Epoch 236, Loss(train/val) 0.25570/0.16697. Took 0.04 sec\n",
            "Epoch 237, Loss(train/val) 0.25196/0.16741. Took 0.05 sec\n",
            "Epoch 238, Loss(train/val) 0.25164/0.16714. Took 0.04 sec\n",
            "Epoch 239, Loss(train/val) 0.24899/0.16656. Took 0.04 sec\n",
            "Epoch 240, Loss(train/val) 0.25397/0.16601. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 0.27375/0.16512. Took 0.04 sec\n",
            "Epoch 242, Loss(train/val) 0.26408/0.16426. Took 0.04 sec\n",
            "Epoch 243, Loss(train/val) 0.26349/0.16373. Took 0.04 sec\n",
            "Epoch 244, Loss(train/val) 0.25980/0.16593. Took 0.04 sec\n",
            "Epoch 245, Loss(train/val) 0.24889/0.16713. Took 0.04 sec\n",
            "Epoch 246, Loss(train/val) 0.27370/0.16651. Took 0.04 sec\n",
            "Epoch 247, Loss(train/val) 0.25629/0.16577. Took 0.05 sec\n",
            "Epoch 248, Loss(train/val) 0.24991/0.16538. Took 0.04 sec\n",
            "Epoch 249, Loss(train/val) 0.24601/0.16482. Took 0.04 sec\n",
            "Epoch 250, Loss(train/val) 0.25111/0.16506. Took 0.04 sec\n",
            "Epoch 251, Loss(train/val) 0.24993/0.16608. Took 0.04 sec\n",
            "Epoch 252, Loss(train/val) 0.24972/0.16567. Took 0.04 sec\n",
            "Epoch 253, Loss(train/val) 0.25472/0.16625. Took 0.04 sec\n",
            "Epoch 254, Loss(train/val) 0.25063/0.16855. Took 0.04 sec\n",
            "Epoch 255, Loss(train/val) 0.25798/0.16851. Took 0.04 sec\n",
            "Epoch 256, Loss(train/val) 0.24653/0.16854. Took 0.04 sec\n",
            "Epoch 257, Loss(train/val) 0.25596/0.16656. Took 0.04 sec\n",
            "Epoch 258, Loss(train/val) 0.25017/0.16595. Took 0.05 sec\n",
            "Epoch 259, Loss(train/val) 0.25514/0.16458. Took 0.04 sec\n",
            "Epoch 260, Loss(train/val) 0.25778/0.16507. Took 0.04 sec\n",
            "Epoch 261, Loss(train/val) 0.24647/0.16523. Took 0.04 sec\n",
            "Epoch 262, Loss(train/val) 0.27350/0.16773. Took 0.04 sec\n",
            "Epoch 263, Loss(train/val) 0.24908/0.16710. Took 0.04 sec\n",
            "Epoch 264, Loss(train/val) 0.25271/0.16540. Took 0.04 sec\n",
            "Epoch 265, Loss(train/val) 0.26222/0.16548. Took 0.04 sec\n",
            "Epoch 266, Loss(train/val) 0.24703/0.16426. Took 0.04 sec\n",
            "Epoch 267, Loss(train/val) 0.24255/0.16369. Took 0.04 sec\n",
            "Epoch 268, Loss(train/val) 0.24886/0.16463. Took 0.04 sec\n",
            "Epoch 269, Loss(train/val) 0.25386/0.16568. Took 0.04 sec\n",
            "Epoch 270, Loss(train/val) 0.25132/0.16453. Took 0.04 sec\n",
            "Epoch 271, Loss(train/val) 0.25153/0.16310. Took 0.04 sec\n",
            "Epoch 272, Loss(train/val) 0.24901/0.16258. Took 0.04 sec\n",
            "Epoch 273, Loss(train/val) 0.24639/0.16244. Took 0.04 sec\n",
            "Epoch 274, Loss(train/val) 0.24169/0.16386. Took 0.04 sec\n",
            "Epoch 275, Loss(train/val) 0.25657/0.16637. Took 0.04 sec\n",
            "Epoch 276, Loss(train/val) 0.24740/0.16616. Took 0.05 sec\n",
            "Epoch 277, Loss(train/val) 0.27131/0.16484. Took 0.04 sec\n",
            "Epoch 278, Loss(train/val) 0.25130/0.16453. Took 0.04 sec\n",
            "Epoch 279, Loss(train/val) 0.24925/0.16522. Took 0.04 sec\n",
            "Epoch 280, Loss(train/val) 0.24822/0.16449. Took 0.04 sec\n",
            "Epoch 281, Loss(train/val) 0.25802/0.16383. Took 0.04 sec\n",
            "Epoch 282, Loss(train/val) 0.24482/0.16379. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 0.24901/0.16398. Took 0.04 sec\n",
            "Epoch 284, Loss(train/val) 0.27250/0.16200. Took 0.04 sec\n",
            "Epoch 285, Loss(train/val) 0.24647/0.16323. Took 0.04 sec\n",
            "Epoch 286, Loss(train/val) 0.25928/0.16428. Took 0.04 sec\n",
            "Epoch 287, Loss(train/val) 0.25185/0.16435. Took 0.04 sec\n",
            "Epoch 288, Loss(train/val) 0.26283/0.16334. Took 0.05 sec\n",
            "Epoch 289, Loss(train/val) 0.25214/0.16220. Took 0.04 sec\n",
            "Epoch 290, Loss(train/val) 0.24529/0.16349. Took 0.04 sec\n",
            "Epoch 291, Loss(train/val) 0.24100/0.16505. Took 0.04 sec\n",
            "Epoch 292, Loss(train/val) 0.24646/0.16486. Took 0.04 sec\n",
            "Epoch 293, Loss(train/val) 0.24514/0.16549. Took 0.04 sec\n",
            "Epoch 294, Loss(train/val) 0.23837/0.16349. Took 0.04 sec\n",
            "Epoch 295, Loss(train/val) 0.24207/0.16299. Took 0.04 sec\n",
            "Epoch 296, Loss(train/val) 0.25701/0.16204. Took 0.04 sec\n",
            "Epoch 297, Loss(train/val) 0.24008/0.16319. Took 0.04 sec\n",
            "Epoch 298, Loss(train/val) 0.24833/0.16560. Took 0.04 sec\n",
            "Epoch 299, Loss(train/val) 0.24980/0.16566. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.24433/0.16381. Took 0.04 sec\n",
            "Epoch 301, Loss(train/val) 0.25839/0.16319. Took 0.04 sec\n",
            "Epoch 302, Loss(train/val) 0.25075/0.16234. Took 0.04 sec\n",
            "Epoch 303, Loss(train/val) 0.25116/0.16258. Took 0.04 sec\n",
            "Epoch 304, Loss(train/val) 0.24505/0.16313. Took 0.04 sec\n",
            "Epoch 305, Loss(train/val) 0.25782/0.16420. Took 0.04 sec\n",
            "Epoch 306, Loss(train/val) 0.24178/0.16414. Took 0.05 sec\n",
            "Epoch 307, Loss(train/val) 0.24143/0.16367. Took 0.04 sec\n",
            "Epoch 308, Loss(train/val) 0.25399/0.16276. Took 0.04 sec\n",
            "Epoch 309, Loss(train/val) 0.24644/0.16253. Took 0.04 sec\n",
            "Epoch 310, Loss(train/val) 0.24501/0.16238. Took 0.04 sec\n",
            "Epoch 311, Loss(train/val) 0.24720/0.16440. Took 0.04 sec\n",
            "Epoch 312, Loss(train/val) 0.25303/0.16468. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 0.24831/0.16261. Took 0.04 sec\n",
            "Epoch 314, Loss(train/val) 0.24167/0.16298. Took 0.04 sec\n",
            "Epoch 315, Loss(train/val) 0.23775/0.16244. Took 0.04 sec\n",
            "Epoch 316, Loss(train/val) 0.24038/0.16221. Took 0.04 sec\n",
            "Epoch 317, Loss(train/val) 0.24156/0.16217. Took 0.04 sec\n",
            "Epoch 318, Loss(train/val) 0.24001/0.16319. Took 0.04 sec\n",
            "Epoch 319, Loss(train/val) 0.25580/0.16428. Took 0.04 sec\n",
            "Epoch 320, Loss(train/val) 0.24624/0.16345. Took 0.04 sec\n",
            "Epoch 321, Loss(train/val) 0.24354/0.16214. Took 0.04 sec\n",
            "Epoch 322, Loss(train/val) 0.24221/0.16308. Took 0.04 sec\n",
            "Epoch 323, Loss(train/val) 0.25004/0.16379. Took 0.04 sec\n",
            "Epoch 324, Loss(train/val) 0.25077/0.16112. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.24697/0.16118. Took 0.04 sec\n",
            "Epoch 326, Loss(train/val) 0.24665/0.16161. Took 0.04 sec\n",
            "Epoch 327, Loss(train/val) 0.24257/0.16262. Took 0.04 sec\n",
            "Epoch 328, Loss(train/val) 0.24054/0.16358. Took 0.04 sec\n",
            "Epoch 329, Loss(train/val) 0.24753/0.16126. Took 0.04 sec\n",
            "Epoch 330, Loss(train/val) 0.23821/0.16107. Took 0.04 sec\n",
            "Epoch 331, Loss(train/val) 0.24431/0.16121. Took 0.04 sec\n",
            "Epoch 332, Loss(train/val) 0.26399/0.16166. Took 0.04 sec\n",
            "Epoch 333, Loss(train/val) 0.24120/0.16138. Took 0.04 sec\n",
            "Epoch 334, Loss(train/val) 0.24851/0.16192. Took 0.04 sec\n",
            "Epoch 335, Loss(train/val) 0.24254/0.16112. Took 0.04 sec\n",
            "Epoch 336, Loss(train/val) 0.23553/0.16157. Took 0.04 sec\n",
            "Epoch 337, Loss(train/val) 0.23950/0.16166. Took 0.04 sec\n",
            "Epoch 338, Loss(train/val) 0.24112/0.16261. Took 0.04 sec\n",
            "Epoch 339, Loss(train/val) 0.24077/0.16343. Took 0.04 sec\n",
            "Epoch 340, Loss(train/val) 0.24986/0.16417. Took 0.04 sec\n",
            "Epoch 341, Loss(train/val) 0.25054/0.16167. Took 0.04 sec\n",
            "Epoch 342, Loss(train/val) 0.24532/0.16014. Took 0.04 sec\n",
            "Epoch 343, Loss(train/val) 0.24160/0.16071. Took 0.04 sec\n",
            "Epoch 344, Loss(train/val) 0.24002/0.16139. Took 0.04 sec\n",
            "Epoch 345, Loss(train/val) 0.23388/0.16338. Took 0.04 sec\n",
            "Epoch 346, Loss(train/val) 0.24549/0.16273. Took 0.04 sec\n",
            "Epoch 347, Loss(train/val) 0.25352/0.16277. Took 0.04 sec\n",
            "Epoch 348, Loss(train/val) 0.24132/0.16321. Took 0.04 sec\n",
            "Epoch 349, Loss(train/val) 0.24043/0.16328. Took 0.04 sec\n",
            "Epoch 350, Loss(train/val) 0.24019/0.16253. Took 0.04 sec\n",
            "Epoch 351, Loss(train/val) 0.24035/0.16219. Took 0.04 sec\n",
            "Epoch 352, Loss(train/val) 0.24078/0.16091. Took 0.04 sec\n",
            "Epoch 353, Loss(train/val) 0.24376/0.16142. Took 0.04 sec\n",
            "Epoch 354, Loss(train/val) 0.23710/0.16174. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 0.23938/0.16144. Took 0.04 sec\n",
            "Epoch 356, Loss(train/val) 0.24087/0.16163. Took 0.04 sec\n",
            "Epoch 357, Loss(train/val) 0.23280/0.16169. Took 0.04 sec\n",
            "Epoch 358, Loss(train/val) 0.24040/0.16116. Took 0.04 sec\n",
            "Epoch 359, Loss(train/val) 0.24841/0.16092. Took 0.04 sec\n",
            "Epoch 360, Loss(train/val) 0.23648/0.16137. Took 0.05 sec\n",
            "Epoch 361, Loss(train/val) 0.24725/0.16314. Took 0.04 sec\n",
            "Epoch 362, Loss(train/val) 0.23213/0.16274. Took 0.04 sec\n",
            "Epoch 363, Loss(train/val) 0.23807/0.16148. Took 0.04 sec\n",
            "Epoch 364, Loss(train/val) 0.27035/0.16081. Took 0.04 sec\n",
            "Epoch 365, Loss(train/val) 0.24187/0.16011. Took 0.04 sec\n",
            "Epoch 366, Loss(train/val) 0.23568/0.16070. Took 0.04 sec\n",
            "Epoch 367, Loss(train/val) 0.24733/0.16214. Took 0.04 sec\n",
            "Epoch 368, Loss(train/val) 0.24999/0.16244. Took 0.04 sec\n",
            "Epoch 369, Loss(train/val) 0.25945/0.16096. Took 0.04 sec\n",
            "Epoch 370, Loss(train/val) 0.24181/0.16139. Took 0.04 sec\n",
            "Epoch 371, Loss(train/val) 0.25111/0.16014. Took 0.04 sec\n",
            "Epoch 372, Loss(train/val) 0.24873/0.16062. Took 0.04 sec\n",
            "Epoch 373, Loss(train/val) 0.23635/0.16130. Took 0.04 sec\n",
            "Epoch 374, Loss(train/val) 0.24259/0.16092. Took 0.04 sec\n",
            "Epoch 375, Loss(train/val) 0.27358/0.16098. Took 0.04 sec\n",
            "Epoch 376, Loss(train/val) 0.23896/0.16122. Took 0.04 sec\n",
            "Epoch 377, Loss(train/val) 0.24684/0.16321. Took 0.04 sec\n",
            "Epoch 378, Loss(train/val) 0.24221/0.16252. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 0.24381/0.16090. Took 0.04 sec\n",
            "Epoch 380, Loss(train/val) 0.23371/0.16070. Took 0.04 sec\n",
            "Epoch 381, Loss(train/val) 0.23914/0.16079. Took 0.04 sec\n",
            "Epoch 382, Loss(train/val) 0.23522/0.16067. Took 0.04 sec\n",
            "Epoch 383, Loss(train/val) 0.23818/0.16056. Took 0.04 sec\n",
            "Epoch 384, Loss(train/val) 0.23418/0.16200. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 0.23234/0.16205. Took 0.04 sec\n",
            "Epoch 386, Loss(train/val) 0.23949/0.16226. Took 0.04 sec\n",
            "Epoch 387, Loss(train/val) 0.23657/0.16118. Took 0.04 sec\n",
            "Epoch 388, Loss(train/val) 0.23496/0.16121. Took 0.04 sec\n",
            "Epoch 389, Loss(train/val) 0.23161/0.16110. Took 0.04 sec\n",
            "Epoch 390, Loss(train/val) 0.24826/0.16164. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.24310/0.16148. Took 0.04 sec\n",
            "Epoch 392, Loss(train/val) 0.23542/0.16155. Took 0.04 sec\n",
            "Epoch 393, Loss(train/val) 0.22937/0.16173. Took 0.04 sec\n",
            "Epoch 394, Loss(train/val) 0.23456/0.16158. Took 0.04 sec\n",
            "Epoch 395, Loss(train/val) 0.23042/0.16078. Took 0.04 sec\n",
            "Epoch 396, Loss(train/val) 0.25608/0.16026. Took 0.04 sec\n",
            "Epoch 397, Loss(train/val) 0.23912/0.15957. Took 0.04 sec\n",
            "Epoch 398, Loss(train/val) 0.23554/0.16016. Took 0.04 sec\n",
            "Epoch 399, Loss(train/val) 0.23553/0.16074. Took 0.04 sec\n",
            "Epoch 400, Loss(train/val) 0.23141/0.16270. Took 0.04 sec\n",
            "Epoch 401, Loss(train/val) 0.25095/0.16157. Took 0.04 sec\n",
            "Epoch 402, Loss(train/val) 0.23141/0.16131. Took 0.05 sec\n",
            "Epoch 403, Loss(train/val) 0.24508/0.15964. Took 0.04 sec\n",
            "Epoch 404, Loss(train/val) 0.23616/0.16018. Took 0.04 sec\n",
            "Epoch 405, Loss(train/val) 0.24010/0.16058. Took 0.04 sec\n",
            "Epoch 406, Loss(train/val) 0.25136/0.16116. Took 0.04 sec\n",
            "Epoch 407, Loss(train/val) 0.23767/0.16085. Took 0.04 sec\n",
            "Epoch 408, Loss(train/val) 0.25557/0.16062. Took 0.04 sec\n",
            "Epoch 409, Loss(train/val) 0.23783/0.15948. Took 0.04 sec\n",
            "Epoch 410, Loss(train/val) 0.23035/0.15957. Took 0.04 sec\n",
            "Epoch 411, Loss(train/val) 0.26140/0.15974. Took 0.04 sec\n",
            "Epoch 412, Loss(train/val) 0.23141/0.16091. Took 0.04 sec\n",
            "Epoch 413, Loss(train/val) 0.24281/0.16154. Took 0.04 sec\n",
            "Epoch 414, Loss(train/val) 0.23901/0.16120. Took 0.05 sec\n",
            "Epoch 415, Loss(train/val) 0.23316/0.16062. Took 0.04 sec\n",
            "Epoch 416, Loss(train/val) 0.25866/0.16059. Took 0.04 sec\n",
            "Epoch 417, Loss(train/val) 0.23714/0.15955. Took 0.04 sec\n",
            "Epoch 418, Loss(train/val) 0.23694/0.15889. Took 0.04 sec\n",
            "Epoch 419, Loss(train/val) 0.25574/0.15912. Took 0.04 sec\n",
            "Epoch 420, Loss(train/val) 0.25793/0.15904. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.23996/0.15978. Took 0.04 sec\n",
            "Epoch 422, Loss(train/val) 0.23799/0.15999. Took 0.04 sec\n",
            "Epoch 423, Loss(train/val) 0.23528/0.16040. Took 0.04 sec\n",
            "Epoch 424, Loss(train/val) 0.23206/0.16133. Took 0.04 sec\n",
            "Epoch 425, Loss(train/val) 0.23427/0.16106. Took 0.04 sec\n",
            "Epoch 426, Loss(train/val) 0.24888/0.15941. Took 0.04 sec\n",
            "Epoch 427, Loss(train/val) 0.24974/0.15866. Took 0.04 sec\n",
            "Epoch 428, Loss(train/val) 0.23408/0.15909. Took 0.04 sec\n",
            "Epoch 429, Loss(train/val) 0.25124/0.15885. Took 0.04 sec\n",
            "Epoch 430, Loss(train/val) 0.23502/0.15928. Took 0.04 sec\n",
            "Epoch 431, Loss(train/val) 0.23595/0.16078. Took 0.04 sec\n",
            "Epoch 432, Loss(train/val) 0.25730/0.16133. Took 0.04 sec\n",
            "Epoch 433, Loss(train/val) 0.25048/0.16034. Took 0.04 sec\n",
            "Epoch 434, Loss(train/val) 0.23558/0.15950. Took 0.04 sec\n",
            "Epoch 435, Loss(train/val) 0.23130/0.15956. Took 0.04 sec\n",
            "Epoch 436, Loss(train/val) 0.23726/0.15899. Took 0.04 sec\n",
            "Epoch 437, Loss(train/val) 0.24125/0.15988. Took 0.04 sec\n",
            "Epoch 438, Loss(train/val) 0.23198/0.15952. Took 0.05 sec\n",
            "Epoch 439, Loss(train/val) 0.24573/0.15885. Took 0.05 sec\n",
            "Epoch 440, Loss(train/val) 0.22551/0.15939. Took 0.04 sec\n",
            "Epoch 441, Loss(train/val) 0.24360/0.15904. Took 0.04 sec\n",
            "Epoch 442, Loss(train/val) 0.24964/0.15956. Took 0.04 sec\n",
            "Epoch 443, Loss(train/val) 0.24087/0.15998. Took 0.04 sec\n",
            "Epoch 444, Loss(train/val) 0.23499/0.15926. Took 0.05 sec\n",
            "Epoch 445, Loss(train/val) 0.26017/0.15854. Took 0.04 sec\n",
            "Epoch 446, Loss(train/val) 0.23614/0.15868. Took 0.04 sec\n",
            "Epoch 447, Loss(train/val) 0.24676/0.15897. Took 0.04 sec\n",
            "Epoch 448, Loss(train/val) 0.24123/0.15989. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 0.23565/0.15926. Took 0.04 sec\n",
            "Epoch 450, Loss(train/val) 0.23646/0.15956. Took 0.04 sec\n",
            "Epoch 451, Loss(train/val) 0.23276/0.15964. Took 0.04 sec\n",
            "Epoch 452, Loss(train/val) 0.24601/0.15958. Took 0.04 sec\n",
            "Epoch 453, Loss(train/val) 0.23152/0.16011. Took 0.04 sec\n",
            "Epoch 454, Loss(train/val) 0.25159/0.15966. Took 0.04 sec\n",
            "Epoch 455, Loss(train/val) 0.23251/0.15894. Took 0.04 sec\n",
            "Epoch 456, Loss(train/val) 0.23802/0.15954. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.24469/0.15928. Took 0.04 sec\n",
            "Epoch 458, Loss(train/val) 0.22833/0.15974. Took 0.04 sec\n",
            "Epoch 459, Loss(train/val) 0.23790/0.16035. Took 0.04 sec\n",
            "Epoch 460, Loss(train/val) 0.24626/0.15927. Took 0.04 sec\n",
            "Epoch 461, Loss(train/val) 0.22231/0.15884. Took 0.04 sec\n",
            "Epoch 462, Loss(train/val) 0.23924/0.15862. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.24030/0.15818. Took 0.04 sec\n",
            "Epoch 464, Loss(train/val) 0.23849/0.15904. Took 0.04 sec\n",
            "Epoch 465, Loss(train/val) 0.23363/0.16187. Took 0.04 sec\n",
            "Epoch 466, Loss(train/val) 0.22988/0.16066. Took 0.04 sec\n",
            "Epoch 467, Loss(train/val) 0.24858/0.15924. Took 0.04 sec\n",
            "Epoch 468, Loss(train/val) 0.23933/0.15931. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 0.23381/0.15827. Took 0.04 sec\n",
            "Epoch 470, Loss(train/val) 0.23736/0.15798. Took 0.04 sec\n",
            "Epoch 471, Loss(train/val) 0.23520/0.15846. Took 0.04 sec\n",
            "Epoch 472, Loss(train/val) 0.22768/0.16000. Took 0.04 sec\n",
            "Epoch 473, Loss(train/val) 0.23174/0.16056. Took 0.04 sec\n",
            "Epoch 474, Loss(train/val) 0.23500/0.15982. Took 0.04 sec\n",
            "Epoch 475, Loss(train/val) 0.24878/0.15879. Took 0.04 sec\n",
            "Epoch 476, Loss(train/val) 0.23289/0.15803. Took 0.04 sec\n",
            "Epoch 477, Loss(train/val) 0.23633/0.15907. Took 0.04 sec\n",
            "Epoch 478, Loss(train/val) 0.22657/0.15908. Took 0.04 sec\n",
            "Epoch 479, Loss(train/val) 0.25156/0.15894. Took 0.04 sec\n",
            "Epoch 480, Loss(train/val) 0.22420/0.15962. Took 0.05 sec\n",
            "Epoch 481, Loss(train/val) 0.23427/0.15942. Took 0.05 sec\n",
            "Epoch 482, Loss(train/val) 0.22985/0.15843. Took 0.04 sec\n",
            "Epoch 483, Loss(train/val) 0.23525/0.15855. Took 0.04 sec\n",
            "Epoch 484, Loss(train/val) 0.23072/0.15913. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.23086/0.16007. Took 0.05 sec\n",
            "Epoch 486, Loss(train/val) 0.24293/0.15952. Took 0.04 sec\n",
            "Epoch 487, Loss(train/val) 0.23262/0.15967. Took 0.04 sec\n",
            "Epoch 488, Loss(train/val) 0.23747/0.15905. Took 0.04 sec\n",
            "Epoch 489, Loss(train/val) 0.23131/0.15834. Took 0.04 sec\n",
            "Epoch 490, Loss(train/val) 0.23103/0.15865. Took 0.04 sec\n",
            "Epoch 491, Loss(train/val) 0.23258/0.15822. Took 0.05 sec\n",
            "Epoch 492, Loss(train/val) 0.22485/0.15843. Took 0.04 sec\n",
            "Epoch 493, Loss(train/val) 0.23823/0.15854. Took 0.04 sec\n",
            "Epoch 494, Loss(train/val) 0.23014/0.15912. Took 0.04 sec\n",
            "Epoch 495, Loss(train/val) 0.24058/0.16014. Took 0.04 sec\n",
            "Epoch 496, Loss(train/val) 0.23536/0.15913. Took 0.04 sec\n",
            "Epoch 497, Loss(train/val) 0.24424/0.15935. Took 0.04 sec\n",
            "Epoch 498, Loss(train/val) 0.23119/0.15990. Took 0.04 sec\n",
            "Epoch 499, Loss(train/val) 0.22912/0.16008. Took 0.04 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=500, exp_name='exp2_hidn', hid_dim=64, input_dim=1, l2=1e-05, lr=0.0001, n_layers=4, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.07375/0.34634. Took 0.04 sec\n",
            "Epoch 1, Loss(train/val) 1.06120/0.34627. Took 0.04 sec\n",
            "Epoch 2, Loss(train/val) 1.05840/0.34622. Took 0.04 sec\n",
            "Epoch 3, Loss(train/val) 1.03261/0.34616. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.04081/0.34609. Took 0.04 sec\n",
            "Epoch 5, Loss(train/val) 1.02492/0.34602. Took 0.04 sec\n",
            "Epoch 6, Loss(train/val) 1.01867/0.34594. Took 0.05 sec\n",
            "Epoch 7, Loss(train/val) 1.01603/0.34586. Took 0.04 sec\n",
            "Epoch 8, Loss(train/val) 1.00455/0.34576. Took 0.05 sec\n",
            "Epoch 9, Loss(train/val) 0.99149/0.34565. Took 0.04 sec\n",
            "Epoch 10, Loss(train/val) 0.96522/0.34551. Took 0.04 sec\n",
            "Epoch 11, Loss(train/val) 0.97276/0.34532. Took 0.04 sec\n",
            "Epoch 12, Loss(train/val) 0.95635/0.34503. Took 0.04 sec\n",
            "Epoch 13, Loss(train/val) 0.94593/0.34462. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 0.92598/0.34405. Took 0.04 sec\n",
            "Epoch 15, Loss(train/val) 0.90617/0.34332. Took 0.04 sec\n",
            "Epoch 16, Loss(train/val) 0.89014/0.34238. Took 0.04 sec\n",
            "Epoch 17, Loss(train/val) 0.86799/0.34121. Took 0.04 sec\n",
            "Epoch 18, Loss(train/val) 0.83896/0.33973. Took 0.04 sec\n",
            "Epoch 19, Loss(train/val) 0.82146/0.33777. Took 0.04 sec\n",
            "Epoch 20, Loss(train/val) 0.79171/0.33510. Took 0.04 sec\n",
            "Epoch 21, Loss(train/val) 0.76921/0.33159. Took 0.04 sec\n",
            "Epoch 22, Loss(train/val) 0.73518/0.32696. Took 0.04 sec\n",
            "Epoch 23, Loss(train/val) 0.70944/0.32125. Took 0.04 sec\n",
            "Epoch 24, Loss(train/val) 0.68040/0.31434. Took 0.04 sec\n",
            "Epoch 25, Loss(train/val) 0.65197/0.30607. Took 0.04 sec\n",
            "Epoch 26, Loss(train/val) 0.62122/0.29641. Took 0.04 sec\n",
            "Epoch 27, Loss(train/val) 0.59850/0.28586. Took 0.04 sec\n",
            "Epoch 28, Loss(train/val) 0.56301/0.27536. Took 0.04 sec\n",
            "Epoch 29, Loss(train/val) 0.53919/0.26473. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 0.51565/0.25703. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 0.49846/0.25176. Took 0.04 sec\n",
            "Epoch 32, Loss(train/val) 0.47588/0.24864. Took 0.04 sec\n",
            "Epoch 33, Loss(train/val) 0.45413/0.24720. Took 0.04 sec\n",
            "Epoch 34, Loss(train/val) 0.43840/0.24550. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 0.43806/0.24432. Took 0.04 sec\n",
            "Epoch 36, Loss(train/val) 0.41128/0.24515. Took 0.04 sec\n",
            "Epoch 37, Loss(train/val) 0.39078/0.24224. Took 0.04 sec\n",
            "Epoch 38, Loss(train/val) 0.38794/0.24150. Took 0.04 sec\n",
            "Epoch 39, Loss(train/val) 0.38333/0.23918. Took 0.04 sec\n",
            "Epoch 40, Loss(train/val) 0.37790/0.23658. Took 0.04 sec\n",
            "Epoch 41, Loss(train/val) 0.37445/0.23534. Took 0.04 sec\n",
            "Epoch 42, Loss(train/val) 0.36873/0.23356. Took 0.04 sec\n",
            "Epoch 43, Loss(train/val) 0.36611/0.22978. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 0.35698/0.22588. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 0.35493/0.22395. Took 0.04 sec\n",
            "Epoch 46, Loss(train/val) 0.34796/0.22232. Took 0.04 sec\n",
            "Epoch 47, Loss(train/val) 0.34561/0.22126. Took 0.04 sec\n",
            "Epoch 48, Loss(train/val) 0.34530/0.22074. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 0.35540/0.22010. Took 0.04 sec\n",
            "Epoch 50, Loss(train/val) 0.34752/0.21923. Took 0.04 sec\n",
            "Epoch 51, Loss(train/val) 0.35330/0.21919. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 0.34643/0.21904. Took 0.04 sec\n",
            "Epoch 53, Loss(train/val) 0.35568/0.21905. Took 0.04 sec\n",
            "Epoch 54, Loss(train/val) 0.35073/0.21907. Took 0.06 sec\n",
            "Epoch 55, Loss(train/val) 0.34141/0.21827. Took 0.04 sec\n",
            "Epoch 56, Loss(train/val) 0.34552/0.21798. Took 0.04 sec\n",
            "Epoch 57, Loss(train/val) 0.34061/0.21784. Took 0.04 sec\n",
            "Epoch 58, Loss(train/val) 0.34206/0.21800. Took 0.04 sec\n",
            "Epoch 59, Loss(train/val) 0.34623/0.21816. Took 0.05 sec\n",
            "Epoch 60, Loss(train/val) 0.33449/0.21811. Took 0.04 sec\n",
            "Epoch 61, Loss(train/val) 0.34381/0.21813. Took 0.04 sec\n",
            "Epoch 62, Loss(train/val) 0.34083/0.21790. Took 0.04 sec\n",
            "Epoch 63, Loss(train/val) 0.34490/0.21755. Took 0.04 sec\n",
            "Epoch 64, Loss(train/val) 0.35117/0.21816. Took 0.04 sec\n",
            "Epoch 65, Loss(train/val) 0.34012/0.21980. Took 0.05 sec\n",
            "Epoch 66, Loss(train/val) 0.34046/0.21976. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 0.33500/0.22066. Took 0.04 sec\n",
            "Epoch 68, Loss(train/val) 0.33969/0.22328. Took 0.04 sec\n",
            "Epoch 69, Loss(train/val) 0.34627/0.22335. Took 0.05 sec\n",
            "Epoch 70, Loss(train/val) 0.33420/0.22414. Took 0.04 sec\n",
            "Epoch 71, Loss(train/val) 0.33111/0.22442. Took 0.04 sec\n",
            "Epoch 72, Loss(train/val) 0.33949/0.22516. Took 0.04 sec\n",
            "Epoch 73, Loss(train/val) 0.33786/0.22439. Took 0.04 sec\n",
            "Epoch 74, Loss(train/val) 0.34220/0.22407. Took 0.04 sec\n",
            "Epoch 75, Loss(train/val) 0.33528/0.22281. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 0.34408/0.22529. Took 0.04 sec\n",
            "Epoch 77, Loss(train/val) 0.32732/0.22445. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 0.34026/0.22491. Took 0.04 sec\n",
            "Epoch 79, Loss(train/val) 0.33625/0.22434. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 0.34725/0.22311. Took 0.04 sec\n",
            "Epoch 81, Loss(train/val) 0.34103/0.22652. Took 0.04 sec\n",
            "Epoch 82, Loss(train/val) 0.33744/0.22783. Took 0.04 sec\n",
            "Epoch 83, Loss(train/val) 0.34099/0.22597. Took 0.04 sec\n",
            "Epoch 84, Loss(train/val) 0.33113/0.22793. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 0.34720/0.22613. Took 0.04 sec\n",
            "Epoch 86, Loss(train/val) 0.33601/0.22697. Took 0.04 sec\n",
            "Epoch 87, Loss(train/val) 0.33896/0.22465. Took 0.04 sec\n",
            "Epoch 88, Loss(train/val) 0.33487/0.22466. Took 0.04 sec\n",
            "Epoch 89, Loss(train/val) 0.33524/0.22395. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 0.33044/0.22458. Took 0.04 sec\n",
            "Epoch 91, Loss(train/val) 0.33497/0.22457. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 0.33840/0.22490. Took 0.04 sec\n",
            "Epoch 93, Loss(train/val) 0.34640/0.22499. Took 0.04 sec\n",
            "Epoch 94, Loss(train/val) 0.34242/0.22238. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 0.33133/0.22274. Took 0.04 sec\n",
            "Epoch 96, Loss(train/val) 0.34018/0.21794. Took 0.04 sec\n",
            "Epoch 97, Loss(train/val) 0.34020/0.21495. Took 0.04 sec\n",
            "Epoch 98, Loss(train/val) 0.33712/0.21536. Took 0.04 sec\n",
            "Epoch 99, Loss(train/val) 0.32710/0.21818. Took 0.04 sec\n",
            "Epoch 100, Loss(train/val) 0.33776/0.21824. Took 0.06 sec\n",
            "Epoch 101, Loss(train/val) 0.34219/0.22215. Took 0.04 sec\n",
            "Epoch 102, Loss(train/val) 0.33631/0.22218. Took 0.04 sec\n",
            "Epoch 103, Loss(train/val) 0.33450/0.22119. Took 0.04 sec\n",
            "Epoch 104, Loss(train/val) 0.32978/0.22253. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 0.33926/0.22033. Took 0.04 sec\n",
            "Epoch 106, Loss(train/val) 0.33214/0.22149. Took 0.04 sec\n",
            "Epoch 107, Loss(train/val) 0.32809/0.21921. Took 0.04 sec\n",
            "Epoch 108, Loss(train/val) 0.33039/0.21902. Took 0.04 sec\n",
            "Epoch 109, Loss(train/val) 0.33987/0.21903. Took 0.05 sec\n",
            "Epoch 110, Loss(train/val) 0.35232/0.21920. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 0.32473/0.21878. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 0.32996/0.21596. Took 0.04 sec\n",
            "Epoch 113, Loss(train/val) 0.33219/0.21654. Took 0.04 sec\n",
            "Epoch 114, Loss(train/val) 0.32988/0.21597. Took 0.04 sec\n",
            "Epoch 115, Loss(train/val) 0.34148/0.21319. Took 0.04 sec\n",
            "Epoch 116, Loss(train/val) 0.33400/0.21363. Took 0.04 sec\n",
            "Epoch 117, Loss(train/val) 0.31826/0.21343. Took 0.04 sec\n",
            "Epoch 118, Loss(train/val) 0.32670/0.21372. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 0.32360/0.21455. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 0.32205/0.21583. Took 0.04 sec\n",
            "Epoch 121, Loss(train/val) 0.32391/0.21768. Took 0.04 sec\n",
            "Epoch 122, Loss(train/val) 0.32846/0.22068. Took 0.04 sec\n",
            "Epoch 123, Loss(train/val) 0.32718/0.22377. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 0.32578/0.22360. Took 0.06 sec\n",
            "Epoch 125, Loss(train/val) 0.32637/0.22069. Took 0.04 sec\n",
            "Epoch 126, Loss(train/val) 0.33055/0.21759. Took 0.04 sec\n",
            "Epoch 127, Loss(train/val) 0.32821/0.21614. Took 0.04 sec\n",
            "Epoch 128, Loss(train/val) 0.33634/0.21308. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 0.32350/0.21145. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 0.32027/0.21149. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 0.32063/0.21107. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 0.31688/0.20953. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 0.31408/0.20941. Took 0.05 sec\n",
            "Epoch 134, Loss(train/val) 0.31357/0.21054. Took 0.06 sec\n",
            "Epoch 135, Loss(train/val) 0.32747/0.20716. Took 0.05 sec\n",
            "Epoch 136, Loss(train/val) 0.32315/0.20817. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 0.31524/0.20914. Took 0.04 sec\n",
            "Epoch 138, Loss(train/val) 0.31966/0.20840. Took 0.04 sec\n",
            "Epoch 139, Loss(train/val) 0.33080/0.20958. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 0.32964/0.20848. Took 0.05 sec\n",
            "Epoch 141, Loss(train/val) 0.32210/0.20473. Took 0.05 sec\n",
            "Epoch 142, Loss(train/val) 0.33474/0.20549. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 0.32087/0.20471. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 0.32313/0.20365. Took 0.06 sec\n",
            "Epoch 145, Loss(train/val) 0.30522/0.20574. Took 0.06 sec\n",
            "Epoch 146, Loss(train/val) 0.31694/0.20674. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 0.31267/0.21107. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 0.32008/0.21184. Took 0.05 sec\n",
            "Epoch 149, Loss(train/val) 0.30724/0.21004. Took 0.06 sec\n",
            "Epoch 150, Loss(train/val) 0.31886/0.20637. Took 0.04 sec\n",
            "Epoch 151, Loss(train/val) 0.31984/0.20330. Took 0.04 sec\n",
            "Epoch 152, Loss(train/val) 0.31598/0.20509. Took 0.04 sec\n",
            "Epoch 153, Loss(train/val) 0.30882/0.20316. Took 0.04 sec\n",
            "Epoch 154, Loss(train/val) 0.30633/0.20434. Took 0.04 sec\n",
            "Epoch 155, Loss(train/val) 0.30633/0.20634. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 0.30099/0.20366. Took 0.04 sec\n",
            "Epoch 157, Loss(train/val) 0.31746/0.20009. Took 0.04 sec\n",
            "Epoch 158, Loss(train/val) 0.30440/0.20073. Took 0.04 sec\n",
            "Epoch 159, Loss(train/val) 0.30515/0.20378. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 0.31229/0.19954. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 0.29832/0.20023. Took 0.04 sec\n",
            "Epoch 162, Loss(train/val) 0.30067/0.20338. Took 0.04 sec\n",
            "Epoch 163, Loss(train/val) 0.30964/0.20598. Took 0.04 sec\n",
            "Epoch 164, Loss(train/val) 0.29862/0.20176. Took 0.04 sec\n",
            "Epoch 165, Loss(train/val) 0.30797/0.19828. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.30068/0.19899. Took 0.04 sec\n",
            "Epoch 167, Loss(train/val) 0.30490/0.19665. Took 0.04 sec\n",
            "Epoch 168, Loss(train/val) 0.30368/0.19604. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 0.29820/0.20377. Took 0.04 sec\n",
            "Epoch 170, Loss(train/val) 0.29133/0.19573. Took 0.04 sec\n",
            "Epoch 171, Loss(train/val) 0.31693/0.19325. Took 0.04 sec\n",
            "Epoch 172, Loss(train/val) 0.30926/0.19125. Took 0.04 sec\n",
            "Epoch 173, Loss(train/val) 0.29406/0.19463. Took 0.04 sec\n",
            "Epoch 174, Loss(train/val) 0.29668/0.20509. Took 0.05 sec\n",
            "Epoch 175, Loss(train/val) 0.29402/0.20131. Took 0.04 sec\n",
            "Epoch 176, Loss(train/val) 0.29285/0.19117. Took 0.04 sec\n",
            "Epoch 177, Loss(train/val) 0.28753/0.18996. Took 0.04 sec\n",
            "Epoch 178, Loss(train/val) 0.29570/0.19123. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 0.29487/0.19372. Took 0.04 sec\n",
            "Epoch 180, Loss(train/val) 0.28808/0.19543. Took 0.04 sec\n",
            "Epoch 181, Loss(train/val) 0.29321/0.19023. Took 0.04 sec\n",
            "Epoch 182, Loss(train/val) 0.29395/0.18760. Took 0.04 sec\n",
            "Epoch 183, Loss(train/val) 0.28680/0.18606. Took 0.04 sec\n",
            "Epoch 184, Loss(train/val) 0.28675/0.18546. Took 0.04 sec\n",
            "Epoch 185, Loss(train/val) 0.28229/0.19248. Took 0.06 sec\n",
            "Epoch 186, Loss(train/val) 0.28283/0.18942. Took 0.04 sec\n",
            "Epoch 187, Loss(train/val) 0.27696/0.18838. Took 0.04 sec\n",
            "Epoch 188, Loss(train/val) 0.29380/0.18516. Took 0.04 sec\n",
            "Epoch 189, Loss(train/val) 0.27492/0.18302. Took 0.05 sec\n",
            "Epoch 190, Loss(train/val) 0.28740/0.18485. Took 0.04 sec\n",
            "Epoch 191, Loss(train/val) 0.27953/0.19717. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 0.28493/0.18780. Took 0.04 sec\n",
            "Epoch 193, Loss(train/val) 0.27696/0.18130. Took 0.04 sec\n",
            "Epoch 194, Loss(train/val) 0.27631/0.18016. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 0.27463/0.18776. Took 0.04 sec\n",
            "Epoch 196, Loss(train/val) 0.28007/0.19101. Took 0.04 sec\n",
            "Epoch 197, Loss(train/val) 0.27431/0.17974. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 0.29125/0.17888. Took 0.04 sec\n",
            "Epoch 199, Loss(train/val) 0.27816/0.18414. Took 0.04 sec\n",
            "Epoch 200, Loss(train/val) 0.28461/0.18721. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 0.28429/0.17951. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 0.27821/0.17700. Took 0.05 sec\n",
            "Epoch 203, Loss(train/val) 0.28312/0.18443. Took 0.04 sec\n",
            "Epoch 204, Loss(train/val) 0.27273/0.17977. Took 0.04 sec\n",
            "Epoch 205, Loss(train/val) 0.27171/0.17445. Took 0.04 sec\n",
            "Epoch 206, Loss(train/val) 0.29329/0.17450. Took 0.04 sec\n",
            "Epoch 207, Loss(train/val) 0.27039/0.18482. Took 0.04 sec\n",
            "Epoch 208, Loss(train/val) 0.28051/0.18846. Took 0.04 sec\n",
            "Epoch 209, Loss(train/val) 0.25964/0.17585. Took 0.04 sec\n",
            "Epoch 210, Loss(train/val) 0.27338/0.17250. Took 0.04 sec\n",
            "Epoch 211, Loss(train/val) 0.28570/0.17329. Took 0.04 sec\n",
            "Epoch 212, Loss(train/val) 0.28792/0.18234. Took 0.04 sec\n",
            "Epoch 213, Loss(train/val) 0.27521/0.18304. Took 0.04 sec\n",
            "Epoch 214, Loss(train/val) 0.26917/0.17221. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.27799/0.17084. Took 0.05 sec\n",
            "Epoch 216, Loss(train/val) 0.27410/0.17466. Took 0.04 sec\n",
            "Epoch 217, Loss(train/val) 0.27468/0.18855. Took 0.04 sec\n",
            "Epoch 218, Loss(train/val) 0.27982/0.19042. Took 0.04 sec\n",
            "Epoch 219, Loss(train/val) 0.26887/0.17429. Took 0.04 sec\n",
            "Epoch 220, Loss(train/val) 0.26571/0.17068. Took 0.05 sec\n",
            "Epoch 221, Loss(train/val) 0.26744/0.17794. Took 0.04 sec\n",
            "Epoch 222, Loss(train/val) 0.27329/0.18452. Took 0.04 sec\n",
            "Epoch 223, Loss(train/val) 0.27776/0.17615. Took 0.04 sec\n",
            "Epoch 224, Loss(train/val) 0.28453/0.17182. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 0.26479/0.17880. Took 0.04 sec\n",
            "Epoch 226, Loss(train/val) 0.26400/0.17741. Took 0.04 sec\n",
            "Epoch 227, Loss(train/val) 0.27177/0.17629. Took 0.04 sec\n",
            "Epoch 228, Loss(train/val) 0.27688/0.16998. Took 0.04 sec\n",
            "Epoch 229, Loss(train/val) 0.27628/0.16921. Took 0.04 sec\n",
            "Epoch 230, Loss(train/val) 0.25578/0.17225. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 0.26619/0.17994. Took 0.04 sec\n",
            "Epoch 232, Loss(train/val) 0.26138/0.17744. Took 0.04 sec\n",
            "Epoch 233, Loss(train/val) 0.27537/0.16986. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 0.26119/0.17205. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 0.27180/0.18486. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 0.27762/0.17351. Took 0.04 sec\n",
            "Epoch 237, Loss(train/val) 0.26483/0.16963. Took 0.04 sec\n",
            "Epoch 238, Loss(train/val) 0.27275/0.16815. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 0.25847/0.17878. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 0.26196/0.17815. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 0.26316/0.17198. Took 0.04 sec\n",
            "Epoch 242, Loss(train/val) 0.30925/0.16877. Took 0.04 sec\n",
            "Epoch 243, Loss(train/val) 0.27685/0.16790. Took 0.04 sec\n",
            "Epoch 244, Loss(train/val) 0.26169/0.17045. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.26379/0.17773. Took 0.04 sec\n",
            "Epoch 246, Loss(train/val) 0.27056/0.17440. Took 0.04 sec\n",
            "Epoch 247, Loss(train/val) 0.26147/0.17184. Took 0.04 sec\n",
            "Epoch 248, Loss(train/val) 0.26614/0.16897. Took 0.04 sec\n",
            "Epoch 249, Loss(train/val) 0.26527/0.16796. Took 0.05 sec\n",
            "Epoch 250, Loss(train/val) 0.26743/0.17236. Took 0.04 sec\n",
            "Epoch 251, Loss(train/val) 0.27066/0.17571. Took 0.04 sec\n",
            "Epoch 252, Loss(train/val) 0.27084/0.16917. Took 0.04 sec\n",
            "Epoch 253, Loss(train/val) 0.25437/0.16794. Took 0.04 sec\n",
            "Epoch 254, Loss(train/val) 0.27014/0.17091. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.25804/0.17063. Took 0.04 sec\n",
            "Epoch 256, Loss(train/val) 0.26197/0.17117. Took 0.04 sec\n",
            "Epoch 257, Loss(train/val) 0.25616/0.17838. Took 0.04 sec\n",
            "Epoch 258, Loss(train/val) 0.27267/0.17853. Took 0.04 sec\n",
            "Epoch 259, Loss(train/val) 0.27317/0.17096. Took 0.04 sec\n",
            "Epoch 260, Loss(train/val) 0.26219/0.16690. Took 0.04 sec\n",
            "Epoch 261, Loss(train/val) 0.27921/0.16906. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.26690/0.17331. Took 0.05 sec\n",
            "Epoch 263, Loss(train/val) 0.26934/0.17268. Took 0.04 sec\n",
            "Epoch 264, Loss(train/val) 0.25696/0.16913. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.26299/0.16825. Took 0.04 sec\n",
            "Epoch 266, Loss(train/val) 0.25937/0.16931. Took 0.04 sec\n",
            "Epoch 267, Loss(train/val) 0.25756/0.17237. Took 0.04 sec\n",
            "Epoch 268, Loss(train/val) 0.26212/0.17742. Took 0.04 sec\n",
            "Epoch 269, Loss(train/val) 0.26288/0.17329. Took 0.04 sec\n",
            "Epoch 270, Loss(train/val) 0.26420/0.16719. Took 0.04 sec\n",
            "Epoch 271, Loss(train/val) 0.29504/0.16686. Took 0.04 sec\n",
            "Epoch 272, Loss(train/val) 0.26201/0.17203. Took 0.04 sec\n",
            "Epoch 273, Loss(train/val) 0.25812/0.16884. Took 0.05 sec\n",
            "Epoch 274, Loss(train/val) 0.26589/0.16908. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.26399/0.16769. Took 0.04 sec\n",
            "Epoch 276, Loss(train/val) 0.25710/0.16850. Took 0.04 sec\n",
            "Epoch 277, Loss(train/val) 0.25997/0.16968. Took 0.04 sec\n",
            "Epoch 278, Loss(train/val) 0.25774/0.16866. Took 0.04 sec\n",
            "Epoch 279, Loss(train/val) 0.25632/0.17518. Took 0.04 sec\n",
            "Epoch 280, Loss(train/val) 0.26054/0.16859. Took 0.04 sec\n",
            "Epoch 281, Loss(train/val) 0.26406/0.16730. Took 0.04 sec\n",
            "Epoch 282, Loss(train/val) 0.25767/0.17111. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 0.26175/0.17205. Took 0.04 sec\n",
            "Epoch 284, Loss(train/val) 0.25172/0.16604. Took 0.04 sec\n",
            "Epoch 285, Loss(train/val) 0.25482/0.16695. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.25825/0.17270. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.26204/0.17429. Took 0.04 sec\n",
            "Epoch 288, Loss(train/val) 0.26766/0.18063. Took 0.04 sec\n",
            "Epoch 289, Loss(train/val) 0.25595/0.16810. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.28030/0.16727. Took 0.04 sec\n",
            "Epoch 291, Loss(train/val) 0.25952/0.17382. Took 0.04 sec\n",
            "Epoch 292, Loss(train/val) 0.27474/0.17582. Took 0.04 sec\n",
            "Epoch 293, Loss(train/val) 0.28588/0.16746. Took 0.04 sec\n",
            "Epoch 294, Loss(train/val) 0.26841/0.16545. Took 0.05 sec\n",
            "Epoch 295, Loss(train/val) 0.25764/0.16660. Took 0.04 sec\n",
            "Epoch 296, Loss(train/val) 0.25823/0.17652. Took 0.04 sec\n",
            "Epoch 297, Loss(train/val) 0.25536/0.17004. Took 0.04 sec\n",
            "Epoch 298, Loss(train/val) 0.25801/0.16492. Took 0.04 sec\n",
            "Epoch 299, Loss(train/val) 0.25195/0.16492. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.25174/0.16585. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.27069/0.17212. Took 0.04 sec\n",
            "Epoch 302, Loss(train/val) 0.26128/0.16664. Took 0.04 sec\n",
            "Epoch 303, Loss(train/val) 0.25510/0.16632. Took 0.04 sec\n",
            "Epoch 304, Loss(train/val) 0.24434/0.16620. Took 0.04 sec\n",
            "Epoch 305, Loss(train/val) 0.26197/0.16572. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 0.25921/0.17282. Took 0.04 sec\n",
            "Epoch 307, Loss(train/val) 0.25215/0.17001. Took 0.04 sec\n",
            "Epoch 308, Loss(train/val) 0.25311/0.16774. Took 0.04 sec\n",
            "Epoch 309, Loss(train/val) 0.24797/0.16532. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.25814/0.16652. Took 0.05 sec\n",
            "Epoch 311, Loss(train/val) 0.24959/0.16630. Took 0.04 sec\n",
            "Epoch 312, Loss(train/val) 0.26440/0.16856. Took 0.04 sec\n",
            "Epoch 313, Loss(train/val) 0.24970/0.16849. Took 0.04 sec\n",
            "Epoch 314, Loss(train/val) 0.25123/0.16744. Took 0.04 sec\n",
            "Epoch 315, Loss(train/val) 0.24851/0.16682. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.25183/0.16582. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.25210/0.16748. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.25224/0.17176. Took 0.04 sec\n",
            "Epoch 319, Loss(train/val) 0.25129/0.16881. Took 0.04 sec\n",
            "Epoch 320, Loss(train/val) 0.25205/0.16455. Took 0.05 sec\n",
            "Epoch 321, Loss(train/val) 0.24912/0.16453. Took 0.04 sec\n",
            "Epoch 322, Loss(train/val) 0.26308/0.17104. Took 0.04 sec\n",
            "Epoch 323, Loss(train/val) 0.24937/0.17030. Took 0.04 sec\n",
            "Epoch 324, Loss(train/val) 0.25357/0.16761. Took 0.04 sec\n",
            "Epoch 325, Loss(train/val) 0.26020/0.16521. Took 0.04 sec\n",
            "Epoch 326, Loss(train/val) 0.27375/0.16599. Took 0.05 sec\n",
            "Epoch 327, Loss(train/val) 0.24645/0.16547. Took 0.04 sec\n",
            "Epoch 328, Loss(train/val) 0.26088/0.16558. Took 0.04 sec\n",
            "Epoch 329, Loss(train/val) 0.24883/0.16810. Took 0.04 sec\n",
            "Epoch 330, Loss(train/val) 0.24662/0.16489. Took 0.06 sec\n",
            "Epoch 331, Loss(train/val) 0.24955/0.16768. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 0.26004/0.16615. Took 0.05 sec\n",
            "Epoch 333, Loss(train/val) 0.25669/0.16519. Took 0.04 sec\n",
            "Epoch 334, Loss(train/val) 0.24195/0.16442. Took 0.04 sec\n",
            "Epoch 335, Loss(train/val) 0.25727/0.16543. Took 0.05 sec\n",
            "Epoch 336, Loss(train/val) 0.25291/0.17232. Took 0.04 sec\n",
            "Epoch 337, Loss(train/val) 0.24826/0.16978. Took 0.04 sec\n",
            "Epoch 338, Loss(train/val) 0.25023/0.16345. Took 0.04 sec\n",
            "Epoch 339, Loss(train/val) 0.24579/0.16532. Took 0.04 sec\n",
            "Epoch 340, Loss(train/val) 0.24348/0.16893. Took 0.05 sec\n",
            "Epoch 341, Loss(train/val) 0.25253/0.17287. Took 0.04 sec\n",
            "Epoch 342, Loss(train/val) 0.24803/0.16440. Took 0.04 sec\n",
            "Epoch 343, Loss(train/val) 0.26040/0.16339. Took 0.04 sec\n",
            "Epoch 344, Loss(train/val) 0.24782/0.16475. Took 0.04 sec\n",
            "Epoch 345, Loss(train/val) 0.24711/0.16562. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.25256/0.16646. Took 0.04 sec\n",
            "Epoch 347, Loss(train/val) 0.24561/0.16691. Took 0.04 sec\n",
            "Epoch 348, Loss(train/val) 0.24471/0.16566. Took 0.04 sec\n",
            "Epoch 349, Loss(train/val) 0.24408/0.16738. Took 0.05 sec\n",
            "Epoch 350, Loss(train/val) 0.25009/0.16624. Took 0.05 sec\n",
            "Epoch 351, Loss(train/val) 0.26164/0.16960. Took 0.04 sec\n",
            "Epoch 352, Loss(train/val) 0.25087/0.16391. Took 0.04 sec\n",
            "Epoch 353, Loss(train/val) 0.25056/0.16373. Took 0.04 sec\n",
            "Epoch 354, Loss(train/val) 0.25929/0.16371. Took 0.04 sec\n",
            "Epoch 355, Loss(train/val) 0.25692/0.16464. Took 0.06 sec\n",
            "Epoch 356, Loss(train/val) 0.24813/0.16762. Took 0.04 sec\n",
            "Epoch 357, Loss(train/val) 0.25016/0.16556. Took 0.04 sec\n",
            "Epoch 358, Loss(train/val) 0.24142/0.16350. Took 0.04 sec\n",
            "Epoch 359, Loss(train/val) 0.24239/0.16457. Took 0.05 sec\n",
            "Epoch 360, Loss(train/val) 0.25319/0.16428. Took 0.05 sec\n",
            "Epoch 361, Loss(train/val) 0.24862/0.16545. Took 0.04 sec\n",
            "Epoch 362, Loss(train/val) 0.25364/0.16653. Took 0.04 sec\n",
            "Epoch 363, Loss(train/val) 0.25783/0.16818. Took 0.04 sec\n",
            "Epoch 364, Loss(train/val) 0.25362/0.17235. Took 0.04 sec\n",
            "Epoch 365, Loss(train/val) 0.25380/0.16442. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.24858/0.16315. Took 0.04 sec\n",
            "Epoch 367, Loss(train/val) 0.24192/0.16622. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.24254/0.16907. Took 0.04 sec\n",
            "Epoch 369, Loss(train/val) 0.24125/0.16612. Took 0.04 sec\n",
            "Epoch 370, Loss(train/val) 0.24289/0.16293. Took 0.05 sec\n",
            "Epoch 371, Loss(train/val) 0.26218/0.16468. Took 0.04 sec\n",
            "Epoch 372, Loss(train/val) 0.24387/0.16240. Took 0.04 sec\n",
            "Epoch 373, Loss(train/val) 0.23724/0.16933. Took 0.04 sec\n",
            "Epoch 374, Loss(train/val) 0.24248/0.17377. Took 0.04 sec\n",
            "Epoch 375, Loss(train/val) 0.24456/0.16331. Took 0.04 sec\n",
            "Epoch 376, Loss(train/val) 0.24520/0.16500. Took 0.04 sec\n",
            "Epoch 377, Loss(train/val) 0.24873/0.16183. Took 0.04 sec\n",
            "Epoch 378, Loss(train/val) 0.23686/0.17176. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 0.24475/0.16486. Took 0.04 sec\n",
            "Epoch 380, Loss(train/val) 0.24209/0.16222. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 0.25488/0.16205. Took 0.04 sec\n",
            "Epoch 382, Loss(train/val) 0.24291/0.16298. Took 0.04 sec\n",
            "Epoch 383, Loss(train/val) 0.23829/0.16268. Took 0.04 sec\n",
            "Epoch 384, Loss(train/val) 0.24084/0.16507. Took 0.04 sec\n",
            "Epoch 385, Loss(train/val) 0.25009/0.16658. Took 0.04 sec\n",
            "Epoch 386, Loss(train/val) 0.24321/0.16252. Took 0.04 sec\n",
            "Epoch 387, Loss(train/val) 0.24874/0.16209. Took 0.04 sec\n",
            "Epoch 388, Loss(train/val) 0.24289/0.16199. Took 0.04 sec\n",
            "Epoch 389, Loss(train/val) 0.24229/0.16197. Took 0.04 sec\n",
            "Epoch 390, Loss(train/val) 0.24504/0.16321. Took 0.04 sec\n",
            "Epoch 391, Loss(train/val) 0.23786/0.16536. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.24720/0.16292. Took 0.04 sec\n",
            "Epoch 393, Loss(train/val) 0.24327/0.16583. Took 0.04 sec\n",
            "Epoch 394, Loss(train/val) 0.24750/0.17030. Took 0.04 sec\n",
            "Epoch 395, Loss(train/val) 0.23648/0.16194. Took 0.05 sec\n",
            "Epoch 396, Loss(train/val) 0.25935/0.16042. Took 0.05 sec\n",
            "Epoch 397, Loss(train/val) 0.24248/0.16763. Took 0.04 sec\n",
            "Epoch 398, Loss(train/val) 0.24473/0.16871. Took 0.04 sec\n",
            "Epoch 399, Loss(train/val) 0.24017/0.16892. Took 0.05 sec\n",
            "Epoch 400, Loss(train/val) 0.25952/0.16416. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 0.23637/0.16353. Took 0.07 sec\n",
            "Epoch 402, Loss(train/val) 0.24323/0.16207. Took 0.04 sec\n",
            "Epoch 403, Loss(train/val) 0.24418/0.16237. Took 0.04 sec\n",
            "Epoch 404, Loss(train/val) 0.24767/0.16179. Took 0.04 sec\n",
            "Epoch 405, Loss(train/val) 0.24522/0.16320. Took 0.05 sec\n",
            "Epoch 406, Loss(train/val) 0.24651/0.16436. Took 0.04 sec\n",
            "Epoch 407, Loss(train/val) 0.24820/0.16239. Took 0.04 sec\n",
            "Epoch 408, Loss(train/val) 0.26163/0.16407. Took 0.04 sec\n",
            "Epoch 409, Loss(train/val) 0.24921/0.16269. Took 0.04 sec\n",
            "Epoch 410, Loss(train/val) 0.23752/0.16132. Took 0.05 sec\n",
            "Epoch 411, Loss(train/val) 0.23798/0.16051. Took 0.04 sec\n",
            "Epoch 412, Loss(train/val) 0.23616/0.16046. Took 0.04 sec\n",
            "Epoch 413, Loss(train/val) 0.24465/0.16077. Took 0.04 sec\n",
            "Epoch 414, Loss(train/val) 0.24159/0.16437. Took 0.04 sec\n",
            "Epoch 415, Loss(train/val) 0.24104/0.16351. Took 0.05 sec\n",
            "Epoch 416, Loss(train/val) 0.23733/0.16938. Took 0.04 sec\n",
            "Epoch 417, Loss(train/val) 0.23260/0.16931. Took 0.04 sec\n",
            "Epoch 418, Loss(train/val) 0.24507/0.16811. Took 0.04 sec\n",
            "Epoch 419, Loss(train/val) 0.24112/0.16571. Took 0.05 sec\n",
            "Epoch 420, Loss(train/val) 0.23816/0.16148. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.23579/0.16093. Took 0.04 sec\n",
            "Epoch 422, Loss(train/val) 0.24476/0.16165. Took 0.04 sec\n",
            "Epoch 423, Loss(train/val) 0.24407/0.16101. Took 0.04 sec\n",
            "Epoch 424, Loss(train/val) 0.24444/0.16068. Took 0.04 sec\n",
            "Epoch 425, Loss(train/val) 0.24340/0.16168. Took 0.05 sec\n",
            "Epoch 426, Loss(train/val) 0.25403/0.16516. Took 0.04 sec\n",
            "Epoch 427, Loss(train/val) 0.23719/0.16048. Took 0.04 sec\n",
            "Epoch 428, Loss(train/val) 0.25256/0.16131. Took 0.04 sec\n",
            "Epoch 429, Loss(train/val) 0.23399/0.17071. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.24793/0.17004. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.23565/0.16320. Took 0.04 sec\n",
            "Epoch 432, Loss(train/val) 0.24858/0.16053. Took 0.04 sec\n",
            "Epoch 433, Loss(train/val) 0.23322/0.16094. Took 0.06 sec\n",
            "Epoch 434, Loss(train/val) 0.25241/0.16041. Took 0.04 sec\n",
            "Epoch 435, Loss(train/val) 0.23494/0.16027. Took 0.05 sec\n",
            "Epoch 436, Loss(train/val) 0.23436/0.15933. Took 0.04 sec\n",
            "Epoch 437, Loss(train/val) 0.24071/0.16474. Took 0.04 sec\n",
            "Epoch 438, Loss(train/val) 0.23965/0.16108. Took 0.04 sec\n",
            "Epoch 439, Loss(train/val) 0.23238/0.16041. Took 0.04 sec\n",
            "Epoch 440, Loss(train/val) 0.23634/0.16150. Took 0.05 sec\n",
            "Epoch 441, Loss(train/val) 0.24327/0.16194. Took 0.04 sec\n",
            "Epoch 442, Loss(train/val) 0.23386/0.16187. Took 0.04 sec\n",
            "Epoch 443, Loss(train/val) 0.23372/0.16064. Took 0.04 sec\n",
            "Epoch 444, Loss(train/val) 0.23067/0.16239. Took 0.04 sec\n",
            "Epoch 445, Loss(train/val) 0.23674/0.16609. Took 0.05 sec\n",
            "Epoch 446, Loss(train/val) 0.24259/0.16846. Took 0.05 sec\n",
            "Epoch 447, Loss(train/val) 0.24250/0.16241. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 0.22436/0.16409. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 0.23277/0.17198. Took 0.04 sec\n",
            "Epoch 450, Loss(train/val) 0.23490/0.17294. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 0.23491/0.16844. Took 0.05 sec\n",
            "Epoch 452, Loss(train/val) 0.23534/0.15841. Took 0.04 sec\n",
            "Epoch 453, Loss(train/val) 0.25022/0.16422. Took 0.04 sec\n",
            "Epoch 454, Loss(train/val) 0.23649/0.15870. Took 0.04 sec\n",
            "Epoch 455, Loss(train/val) 0.24536/0.16557. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.23282/0.16003. Took 0.04 sec\n",
            "Epoch 457, Loss(train/val) 0.24351/0.16083. Took 0.04 sec\n",
            "Epoch 458, Loss(train/val) 0.23593/0.15848. Took 0.04 sec\n",
            "Epoch 459, Loss(train/val) 0.23265/0.16344. Took 0.04 sec\n",
            "Epoch 460, Loss(train/val) 0.23414/0.16745. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.23203/0.15976. Took 0.04 sec\n",
            "Epoch 462, Loss(train/val) 0.23991/0.15890. Took 0.04 sec\n",
            "Epoch 463, Loss(train/val) 0.23367/0.16075. Took 0.04 sec\n",
            "Epoch 464, Loss(train/val) 0.23224/0.16153. Took 0.04 sec\n",
            "Epoch 465, Loss(train/val) 0.22900/0.15814. Took 0.04 sec\n",
            "Epoch 466, Loss(train/val) 0.23487/0.15784. Took 0.05 sec\n",
            "Epoch 467, Loss(train/val) 0.23581/0.15977. Took 0.04 sec\n",
            "Epoch 468, Loss(train/val) 0.23615/0.15899. Took 0.04 sec\n",
            "Epoch 469, Loss(train/val) 0.23458/0.15933. Took 0.04 sec\n",
            "Epoch 470, Loss(train/val) 0.23090/0.16062. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 0.23759/0.16139. Took 0.04 sec\n",
            "Epoch 472, Loss(train/val) 0.23748/0.16051. Took 0.04 sec\n",
            "Epoch 473, Loss(train/val) 0.23282/0.16033. Took 0.04 sec\n",
            "Epoch 474, Loss(train/val) 0.23272/0.15847. Took 0.04 sec\n",
            "Epoch 475, Loss(train/val) 0.24059/0.16701. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.23431/0.16091. Took 0.04 sec\n",
            "Epoch 477, Loss(train/val) 0.23225/0.15767. Took 0.04 sec\n",
            "Epoch 478, Loss(train/val) 0.23610/0.15758. Took 0.04 sec\n",
            "Epoch 479, Loss(train/val) 0.23011/0.15979. Took 0.04 sec\n",
            "Epoch 480, Loss(train/val) 0.23123/0.16440. Took 0.05 sec\n",
            "Epoch 481, Loss(train/val) 0.24815/0.15957. Took 0.05 sec\n",
            "Epoch 482, Loss(train/val) 0.23546/0.15933. Took 0.04 sec\n",
            "Epoch 483, Loss(train/val) 0.23243/0.15923. Took 0.04 sec\n",
            "Epoch 484, Loss(train/val) 0.23093/0.15699. Took 0.04 sec\n",
            "Epoch 485, Loss(train/val) 0.23715/0.15825. Took 0.04 sec\n",
            "Epoch 486, Loss(train/val) 0.22965/0.16666. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.24156/0.16536. Took 0.04 sec\n",
            "Epoch 488, Loss(train/val) 0.23734/0.15830. Took 0.04 sec\n",
            "Epoch 489, Loss(train/val) 0.24805/0.15922. Took 0.04 sec\n",
            "Epoch 490, Loss(train/val) 0.23570/0.15729. Took 0.04 sec\n",
            "Epoch 491, Loss(train/val) 0.23891/0.16702. Took 0.04 sec\n",
            "Epoch 492, Loss(train/val) 0.24898/0.16718. Took 0.04 sec\n",
            "Epoch 493, Loss(train/val) 0.23284/0.16096. Took 0.04 sec\n",
            "Epoch 494, Loss(train/val) 0.22661/0.15641. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.23194/0.15887. Took 0.05 sec\n",
            "Epoch 496, Loss(train/val) 0.22784/0.18539. Took 0.04 sec\n",
            "Epoch 497, Loss(train/val) 0.23766/0.19219. Took 0.04 sec\n",
            "Epoch 498, Loss(train/val) 0.23614/0.15858. Took 0.04 sec\n",
            "Epoch 499, Loss(train/val) 0.23581/0.15691. Took 0.04 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=500, exp_name='exp2_hidn', hid_dim=64, input_dim=1, l2=1e-05, lr=0.0001, n_layers=6, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.06689/0.35434. Took 0.05 sec\n",
            "Epoch 1, Loss(train/val) 1.04344/0.35426. Took 0.05 sec\n",
            "Epoch 2, Loss(train/val) 1.04886/0.35419. Took 0.05 sec\n",
            "Epoch 3, Loss(train/val) 1.05578/0.35414. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.05384/0.35409. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.06229/0.35405. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.05838/0.35401. Took 0.05 sec\n",
            "Epoch 7, Loss(train/val) 1.05991/0.35398. Took 0.05 sec\n",
            "Epoch 8, Loss(train/val) 1.05224/0.35398. Took 0.05 sec\n",
            "Epoch 9, Loss(train/val) 1.05736/0.35400. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.05417/0.35403. Took 0.06 sec\n",
            "Epoch 11, Loss(train/val) 1.03692/0.35409. Took 0.05 sec\n",
            "Epoch 12, Loss(train/val) 1.04424/0.35415. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 1.04020/0.35422. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.04058/0.35432. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.03787/0.35447. Took 0.06 sec\n",
            "Epoch 16, Loss(train/val) 1.03384/0.35454. Took 0.05 sec\n",
            "Epoch 17, Loss(train/val) 1.02729/0.35456. Took 0.04 sec\n",
            "Epoch 18, Loss(train/val) 1.00961/0.35456. Took 0.05 sec\n",
            "Epoch 19, Loss(train/val) 1.00643/0.35454. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 0.97737/0.35408. Took 0.05 sec\n",
            "Epoch 21, Loss(train/val) 0.96655/0.35342. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 0.94613/0.35244. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 0.93535/0.35162. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 0.90395/0.35099. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 0.86205/0.35010. Took 0.05 sec\n",
            "Epoch 26, Loss(train/val) 0.82519/0.34859. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 0.78082/0.34674. Took 0.05 sec\n",
            "Epoch 28, Loss(train/val) 0.72952/0.34537. Took 0.05 sec\n",
            "Epoch 29, Loss(train/val) 0.69532/0.34580. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 0.64809/0.34920. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 0.59983/0.35538. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 0.56493/0.35967. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 0.55104/0.35641. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 0.51658/0.33994. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 0.50696/0.31198. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 0.47341/0.27672. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 0.47980/0.24748. Took 0.05 sec\n",
            "Epoch 38, Loss(train/val) 0.45913/0.24411. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 0.44594/0.26856. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 0.42142/0.30931. Took 0.05 sec\n",
            "Epoch 41, Loss(train/val) 0.42063/0.35356. Took 0.05 sec\n",
            "Epoch 42, Loss(train/val) 0.40631/0.38497. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 0.41476/0.40103. Took 0.05 sec\n",
            "Epoch 44, Loss(train/val) 0.39742/0.40842. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 0.37888/0.40060. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 0.38485/0.38333. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 0.38388/0.36753. Took 0.04 sec\n",
            "Epoch 48, Loss(train/val) 0.38456/0.34478. Took 0.05 sec\n",
            "Epoch 49, Loss(train/val) 0.37309/0.32832. Took 0.05 sec\n",
            "Epoch 50, Loss(train/val) 0.37902/0.31465. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 0.35909/0.30162. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 0.37966/0.28744. Took 0.05 sec\n",
            "Epoch 53, Loss(train/val) 0.36527/0.27296. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 0.37289/0.26214. Took 0.05 sec\n",
            "Epoch 55, Loss(train/val) 0.35899/0.25899. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 0.36606/0.25221. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 0.37283/0.24181. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 0.36346/0.24060. Took 0.05 sec\n",
            "Epoch 59, Loss(train/val) 0.37017/0.23964. Took 0.05 sec\n",
            "Epoch 60, Loss(train/val) 0.36253/0.23963. Took 0.05 sec\n",
            "Epoch 61, Loss(train/val) 0.35480/0.23903. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 0.36409/0.23836. Took 0.05 sec\n",
            "Epoch 63, Loss(train/val) 0.36680/0.23809. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 0.35923/0.23905. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 0.36313/0.23932. Took 0.06 sec\n",
            "Epoch 66, Loss(train/val) 0.36346/0.24109. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 0.35287/0.24377. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 0.35806/0.24905. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 0.36101/0.25338. Took 0.05 sec\n",
            "Epoch 70, Loss(train/val) 0.35775/0.25832. Took 0.06 sec\n",
            "Epoch 71, Loss(train/val) 0.35609/0.26069. Took 0.04 sec\n",
            "Epoch 72, Loss(train/val) 0.36146/0.25998. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 0.36055/0.25582. Took 0.05 sec\n",
            "Epoch 74, Loss(train/val) 0.35838/0.25320. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 0.37683/0.24728. Took 0.06 sec\n",
            "Epoch 76, Loss(train/val) 0.34930/0.24290. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 0.37717/0.24134. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 0.36059/0.24396. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 0.35621/0.24533. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 0.36029/0.25061. Took 0.06 sec\n",
            "Epoch 81, Loss(train/val) 0.34694/0.25439. Took 0.05 sec\n",
            "Epoch 82, Loss(train/val) 0.36101/0.24734. Took 0.05 sec\n",
            "Epoch 83, Loss(train/val) 0.36745/0.24376. Took 0.05 sec\n",
            "Epoch 84, Loss(train/val) 0.35758/0.24535. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 0.35781/0.24560. Took 0.05 sec\n",
            "Epoch 86, Loss(train/val) 0.35423/0.24500. Took 0.05 sec\n",
            "Epoch 87, Loss(train/val) 0.35079/0.24530. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 0.36425/0.24376. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 0.36101/0.24086. Took 0.06 sec\n",
            "Epoch 90, Loss(train/val) 0.35794/0.24099. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 0.35512/0.24214. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 0.35640/0.24485. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 0.35795/0.24484. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 0.36367/0.24623. Took 0.06 sec\n",
            "Epoch 95, Loss(train/val) 0.35846/0.24885. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 0.35437/0.24285. Took 0.05 sec\n",
            "Epoch 97, Loss(train/val) 0.34366/0.24264. Took 0.05 sec\n",
            "Epoch 98, Loss(train/val) 0.35637/0.23859. Took 0.05 sec\n",
            "Epoch 99, Loss(train/val) 0.35550/0.23852. Took 0.06 sec\n",
            "Epoch 100, Loss(train/val) 0.35068/0.23783. Took 0.05 sec\n",
            "Epoch 101, Loss(train/val) 0.35350/0.23688. Took 0.05 sec\n",
            "Epoch 102, Loss(train/val) 0.36171/0.23696. Took 0.05 sec\n",
            "Epoch 103, Loss(train/val) 0.35474/0.23504. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 0.34326/0.23751. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 0.36164/0.23808. Took 0.05 sec\n",
            "Epoch 106, Loss(train/val) 0.34049/0.23896. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 0.35895/0.23616. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 0.35231/0.23855. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 0.35361/0.23938. Took 0.05 sec\n",
            "Epoch 110, Loss(train/val) 0.34404/0.23913. Took 0.05 sec\n",
            "Epoch 111, Loss(train/val) 0.35475/0.23879. Took 0.05 sec\n",
            "Epoch 112, Loss(train/val) 0.34845/0.24193. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 0.35046/0.24168. Took 0.04 sec\n",
            "Epoch 114, Loss(train/val) 0.35235/0.24187. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 0.35712/0.23967. Took 0.05 sec\n",
            "Epoch 116, Loss(train/val) 0.34559/0.23655. Took 0.05 sec\n",
            "Epoch 117, Loss(train/val) 0.33601/0.23494. Took 0.05 sec\n",
            "Epoch 118, Loss(train/val) 0.35763/0.23397. Took 0.05 sec\n",
            "Epoch 119, Loss(train/val) 0.34440/0.23359. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 0.35105/0.23435. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 0.35804/0.23545. Took 0.06 sec\n",
            "Epoch 122, Loss(train/val) 0.35131/0.23701. Took 0.04 sec\n",
            "Epoch 123, Loss(train/val) 0.35073/0.23738. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 0.34948/0.23729. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 0.34842/0.23429. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 0.35080/0.23088. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 0.35199/0.22938. Took 0.05 sec\n",
            "Epoch 128, Loss(train/val) 0.36793/0.22856. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 0.35389/0.23030. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 0.34727/0.23024. Took 0.05 sec\n",
            "Epoch 131, Loss(train/val) 0.35767/0.22958. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 0.35088/0.23065. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 0.35183/0.23216. Took 0.06 sec\n",
            "Epoch 134, Loss(train/val) 0.34879/0.23308. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 0.35250/0.23653. Took 0.05 sec\n",
            "Epoch 136, Loss(train/val) 0.34431/0.23675. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 0.34419/0.23666. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 0.34240/0.23482. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 0.34830/0.23692. Took 0.06 sec\n",
            "Epoch 140, Loss(train/val) 0.34201/0.23654. Took 0.05 sec\n",
            "Epoch 141, Loss(train/val) 0.34127/0.23549. Took 0.06 sec\n",
            "Epoch 142, Loss(train/val) 0.34866/0.23452. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 0.34786/0.23178. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 0.35215/0.23037. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 0.34749/0.22936. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 0.34356/0.22928. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 0.34280/0.23061. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 0.34632/0.23012. Took 0.06 sec\n",
            "Epoch 149, Loss(train/val) 0.34240/0.22923. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 0.34106/0.22843. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 0.34306/0.22925. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 0.33672/0.23002. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 0.34822/0.22964. Took 0.06 sec\n",
            "Epoch 154, Loss(train/val) 0.34801/0.22778. Took 0.05 sec\n",
            "Epoch 155, Loss(train/val) 0.34330/0.22795. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 0.35121/0.22739. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 0.34740/0.22624. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 0.34043/0.22693. Took 0.06 sec\n",
            "Epoch 159, Loss(train/val) 0.33679/0.22602. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 0.34079/0.22469. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 0.34894/0.22277. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 0.33793/0.22278. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 0.33695/0.22230. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 0.34611/0.22114. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 0.34393/0.22084. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.33890/0.22098. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 0.33995/0.21967. Took 0.05 sec\n",
            "Epoch 168, Loss(train/val) 0.34670/0.21982. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 0.34736/0.22297. Took 0.05 sec\n",
            "Epoch 170, Loss(train/val) 0.33660/0.22022. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 0.34174/0.21977. Took 0.05 sec\n",
            "Epoch 172, Loss(train/val) 0.33874/0.22091. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 0.34033/0.22183. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 0.34753/0.22275. Took 0.05 sec\n",
            "Epoch 175, Loss(train/val) 0.34845/0.22343. Took 0.05 sec\n",
            "Epoch 176, Loss(train/val) 0.34705/0.22139. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 0.33581/0.22170. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 0.33698/0.22021. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 0.36163/0.22104. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 0.34332/0.21803. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 0.33647/0.21529. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 0.34120/0.21506. Took 0.05 sec\n",
            "Epoch 183, Loss(train/val) 0.33635/0.21587. Took 0.06 sec\n",
            "Epoch 184, Loss(train/val) 0.33710/0.21728. Took 0.05 sec\n",
            "Epoch 185, Loss(train/val) 0.34260/0.21722. Took 0.04 sec\n",
            "Epoch 186, Loss(train/val) 0.33799/0.21593. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 0.33219/0.21650. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 0.31888/0.21622. Took 0.05 sec\n",
            "Epoch 189, Loss(train/val) 0.33574/0.21759. Took 0.05 sec\n",
            "Epoch 190, Loss(train/val) 0.32953/0.21811. Took 0.05 sec\n",
            "Epoch 191, Loss(train/val) 0.33233/0.21663. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 0.33136/0.21717. Took 0.05 sec\n",
            "Epoch 193, Loss(train/val) 0.33165/0.21560. Took 0.05 sec\n",
            "Epoch 194, Loss(train/val) 0.33743/0.21484. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 0.32778/0.21515. Took 0.05 sec\n",
            "Epoch 196, Loss(train/val) 0.33597/0.21616. Took 0.04 sec\n",
            "Epoch 197, Loss(train/val) 0.34036/0.21763. Took 0.05 sec\n",
            "Epoch 198, Loss(train/val) 0.33421/0.21742. Took 0.05 sec\n",
            "Epoch 199, Loss(train/val) 0.31475/0.21617. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 0.32278/0.21536. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 0.32708/0.21427. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 0.33422/0.21260. Took 0.05 sec\n",
            "Epoch 203, Loss(train/val) 0.32594/0.21476. Took 0.06 sec\n",
            "Epoch 204, Loss(train/val) 0.32473/0.21217. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.32894/0.20982. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 0.31930/0.21059. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 0.33893/0.20899. Took 0.05 sec\n",
            "Epoch 208, Loss(train/val) 0.32249/0.21127. Took 0.05 sec\n",
            "Epoch 209, Loss(train/val) 0.31811/0.21416. Took 0.05 sec\n",
            "Epoch 210, Loss(train/val) 0.31479/0.21702. Took 0.05 sec\n",
            "Epoch 211, Loss(train/val) 0.32189/0.21052. Took 0.05 sec\n",
            "Epoch 212, Loss(train/val) 0.31426/0.20677. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 0.31390/0.20527. Took 0.06 sec\n",
            "Epoch 214, Loss(train/val) 0.31722/0.20717. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.31432/0.21061. Took 0.05 sec\n",
            "Epoch 216, Loss(train/val) 0.31953/0.21447. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 0.31637/0.21519. Took 0.05 sec\n",
            "Epoch 218, Loss(train/val) 0.31190/0.20847. Took 0.06 sec\n",
            "Epoch 219, Loss(train/val) 0.30680/0.20427. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 0.30744/0.20765. Took 0.05 sec\n",
            "Epoch 221, Loss(train/val) 0.31102/0.21254. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 0.30852/0.22726. Took 0.05 sec\n",
            "Epoch 223, Loss(train/val) 0.31497/0.22975. Took 0.06 sec\n",
            "Epoch 224, Loss(train/val) 0.30362/0.21690. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 0.29414/0.22170. Took 0.05 sec\n",
            "Epoch 226, Loss(train/val) 0.29789/0.21918. Took 0.05 sec\n",
            "Epoch 227, Loss(train/val) 0.30041/0.23283. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 0.29247/0.23190. Took 0.05 sec\n",
            "Epoch 229, Loss(train/val) 0.29140/0.23307. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 0.29534/0.23693. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 0.29039/0.25016. Took 0.05 sec\n",
            "Epoch 232, Loss(train/val) 0.28682/0.27610. Took 0.05 sec\n",
            "Epoch 233, Loss(train/val) 0.28420/0.28956. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 0.27170/0.29727. Took 0.04 sec\n",
            "Epoch 235, Loss(train/val) 0.28687/0.29936. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 0.27939/0.31638. Took 0.04 sec\n",
            "Epoch 237, Loss(train/val) 0.27648/0.33504. Took 0.04 sec\n",
            "Epoch 238, Loss(train/val) 0.28125/0.31123. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 0.27801/0.26534. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 0.26834/0.25432. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 0.27072/0.26861. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 0.27049/0.27493. Took 0.04 sec\n",
            "Epoch 243, Loss(train/val) 0.27296/0.25974. Took 0.05 sec\n",
            "Epoch 244, Loss(train/val) 0.28242/0.24275. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.27731/0.24160. Took 0.05 sec\n",
            "Epoch 246, Loss(train/val) 0.27009/0.24361. Took 0.05 sec\n",
            "Epoch 247, Loss(train/val) 0.26768/0.25654. Took 0.05 sec\n",
            "Epoch 248, Loss(train/val) 0.26184/0.24480. Took 0.05 sec\n",
            "Epoch 249, Loss(train/val) 0.27128/0.21063. Took 0.05 sec\n",
            "Epoch 250, Loss(train/val) 0.26807/0.20383. Took 0.05 sec\n",
            "Epoch 251, Loss(train/val) 0.26058/0.21525. Took 0.05 sec\n",
            "Epoch 252, Loss(train/val) 0.27323/0.21148. Took 0.04 sec\n",
            "Epoch 253, Loss(train/val) 0.26764/0.20061. Took 0.05 sec\n",
            "Epoch 254, Loss(train/val) 0.26280/0.20859. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.26356/0.20224. Took 0.05 sec\n",
            "Epoch 256, Loss(train/val) 0.25983/0.19264. Took 0.05 sec\n",
            "Epoch 257, Loss(train/val) 0.26412/0.20501. Took 0.04 sec\n",
            "Epoch 258, Loss(train/val) 0.25983/0.20626. Took 0.05 sec\n",
            "Epoch 259, Loss(train/val) 0.26158/0.18839. Took 0.05 sec\n",
            "Epoch 260, Loss(train/val) 0.27230/0.18063. Took 0.05 sec\n",
            "Epoch 261, Loss(train/val) 0.25547/0.18059. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.28647/0.17467. Took 0.05 sec\n",
            "Epoch 263, Loss(train/val) 0.25845/0.17528. Took 0.06 sec\n",
            "Epoch 264, Loss(train/val) 0.25759/0.17415. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.27347/0.17061. Took 0.04 sec\n",
            "Epoch 266, Loss(train/val) 0.25499/0.17335. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 0.25234/0.17976. Took 0.05 sec\n",
            "Epoch 268, Loss(train/val) 0.25624/0.18108. Took 0.05 sec\n",
            "Epoch 269, Loss(train/val) 0.25351/0.18150. Took 0.05 sec\n",
            "Epoch 270, Loss(train/val) 0.25606/0.17165. Took 0.05 sec\n",
            "Epoch 271, Loss(train/val) 0.25929/0.16701. Took 0.04 sec\n",
            "Epoch 272, Loss(train/val) 0.25670/0.16646. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 0.25914/0.18123. Took 0.06 sec\n",
            "Epoch 274, Loss(train/val) 0.24937/0.18966. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.25074/0.18736. Took 0.05 sec\n",
            "Epoch 276, Loss(train/val) 0.25122/0.17621. Took 0.05 sec\n",
            "Epoch 277, Loss(train/val) 0.26887/0.16569. Took 0.05 sec\n",
            "Epoch 278, Loss(train/val) 0.25320/0.16870. Took 0.05 sec\n",
            "Epoch 279, Loss(train/val) 0.25060/0.17962. Took 0.05 sec\n",
            "Epoch 280, Loss(train/val) 0.25139/0.17411. Took 0.05 sec\n",
            "Epoch 281, Loss(train/val) 0.25633/0.17205. Took 0.05 sec\n",
            "Epoch 282, Loss(train/val) 0.25625/0.17262. Took 0.04 sec\n",
            "Epoch 283, Loss(train/val) 0.24170/0.19064. Took 0.06 sec\n",
            "Epoch 284, Loss(train/val) 0.24981/0.21162. Took 0.05 sec\n",
            "Epoch 285, Loss(train/val) 0.25698/0.19263. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.24959/0.17741. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.24964/0.17000. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.24141/0.16843. Took 0.06 sec\n",
            "Epoch 289, Loss(train/val) 0.24333/0.16815. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.24765/0.16576. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.25554/0.16428. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.25083/0.16438. Took 0.04 sec\n",
            "Epoch 293, Loss(train/val) 0.24649/0.16781. Took 0.05 sec\n",
            "Epoch 294, Loss(train/val) 0.25844/0.17853. Took 0.05 sec\n",
            "Epoch 295, Loss(train/val) 0.25578/0.17315. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 0.23818/0.16971. Took 0.04 sec\n",
            "Epoch 297, Loss(train/val) 0.24155/0.17075. Took 0.05 sec\n",
            "Epoch 298, Loss(train/val) 0.23830/0.18029. Took 0.06 sec\n",
            "Epoch 299, Loss(train/val) 0.24470/0.18905. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.24593/0.18288. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.25348/0.17454. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.24080/0.16815. Took 0.05 sec\n",
            "Epoch 303, Loss(train/val) 0.22885/0.16978. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 0.24261/0.17230. Took 0.05 sec\n",
            "Epoch 305, Loss(train/val) 0.25440/0.17346. Took 0.04 sec\n",
            "Epoch 306, Loss(train/val) 0.23637/0.17063. Took 0.05 sec\n",
            "Epoch 307, Loss(train/val) 0.24242/0.17133. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.23765/0.17027. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.25245/0.16544. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.23725/0.17093. Took 0.05 sec\n",
            "Epoch 311, Loss(train/val) 0.24400/0.16719. Took 0.04 sec\n",
            "Epoch 312, Loss(train/val) 0.25775/0.16946. Took 0.04 sec\n",
            "Epoch 313, Loss(train/val) 0.23323/0.16833. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.24644/0.16970. Took 0.05 sec\n",
            "Epoch 315, Loss(train/val) 0.25159/0.16770. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.25087/0.16540. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.24459/0.16902. Took 0.04 sec\n",
            "Epoch 318, Loss(train/val) 0.23532/0.16813. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.24173/0.16495. Took 0.05 sec\n",
            "Epoch 320, Loss(train/val) 0.24419/0.16124. Took 0.05 sec\n",
            "Epoch 321, Loss(train/val) 0.24184/0.16292. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.25050/0.17248. Took 0.05 sec\n",
            "Epoch 323, Loss(train/val) 0.24346/0.18532. Took 0.05 sec\n",
            "Epoch 324, Loss(train/val) 0.24189/0.18984. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.23916/0.17379. Took 0.04 sec\n",
            "Epoch 326, Loss(train/val) 0.24696/0.16349. Took 0.05 sec\n",
            "Epoch 327, Loss(train/val) 0.24259/0.16130. Took 0.05 sec\n",
            "Epoch 328, Loss(train/val) 0.24165/0.16087. Took 0.05 sec\n",
            "Epoch 329, Loss(train/val) 0.22735/0.16153. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.23202/0.16061. Took 0.05 sec\n",
            "Epoch 331, Loss(train/val) 0.24785/0.16185. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 0.25661/0.17437. Took 0.05 sec\n",
            "Epoch 333, Loss(train/val) 0.24063/0.17743. Took 0.06 sec\n",
            "Epoch 334, Loss(train/val) 0.24219/0.17715. Took 0.05 sec\n",
            "Epoch 335, Loss(train/val) 0.25865/0.16555. Took 0.05 sec\n",
            "Epoch 336, Loss(train/val) 0.24228/0.16405. Took 0.05 sec\n",
            "Epoch 337, Loss(train/val) 0.23937/0.16849. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.24359/0.17564. Took 0.05 sec\n",
            "Epoch 339, Loss(train/val) 0.23485/0.16708. Took 0.05 sec\n",
            "Epoch 340, Loss(train/val) 0.23235/0.16363. Took 0.04 sec\n",
            "Epoch 341, Loss(train/val) 0.23437/0.16136. Took 0.04 sec\n",
            "Epoch 342, Loss(train/val) 0.23949/0.15948. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.23942/0.15981. Took 0.05 sec\n",
            "Epoch 344, Loss(train/val) 0.24277/0.16229. Took 0.05 sec\n",
            "Epoch 345, Loss(train/val) 0.22474/0.16501. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.23351/0.16342. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 0.23365/0.15863. Took 0.04 sec\n",
            "Epoch 348, Loss(train/val) 0.23976/0.15856. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 0.23516/0.15984. Took 0.05 sec\n",
            "Epoch 350, Loss(train/val) 0.25143/0.15904. Took 0.04 sec\n",
            "Epoch 351, Loss(train/val) 0.23039/0.16124. Took 0.05 sec\n",
            "Epoch 352, Loss(train/val) 0.22766/0.16265. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 0.23009/0.16269. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 0.22611/0.16228. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 0.24610/0.16119. Took 0.04 sec\n",
            "Epoch 356, Loss(train/val) 0.22443/0.15894. Took 0.05 sec\n",
            "Epoch 357, Loss(train/val) 0.23581/0.16144. Took 0.05 sec\n",
            "Epoch 358, Loss(train/val) 0.23058/0.16587. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 0.23264/0.16140. Took 0.05 sec\n",
            "Epoch 360, Loss(train/val) 0.23546/0.16447. Took 0.04 sec\n",
            "Epoch 361, Loss(train/val) 0.23657/0.16360. Took 0.04 sec\n",
            "Epoch 362, Loss(train/val) 0.25164/0.15766. Took 0.04 sec\n",
            "Epoch 363, Loss(train/val) 0.23899/0.15773. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 0.23699/0.16047. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.23013/0.16695. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.23535/0.16451. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 0.23388/0.16024. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.23886/0.15952. Took 0.05 sec\n",
            "Epoch 369, Loss(train/val) 0.23136/0.15761. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.22930/0.15751. Took 0.05 sec\n",
            "Epoch 371, Loss(train/val) 0.23747/0.15882. Took 0.05 sec\n",
            "Epoch 372, Loss(train/val) 0.25303/0.15735. Took 0.04 sec\n",
            "Epoch 373, Loss(train/val) 0.23158/0.15699. Took 0.06 sec\n",
            "Epoch 374, Loss(train/val) 0.24073/0.15849. Took 0.05 sec\n",
            "Epoch 375, Loss(train/val) 0.24962/0.15718. Took 0.05 sec\n",
            "Epoch 376, Loss(train/val) 0.22973/0.15733. Took 0.05 sec\n",
            "Epoch 377, Loss(train/val) 0.23880/0.15813. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.23413/0.15881. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 0.22205/0.16318. Took 0.06 sec\n",
            "Epoch 380, Loss(train/val) 0.23276/0.16114. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 0.24288/0.16028. Took 0.05 sec\n",
            "Epoch 382, Loss(train/val) 0.23279/0.16033. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.23552/0.15766. Took 0.05 sec\n",
            "Epoch 384, Loss(train/val) 0.23907/0.15867. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 0.22877/0.15968. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.22983/0.15693. Took 0.04 sec\n",
            "Epoch 387, Loss(train/val) 0.23826/0.15686. Took 0.05 sec\n",
            "Epoch 388, Loss(train/val) 0.22777/0.15598. Took 0.05 sec\n",
            "Epoch 389, Loss(train/val) 0.24335/0.15633. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.22905/0.15636. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.22413/0.15681. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.23432/0.15702. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.23702/0.15669. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 0.22352/0.15945. Took 0.05 sec\n",
            "Epoch 395, Loss(train/val) 0.23278/0.16227. Took 0.05 sec\n",
            "Epoch 396, Loss(train/val) 0.23545/0.15942. Took 0.04 sec\n",
            "Epoch 397, Loss(train/val) 0.24027/0.15584. Took 0.05 sec\n",
            "Epoch 398, Loss(train/val) 0.26776/0.16252. Took 0.05 sec\n",
            "Epoch 399, Loss(train/val) 0.24389/0.15975. Took 0.05 sec\n",
            "Epoch 400, Loss(train/val) 0.23079/0.15988. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 0.24298/0.16922. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.22668/0.16176. Took 0.05 sec\n",
            "Epoch 403, Loss(train/val) 0.23330/0.15608. Took 0.04 sec\n",
            "Epoch 404, Loss(train/val) 0.22711/0.15852. Took 0.05 sec\n",
            "Epoch 405, Loss(train/val) 0.23670/0.16641. Took 0.05 sec\n",
            "Epoch 406, Loss(train/val) 0.24262/0.17168. Took 0.05 sec\n",
            "Epoch 407, Loss(train/val) 0.23725/0.16529. Took 0.05 sec\n",
            "Epoch 408, Loss(train/val) 0.24168/0.15823. Took 0.05 sec\n",
            "Epoch 409, Loss(train/val) 0.22712/0.15676. Took 0.04 sec\n",
            "Epoch 410, Loss(train/val) 0.22771/0.15930. Took 0.05 sec\n",
            "Epoch 411, Loss(train/val) 0.23256/0.15881. Took 0.04 sec\n",
            "Epoch 412, Loss(train/val) 0.22864/0.15744. Took 0.05 sec\n",
            "Epoch 413, Loss(train/val) 0.23114/0.15645. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.27112/0.15847. Took 0.05 sec\n",
            "Epoch 415, Loss(train/val) 0.23082/0.15617. Took 0.05 sec\n",
            "Epoch 416, Loss(train/val) 0.23247/0.15651. Took 0.05 sec\n",
            "Epoch 417, Loss(train/val) 0.22829/0.16314. Took 0.05 sec\n",
            "Epoch 418, Loss(train/val) 0.23410/0.15894. Took 0.04 sec\n",
            "Epoch 419, Loss(train/val) 0.22705/0.15752. Took 0.04 sec\n",
            "Epoch 420, Loss(train/val) 0.22820/0.15676. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.22462/0.15603. Took 0.04 sec\n",
            "Epoch 422, Loss(train/val) 0.23510/0.15693. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.22729/0.15777. Took 0.05 sec\n",
            "Epoch 424, Loss(train/val) 0.22989/0.15632. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.23447/0.15639. Took 0.05 sec\n",
            "Epoch 426, Loss(train/val) 0.23518/0.15828. Took 0.05 sec\n",
            "Epoch 427, Loss(train/val) 0.23200/0.15635. Took 0.06 sec\n",
            "Epoch 428, Loss(train/val) 0.23526/0.15749. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.22618/0.15779. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.23778/0.15568. Took 0.04 sec\n",
            "Epoch 431, Loss(train/val) 0.23262/0.15583. Took 0.05 sec\n",
            "Epoch 432, Loss(train/val) 0.24111/0.15649. Took 0.05 sec\n",
            "Epoch 433, Loss(train/val) 0.22975/0.15597. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.23488/0.15742. Took 0.05 sec\n",
            "Epoch 435, Loss(train/val) 0.23654/0.15747. Took 0.04 sec\n",
            "Epoch 436, Loss(train/val) 0.23475/0.16145. Took 0.04 sec\n",
            "Epoch 437, Loss(train/val) 0.24553/0.15692. Took 0.06 sec\n",
            "Epoch 438, Loss(train/val) 0.22955/0.15577. Took 0.05 sec\n",
            "Epoch 439, Loss(train/val) 0.22678/0.15585. Took 0.05 sec\n",
            "Epoch 440, Loss(train/val) 0.22915/0.15556. Took 0.04 sec\n",
            "Epoch 441, Loss(train/val) 0.23492/0.15538. Took 0.05 sec\n",
            "Epoch 442, Loss(train/val) 0.22513/0.15474. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 0.23069/0.15488. Took 0.04 sec\n",
            "Epoch 444, Loss(train/val) 0.21774/0.15525. Took 0.04 sec\n",
            "Epoch 445, Loss(train/val) 0.23342/0.15599. Took 0.04 sec\n",
            "Epoch 446, Loss(train/val) 0.22065/0.15611. Took 0.05 sec\n",
            "Epoch 447, Loss(train/val) 0.22501/0.15720. Took 0.06 sec\n",
            "Epoch 448, Loss(train/val) 0.23515/0.15629. Took 0.05 sec\n",
            "Epoch 449, Loss(train/val) 0.23229/0.15473. Took 0.05 sec\n",
            "Epoch 450, Loss(train/val) 0.22875/0.15871. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 0.24331/0.16330. Took 0.05 sec\n",
            "Epoch 452, Loss(train/val) 0.22836/0.15658. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.23439/0.15709. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.22860/0.15867. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 0.22640/0.15823. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.22832/0.15532. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.22488/0.15489. Took 0.06 sec\n",
            "Epoch 458, Loss(train/val) 0.23446/0.15496. Took 0.06 sec\n",
            "Epoch 459, Loss(train/val) 0.22420/0.15600. Took 0.05 sec\n",
            "Epoch 460, Loss(train/val) 0.23050/0.15728. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.22830/0.15586. Took 0.05 sec\n",
            "Epoch 462, Loss(train/val) 0.23008/0.15586. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.22617/0.15639. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.23055/0.15628. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.23309/0.15504. Took 0.05 sec\n",
            "Epoch 466, Loss(train/val) 0.26363/0.16078. Took 0.05 sec\n",
            "Epoch 467, Loss(train/val) 0.23037/0.16687. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.22766/0.16135. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 0.23256/0.15648. Took 0.05 sec\n",
            "Epoch 470, Loss(train/val) 0.22735/0.15703. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 0.23050/0.15533. Took 0.05 sec\n",
            "Epoch 472, Loss(train/val) 0.23072/0.15604. Took 0.06 sec\n",
            "Epoch 473, Loss(train/val) 0.22491/0.15500. Took 0.05 sec\n",
            "Epoch 474, Loss(train/val) 0.22196/0.15489. Took 0.04 sec\n",
            "Epoch 475, Loss(train/val) 0.22132/0.15502. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.23519/0.15464. Took 0.04 sec\n",
            "Epoch 477, Loss(train/val) 0.22547/0.15459. Took 0.05 sec\n",
            "Epoch 478, Loss(train/val) 0.23105/0.15451. Took 0.05 sec\n",
            "Epoch 479, Loss(train/val) 0.22424/0.15688. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.22326/0.16019. Took 0.04 sec\n",
            "Epoch 481, Loss(train/val) 0.22633/0.16133. Took 0.05 sec\n",
            "Epoch 482, Loss(train/val) 0.24203/0.15571. Took 0.05 sec\n",
            "Epoch 483, Loss(train/val) 0.22872/0.15419. Took 0.05 sec\n",
            "Epoch 484, Loss(train/val) 0.22819/0.15475. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.22656/0.15552. Took 0.05 sec\n",
            "Epoch 486, Loss(train/val) 0.22504/0.15539. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.22348/0.15616. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.23911/0.15717. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.22487/0.15908. Took 0.05 sec\n",
            "Epoch 490, Loss(train/val) 0.23125/0.15757. Took 0.05 sec\n",
            "Epoch 491, Loss(train/val) 0.23522/0.15605. Took 0.05 sec\n",
            "Epoch 492, Loss(train/val) 0.22724/0.15508. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.22977/0.15658. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.22548/0.15531. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.22770/0.15496. Took 0.04 sec\n",
            "Epoch 496, Loss(train/val) 0.23080/0.15489. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 0.23224/0.15625. Took 0.05 sec\n",
            "Epoch 498, Loss(train/val) 0.22452/0.15701. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.22372/0.15502. Took 0.05 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=500, exp_name='exp2_hidn', hid_dim=64, input_dim=1, l2=1e-05, lr=0.0001, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.08942/0.35713. Took 0.06 sec\n",
            "Epoch 1, Loss(train/val) 1.08157/0.35679. Took 0.06 sec\n",
            "Epoch 2, Loss(train/val) 1.07880/0.35647. Took 0.05 sec\n",
            "Epoch 3, Loss(train/val) 1.08277/0.35616. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.08335/0.35587. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.08455/0.35558. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.06012/0.35529. Took 0.06 sec\n",
            "Epoch 7, Loss(train/val) 1.07468/0.35501. Took 0.05 sec\n",
            "Epoch 8, Loss(train/val) 1.07582/0.35473. Took 0.06 sec\n",
            "Epoch 9, Loss(train/val) 1.08143/0.35441. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.07754/0.35408. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.07728/0.35375. Took 0.06 sec\n",
            "Epoch 12, Loss(train/val) 1.08428/0.35340. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 1.08014/0.35305. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.07853/0.35269. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.07833/0.35231. Took 0.06 sec\n",
            "Epoch 16, Loss(train/val) 1.07803/0.35190. Took 0.05 sec\n",
            "Epoch 17, Loss(train/val) 1.07607/0.35145. Took 0.05 sec\n",
            "Epoch 18, Loss(train/val) 1.07558/0.35099. Took 0.05 sec\n",
            "Epoch 19, Loss(train/val) 1.07729/0.35047. Took 0.06 sec\n",
            "Epoch 20, Loss(train/val) 1.07533/0.34990. Took 0.05 sec\n",
            "Epoch 21, Loss(train/val) 1.07403/0.34927. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 1.06737/0.34859. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 1.06307/0.34804. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 1.05739/0.34736. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 1.05999/0.34685. Took 0.05 sec\n",
            "Epoch 26, Loss(train/val) 1.05749/0.34652. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.05528/0.34628. Took 0.06 sec\n",
            "Epoch 28, Loss(train/val) 1.03715/0.34675. Took 0.05 sec\n",
            "Epoch 29, Loss(train/val) 1.03344/0.34930. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 1.01491/0.35419. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 1.00002/0.35993. Took 0.06 sec\n",
            "Epoch 32, Loss(train/val) 0.96623/0.36855. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 0.93270/0.37956. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 0.90793/0.39272. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 0.85686/0.40702. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 0.81839/0.42746. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 0.77506/0.45586. Took 0.05 sec\n",
            "Epoch 38, Loss(train/val) 0.74130/0.48743. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 0.71589/0.50419. Took 0.06 sec\n",
            "Epoch 40, Loss(train/val) 0.68193/0.49809. Took 0.06 sec\n",
            "Epoch 41, Loss(train/val) 0.66752/0.46583. Took 0.05 sec\n",
            "Epoch 42, Loss(train/val) 0.64500/0.41894. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 0.61801/0.39084. Took 0.05 sec\n",
            "Epoch 44, Loss(train/val) 0.59914/0.35707. Took 0.06 sec\n",
            "Epoch 45, Loss(train/val) 0.58742/0.33739. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 0.56307/0.33310. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 0.54426/0.33292. Took 0.05 sec\n",
            "Epoch 48, Loss(train/val) 0.52603/0.32993. Took 0.06 sec\n",
            "Epoch 49, Loss(train/val) 0.51910/0.34815. Took 0.05 sec\n",
            "Epoch 50, Loss(train/val) 0.50096/0.34134. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 0.48223/0.33933. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 0.47022/0.33919. Took 0.05 sec\n",
            "Epoch 53, Loss(train/val) 0.45707/0.30591. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 0.44204/0.28046. Took 0.05 sec\n",
            "Epoch 55, Loss(train/val) 0.43413/0.26877. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 0.41596/0.25530. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 0.40609/0.25406. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 0.40478/0.26353. Took 0.06 sec\n",
            "Epoch 59, Loss(train/val) 0.40287/0.28245. Took 0.05 sec\n",
            "Epoch 60, Loss(train/val) 0.39550/0.30088. Took 0.06 sec\n",
            "Epoch 61, Loss(train/val) 0.40083/0.33425. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 0.38568/0.34300. Took 0.05 sec\n",
            "Epoch 63, Loss(train/val) 0.38374/0.34287. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 0.38028/0.30183. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 0.37001/0.27929. Took 0.06 sec\n",
            "Epoch 66, Loss(train/val) 0.36928/0.26901. Took 0.05 sec\n",
            "Epoch 67, Loss(train/val) 0.36999/0.25967. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 0.36543/0.25141. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 0.36604/0.25243. Took 0.06 sec\n",
            "Epoch 70, Loss(train/val) 0.36455/0.25465. Took 0.05 sec\n",
            "Epoch 71, Loss(train/val) 0.36136/0.28374. Took 0.05 sec\n",
            "Epoch 72, Loss(train/val) 0.35915/0.32558. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 0.36702/0.32053. Took 0.06 sec\n",
            "Epoch 74, Loss(train/val) 0.36010/0.28701. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 0.36490/0.27461. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 0.37657/0.24391. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 0.35945/0.23870. Took 0.07 sec\n",
            "Epoch 78, Loss(train/val) 0.36536/0.25363. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 0.36859/0.25265. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 0.35479/0.24378. Took 0.05 sec\n",
            "Epoch 81, Loss(train/val) 0.36374/0.23845. Took 0.05 sec\n",
            "Epoch 82, Loss(train/val) 0.35717/0.23747. Took 0.06 sec\n",
            "Epoch 83, Loss(train/val) 0.36588/0.23646. Took 0.05 sec\n",
            "Epoch 84, Loss(train/val) 0.35132/0.23982. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 0.36765/0.23609. Took 0.06 sec\n",
            "Epoch 86, Loss(train/val) 0.35038/0.23511. Took 0.06 sec\n",
            "Epoch 87, Loss(train/val) 0.34585/0.23546. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 0.35878/0.23513. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 0.35403/0.23952. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 0.35450/0.24744. Took 0.07 sec\n",
            "Epoch 91, Loss(train/val) 0.38221/0.26053. Took 0.05 sec\n",
            "Epoch 92, Loss(train/val) 0.35958/0.25439. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 0.35656/0.24373. Took 0.06 sec\n",
            "Epoch 94, Loss(train/val) 0.38192/0.23619. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 0.36326/0.23493. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 0.35838/0.23416. Took 0.06 sec\n",
            "Epoch 97, Loss(train/val) 0.34812/0.23577. Took 0.06 sec\n",
            "Epoch 98, Loss(train/val) 0.35843/0.24221. Took 0.05 sec\n",
            "Epoch 99, Loss(train/val) 0.36813/0.24435. Took 0.05 sec\n",
            "Epoch 100, Loss(train/val) 0.35317/0.24100. Took 0.05 sec\n",
            "Epoch 101, Loss(train/val) 0.35844/0.23573. Took 0.06 sec\n",
            "Epoch 102, Loss(train/val) 0.35252/0.23397. Took 0.05 sec\n",
            "Epoch 103, Loss(train/val) 0.35419/0.23356. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 0.36022/0.23516. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 0.37927/0.24344. Took 0.05 sec\n",
            "Epoch 106, Loss(train/val) 0.35623/0.24768. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 0.36055/0.24934. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 0.36381/0.24997. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 0.35638/0.25320. Took 0.05 sec\n",
            "Epoch 110, Loss(train/val) 0.35188/0.24946. Took 0.05 sec\n",
            "Epoch 111, Loss(train/val) 0.35596/0.23841. Took 0.06 sec\n",
            "Epoch 112, Loss(train/val) 0.35062/0.23421. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 0.36573/0.23639. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 0.34450/0.23712. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 0.35635/0.23479. Took 0.06 sec\n",
            "Epoch 116, Loss(train/val) 0.35344/0.23309. Took 0.05 sec\n",
            "Epoch 117, Loss(train/val) 0.36597/0.23370. Took 0.05 sec\n",
            "Epoch 118, Loss(train/val) 0.35743/0.23253. Took 0.05 sec\n",
            "Epoch 119, Loss(train/val) 0.35143/0.23153. Took 0.06 sec\n",
            "Epoch 120, Loss(train/val) 0.35174/0.23365. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 0.35276/0.23798. Took 0.06 sec\n",
            "Epoch 122, Loss(train/val) 0.35121/0.23530. Took 0.06 sec\n",
            "Epoch 123, Loss(train/val) 0.34759/0.23816. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 0.35459/0.23505. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 0.36330/0.23280. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 0.35746/0.23211. Took 0.06 sec\n",
            "Epoch 127, Loss(train/val) 0.35205/0.23228. Took 0.05 sec\n",
            "Epoch 128, Loss(train/val) 0.35287/0.23433. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 0.36213/0.23638. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 0.35750/0.23362. Took 0.05 sec\n",
            "Epoch 131, Loss(train/val) 0.34544/0.23792. Took 0.06 sec\n",
            "Epoch 132, Loss(train/val) 0.35166/0.23316. Took 0.05 sec\n",
            "Epoch 133, Loss(train/val) 0.35447/0.23408. Took 0.05 sec\n",
            "Epoch 134, Loss(train/val) 0.34140/0.23175. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 0.36675/0.23111. Took 0.06 sec\n",
            "Epoch 136, Loss(train/val) 0.33946/0.23091. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 0.35622/0.23374. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 0.34958/0.23676. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 0.36457/0.23376. Took 0.06 sec\n",
            "Epoch 140, Loss(train/val) 0.35093/0.23167. Took 0.05 sec\n",
            "Epoch 141, Loss(train/val) 0.34595/0.23137. Took 0.05 sec\n",
            "Epoch 142, Loss(train/val) 0.35877/0.23020. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 0.34911/0.22981. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 0.34175/0.23028. Took 0.06 sec\n",
            "Epoch 145, Loss(train/val) 0.35007/0.22954. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 0.34412/0.23020. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 0.34648/0.22942. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 0.34395/0.22891. Took 0.05 sec\n",
            "Epoch 149, Loss(train/val) 0.34758/0.23004. Took 0.06 sec\n",
            "Epoch 150, Loss(train/val) 0.34269/0.23372. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 0.35566/0.23012. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 0.35301/0.22940. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 0.34691/0.22871. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 0.34617/0.22876. Took 0.06 sec\n",
            "Epoch 155, Loss(train/val) 0.35704/0.22853. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 0.34522/0.22902. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 0.34974/0.23204. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 0.34794/0.23089. Took 0.06 sec\n",
            "Epoch 159, Loss(train/val) 0.34652/0.23057. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 0.35601/0.22787. Took 0.05 sec\n",
            "Epoch 161, Loss(train/val) 0.33812/0.22762. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 0.35409/0.22786. Took 0.06 sec\n",
            "Epoch 163, Loss(train/val) 0.35264/0.22721. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 0.34208/0.22698. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 0.34256/0.22794. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.33390/0.22705. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 0.34709/0.22689. Took 0.05 sec\n",
            "Epoch 168, Loss(train/val) 0.33973/0.22694. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 0.33459/0.22673. Took 0.05 sec\n",
            "Epoch 170, Loss(train/val) 0.35213/0.22587. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 0.34882/0.22862. Took 0.05 sec\n",
            "Epoch 172, Loss(train/val) 0.34324/0.23143. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 0.33779/0.23135. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 0.34647/0.23007. Took 0.06 sec\n",
            "Epoch 175, Loss(train/val) 0.34547/0.22825. Took 0.06 sec\n",
            "Epoch 176, Loss(train/val) 0.34816/0.22796. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 0.33174/0.22593. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 0.35459/0.22744. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 0.33826/0.22676. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 0.34337/0.22640. Took 0.06 sec\n",
            "Epoch 181, Loss(train/val) 0.33755/0.22466. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 0.33936/0.22397. Took 0.05 sec\n",
            "Epoch 183, Loss(train/val) 0.34765/0.22595. Took 0.05 sec\n",
            "Epoch 184, Loss(train/val) 0.33557/0.22581. Took 0.05 sec\n",
            "Epoch 185, Loss(train/val) 0.33218/0.22473. Took 0.05 sec\n",
            "Epoch 186, Loss(train/val) 0.34410/0.22368. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 0.33530/0.22295. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 0.33897/0.22283. Took 0.05 sec\n",
            "Epoch 189, Loss(train/val) 0.33530/0.22400. Took 0.05 sec\n",
            "Epoch 190, Loss(train/val) 0.32506/0.22348. Took 0.07 sec\n",
            "Epoch 191, Loss(train/val) 0.33773/0.22283. Took 0.06 sec\n",
            "Epoch 192, Loss(train/val) 0.33292/0.22254. Took 0.06 sec\n",
            "Epoch 193, Loss(train/val) 0.33212/0.22177. Took 0.06 sec\n",
            "Epoch 194, Loss(train/val) 0.33114/0.22181. Took 0.06 sec\n",
            "Epoch 195, Loss(train/val) 0.33960/0.22231. Took 0.05 sec\n",
            "Epoch 196, Loss(train/val) 0.33874/0.22167. Took 0.06 sec\n",
            "Epoch 197, Loss(train/val) 0.32821/0.22217. Took 0.05 sec\n",
            "Epoch 198, Loss(train/val) 0.32447/0.22208. Took 0.06 sec\n",
            "Epoch 199, Loss(train/val) 0.33664/0.22256. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 0.32645/0.22277. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 0.33113/0.22083. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 0.34569/0.22056. Took 0.06 sec\n",
            "Epoch 203, Loss(train/val) 0.33510/0.21986. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 0.33226/0.22067. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.33095/0.22309. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 0.32822/0.22405. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 0.32602/0.21976. Took 0.06 sec\n",
            "Epoch 208, Loss(train/val) 0.32441/0.21884. Took 0.05 sec\n",
            "Epoch 209, Loss(train/val) 0.32485/0.21928. Took 0.05 sec\n",
            "Epoch 210, Loss(train/val) 0.33416/0.21765. Took 0.05 sec\n",
            "Epoch 211, Loss(train/val) 0.31819/0.21848. Took 0.06 sec\n",
            "Epoch 212, Loss(train/val) 0.33678/0.22244. Took 0.06 sec\n",
            "Epoch 213, Loss(train/val) 0.32789/0.21974. Took 0.05 sec\n",
            "Epoch 214, Loss(train/val) 0.33963/0.21756. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.30765/0.21597. Took 0.05 sec\n",
            "Epoch 216, Loss(train/val) 0.33427/0.21616. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 0.34448/0.21802. Took 0.05 sec\n",
            "Epoch 218, Loss(train/val) 0.32807/0.22156. Took 0.06 sec\n",
            "Epoch 219, Loss(train/val) 0.31784/0.22065. Took 0.06 sec\n",
            "Epoch 220, Loss(train/val) 0.33708/0.21511. Took 0.05 sec\n",
            "Epoch 221, Loss(train/val) 0.31430/0.21431. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 0.31238/0.21570. Took 0.05 sec\n",
            "Epoch 223, Loss(train/val) 0.32560/0.21752. Took 0.06 sec\n",
            "Epoch 224, Loss(train/val) 0.31784/0.21427. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 0.30883/0.21396. Took 0.05 sec\n",
            "Epoch 226, Loss(train/val) 0.32907/0.21309. Took 0.05 sec\n",
            "Epoch 227, Loss(train/val) 0.31704/0.21553. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 0.31477/0.21623. Took 0.06 sec\n",
            "Epoch 229, Loss(train/val) 0.32391/0.21159. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 0.32272/0.21128. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 0.30266/0.21083. Took 0.06 sec\n",
            "Epoch 232, Loss(train/val) 0.33459/0.20965. Took 0.05 sec\n",
            "Epoch 233, Loss(train/val) 0.30440/0.21242. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 0.30795/0.21511. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 0.30648/0.21810. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 0.31717/0.21245. Took 0.05 sec\n",
            "Epoch 237, Loss(train/val) 0.30538/0.20881. Took 0.05 sec\n",
            "Epoch 238, Loss(train/val) 0.30567/0.20695. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 0.30239/0.20877. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 0.30580/0.21086. Took 0.06 sec\n",
            "Epoch 241, Loss(train/val) 0.30663/0.21254. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 0.31044/0.20709. Took 0.05 sec\n",
            "Epoch 243, Loss(train/val) 0.30216/0.20440. Took 0.05 sec\n",
            "Epoch 244, Loss(train/val) 0.30081/0.20520. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.31249/0.20500. Took 0.06 sec\n",
            "Epoch 246, Loss(train/val) 0.29157/0.20530. Took 0.05 sec\n",
            "Epoch 247, Loss(train/val) 0.29938/0.20174. Took 0.05 sec\n",
            "Epoch 248, Loss(train/val) 0.28714/0.20245. Took 0.05 sec\n",
            "Epoch 249, Loss(train/val) 0.29759/0.20181. Took 0.06 sec\n",
            "Epoch 250, Loss(train/val) 0.29373/0.20125. Took 0.06 sec\n",
            "Epoch 251, Loss(train/val) 0.30254/0.19879. Took 0.06 sec\n",
            "Epoch 252, Loss(train/val) 0.30799/0.20000. Took 0.05 sec\n",
            "Epoch 253, Loss(train/val) 0.28961/0.21050. Took 0.07 sec\n",
            "Epoch 254, Loss(train/val) 0.29253/0.21522. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.30118/0.21126. Took 0.05 sec\n",
            "Epoch 256, Loss(train/val) 0.29052/0.20072. Took 0.05 sec\n",
            "Epoch 257, Loss(train/val) 0.28648/0.19670. Took 0.05 sec\n",
            "Epoch 258, Loss(train/val) 0.28226/0.19761. Took 0.06 sec\n",
            "Epoch 259, Loss(train/val) 0.29518/0.20306. Took 0.05 sec\n",
            "Epoch 260, Loss(train/val) 0.27980/0.19614. Took 0.05 sec\n",
            "Epoch 261, Loss(train/val) 0.28302/0.19891. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.27921/0.20312. Took 0.05 sec\n",
            "Epoch 263, Loss(train/val) 0.29161/0.19444. Took 0.05 sec\n",
            "Epoch 264, Loss(train/val) 0.28961/0.18990. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.28067/0.19403. Took 0.05 sec\n",
            "Epoch 266, Loss(train/val) 0.27510/0.20645. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 0.27964/0.21319. Took 0.06 sec\n",
            "Epoch 268, Loss(train/val) 0.28413/0.20136. Took 0.05 sec\n",
            "Epoch 269, Loss(train/val) 0.27195/0.18881. Took 0.05 sec\n",
            "Epoch 270, Loss(train/val) 0.28804/0.18905. Took 0.06 sec\n",
            "Epoch 271, Loss(train/val) 0.27377/0.19524. Took 0.06 sec\n",
            "Epoch 272, Loss(train/val) 0.27155/0.19647. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 0.29255/0.19387. Took 0.05 sec\n",
            "Epoch 274, Loss(train/val) 0.27412/0.18709. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.26801/0.18728. Took 0.06 sec\n",
            "Epoch 276, Loss(train/val) 0.26307/0.19060. Took 0.05 sec\n",
            "Epoch 277, Loss(train/val) 0.26720/0.20542. Took 0.06 sec\n",
            "Epoch 278, Loss(train/val) 0.26222/0.19537. Took 0.05 sec\n",
            "Epoch 279, Loss(train/val) 0.27149/0.18747. Took 0.05 sec\n",
            "Epoch 280, Loss(train/val) 0.27901/0.18518. Took 0.06 sec\n",
            "Epoch 281, Loss(train/val) 0.27467/0.19616. Took 0.05 sec\n",
            "Epoch 282, Loss(train/val) 0.26099/0.19852. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 0.25688/0.19648. Took 0.05 sec\n",
            "Epoch 284, Loss(train/val) 0.26494/0.19306. Took 0.06 sec\n",
            "Epoch 285, Loss(train/val) 0.26915/0.18277. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.26202/0.18127. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.27195/0.18439. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.25373/0.18804. Took 0.07 sec\n",
            "Epoch 289, Loss(train/val) 0.27000/0.19581. Took 0.06 sec\n",
            "Epoch 290, Loss(train/val) 0.26413/0.19132. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.25980/0.17884. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.25791/0.17985. Took 0.06 sec\n",
            "Epoch 293, Loss(train/val) 0.28307/0.18353. Took 0.05 sec\n",
            "Epoch 294, Loss(train/val) 0.25645/0.18121. Took 0.05 sec\n",
            "Epoch 295, Loss(train/val) 0.25516/0.18696. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 0.25478/0.21878. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.25615/0.18700. Took 0.05 sec\n",
            "Epoch 298, Loss(train/val) 0.26722/0.17638. Took 0.05 sec\n",
            "Epoch 299, Loss(train/val) 0.25861/0.17595. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.26606/0.17652. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.26685/0.18085. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.25074/0.18349. Took 0.05 sec\n",
            "Epoch 303, Loss(train/val) 0.24939/0.19458. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 0.24861/0.20768. Took 0.05 sec\n",
            "Epoch 305, Loss(train/val) 0.25058/0.20680. Took 0.06 sec\n",
            "Epoch 306, Loss(train/val) 0.25210/0.17491. Took 0.05 sec\n",
            "Epoch 307, Loss(train/val) 0.25073/0.18639. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.25506/0.18208. Took 0.06 sec\n",
            "Epoch 309, Loss(train/val) 0.26330/0.18305. Took 0.06 sec\n",
            "Epoch 310, Loss(train/val) 0.24392/0.21143. Took 0.06 sec\n",
            "Epoch 311, Loss(train/val) 0.25672/0.20716. Took 0.05 sec\n",
            "Epoch 312, Loss(train/val) 0.25281/0.17266. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 0.24717/0.17445. Took 0.06 sec\n",
            "Epoch 314, Loss(train/val) 0.23925/0.17198. Took 0.05 sec\n",
            "Epoch 315, Loss(train/val) 0.25430/0.18982. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.25165/0.19566. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.25310/0.17804. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.25750/0.17374. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.24746/0.17143. Took 0.05 sec\n",
            "Epoch 320, Loss(train/val) 0.26055/0.17371. Took 0.05 sec\n",
            "Epoch 321, Loss(train/val) 0.24150/0.17258. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.24284/0.17231. Took 0.05 sec\n",
            "Epoch 323, Loss(train/val) 0.24456/0.17223. Took 0.05 sec\n",
            "Epoch 324, Loss(train/val) 0.24483/0.17176. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.24552/0.17060. Took 0.05 sec\n",
            "Epoch 326, Loss(train/val) 0.24865/0.17514. Took 0.05 sec\n",
            "Epoch 327, Loss(train/val) 0.24509/0.18521. Took 0.06 sec\n",
            "Epoch 328, Loss(train/val) 0.24608/0.17996. Took 0.05 sec\n",
            "Epoch 329, Loss(train/val) 0.25058/0.16958. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.25548/0.17041. Took 0.05 sec\n",
            "Epoch 331, Loss(train/val) 0.24106/0.17115. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 0.25027/0.17027. Took 0.05 sec\n",
            "Epoch 333, Loss(train/val) 0.23730/0.16894. Took 0.05 sec\n",
            "Epoch 334, Loss(train/val) 0.24488/0.17396. Took 0.06 sec\n",
            "Epoch 335, Loss(train/val) 0.24818/0.18314. Took 0.05 sec\n",
            "Epoch 336, Loss(train/val) 0.24136/0.18020. Took 0.05 sec\n",
            "Epoch 337, Loss(train/val) 0.24228/0.16917. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.23671/0.16928. Took 0.05 sec\n",
            "Epoch 339, Loss(train/val) 0.25553/0.16849. Took 0.05 sec\n",
            "Epoch 340, Loss(train/val) 0.23788/0.16947. Took 0.05 sec\n",
            "Epoch 341, Loss(train/val) 0.23819/0.17640. Took 0.05 sec\n",
            "Epoch 342, Loss(train/val) 0.24666/0.16827. Took 0.06 sec\n",
            "Epoch 343, Loss(train/val) 0.23233/0.16793. Took 0.05 sec\n",
            "Epoch 344, Loss(train/val) 0.24009/0.16776. Took 0.06 sec\n",
            "Epoch 345, Loss(train/val) 0.25199/0.16881. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.23916/0.16891. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 0.24116/0.16754. Took 0.06 sec\n",
            "Epoch 348, Loss(train/val) 0.24416/0.16814. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 0.24253/0.17160. Took 0.05 sec\n",
            "Epoch 350, Loss(train/val) 0.24387/0.17969. Took 0.06 sec\n",
            "Epoch 351, Loss(train/val) 0.23789/0.17646. Took 0.05 sec\n",
            "Epoch 352, Loss(train/val) 0.26570/0.16697. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 0.26120/0.16685. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 0.23823/0.17280. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 0.26154/0.17370. Took 0.06 sec\n",
            "Epoch 356, Loss(train/val) 0.24873/0.17108. Took 0.05 sec\n",
            "Epoch 357, Loss(train/val) 0.23846/0.16644. Took 0.05 sec\n",
            "Epoch 358, Loss(train/val) 0.25502/0.16745. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 0.25268/0.17733. Took 0.05 sec\n",
            "Epoch 360, Loss(train/val) 0.23533/0.18032. Took 0.05 sec\n",
            "Epoch 361, Loss(train/val) 0.23572/0.16886. Took 0.06 sec\n",
            "Epoch 362, Loss(train/val) 0.23571/0.18470. Took 0.06 sec\n",
            "Epoch 363, Loss(train/val) 0.26235/0.18219. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 0.23284/0.16863. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.24590/0.18579. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.24510/0.17609. Took 0.07 sec\n",
            "Epoch 367, Loss(train/val) 0.23462/0.17276. Took 0.06 sec\n",
            "Epoch 368, Loss(train/val) 0.23664/0.16768. Took 0.05 sec\n",
            "Epoch 369, Loss(train/val) 0.25229/0.16701. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.23556/0.16670. Took 0.07 sec\n",
            "Epoch 371, Loss(train/val) 0.24245/0.17418. Took 0.06 sec\n",
            "Epoch 372, Loss(train/val) 0.22778/0.18346. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 0.23795/0.17355. Took 0.05 sec\n",
            "Epoch 374, Loss(train/val) 0.23597/0.16989. Took 0.06 sec\n",
            "Epoch 375, Loss(train/val) 0.23415/0.17259. Took 0.05 sec\n",
            "Epoch 376, Loss(train/val) 0.22987/0.16605. Took 0.05 sec\n",
            "Epoch 377, Loss(train/val) 0.25459/0.16433. Took 0.06 sec\n",
            "Epoch 378, Loss(train/val) 0.23077/0.16458. Took 0.06 sec\n",
            "Epoch 379, Loss(train/val) 0.23606/0.17261. Took 0.06 sec\n",
            "Epoch 380, Loss(train/val) 0.24291/0.16887. Took 0.06 sec\n",
            "Epoch 381, Loss(train/val) 0.23543/0.16537. Took 0.06 sec\n",
            "Epoch 382, Loss(train/val) 0.22811/0.16517. Took 0.07 sec\n",
            "Epoch 383, Loss(train/val) 0.24047/0.16441. Took 0.07 sec\n",
            "Epoch 384, Loss(train/val) 0.24615/0.16548. Took 0.06 sec\n",
            "Epoch 385, Loss(train/val) 0.22656/0.16662. Took 0.06 sec\n",
            "Epoch 386, Loss(train/val) 0.24287/0.17473. Took 0.06 sec\n",
            "Epoch 387, Loss(train/val) 0.23212/0.16654. Took 0.05 sec\n",
            "Epoch 388, Loss(train/val) 0.23287/0.16448. Took 0.06 sec\n",
            "Epoch 389, Loss(train/val) 0.23832/0.16671. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.22977/0.16401. Took 0.06 sec\n",
            "Epoch 391, Loss(train/val) 0.22953/0.16631. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.23010/0.16426. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.24969/0.16511. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 0.22682/0.16970. Took 0.05 sec\n",
            "Epoch 395, Loss(train/val) 0.23568/0.16397. Took 0.06 sec\n",
            "Epoch 396, Loss(train/val) 0.23458/0.16406. Took 0.05 sec\n",
            "Epoch 397, Loss(train/val) 0.23530/0.16380. Took 0.05 sec\n",
            "Epoch 398, Loss(train/val) 0.23051/0.16316. Took 0.05 sec\n",
            "Epoch 399, Loss(train/val) 0.23146/0.16443. Took 0.05 sec\n",
            "Epoch 400, Loss(train/val) 0.24001/0.17690. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 0.23143/0.17847. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.22996/0.16773. Took 0.06 sec\n",
            "Epoch 403, Loss(train/val) 0.22700/0.16172. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 0.22519/0.16495. Took 0.06 sec\n",
            "Epoch 405, Loss(train/val) 0.22699/0.16480. Took 0.05 sec\n",
            "Epoch 406, Loss(train/val) 0.21933/0.16472. Took 0.06 sec\n",
            "Epoch 407, Loss(train/val) 0.23763/0.16151. Took 0.06 sec\n",
            "Epoch 408, Loss(train/val) 0.23123/0.16115. Took 0.05 sec\n",
            "Epoch 409, Loss(train/val) 0.22828/0.16113. Took 0.05 sec\n",
            "Epoch 410, Loss(train/val) 0.23420/0.16116. Took 0.05 sec\n",
            "Epoch 411, Loss(train/val) 0.22925/0.17043. Took 0.06 sec\n",
            "Epoch 412, Loss(train/val) 0.23189/0.17227. Took 0.05 sec\n",
            "Epoch 413, Loss(train/val) 0.23910/0.16239. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.23062/0.16155. Took 0.06 sec\n",
            "Epoch 415, Loss(train/val) 0.24961/0.16116. Took 0.06 sec\n",
            "Epoch 416, Loss(train/val) 0.22762/0.16483. Took 0.05 sec\n",
            "Epoch 417, Loss(train/val) 0.22698/0.16569. Took 0.05 sec\n",
            "Epoch 418, Loss(train/val) 0.23700/0.16389. Took 0.05 sec\n",
            "Epoch 419, Loss(train/val) 0.22641/0.16254. Took 0.05 sec\n",
            "Epoch 420, Loss(train/val) 0.24051/0.16303. Took 0.06 sec\n",
            "Epoch 421, Loss(train/val) 0.23037/0.16611. Took 0.06 sec\n",
            "Epoch 422, Loss(train/val) 0.22691/0.16327. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.22264/0.16652. Took 0.05 sec\n",
            "Epoch 424, Loss(train/val) 0.25192/0.16337. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.22212/0.16805. Took 0.05 sec\n",
            "Epoch 426, Loss(train/val) 0.24387/0.16242. Took 0.05 sec\n",
            "Epoch 427, Loss(train/val) 0.23883/0.16400. Took 0.05 sec\n",
            "Epoch 428, Loss(train/val) 0.23020/0.17973. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.23785/0.16312. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.23401/0.17101. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.22642/0.17916. Took 0.05 sec\n",
            "Epoch 432, Loss(train/val) 0.22488/0.15992. Took 0.06 sec\n",
            "Epoch 433, Loss(train/val) 0.24033/0.16796. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.22968/0.16327. Took 0.05 sec\n",
            "Epoch 435, Loss(train/val) 0.22513/0.16904. Took 0.05 sec\n",
            "Epoch 436, Loss(train/val) 0.23807/0.17008. Took 0.05 sec\n",
            "Epoch 437, Loss(train/val) 0.22465/0.16051. Took 0.05 sec\n",
            "Epoch 438, Loss(train/val) 0.23213/0.16030. Took 0.05 sec\n",
            "Epoch 439, Loss(train/val) 0.23441/0.16056. Took 0.05 sec\n",
            "Epoch 440, Loss(train/val) 0.22483/0.15922. Took 0.06 sec\n",
            "Epoch 441, Loss(train/val) 0.22480/0.15888. Took 0.05 sec\n",
            "Epoch 442, Loss(train/val) 0.21951/0.16771. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 0.22776/0.17179. Took 0.05 sec\n",
            "Epoch 444, Loss(train/val) 0.22778/0.16163. Took 0.06 sec\n",
            "Epoch 445, Loss(train/val) 0.22865/0.15902. Took 0.05 sec\n",
            "Epoch 446, Loss(train/val) 0.22235/0.16058. Took 0.05 sec\n",
            "Epoch 447, Loss(train/val) 0.23408/0.16012. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 0.23180/0.15957. Took 0.05 sec\n",
            "Epoch 449, Loss(train/val) 0.21795/0.15886. Took 0.05 sec\n",
            "Epoch 450, Loss(train/val) 0.23863/0.15872. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 0.22963/0.16430. Took 0.05 sec\n",
            "Epoch 452, Loss(train/val) 0.23033/0.16146. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.22952/0.16279. Took 0.06 sec\n",
            "Epoch 454, Loss(train/val) 0.24142/0.16057. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 0.23004/0.16594. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.23105/0.17059. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.22491/0.16021. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.22456/0.19707. Took 0.05 sec\n",
            "Epoch 459, Loss(train/val) 0.24432/0.19476. Took 0.05 sec\n",
            "Epoch 460, Loss(train/val) 0.24297/0.16065. Took 0.06 sec\n",
            "Epoch 461, Loss(train/val) 0.23355/0.16514. Took 0.06 sec\n",
            "Epoch 462, Loss(train/val) 0.22399/0.18770. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.23500/0.16810. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.22766/0.16043. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.23032/0.18030. Took 0.05 sec\n",
            "Epoch 466, Loss(train/val) 0.23602/0.18431. Took 0.05 sec\n",
            "Epoch 467, Loss(train/val) 0.22777/0.16189. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.22412/0.15741. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 0.24122/0.15713. Took 0.06 sec\n",
            "Epoch 470, Loss(train/val) 0.27684/0.16540. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 0.21913/0.16113. Took 0.05 sec\n",
            "Epoch 472, Loss(train/val) 0.22260/0.15910. Took 0.05 sec\n",
            "Epoch 473, Loss(train/val) 0.22324/0.16191. Took 0.05 sec\n",
            "Epoch 474, Loss(train/val) 0.22500/0.15737. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 0.22147/0.16039. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.22394/0.16067. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.24428/0.16037. Took 0.05 sec\n",
            "Epoch 478, Loss(train/val) 0.22168/0.16211. Took 0.05 sec\n",
            "Epoch 479, Loss(train/val) 0.23420/0.15847. Took 0.06 sec\n",
            "Epoch 480, Loss(train/val) 0.22366/0.16263. Took 0.05 sec\n",
            "Epoch 481, Loss(train/val) 0.24088/0.15946. Took 0.05 sec\n",
            "Epoch 482, Loss(train/val) 0.22276/0.16102. Took 0.06 sec\n",
            "Epoch 483, Loss(train/val) 0.22382/0.15700. Took 0.06 sec\n",
            "Epoch 484, Loss(train/val) 0.22237/0.15763. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.22355/0.15883. Took 0.06 sec\n",
            "Epoch 486, Loss(train/val) 0.22704/0.15915. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.23321/0.15744. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.22088/0.15728. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.22594/0.15838. Took 0.05 sec\n",
            "Epoch 490, Loss(train/val) 0.21704/0.15919. Took 0.06 sec\n",
            "Epoch 491, Loss(train/val) 0.24461/0.16184. Took 0.05 sec\n",
            "Epoch 492, Loss(train/val) 0.22104/0.15822. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.23230/0.15790. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.22987/0.15885. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.23960/0.15713. Took 0.06 sec\n",
            "Epoch 496, Loss(train/val) 0.23183/0.15953. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 0.22464/0.15892. Took 0.05 sec\n",
            "Epoch 498, Loss(train/val) 0.23814/0.15839. Took 0.06 sec\n",
            "Epoch 499, Loss(train/val) 0.21480/0.15678. Took 0.06 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var1 = 'hid_dim'\n",
        "var2 = 'n_layers'\n",
        "df = load_exp_result('exp2_hidn')\n",
        "\n",
        "plot_loss_variation2(var1, var2, df, sharey=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "wfzSbFtctSUz",
        "outputId": "fe37e391-f183-49be-de42-a3b74fe5bcff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1355.38x864 with 24 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "### hid_dim = 16\n",
        "### n_layers = 8"
      ],
      "metadata": {
        "id": "e4ubljtrstSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Learning Rate(lr)"
      ],
      "metadata": {
        "id": "7HLZ874As0m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 666\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp3_lr\"\n",
        "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "args.batch_size = 193\n",
        "args.x_frames = 4\n",
        "args.y_frames = 4\n",
        "trainset = MyDataset(args.x_frames, args.y_frames, mydata, 0, int(0.6*len(mydata)))\n",
        "valset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.6*len(mydata)), int(0.8*len(mydata)))\n",
        "testset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.8*len(mydata)), int(len(mydata)))\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
        "\n",
        "args.input_dim = 1\n",
        "args.hid_dim = 16\n",
        "args.n_layers = 8\n",
        "\n",
        "args.l2 = 0.00001\n",
        "args.dropout = 0.0\n",
        "args.use_bn = True\n",
        "\n",
        "args.optim = 'Adam'\n",
        "args.lr = 0.0001\n",
        "args.epoch = 700\n",
        "\n",
        "name_var1 = 'lr'\n",
        "list_var1 = [0.00001, 0.00003, 0.0001, 0.0003]\n",
        "\n",
        "for var1 in list_var1:\n",
        "    setattr(args, name_var1, var1)\n",
        "    print(args)\n",
        "                \n",
        "    setting, result = experiment(partition, deepcopy(args))\n",
        "    save_exp_result(setting, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh7iJX7KyhRM",
        "outputId": "27533f9b-e2f8-428e-f174-f240369a6d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=700, exp_name='exp3_lr', hid_dim=16, input_dim=1, l2=1e-05, lr=1e-05, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.03470/0.38240. Took 0.05 sec\n",
            "Epoch 1, Loss(train/val) 1.03300/0.38340. Took 0.05 sec\n",
            "Epoch 2, Loss(train/val) 1.02345/0.38426. Took 0.05 sec\n",
            "Epoch 3, Loss(train/val) 1.03088/0.38500. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.02085/0.38565. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.03315/0.38621. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.02412/0.38669. Took 0.05 sec\n",
            "Epoch 7, Loss(train/val) 1.02083/0.38710. Took 0.05 sec\n",
            "Epoch 8, Loss(train/val) 1.03290/0.38747. Took 0.04 sec\n",
            "Epoch 9, Loss(train/val) 1.02382/0.38778. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.02060/0.38805. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.03135/0.38829. Took 0.05 sec\n",
            "Epoch 12, Loss(train/val) 1.02572/0.38849. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 1.03091/0.38867. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.02531/0.38884. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.01700/0.38898. Took 0.05 sec\n",
            "Epoch 16, Loss(train/val) 1.02206/0.38912. Took 0.05 sec\n",
            "Epoch 17, Loss(train/val) 1.02862/0.38924. Took 0.05 sec\n",
            "Epoch 18, Loss(train/val) 1.02225/0.38936. Took 0.04 sec\n",
            "Epoch 19, Loss(train/val) 1.02491/0.38947. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 1.02016/0.38957. Took 0.05 sec\n",
            "Epoch 21, Loss(train/val) 1.01771/0.38968. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 1.03329/0.38979. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 1.02726/0.38990. Took 0.04 sec\n",
            "Epoch 24, Loss(train/val) 1.01356/0.39002. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 1.03447/0.39015. Took 0.05 sec\n",
            "Epoch 26, Loss(train/val) 1.02506/0.39028. Took 0.04 sec\n",
            "Epoch 27, Loss(train/val) 1.03145/0.39038. Took 0.05 sec\n",
            "Epoch 28, Loss(train/val) 1.03033/0.39048. Took 0.05 sec\n",
            "Epoch 29, Loss(train/val) 1.02408/0.39060. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 1.02566/0.39073. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 1.02861/0.39088. Took 0.04 sec\n",
            "Epoch 32, Loss(train/val) 1.02803/0.39103. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 1.02904/0.39119. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 1.03189/0.39135. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 1.02424/0.39150. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 1.03248/0.39165. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.02843/0.39177. Took 0.05 sec\n",
            "Epoch 38, Loss(train/val) 1.02575/0.39189. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 1.02435/0.39198. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 1.03485/0.39206. Took 0.05 sec\n",
            "Epoch 41, Loss(train/val) 1.03204/0.39212. Took 0.05 sec\n",
            "Epoch 42, Loss(train/val) 1.02647/0.39216. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 1.01570/0.39219. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 1.03297/0.39223. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 1.03170/0.39225. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 1.02809/0.39226. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 1.03213/0.39227. Took 0.05 sec\n",
            "Epoch 48, Loss(train/val) 1.01632/0.39228. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 1.02653/0.39228. Took 0.05 sec\n",
            "Epoch 50, Loss(train/val) 1.03093/0.39228. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 1.02827/0.39229. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 1.02533/0.39227. Took 0.05 sec\n",
            "Epoch 53, Loss(train/val) 1.02530/0.39227. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 1.02464/0.39227. Took 0.05 sec\n",
            "Epoch 55, Loss(train/val) 1.02755/0.39226. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 1.03300/0.39225. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 1.03057/0.39225. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 1.03516/0.39224. Took 0.05 sec\n",
            "Epoch 59, Loss(train/val) 1.03282/0.39224. Took 0.05 sec\n",
            "Epoch 60, Loss(train/val) 1.02304/0.39223. Took 0.04 sec\n",
            "Epoch 61, Loss(train/val) 1.03018/0.39223. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 1.02911/0.39223. Took 0.05 sec\n",
            "Epoch 63, Loss(train/val) 1.03271/0.39222. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 1.03054/0.39222. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 1.02263/0.39221. Took 0.05 sec\n",
            "Epoch 66, Loss(train/val) 1.02722/0.39221. Took 0.05 sec\n",
            "Epoch 67, Loss(train/val) 1.02837/0.39220. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 1.02492/0.39220. Took 0.06 sec\n",
            "Epoch 69, Loss(train/val) 1.03047/0.39220. Took 0.05 sec\n",
            "Epoch 70, Loss(train/val) 1.02114/0.39220. Took 0.05 sec\n",
            "Epoch 71, Loss(train/val) 1.02188/0.39221. Took 0.05 sec\n",
            "Epoch 72, Loss(train/val) 1.01671/0.39220. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 1.02958/0.39221. Took 0.05 sec\n",
            "Epoch 74, Loss(train/val) 1.02973/0.39221. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 1.02003/0.39222. Took 0.06 sec\n",
            "Epoch 76, Loss(train/val) 1.02875/0.39223. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 1.03138/0.39224. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 1.02778/0.39226. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 1.02324/0.39227. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 1.02549/0.39228. Took 0.05 sec\n",
            "Epoch 81, Loss(train/val) 1.02998/0.39230. Took 0.04 sec\n",
            "Epoch 82, Loss(train/val) 1.02737/0.39231. Took 0.05 sec\n",
            "Epoch 83, Loss(train/val) 1.01851/0.39232. Took 0.05 sec\n",
            "Epoch 84, Loss(train/val) 1.02172/0.39234. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 1.03102/0.39236. Took 0.04 sec\n",
            "Epoch 86, Loss(train/val) 1.02718/0.39238. Took 0.05 sec\n",
            "Epoch 87, Loss(train/val) 1.02905/0.39240. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 1.02200/0.39242. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 1.02623/0.39244. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 1.03057/0.39244. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 1.03084/0.39244. Took 0.05 sec\n",
            "Epoch 92, Loss(train/val) 1.02871/0.39246. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 1.03226/0.39248. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 1.02403/0.39250. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 1.02572/0.39252. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 1.02824/0.39255. Took 0.05 sec\n",
            "Epoch 97, Loss(train/val) 1.02461/0.39258. Took 0.05 sec\n",
            "Epoch 98, Loss(train/val) 1.02866/0.39259. Took 0.05 sec\n",
            "Epoch 99, Loss(train/val) 1.02759/0.39262. Took 0.05 sec\n",
            "Epoch 100, Loss(train/val) 1.03292/0.39264. Took 0.05 sec\n",
            "Epoch 101, Loss(train/val) 1.03263/0.39266. Took 0.05 sec\n",
            "Epoch 102, Loss(train/val) 1.02186/0.39267. Took 0.05 sec\n",
            "Epoch 103, Loss(train/val) 1.02243/0.39269. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 1.02203/0.39272. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 1.02869/0.39275. Took 0.05 sec\n",
            "Epoch 106, Loss(train/val) 1.02809/0.39279. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 1.03039/0.39282. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 1.03178/0.39283. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 1.01200/0.39285. Took 0.06 sec\n",
            "Epoch 110, Loss(train/val) 1.02065/0.39288. Took 0.06 sec\n",
            "Epoch 111, Loss(train/val) 1.02200/0.39291. Took 0.05 sec\n",
            "Epoch 112, Loss(train/val) 1.02976/0.39294. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 1.02634/0.39295. Took 0.06 sec\n",
            "Epoch 114, Loss(train/val) 1.03190/0.39297. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 1.01399/0.39300. Took 0.05 sec\n",
            "Epoch 116, Loss(train/val) 1.02721/0.39303. Took 0.05 sec\n",
            "Epoch 117, Loss(train/val) 1.03379/0.39305. Took 0.05 sec\n",
            "Epoch 118, Loss(train/val) 1.01606/0.39309. Took 0.05 sec\n",
            "Epoch 119, Loss(train/val) 1.02040/0.39311. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 1.02506/0.39313. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 1.03133/0.39315. Took 0.06 sec\n",
            "Epoch 122, Loss(train/val) 1.02436/0.39317. Took 0.05 sec\n",
            "Epoch 123, Loss(train/val) 1.02437/0.39320. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 1.03376/0.39323. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 1.02569/0.39324. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 1.02632/0.39325. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 1.01468/0.39328. Took 0.05 sec\n",
            "Epoch 128, Loss(train/val) 1.03649/0.39332. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 1.01933/0.39334. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 1.02965/0.39337. Took 0.05 sec\n",
            "Epoch 131, Loss(train/val) 1.02745/0.39339. Took 0.06 sec\n",
            "Epoch 132, Loss(train/val) 1.02952/0.39342. Took 0.07 sec\n",
            "Epoch 133, Loss(train/val) 1.02461/0.39342. Took 0.06 sec\n",
            "Epoch 134, Loss(train/val) 1.03458/0.39345. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 1.02404/0.39348. Took 0.05 sec\n",
            "Epoch 136, Loss(train/val) 1.02042/0.39351. Took 0.06 sec\n",
            "Epoch 137, Loss(train/val) 1.03514/0.39354. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 1.02316/0.39355. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 1.02731/0.39356. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 1.03285/0.39356. Took 0.05 sec\n",
            "Epoch 141, Loss(train/val) 1.02090/0.39359. Took 0.05 sec\n",
            "Epoch 142, Loss(train/val) 1.02840/0.39362. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 1.03138/0.39365. Took 0.04 sec\n",
            "Epoch 144, Loss(train/val) 1.03346/0.39367. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 1.03014/0.39369. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 1.01660/0.39371. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 1.01824/0.39370. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 1.02717/0.39371. Took 0.05 sec\n",
            "Epoch 149, Loss(train/val) 1.02950/0.39373. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 1.03125/0.39376. Took 0.06 sec\n",
            "Epoch 151, Loss(train/val) 1.01706/0.39379. Took 0.06 sec\n",
            "Epoch 152, Loss(train/val) 1.02088/0.39382. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 1.03188/0.39380. Took 0.04 sec\n",
            "Epoch 154, Loss(train/val) 1.02575/0.39378. Took 0.05 sec\n",
            "Epoch 155, Loss(train/val) 1.02010/0.39380. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 1.02747/0.39382. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 1.02860/0.39386. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 1.03503/0.39390. Took 0.04 sec\n",
            "Epoch 159, Loss(train/val) 1.03131/0.39392. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 1.02456/0.39395. Took 0.05 sec\n",
            "Epoch 161, Loss(train/val) 1.01873/0.39395. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 1.02849/0.39396. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 1.01737/0.39396. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 1.02925/0.39399. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 1.02632/0.39401. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 1.02692/0.39402. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 1.02777/0.39404. Took 0.05 sec\n",
            "Epoch 168, Loss(train/val) 1.02528/0.39405. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 1.03171/0.39408. Took 0.04 sec\n",
            "Epoch 170, Loss(train/val) 1.02283/0.39411. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 1.03136/0.39410. Took 0.06 sec\n",
            "Epoch 172, Loss(train/val) 1.03420/0.39411. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 1.03167/0.39411. Took 0.07 sec\n",
            "Epoch 174, Loss(train/val) 1.02992/0.39413. Took 0.12 sec\n",
            "Epoch 175, Loss(train/val) 1.02528/0.39415. Took 0.15 sec\n",
            "Epoch 176, Loss(train/val) 1.03484/0.39418. Took 0.10 sec\n",
            "Epoch 177, Loss(train/val) 1.03461/0.39421. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 1.03559/0.39423. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 1.03322/0.39422. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 1.02551/0.39424. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 1.02783/0.39427. Took 0.06 sec\n",
            "Epoch 182, Loss(train/val) 1.01998/0.39428. Took 0.20 sec\n",
            "Epoch 183, Loss(train/val) 1.03240/0.39428. Took 0.19 sec\n",
            "Epoch 184, Loss(train/val) 1.02911/0.39429. Took 0.16 sec\n",
            "Epoch 185, Loss(train/val) 1.03270/0.39427. Took 0.23 sec\n",
            "Epoch 186, Loss(train/val) 1.02831/0.39430. Took 0.24 sec\n",
            "Epoch 187, Loss(train/val) 1.03270/0.39434. Took 0.14 sec\n",
            "Epoch 188, Loss(train/val) 1.02783/0.39438. Took 0.21 sec\n",
            "Epoch 189, Loss(train/val) 1.02895/0.39438. Took 0.16 sec\n",
            "Epoch 190, Loss(train/val) 1.02276/0.39440. Took 0.15 sec\n",
            "Epoch 191, Loss(train/val) 1.02989/0.39440. Took 0.17 sec\n",
            "Epoch 192, Loss(train/val) 1.03289/0.39438. Took 0.20 sec\n",
            "Epoch 193, Loss(train/val) 1.02779/0.39436. Took 0.20 sec\n",
            "Epoch 194, Loss(train/val) 1.02753/0.39434. Took 0.22 sec\n",
            "Epoch 195, Loss(train/val) 1.03007/0.39434. Took 0.09 sec\n",
            "Epoch 196, Loss(train/val) 1.03381/0.39433. Took 0.09 sec\n",
            "Epoch 197, Loss(train/val) 1.03165/0.39432. Took 0.09 sec\n",
            "Epoch 198, Loss(train/val) 1.02561/0.39432. Took 0.08 sec\n",
            "Epoch 199, Loss(train/val) 1.02997/0.39430. Took 0.09 sec\n",
            "Epoch 200, Loss(train/val) 1.03077/0.39430. Took 0.14 sec\n",
            "Epoch 201, Loss(train/val) 1.02329/0.39428. Took 0.10 sec\n",
            "Epoch 202, Loss(train/val) 1.02663/0.39426. Took 0.10 sec\n",
            "Epoch 203, Loss(train/val) 1.03301/0.39424. Took 0.06 sec\n",
            "Epoch 204, Loss(train/val) 1.03092/0.39423. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 1.02505/0.39417. Took 0.06 sec\n",
            "Epoch 206, Loss(train/val) 1.02360/0.39416. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 1.02245/0.39417. Took 0.06 sec\n",
            "Epoch 208, Loss(train/val) 1.02840/0.39418. Took 0.06 sec\n",
            "Epoch 209, Loss(train/val) 1.02652/0.39419. Took 0.05 sec\n",
            "Epoch 210, Loss(train/val) 1.02724/0.39419. Took 0.05 sec\n",
            "Epoch 211, Loss(train/val) 1.02296/0.39418. Took 0.06 sec\n",
            "Epoch 212, Loss(train/val) 1.02963/0.39416. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 1.02083/0.39413. Took 0.05 sec\n",
            "Epoch 214, Loss(train/val) 1.01788/0.39410. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 1.00268/0.39409. Took 0.05 sec\n",
            "Epoch 216, Loss(train/val) 1.02845/0.39408. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 1.02602/0.39408. Took 0.05 sec\n",
            "Epoch 218, Loss(train/val) 1.02959/0.39409. Took 0.05 sec\n",
            "Epoch 219, Loss(train/val) 1.02326/0.39406. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 1.02138/0.39402. Took 0.05 sec\n",
            "Epoch 221, Loss(train/val) 1.03021/0.39400. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 1.02490/0.39400. Took 0.06 sec\n",
            "Epoch 223, Loss(train/val) 1.01368/0.39401. Took 0.05 sec\n",
            "Epoch 224, Loss(train/val) 1.02151/0.39402. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 1.03171/0.39402. Took 0.06 sec\n",
            "Epoch 226, Loss(train/val) 1.02964/0.39399. Took 0.05 sec\n",
            "Epoch 227, Loss(train/val) 1.01696/0.39395. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 1.01491/0.39394. Took 0.05 sec\n",
            "Epoch 229, Loss(train/val) 1.02597/0.39395. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 1.02908/0.39396. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 1.02720/0.39397. Took 0.06 sec\n",
            "Epoch 232, Loss(train/val) 1.02615/0.39394. Took 0.06 sec\n",
            "Epoch 233, Loss(train/val) 1.02902/0.39392. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 1.03030/0.39391. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 1.03163/0.39389. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 1.03030/0.39390. Took 0.08 sec\n",
            "Epoch 237, Loss(train/val) 1.02691/0.39389. Took 0.11 sec\n",
            "Epoch 238, Loss(train/val) 1.02837/0.39387. Took 0.08 sec\n",
            "Epoch 239, Loss(train/val) 1.03278/0.39384. Took 0.09 sec\n",
            "Epoch 240, Loss(train/val) 1.02126/0.39382. Took 0.10 sec\n",
            "Epoch 241, Loss(train/val) 1.03573/0.39383. Took 0.09 sec\n",
            "Epoch 242, Loss(train/val) 1.02493/0.39384. Took 0.15 sec\n",
            "Epoch 243, Loss(train/val) 1.03004/0.39384. Took 0.10 sec\n",
            "Epoch 244, Loss(train/val) 1.02512/0.39380. Took 0.09 sec\n",
            "Epoch 245, Loss(train/val) 1.02143/0.39379. Took 0.09 sec\n",
            "Epoch 246, Loss(train/val) 1.02906/0.39378. Took 0.12 sec\n",
            "Epoch 247, Loss(train/val) 1.03045/0.39377. Took 0.16 sec\n",
            "Epoch 248, Loss(train/val) 1.03261/0.39376. Took 0.12 sec\n",
            "Epoch 249, Loss(train/val) 1.02499/0.39376. Took 0.09 sec\n",
            "Epoch 250, Loss(train/val) 1.01949/0.39378. Took 0.07 sec\n",
            "Epoch 251, Loss(train/val) 1.02453/0.39378. Took 0.08 sec\n",
            "Epoch 252, Loss(train/val) 1.03166/0.39370. Took 0.08 sec\n",
            "Epoch 253, Loss(train/val) 1.03039/0.39367. Took 0.08 sec\n",
            "Epoch 254, Loss(train/val) 1.02329/0.39364. Took 0.07 sec\n",
            "Epoch 255, Loss(train/val) 1.02795/0.39364. Took 0.05 sec\n",
            "Epoch 256, Loss(train/val) 1.02426/0.39366. Took 0.05 sec\n",
            "Epoch 257, Loss(train/val) 1.02769/0.39369. Took 0.05 sec\n",
            "Epoch 258, Loss(train/val) 1.02358/0.39371. Took 0.05 sec\n",
            "Epoch 259, Loss(train/val) 1.01760/0.39368. Took 0.05 sec\n",
            "Epoch 260, Loss(train/val) 1.02250/0.39368. Took 0.05 sec\n",
            "Epoch 261, Loss(train/val) 1.01810/0.39365. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 1.01813/0.39361. Took 0.05 sec\n",
            "Epoch 263, Loss(train/val) 1.02930/0.39359. Took 0.05 sec\n",
            "Epoch 264, Loss(train/val) 1.02264/0.39358. Took 0.04 sec\n",
            "Epoch 265, Loss(train/val) 1.02969/0.39355. Took 0.06 sec\n",
            "Epoch 266, Loss(train/val) 1.01437/0.39354. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 1.02747/0.39354. Took 0.05 sec\n",
            "Epoch 268, Loss(train/val) 1.02676/0.39356. Took 0.05 sec\n",
            "Epoch 269, Loss(train/val) 1.03100/0.39355. Took 0.05 sec\n",
            "Epoch 270, Loss(train/val) 1.02198/0.39349. Took 0.05 sec\n",
            "Epoch 271, Loss(train/val) 1.02560/0.39347. Took 0.04 sec\n",
            "Epoch 272, Loss(train/val) 1.03229/0.39348. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 1.03068/0.39350. Took 0.05 sec\n",
            "Epoch 274, Loss(train/val) 1.02359/0.39348. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 1.02732/0.39346. Took 0.05 sec\n",
            "Epoch 276, Loss(train/val) 1.02977/0.39343. Took 0.05 sec\n",
            "Epoch 277, Loss(train/val) 1.02521/0.39343. Took 0.04 sec\n",
            "Epoch 278, Loss(train/val) 1.03007/0.39342. Took 0.05 sec\n",
            "Epoch 279, Loss(train/val) 1.01831/0.39341. Took 0.05 sec\n",
            "Epoch 280, Loss(train/val) 1.02231/0.39339. Took 0.05 sec\n",
            "Epoch 281, Loss(train/val) 1.02965/0.39336. Took 0.05 sec\n",
            "Epoch 282, Loss(train/val) 1.02930/0.39337. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 1.00802/0.39338. Took 0.05 sec\n",
            "Epoch 284, Loss(train/val) 1.01859/0.39340. Took 0.04 sec\n",
            "Epoch 285, Loss(train/val) 1.02913/0.39342. Took 0.06 sec\n",
            "Epoch 286, Loss(train/val) 1.02833/0.39343. Took 0.06 sec\n",
            "Epoch 287, Loss(train/val) 1.02693/0.39341. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 1.02697/0.39332. Took 0.04 sec\n",
            "Epoch 289, Loss(train/val) 1.02387/0.39326. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 1.02005/0.39319. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 1.02237/0.39319. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 1.01124/0.39319. Took 0.05 sec\n",
            "Epoch 293, Loss(train/val) 1.01397/0.39322. Took 0.05 sec\n",
            "Epoch 294, Loss(train/val) 1.02278/0.39327. Took 0.04 sec\n",
            "Epoch 295, Loss(train/val) 1.02508/0.39330. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 1.02697/0.39335. Took 0.06 sec\n",
            "Epoch 297, Loss(train/val) 1.02737/0.39338. Took 0.05 sec\n",
            "Epoch 298, Loss(train/val) 1.03377/0.39340. Took 0.05 sec\n",
            "Epoch 299, Loss(train/val) 1.02615/0.39339. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 1.02341/0.39338. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 1.02571/0.39335. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 1.02487/0.39330. Took 0.05 sec\n",
            "Epoch 303, Loss(train/val) 1.01045/0.39321. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 1.03049/0.39317. Took 0.05 sec\n",
            "Epoch 305, Loss(train/val) 1.02192/0.39316. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 1.01787/0.39320. Took 0.05 sec\n",
            "Epoch 307, Loss(train/val) 1.02376/0.39325. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 1.03010/0.39329. Took 0.06 sec\n",
            "Epoch 309, Loss(train/val) 1.01931/0.39331. Took 0.06 sec\n",
            "Epoch 310, Loss(train/val) 1.03061/0.39330. Took 0.05 sec\n",
            "Epoch 311, Loss(train/val) 1.02703/0.39326. Took 0.05 sec\n",
            "Epoch 312, Loss(train/val) 1.02262/0.39324. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 1.02581/0.39321. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 1.02791/0.39321. Took 0.09 sec\n",
            "Epoch 315, Loss(train/val) 1.03111/0.39323. Took 0.09 sec\n",
            "Epoch 316, Loss(train/val) 1.01624/0.39324. Took 0.10 sec\n",
            "Epoch 317, Loss(train/val) 1.01878/0.39325. Took 0.11 sec\n",
            "Epoch 318, Loss(train/val) 1.01539/0.39325. Took 0.09 sec\n",
            "Epoch 319, Loss(train/val) 1.02222/0.39321. Took 0.09 sec\n",
            "Epoch 320, Loss(train/val) 1.03200/0.39318. Took 0.09 sec\n",
            "Epoch 321, Loss(train/val) 1.02847/0.39320. Took 0.09 sec\n",
            "Epoch 322, Loss(train/val) 1.02940/0.39324. Took 0.10 sec\n",
            "Epoch 323, Loss(train/val) 1.02115/0.39329. Took 0.09 sec\n",
            "Epoch 324, Loss(train/val) 1.02295/0.39333. Took 0.10 sec\n",
            "Epoch 325, Loss(train/val) 1.02585/0.39334. Took 0.08 sec\n",
            "Epoch 326, Loss(train/val) 1.02956/0.39331. Took 0.09 sec\n",
            "Epoch 327, Loss(train/val) 1.02759/0.39327. Took 0.10 sec\n",
            "Epoch 328, Loss(train/val) 1.02745/0.39324. Took 0.07 sec\n",
            "Epoch 329, Loss(train/val) 1.02995/0.39320. Took 0.09 sec\n",
            "Epoch 330, Loss(train/val) 1.02791/0.39321. Took 0.08 sec\n",
            "Epoch 331, Loss(train/val) 1.02284/0.39325. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 1.01069/0.39331. Took 0.05 sec\n",
            "Epoch 333, Loss(train/val) 1.02703/0.39337. Took 0.05 sec\n",
            "Epoch 334, Loss(train/val) 1.02640/0.39343. Took 0.06 sec\n",
            "Epoch 335, Loss(train/val) 1.02134/0.39344. Took 0.05 sec\n",
            "Epoch 336, Loss(train/val) 1.02269/0.39346. Took 0.05 sec\n",
            "Epoch 337, Loss(train/val) 1.02515/0.39346. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 1.02875/0.39342. Took 0.05 sec\n",
            "Epoch 339, Loss(train/val) 1.01813/0.39339. Took 0.05 sec\n",
            "Epoch 340, Loss(train/val) 1.02312/0.39340. Took 0.05 sec\n",
            "Epoch 341, Loss(train/val) 1.02958/0.39339. Took 0.05 sec\n",
            "Epoch 342, Loss(train/val) 1.01664/0.39336. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 1.01871/0.39338. Took 0.05 sec\n",
            "Epoch 344, Loss(train/val) 1.02548/0.39339. Took 0.05 sec\n",
            "Epoch 345, Loss(train/val) 1.02377/0.39342. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 1.02643/0.39346. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 1.01817/0.39347. Took 0.05 sec\n",
            "Epoch 348, Loss(train/val) 1.02543/0.39348. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 1.02588/0.39350. Took 0.05 sec\n",
            "Epoch 350, Loss(train/val) 1.02389/0.39353. Took 0.05 sec\n",
            "Epoch 351, Loss(train/val) 1.02631/0.39355. Took 0.05 sec\n",
            "Epoch 352, Loss(train/val) 1.02395/0.39357. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 1.02299/0.39361. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 1.01711/0.39367. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 1.01938/0.39373. Took 0.05 sec\n",
            "Epoch 356, Loss(train/val) 1.02411/0.39380. Took 0.05 sec\n",
            "Epoch 357, Loss(train/val) 1.01838/0.39388. Took 0.05 sec\n",
            "Epoch 358, Loss(train/val) 1.01486/0.39393. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 1.00947/0.39396. Took 0.04 sec\n",
            "Epoch 360, Loss(train/val) 1.00817/0.39402. Took 0.05 sec\n",
            "Epoch 361, Loss(train/val) 1.02534/0.39412. Took 0.05 sec\n",
            "Epoch 362, Loss(train/val) 1.01379/0.39423. Took 0.05 sec\n",
            "Epoch 363, Loss(train/val) 1.02393/0.39435. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 1.01792/0.39445. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 1.02754/0.39456. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 1.02816/0.39468. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 1.02395/0.39482. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 1.02457/0.39495. Took 0.05 sec\n",
            "Epoch 369, Loss(train/val) 1.02089/0.39505. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 1.01858/0.39513. Took 0.05 sec\n",
            "Epoch 371, Loss(train/val) 1.02625/0.39519. Took 0.05 sec\n",
            "Epoch 372, Loss(train/val) 1.02603/0.39525. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 1.02687/0.39531. Took 0.05 sec\n",
            "Epoch 374, Loss(train/val) 1.02769/0.39538. Took 0.05 sec\n",
            "Epoch 375, Loss(train/val) 1.02785/0.39545. Took 0.05 sec\n",
            "Epoch 376, Loss(train/val) 1.01551/0.39551. Took 0.06 sec\n",
            "Epoch 377, Loss(train/val) 1.02693/0.39557. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 1.02277/0.39561. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 1.02755/0.39557. Took 0.05 sec\n",
            "Epoch 380, Loss(train/val) 1.02545/0.39557. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 1.02461/0.39559. Took 0.05 sec\n",
            "Epoch 382, Loss(train/val) 1.01151/0.39562. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 1.02001/0.39564. Took 0.05 sec\n",
            "Epoch 384, Loss(train/val) 1.02928/0.39568. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 1.01802/0.39573. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 1.02090/0.39578. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 1.01604/0.39581. Took 0.05 sec\n",
            "Epoch 388, Loss(train/val) 1.02570/0.39585. Took 0.05 sec\n",
            "Epoch 389, Loss(train/val) 1.01211/0.39590. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 1.01774/0.39594. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 1.01597/0.39598. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 1.00768/0.39599. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 1.01679/0.39601. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 1.01819/0.39603. Took 0.05 sec\n",
            "Epoch 395, Loss(train/val) 1.02602/0.39604. Took 0.05 sec\n",
            "Epoch 396, Loss(train/val) 1.01462/0.39605. Took 0.05 sec\n",
            "Epoch 397, Loss(train/val) 1.02092/0.39606. Took 0.06 sec\n",
            "Epoch 398, Loss(train/val) 1.01775/0.39607. Took 0.05 sec\n",
            "Epoch 399, Loss(train/val) 1.02298/0.39609. Took 0.05 sec\n",
            "Epoch 400, Loss(train/val) 1.01770/0.39610. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 1.02104/0.39612. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 1.02038/0.39612. Took 0.05 sec\n",
            "Epoch 403, Loss(train/val) 1.02451/0.39612. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 1.02501/0.39611. Took 0.05 sec\n",
            "Epoch 405, Loss(train/val) 1.02397/0.39612. Took 0.06 sec\n",
            "Epoch 406, Loss(train/val) 1.01771/0.39612. Took 0.05 sec\n",
            "Epoch 407, Loss(train/val) 1.02522/0.39614. Took 0.05 sec\n",
            "Epoch 408, Loss(train/val) 1.02547/0.39616. Took 0.07 sec\n",
            "Epoch 409, Loss(train/val) 1.02157/0.39618. Took 0.10 sec\n",
            "Epoch 410, Loss(train/val) 1.00768/0.39621. Took 0.08 sec\n",
            "Epoch 411, Loss(train/val) 1.00194/0.39623. Took 0.08 sec\n",
            "Epoch 412, Loss(train/val) 1.01844/0.39624. Took 0.08 sec\n",
            "Epoch 413, Loss(train/val) 1.02094/0.39626. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 1.01477/0.39626. Took 0.06 sec\n",
            "Epoch 415, Loss(train/val) 1.01920/0.39622. Took 0.05 sec\n",
            "Epoch 416, Loss(train/val) 1.01156/0.39622. Took 0.05 sec\n",
            "Epoch 417, Loss(train/val) 1.02049/0.39623. Took 0.06 sec\n",
            "Epoch 418, Loss(train/val) 1.02183/0.39625. Took 0.08 sec\n",
            "Epoch 419, Loss(train/val) 1.00947/0.39626. Took 0.08 sec\n",
            "Epoch 420, Loss(train/val) 1.01614/0.39628. Took 0.10 sec\n",
            "Epoch 421, Loss(train/val) 1.01351/0.39629. Took 0.09 sec\n",
            "Epoch 422, Loss(train/val) 1.01790/0.39632. Took 0.10 sec\n",
            "Epoch 423, Loss(train/val) 1.01446/0.39634. Took 0.11 sec\n",
            "Epoch 424, Loss(train/val) 1.02589/0.39635. Took 0.09 sec\n",
            "Epoch 425, Loss(train/val) 1.01697/0.39637. Took 0.08 sec\n",
            "Epoch 426, Loss(train/val) 1.01944/0.39639. Took 0.08 sec\n",
            "Epoch 427, Loss(train/val) 1.01899/0.39641. Took 0.09 sec\n",
            "Epoch 428, Loss(train/val) 1.01190/0.39641. Took 0.08 sec\n",
            "Epoch 429, Loss(train/val) 1.01491/0.39642. Took 0.10 sec\n",
            "Epoch 430, Loss(train/val) 1.00474/0.39643. Took 0.09 sec\n",
            "Epoch 431, Loss(train/val) 1.00968/0.39644. Took 0.08 sec\n",
            "Epoch 432, Loss(train/val) 1.00786/0.39646. Took 0.08 sec\n",
            "Epoch 433, Loss(train/val) 1.02152/0.39647. Took 0.08 sec\n",
            "Epoch 434, Loss(train/val) 1.01967/0.39649. Took 0.08 sec\n",
            "Epoch 435, Loss(train/val) 1.01730/0.39649. Took 0.07 sec\n",
            "Epoch 436, Loss(train/val) 1.01857/0.39650. Took 0.05 sec\n",
            "Epoch 437, Loss(train/val) 1.01308/0.39651. Took 0.05 sec\n",
            "Epoch 438, Loss(train/val) 1.02052/0.39652. Took 0.05 sec\n",
            "Epoch 439, Loss(train/val) 1.01338/0.39654. Took 0.05 sec\n",
            "Epoch 440, Loss(train/val) 1.01438/0.39654. Took 0.05 sec\n",
            "Epoch 441, Loss(train/val) 0.99913/0.39654. Took 0.05 sec\n",
            "Epoch 442, Loss(train/val) 1.01099/0.39655. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 1.01500/0.39655. Took 0.04 sec\n",
            "Epoch 444, Loss(train/val) 1.01600/0.39655. Took 0.05 sec\n",
            "Epoch 445, Loss(train/val) 1.01634/0.39654. Took 0.05 sec\n",
            "Epoch 446, Loss(train/val) 1.01231/0.39655. Took 0.05 sec\n",
            "Epoch 447, Loss(train/val) 1.01571/0.39655. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 1.01290/0.39655. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 1.00984/0.39655. Took 0.05 sec\n",
            "Epoch 450, Loss(train/val) 1.02258/0.39655. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 1.02192/0.39656. Took 0.05 sec\n",
            "Epoch 452, Loss(train/val) 1.01456/0.39657. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 1.00223/0.39655. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 1.01853/0.39654. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 1.00577/0.39651. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 1.00532/0.39648. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 1.01572/0.39647. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 1.01398/0.39646. Took 0.05 sec\n",
            "Epoch 459, Loss(train/val) 1.01252/0.39647. Took 0.05 sec\n",
            "Epoch 460, Loss(train/val) 0.98581/0.39641. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 1.01682/0.39640. Took 0.05 sec\n",
            "Epoch 462, Loss(train/val) 1.01400/0.39639. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 1.01822/0.39640. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 1.01709/0.39642. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 1.01056/0.39642. Took 0.05 sec\n",
            "Epoch 466, Loss(train/val) 1.01497/0.39643. Took 0.05 sec\n",
            "Epoch 467, Loss(train/val) 0.99991/0.39645. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 1.01854/0.39643. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 1.01131/0.39641. Took 0.05 sec\n",
            "Epoch 470, Loss(train/val) 1.01741/0.39641. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 1.01521/0.39638. Took 0.05 sec\n",
            "Epoch 472, Loss(train/val) 1.01197/0.39639. Took 0.04 sec\n",
            "Epoch 473, Loss(train/val) 1.01223/0.39636. Took 0.05 sec\n",
            "Epoch 474, Loss(train/val) 1.00857/0.39631. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 1.01546/0.39626. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 1.01491/0.39621. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 1.01168/0.39619. Took 0.06 sec\n",
            "Epoch 478, Loss(train/val) 1.01376/0.39612. Took 0.05 sec\n",
            "Epoch 479, Loss(train/val) 1.01353/0.39609. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 1.01253/0.39607. Took 0.05 sec\n",
            "Epoch 481, Loss(train/val) 1.00045/0.39600. Took 0.05 sec\n",
            "Epoch 482, Loss(train/val) 0.99154/0.39593. Took 0.05 sec\n",
            "Epoch 483, Loss(train/val) 1.01387/0.39589. Took 0.04 sec\n",
            "Epoch 484, Loss(train/val) 1.01706/0.39584. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 1.01544/0.39575. Took 0.05 sec\n",
            "Epoch 486, Loss(train/val) 1.00579/0.39568. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 1.01014/0.39562. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 1.00668/0.39555. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 1.01201/0.39550. Took 0.04 sec\n",
            "Epoch 490, Loss(train/val) 1.00481/0.39544. Took 0.05 sec\n",
            "Epoch 491, Loss(train/val) 1.00221/0.39535. Took 0.05 sec\n",
            "Epoch 492, Loss(train/val) 1.00725/0.39532. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 1.00665/0.39521. Took 0.04 sec\n",
            "Epoch 494, Loss(train/val) 1.01256/0.39516. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 1.01673/0.39514. Took 0.05 sec\n",
            "Epoch 496, Loss(train/val) 1.01250/0.39510. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 1.01282/0.39500. Took 0.05 sec\n",
            "Epoch 498, Loss(train/val) 1.01241/0.39497. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.98907/0.39480. Took 0.05 sec\n",
            "Epoch 500, Loss(train/val) 1.00222/0.39470. Took 0.05 sec\n",
            "Epoch 501, Loss(train/val) 1.00408/0.39461. Took 0.05 sec\n",
            "Epoch 502, Loss(train/val) 1.00290/0.39456. Took 0.05 sec\n",
            "Epoch 503, Loss(train/val) 1.00326/0.39452. Took 0.05 sec\n",
            "Epoch 504, Loss(train/val) 1.00842/0.39442. Took 0.05 sec\n",
            "Epoch 505, Loss(train/val) 1.00806/0.39441. Took 0.06 sec\n",
            "Epoch 506, Loss(train/val) 1.01008/0.39434. Took 0.04 sec\n",
            "Epoch 507, Loss(train/val) 1.01322/0.39427. Took 0.05 sec\n",
            "Epoch 508, Loss(train/val) 1.00620/0.39421. Took 0.05 sec\n",
            "Epoch 509, Loss(train/val) 1.00152/0.39409. Took 0.04 sec\n",
            "Epoch 510, Loss(train/val) 1.00385/0.39403. Took 0.05 sec\n",
            "Epoch 511, Loss(train/val) 1.01047/0.39396. Took 0.05 sec\n",
            "Epoch 512, Loss(train/val) 0.99302/0.39379. Took 0.04 sec\n",
            "Epoch 513, Loss(train/val) 1.01096/0.39368. Took 0.05 sec\n",
            "Epoch 514, Loss(train/val) 1.01009/0.39361. Took 0.05 sec\n",
            "Epoch 515, Loss(train/val) 0.99706/0.39349. Took 0.05 sec\n",
            "Epoch 516, Loss(train/val) 1.00785/0.39338. Took 0.05 sec\n",
            "Epoch 517, Loss(train/val) 1.00649/0.39339. Took 0.05 sec\n",
            "Epoch 518, Loss(train/val) 0.99842/0.39332. Took 0.04 sec\n",
            "Epoch 519, Loss(train/val) 1.00950/0.39324. Took 0.05 sec\n",
            "Epoch 520, Loss(train/val) 1.00920/0.39314. Took 0.05 sec\n",
            "Epoch 521, Loss(train/val) 1.00198/0.39302. Took 0.05 sec\n",
            "Epoch 522, Loss(train/val) 0.99969/0.39287. Took 0.05 sec\n",
            "Epoch 523, Loss(train/val) 1.00138/0.39277. Took 0.05 sec\n",
            "Epoch 524, Loss(train/val) 1.01035/0.39270. Took 0.05 sec\n",
            "Epoch 525, Loss(train/val) 1.00388/0.39262. Took 0.05 sec\n",
            "Epoch 526, Loss(train/val) 1.00431/0.39254. Took 0.05 sec\n",
            "Epoch 527, Loss(train/val) 1.00039/0.39233. Took 0.05 sec\n",
            "Epoch 528, Loss(train/val) 1.00592/0.39188. Took 0.05 sec\n",
            "Epoch 529, Loss(train/val) 1.00766/0.39114. Took 0.05 sec\n",
            "Epoch 530, Loss(train/val) 1.00846/0.39001. Took 0.05 sec\n",
            "Epoch 531, Loss(train/val) 1.00178/0.38869. Took 0.04 sec\n",
            "Epoch 532, Loss(train/val) 0.99672/0.38734. Took 0.05 sec\n",
            "Epoch 533, Loss(train/val) 1.00026/0.38603. Took 0.05 sec\n",
            "Epoch 534, Loss(train/val) 1.00299/0.38490. Took 0.04 sec\n",
            "Epoch 535, Loss(train/val) 0.99989/0.38400. Took 0.05 sec\n",
            "Epoch 536, Loss(train/val) 1.00099/0.38335. Took 0.05 sec\n",
            "Epoch 537, Loss(train/val) 1.00232/0.38294. Took 0.05 sec\n",
            "Epoch 538, Loss(train/val) 0.99228/0.38266. Took 0.05 sec\n",
            "Epoch 539, Loss(train/val) 1.00118/0.38251. Took 0.05 sec\n",
            "Epoch 540, Loss(train/val) 0.99918/0.38254. Took 0.05 sec\n",
            "Epoch 541, Loss(train/val) 0.99367/0.38261. Took 0.05 sec\n",
            "Epoch 542, Loss(train/val) 0.99856/0.38268. Took 0.05 sec\n",
            "Epoch 543, Loss(train/val) 0.99614/0.38295. Took 0.05 sec\n",
            "Epoch 544, Loss(train/val) 0.99954/0.38313. Took 0.05 sec\n",
            "Epoch 545, Loss(train/val) 0.99451/0.38330. Took 0.05 sec\n",
            "Epoch 546, Loss(train/val) 1.00056/0.38335. Took 0.05 sec\n",
            "Epoch 547, Loss(train/val) 1.00040/0.38348. Took 0.05 sec\n",
            "Epoch 548, Loss(train/val) 1.00117/0.38345. Took 0.05 sec\n",
            "Epoch 549, Loss(train/val) 1.00041/0.38346. Took 0.05 sec\n",
            "Epoch 550, Loss(train/val) 0.99172/0.38347. Took 0.05 sec\n",
            "Epoch 551, Loss(train/val) 0.99438/0.38346. Took 0.05 sec\n",
            "Epoch 552, Loss(train/val) 0.99721/0.38335. Took 0.05 sec\n",
            "Epoch 553, Loss(train/val) 0.98741/0.38306. Took 0.05 sec\n",
            "Epoch 554, Loss(train/val) 0.99391/0.38288. Took 0.05 sec\n",
            "Epoch 555, Loss(train/val) 0.99135/0.38284. Took 0.05 sec\n",
            "Epoch 556, Loss(train/val) 0.99087/0.38275. Took 0.05 sec\n",
            "Epoch 557, Loss(train/val) 0.98587/0.38266. Took 0.04 sec\n",
            "Epoch 558, Loss(train/val) 0.99191/0.38252. Took 0.05 sec\n",
            "Epoch 559, Loss(train/val) 0.98891/0.38252. Took 0.05 sec\n",
            "Epoch 560, Loss(train/val) 0.99033/0.38259. Took 0.05 sec\n",
            "Epoch 561, Loss(train/val) 0.99509/0.38253. Took 0.05 sec\n",
            "Epoch 562, Loss(train/val) 0.98701/0.38239. Took 0.05 sec\n",
            "Epoch 563, Loss(train/val) 0.98300/0.38238. Took 0.05 sec\n",
            "Epoch 564, Loss(train/val) 0.98706/0.38237. Took 0.04 sec\n",
            "Epoch 565, Loss(train/val) 0.99087/0.38211. Took 0.05 sec\n",
            "Epoch 566, Loss(train/val) 0.98724/0.38204. Took 0.05 sec\n",
            "Epoch 567, Loss(train/val) 0.98378/0.38220. Took 0.05 sec\n",
            "Epoch 568, Loss(train/val) 0.98371/0.38209. Took 0.05 sec\n",
            "Epoch 569, Loss(train/val) 0.98722/0.38177. Took 0.05 sec\n",
            "Epoch 570, Loss(train/val) 0.98888/0.38161. Took 0.06 sec\n",
            "Epoch 571, Loss(train/val) 0.98805/0.38147. Took 0.05 sec\n",
            "Epoch 572, Loss(train/val) 0.99133/0.38137. Took 0.05 sec\n",
            "Epoch 573, Loss(train/val) 0.96954/0.38129. Took 0.05 sec\n",
            "Epoch 574, Loss(train/val) 0.98810/0.38100. Took 0.05 sec\n",
            "Epoch 575, Loss(train/val) 0.98299/0.38063. Took 0.05 sec\n",
            "Epoch 576, Loss(train/val) 0.97674/0.38039. Took 0.05 sec\n",
            "Epoch 577, Loss(train/val) 0.98239/0.38000. Took 0.05 sec\n",
            "Epoch 578, Loss(train/val) 0.98221/0.37999. Took 0.05 sec\n",
            "Epoch 579, Loss(train/val) 0.98483/0.37967. Took 0.05 sec\n",
            "Epoch 580, Loss(train/val) 0.97407/0.37944. Took 0.05 sec\n",
            "Epoch 581, Loss(train/val) 0.97890/0.37916. Took 0.05 sec\n",
            "Epoch 582, Loss(train/val) 0.97054/0.37895. Took 0.04 sec\n",
            "Epoch 583, Loss(train/val) 0.97448/0.37879. Took 0.05 sec\n",
            "Epoch 584, Loss(train/val) 0.97073/0.37840. Took 0.05 sec\n",
            "Epoch 585, Loss(train/val) 0.97663/0.37807. Took 0.05 sec\n",
            "Epoch 586, Loss(train/val) 0.98315/0.37774. Took 0.05 sec\n",
            "Epoch 587, Loss(train/val) 0.97705/0.37746. Took 0.05 sec\n",
            "Epoch 588, Loss(train/val) 0.97881/0.37696. Took 0.05 sec\n",
            "Epoch 589, Loss(train/val) 0.97686/0.37666. Took 0.05 sec\n",
            "Epoch 590, Loss(train/val) 0.96115/0.37614. Took 0.05 sec\n",
            "Epoch 591, Loss(train/val) 0.97427/0.37601. Took 0.06 sec\n",
            "Epoch 592, Loss(train/val) 0.95861/0.37525. Took 0.05 sec\n",
            "Epoch 593, Loss(train/val) 0.96961/0.37478. Took 0.05 sec\n",
            "Epoch 594, Loss(train/val) 0.97218/0.37447. Took 0.05 sec\n",
            "Epoch 595, Loss(train/val) 0.96331/0.37387. Took 0.06 sec\n",
            "Epoch 596, Loss(train/val) 0.96852/0.37348. Took 0.05 sec\n",
            "Epoch 597, Loss(train/val) 0.97409/0.37289. Took 0.05 sec\n",
            "Epoch 598, Loss(train/val) 0.96209/0.37237. Took 0.05 sec\n",
            "Epoch 599, Loss(train/val) 0.96354/0.37172. Took 0.05 sec\n",
            "Epoch 600, Loss(train/val) 0.95868/0.37122. Took 0.06 sec\n",
            "Epoch 601, Loss(train/val) 0.96037/0.37073. Took 0.05 sec\n",
            "Epoch 602, Loss(train/val) 0.94361/0.37036. Took 0.04 sec\n",
            "Epoch 603, Loss(train/val) 0.96301/0.36976. Took 0.04 sec\n",
            "Epoch 604, Loss(train/val) 0.96029/0.36917. Took 0.05 sec\n",
            "Epoch 605, Loss(train/val) 0.94484/0.36825. Took 0.05 sec\n",
            "Epoch 606, Loss(train/val) 0.96536/0.36792. Took 0.05 sec\n",
            "Epoch 607, Loss(train/val) 0.95883/0.36749. Took 0.05 sec\n",
            "Epoch 608, Loss(train/val) 0.96196/0.36689. Took 0.05 sec\n",
            "Epoch 609, Loss(train/val) 0.96156/0.36664. Took 0.05 sec\n",
            "Epoch 610, Loss(train/val) 0.95377/0.36670. Took 0.05 sec\n",
            "Epoch 611, Loss(train/val) 0.95641/0.36668. Took 0.05 sec\n",
            "Epoch 612, Loss(train/val) 0.95501/0.36623. Took 0.06 sec\n",
            "Epoch 613, Loss(train/val) 0.95044/0.36550. Took 0.05 sec\n",
            "Epoch 614, Loss(train/val) 0.95730/0.36520. Took 0.05 sec\n",
            "Epoch 615, Loss(train/val) 0.94721/0.36472. Took 0.05 sec\n",
            "Epoch 616, Loss(train/val) 0.95377/0.36413. Took 0.05 sec\n",
            "Epoch 617, Loss(train/val) 0.95665/0.36367. Took 0.05 sec\n",
            "Epoch 618, Loss(train/val) 0.94957/0.36346. Took 0.05 sec\n",
            "Epoch 619, Loss(train/val) 0.94729/0.36333. Took 0.05 sec\n",
            "Epoch 620, Loss(train/val) 0.94938/0.36268. Took 0.05 sec\n",
            "Epoch 621, Loss(train/val) 0.93516/0.36147. Took 0.05 sec\n",
            "Epoch 622, Loss(train/val) 0.94517/0.36091. Took 0.05 sec\n",
            "Epoch 623, Loss(train/val) 0.94245/0.36038. Took 0.07 sec\n",
            "Epoch 624, Loss(train/val) 0.94297/0.36002. Took 0.05 sec\n",
            "Epoch 625, Loss(train/val) 0.94890/0.35971. Took 0.05 sec\n",
            "Epoch 626, Loss(train/val) 0.94303/0.35933. Took 0.05 sec\n",
            "Epoch 627, Loss(train/val) 0.94269/0.35842. Took 0.05 sec\n",
            "Epoch 628, Loss(train/val) 0.94120/0.35787. Took 0.05 sec\n",
            "Epoch 629, Loss(train/val) 0.93998/0.35723. Took 0.05 sec\n",
            "Epoch 630, Loss(train/val) 0.93533/0.35699. Took 0.05 sec\n",
            "Epoch 631, Loss(train/val) 0.93290/0.35621. Took 0.05 sec\n",
            "Epoch 632, Loss(train/val) 0.93147/0.35574. Took 0.05 sec\n",
            "Epoch 633, Loss(train/val) 0.93698/0.35528. Took 0.06 sec\n",
            "Epoch 634, Loss(train/val) 0.92655/0.35464. Took 0.05 sec\n",
            "Epoch 635, Loss(train/val) 0.93044/0.35460. Took 0.05 sec\n",
            "Epoch 636, Loss(train/val) 0.93045/0.35393. Took 0.05 sec\n",
            "Epoch 637, Loss(train/val) 0.92528/0.35305. Took 0.05 sec\n",
            "Epoch 638, Loss(train/val) 0.91607/0.35223. Took 0.04 sec\n",
            "Epoch 639, Loss(train/val) 0.92519/0.35152. Took 0.05 sec\n",
            "Epoch 640, Loss(train/val) 0.92533/0.35097. Took 0.05 sec\n",
            "Epoch 641, Loss(train/val) 0.92774/0.35055. Took 0.05 sec\n",
            "Epoch 642, Loss(train/val) 0.92495/0.35038. Took 0.05 sec\n",
            "Epoch 643, Loss(train/val) 0.92318/0.34940. Took 0.05 sec\n",
            "Epoch 644, Loss(train/val) 0.92802/0.34859. Took 0.05 sec\n",
            "Epoch 645, Loss(train/val) 0.92706/0.34775. Took 0.05 sec\n",
            "Epoch 646, Loss(train/val) 0.91959/0.34716. Took 0.05 sec\n",
            "Epoch 647, Loss(train/val) 0.91768/0.34665. Took 0.05 sec\n",
            "Epoch 648, Loss(train/val) 0.92200/0.34603. Took 0.05 sec\n",
            "Epoch 649, Loss(train/val) 0.90775/0.34515. Took 0.05 sec\n",
            "Epoch 650, Loss(train/val) 0.92174/0.34483. Took 0.05 sec\n",
            "Epoch 651, Loss(train/val) 0.91864/0.34477. Took 0.05 sec\n",
            "Epoch 652, Loss(train/val) 0.91533/0.34450. Took 0.05 sec\n",
            "Epoch 653, Loss(train/val) 0.89911/0.34331. Took 0.05 sec\n",
            "Epoch 654, Loss(train/val) 0.91577/0.34260. Took 0.06 sec\n",
            "Epoch 655, Loss(train/val) 0.91387/0.34213. Took 0.05 sec\n",
            "Epoch 656, Loss(train/val) 0.91378/0.34178. Took 0.05 sec\n",
            "Epoch 657, Loss(train/val) 0.89989/0.34089. Took 0.05 sec\n",
            "Epoch 658, Loss(train/val) 0.90545/0.34041. Took 0.04 sec\n",
            "Epoch 659, Loss(train/val) 0.90330/0.33971. Took 0.05 sec\n",
            "Epoch 660, Loss(train/val) 0.90248/0.33940. Took 0.04 sec\n",
            "Epoch 661, Loss(train/val) 0.89127/0.33815. Took 0.05 sec\n",
            "Epoch 662, Loss(train/val) 0.90064/0.33737. Took 0.05 sec\n",
            "Epoch 663, Loss(train/val) 0.90285/0.33726. Took 0.05 sec\n",
            "Epoch 664, Loss(train/val) 0.89952/0.33630. Took 0.05 sec\n",
            "Epoch 665, Loss(train/val) 0.89574/0.33539. Took 0.05 sec\n",
            "Epoch 666, Loss(train/val) 0.89568/0.33553. Took 0.05 sec\n",
            "Epoch 667, Loss(train/val) 0.89676/0.33421. Took 0.05 sec\n",
            "Epoch 668, Loss(train/val) 0.89956/0.33375. Took 0.05 sec\n",
            "Epoch 669, Loss(train/val) 0.88856/0.33311. Took 0.05 sec\n",
            "Epoch 670, Loss(train/val) 0.89426/0.33235. Took 0.05 sec\n",
            "Epoch 671, Loss(train/val) 0.89457/0.33172. Took 0.05 sec\n",
            "Epoch 672, Loss(train/val) 0.88104/0.33089. Took 0.05 sec\n",
            "Epoch 673, Loss(train/val) 0.89622/0.33020. Took 0.05 sec\n",
            "Epoch 674, Loss(train/val) 0.88105/0.32957. Took 0.05 sec\n",
            "Epoch 675, Loss(train/val) 0.87751/0.32858. Took 0.05 sec\n",
            "Epoch 676, Loss(train/val) 0.88097/0.32671. Took 0.05 sec\n",
            "Epoch 677, Loss(train/val) 0.88009/0.32586. Took 0.05 sec\n",
            "Epoch 678, Loss(train/val) 0.88587/0.32544. Took 0.05 sec\n",
            "Epoch 679, Loss(train/val) 0.87732/0.32450. Took 0.05 sec\n",
            "Epoch 680, Loss(train/val) 0.88064/0.32370. Took 0.05 sec\n",
            "Epoch 681, Loss(train/val) 0.88147/0.32354. Took 0.05 sec\n",
            "Epoch 682, Loss(train/val) 0.88075/0.32344. Took 0.05 sec\n",
            "Epoch 683, Loss(train/val) 0.87330/0.32321. Took 0.05 sec\n",
            "Epoch 684, Loss(train/val) 0.88119/0.32290. Took 0.06 sec\n",
            "Epoch 685, Loss(train/val) 0.88211/0.32264. Took 0.05 sec\n",
            "Epoch 686, Loss(train/val) 0.87465/0.32244. Took 0.05 sec\n",
            "Epoch 687, Loss(train/val) 0.86776/0.32131. Took 0.05 sec\n",
            "Epoch 688, Loss(train/val) 0.87481/0.32029. Took 0.05 sec\n",
            "Epoch 689, Loss(train/val) 0.87269/0.31996. Took 0.05 sec\n",
            "Epoch 690, Loss(train/val) 0.86719/0.31872. Took 0.05 sec\n",
            "Epoch 691, Loss(train/val) 0.87220/0.31755. Took 0.05 sec\n",
            "Epoch 692, Loss(train/val) 0.87020/0.31677. Took 0.05 sec\n",
            "Epoch 693, Loss(train/val) 0.86176/0.31652. Took 0.05 sec\n",
            "Epoch 694, Loss(train/val) 0.86632/0.31525. Took 0.06 sec\n",
            "Epoch 695, Loss(train/val) 0.87099/0.31430. Took 0.05 sec\n",
            "Epoch 696, Loss(train/val) 0.86247/0.31292. Took 0.06 sec\n",
            "Epoch 697, Loss(train/val) 0.86610/0.31166. Took 0.05 sec\n",
            "Epoch 698, Loss(train/val) 0.85990/0.31118. Took 0.05 sec\n",
            "Epoch 699, Loss(train/val) 0.86025/0.31090. Took 0.05 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=700, exp_name='exp3_lr', hid_dim=16, input_dim=1, l2=1e-05, lr=3e-05, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.02503/0.41964. Took 0.04 sec\n",
            "Epoch 1, Loss(train/val) 1.02348/0.41865. Took 0.04 sec\n",
            "Epoch 2, Loss(train/val) 1.02400/0.41782. Took 0.05 sec\n",
            "Epoch 3, Loss(train/val) 1.02791/0.41714. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.02895/0.41659. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.02383/0.41614. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.02458/0.41576. Took 0.04 sec\n",
            "Epoch 7, Loss(train/val) 1.00815/0.41546. Took 0.05 sec\n",
            "Epoch 8, Loss(train/val) 1.02627/0.41522. Took 0.05 sec\n",
            "Epoch 9, Loss(train/val) 1.02163/0.41501. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.02200/0.41484. Took 0.04 sec\n",
            "Epoch 11, Loss(train/val) 1.02366/0.41472. Took 0.05 sec\n",
            "Epoch 12, Loss(train/val) 1.02737/0.41461. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 1.02520/0.41455. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.02896/0.41451. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.01251/0.41449. Took 0.05 sec\n",
            "Epoch 16, Loss(train/val) 1.01513/0.41451. Took 0.05 sec\n",
            "Epoch 17, Loss(train/val) 1.02647/0.41455. Took 0.05 sec\n",
            "Epoch 18, Loss(train/val) 1.02143/0.41461. Took 0.05 sec\n",
            "Epoch 19, Loss(train/val) 1.01867/0.41470. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 1.02332/0.41482. Took 0.04 sec\n",
            "Epoch 21, Loss(train/val) 1.02392/0.41496. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 1.02526/0.41513. Took 0.04 sec\n",
            "Epoch 23, Loss(train/val) 1.01241/0.41535. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 1.01756/0.41560. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 1.02752/0.41591. Took 0.05 sec\n",
            "Epoch 26, Loss(train/val) 1.02631/0.41625. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.01965/0.41665. Took 0.04 sec\n",
            "Epoch 28, Loss(train/val) 1.02806/0.41710. Took 0.04 sec\n",
            "Epoch 29, Loss(train/val) 1.02045/0.41761. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 1.02350/0.41817. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 1.03013/0.41880. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 1.02494/0.41947. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 1.02699/0.42019. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 1.02474/0.42092. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 1.01812/0.42163. Took 0.04 sec\n",
            "Epoch 36, Loss(train/val) 1.02382/0.42230. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.02798/0.42293. Took 0.04 sec\n",
            "Epoch 38, Loss(train/val) 1.01785/0.42348. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 1.02167/0.42394. Took 0.06 sec\n",
            "Epoch 40, Loss(train/val) 1.02432/0.42429. Took 0.05 sec\n",
            "Epoch 41, Loss(train/val) 1.02473/0.42456. Took 0.05 sec\n",
            "Epoch 42, Loss(train/val) 1.02690/0.42477. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 1.03066/0.42493. Took 0.05 sec\n",
            "Epoch 44, Loss(train/val) 1.01563/0.42501. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 1.02530/0.42507. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 1.02136/0.42510. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 1.01909/0.42511. Took 0.05 sec\n",
            "Epoch 48, Loss(train/val) 1.02984/0.42509. Took 0.05 sec\n",
            "Epoch 49, Loss(train/val) 1.02114/0.42505. Took 0.05 sec\n",
            "Epoch 50, Loss(train/val) 1.02222/0.42500. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 1.02124/0.42497. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 1.02092/0.42495. Took 0.05 sec\n",
            "Epoch 53, Loss(train/val) 1.01479/0.42493. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 1.02270/0.42491. Took 0.06 sec\n",
            "Epoch 55, Loss(train/val) 1.02054/0.42488. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 1.02029/0.42488. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 1.02701/0.42487. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 1.01597/0.42485. Took 0.05 sec\n",
            "Epoch 59, Loss(train/val) 1.01962/0.42484. Took 0.06 sec\n",
            "Epoch 60, Loss(train/val) 1.02588/0.42483. Took 0.05 sec\n",
            "Epoch 61, Loss(train/val) 1.02017/0.42483. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 1.01451/0.42482. Took 0.04 sec\n",
            "Epoch 63, Loss(train/val) 1.02458/0.42481. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 1.01903/0.42480. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 1.01835/0.42478. Took 0.05 sec\n",
            "Epoch 66, Loss(train/val) 1.02516/0.42475. Took 0.05 sec\n",
            "Epoch 67, Loss(train/val) 1.02151/0.42470. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 1.01296/0.42468. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 1.01914/0.42467. Took 0.05 sec\n",
            "Epoch 70, Loss(train/val) 1.02161/0.42466. Took 0.05 sec\n",
            "Epoch 71, Loss(train/val) 1.03137/0.42465. Took 0.04 sec\n",
            "Epoch 72, Loss(train/val) 1.02620/0.42463. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 1.02282/0.42460. Took 0.05 sec\n",
            "Epoch 74, Loss(train/val) 1.02590/0.42456. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 1.02274/0.42451. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 1.02191/0.42447. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 1.01423/0.42441. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 1.02258/0.42437. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 1.02307/0.42431. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 1.01757/0.42424. Took 0.07 sec\n",
            "Epoch 81, Loss(train/val) 1.00937/0.42415. Took 0.05 sec\n",
            "Epoch 82, Loss(train/val) 1.02202/0.42406. Took 0.05 sec\n",
            "Epoch 83, Loss(train/val) 1.01935/0.42396. Took 0.05 sec\n",
            "Epoch 84, Loss(train/val) 1.02724/0.42388. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 1.02510/0.42378. Took 0.05 sec\n",
            "Epoch 86, Loss(train/val) 1.01670/0.42368. Took 0.05 sec\n",
            "Epoch 87, Loss(train/val) 1.00841/0.42355. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 1.01524/0.42343. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 1.02194/0.42331. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 1.02327/0.42316. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 1.02487/0.42300. Took 0.05 sec\n",
            "Epoch 92, Loss(train/val) 1.02288/0.42281. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 1.01778/0.42259. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 1.02099/0.42237. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 1.02514/0.42211. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 1.02150/0.42183. Took 0.04 sec\n",
            "Epoch 97, Loss(train/val) 1.01431/0.42156. Took 0.05 sec\n",
            "Epoch 98, Loss(train/val) 1.02358/0.42128. Took 0.05 sec\n",
            "Epoch 99, Loss(train/val) 1.00946/0.42100. Took 0.05 sec\n",
            "Epoch 100, Loss(train/val) 1.01802/0.42073. Took 0.05 sec\n",
            "Epoch 101, Loss(train/val) 1.02444/0.42046. Took 0.04 sec\n",
            "Epoch 102, Loss(train/val) 1.02145/0.42021. Took 0.05 sec\n",
            "Epoch 103, Loss(train/val) 1.01900/0.41999. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 1.01557/0.41979. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 1.02468/0.41961. Took 0.05 sec\n",
            "Epoch 106, Loss(train/val) 1.02257/0.41945. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 1.01455/0.41933. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 1.01474/0.41923. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 1.01777/0.41917. Took 0.04 sec\n",
            "Epoch 110, Loss(train/val) 1.02140/0.41910. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 1.01961/0.41905. Took 0.05 sec\n",
            "Epoch 112, Loss(train/val) 1.02501/0.41901. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 1.01807/0.41900. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 1.01313/0.41897. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 1.01904/0.41896. Took 0.05 sec\n",
            "Epoch 116, Loss(train/val) 1.01366/0.41897. Took 0.04 sec\n",
            "Epoch 117, Loss(train/val) 1.01593/0.41897. Took 0.05 sec\n",
            "Epoch 118, Loss(train/val) 1.00866/0.41897. Took 0.05 sec\n",
            "Epoch 119, Loss(train/val) 1.02416/0.41898. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 1.02443/0.41900. Took 0.04 sec\n",
            "Epoch 121, Loss(train/val) 1.01898/0.41901. Took 0.05 sec\n",
            "Epoch 122, Loss(train/val) 1.02055/0.41900. Took 0.04 sec\n",
            "Epoch 123, Loss(train/val) 1.01122/0.41897. Took 0.06 sec\n",
            "Epoch 124, Loss(train/val) 1.02323/0.41893. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 1.02365/0.41891. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 1.02037/0.41884. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 1.01991/0.41879. Took 0.05 sec\n",
            "Epoch 128, Loss(train/val) 1.01500/0.41870. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 1.01848/0.41859. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 1.01149/0.41845. Took 0.05 sec\n",
            "Epoch 131, Loss(train/val) 1.01340/0.41831. Took 0.04 sec\n",
            "Epoch 132, Loss(train/val) 1.01572/0.41815. Took 0.05 sec\n",
            "Epoch 133, Loss(train/val) 1.00939/0.41796. Took 0.05 sec\n",
            "Epoch 134, Loss(train/val) 1.01441/0.41774. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 1.01465/0.41752. Took 0.05 sec\n",
            "Epoch 136, Loss(train/val) 1.01586/0.41729. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 1.00203/0.41702. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 1.01528/0.41676. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 1.00819/0.41649. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 1.01118/0.41625. Took 0.05 sec\n",
            "Epoch 141, Loss(train/val) 1.01238/0.41598. Took 0.06 sec\n",
            "Epoch 142, Loss(train/val) 1.00954/0.41574. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 1.01015/0.41547. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 1.00679/0.41520. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 1.00881/0.41497. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 0.99732/0.41469. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 1.00161/0.41431. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 0.99144/0.41391. Took 0.05 sec\n",
            "Epoch 149, Loss(train/val) 1.00709/0.41353. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 0.99501/0.41317. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 1.00883/0.41280. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 1.00176/0.41234. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 1.00427/0.41184. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 1.00365/0.41139. Took 0.05 sec\n",
            "Epoch 155, Loss(train/val) 1.00457/0.41099. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 0.99327/0.41069. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 1.00165/0.41051. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 1.00119/0.41050. Took 0.05 sec\n",
            "Epoch 159, Loss(train/val) 0.99651/0.41056. Took 0.04 sec\n",
            "Epoch 160, Loss(train/val) 0.99638/0.41064. Took 0.05 sec\n",
            "Epoch 161, Loss(train/val) 1.00276/0.41091. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 0.98393/0.41125. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 0.98297/0.41153. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 0.99606/0.41180. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 0.99922/0.41194. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.98790/0.41199. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 0.99708/0.41194. Took 0.05 sec\n",
            "Epoch 168, Loss(train/val) 0.98983/0.41181. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 0.98773/0.41151. Took 0.05 sec\n",
            "Epoch 170, Loss(train/val) 0.98931/0.41114. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 0.98391/0.41074. Took 0.05 sec\n",
            "Epoch 172, Loss(train/val) 0.98614/0.41040. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 0.98678/0.41017. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 0.97683/0.41002. Took 0.05 sec\n",
            "Epoch 175, Loss(train/val) 0.96801/0.40986. Took 0.05 sec\n",
            "Epoch 176, Loss(train/val) 0.97748/0.40979. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 0.96946/0.40969. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 0.98029/0.40970. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 0.97731/0.40973. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 0.97296/0.40978. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 0.97457/0.40983. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 0.97375/0.40988. Took 0.05 sec\n",
            "Epoch 183, Loss(train/val) 0.96857/0.40998. Took 0.04 sec\n",
            "Epoch 184, Loss(train/val) 0.96072/0.41006. Took 0.05 sec\n",
            "Epoch 185, Loss(train/val) 0.96969/0.40999. Took 0.05 sec\n",
            "Epoch 186, Loss(train/val) 0.96764/0.41005. Took 0.06 sec\n",
            "Epoch 187, Loss(train/val) 0.96028/0.41015. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 0.96176/0.41016. Took 0.05 sec\n",
            "Epoch 189, Loss(train/val) 0.95840/0.41013. Took 0.05 sec\n",
            "Epoch 190, Loss(train/val) 0.95489/0.41013. Took 0.05 sec\n",
            "Epoch 191, Loss(train/val) 0.95228/0.41010. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 0.95498/0.41011. Took 0.05 sec\n",
            "Epoch 193, Loss(train/val) 0.94112/0.41013. Took 0.05 sec\n",
            "Epoch 194, Loss(train/val) 0.94755/0.41016. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 0.94561/0.41033. Took 0.05 sec\n",
            "Epoch 196, Loss(train/val) 0.93967/0.41047. Took 0.05 sec\n",
            "Epoch 197, Loss(train/val) 0.92415/0.41068. Took 0.05 sec\n",
            "Epoch 198, Loss(train/val) 0.93674/0.41085. Took 0.05 sec\n",
            "Epoch 199, Loss(train/val) 0.93338/0.41105. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 0.93166/0.41125. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 0.92515/0.41175. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 0.92330/0.41237. Took 0.05 sec\n",
            "Epoch 203, Loss(train/val) 0.91888/0.41327. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 0.91341/0.41445. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.90896/0.41576. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 0.90151/0.41725. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 0.89859/0.41889. Took 0.06 sec\n",
            "Epoch 208, Loss(train/val) 0.89772/0.42074. Took 0.05 sec\n",
            "Epoch 209, Loss(train/val) 0.88650/0.42227. Took 0.05 sec\n",
            "Epoch 210, Loss(train/val) 0.88588/0.42340. Took 0.05 sec\n",
            "Epoch 211, Loss(train/val) 0.87421/0.42476. Took 0.05 sec\n",
            "Epoch 212, Loss(train/val) 0.87116/0.42560. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 0.86745/0.42615. Took 0.05 sec\n",
            "Epoch 214, Loss(train/val) 0.86910/0.42658. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.86274/0.42622. Took 0.05 sec\n",
            "Epoch 216, Loss(train/val) 0.85689/0.42569. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 0.85838/0.42529. Took 0.06 sec\n",
            "Epoch 218, Loss(train/val) 0.85498/0.42402. Took 0.05 sec\n",
            "Epoch 219, Loss(train/val) 0.84577/0.42284. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 0.83357/0.42141. Took 0.05 sec\n",
            "Epoch 221, Loss(train/val) 0.83866/0.42018. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 0.83182/0.41829. Took 0.05 sec\n",
            "Epoch 223, Loss(train/val) 0.82333/0.41671. Took 0.05 sec\n",
            "Epoch 224, Loss(train/val) 0.82316/0.41487. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 0.81642/0.41252. Took 0.05 sec\n",
            "Epoch 226, Loss(train/val) 0.80759/0.41069. Took 0.05 sec\n",
            "Epoch 227, Loss(train/val) 0.81005/0.40855. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 0.80667/0.40672. Took 0.05 sec\n",
            "Epoch 229, Loss(train/val) 0.80472/0.40501. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 0.79071/0.40322. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 0.77851/0.40114. Took 0.05 sec\n",
            "Epoch 232, Loss(train/val) 0.78174/0.39986. Took 0.05 sec\n",
            "Epoch 233, Loss(train/val) 0.78787/0.39821. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 0.78432/0.39645. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 0.76990/0.39489. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 0.76784/0.39377. Took 0.05 sec\n",
            "Epoch 237, Loss(train/val) 0.77012/0.39200. Took 0.05 sec\n",
            "Epoch 238, Loss(train/val) 0.76698/0.39054. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 0.75898/0.38916. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 0.74536/0.38746. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 0.75611/0.38627. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 0.74814/0.38503. Took 0.05 sec\n",
            "Epoch 243, Loss(train/val) 0.74532/0.38385. Took 0.05 sec\n",
            "Epoch 244, Loss(train/val) 0.73252/0.38268. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.74360/0.38148. Took 0.05 sec\n",
            "Epoch 246, Loss(train/val) 0.74209/0.38045. Took 0.05 sec\n",
            "Epoch 247, Loss(train/val) 0.73945/0.37928. Took 0.05 sec\n",
            "Epoch 248, Loss(train/val) 0.73776/0.37834. Took 0.05 sec\n",
            "Epoch 249, Loss(train/val) 0.72656/0.37732. Took 0.06 sec\n",
            "Epoch 250, Loss(train/val) 0.72817/0.37605. Took 0.05 sec\n",
            "Epoch 251, Loss(train/val) 0.73103/0.37526. Took 0.05 sec\n",
            "Epoch 252, Loss(train/val) 0.72143/0.37418. Took 0.05 sec\n",
            "Epoch 253, Loss(train/val) 0.72262/0.37286. Took 0.05 sec\n",
            "Epoch 254, Loss(train/val) 0.72555/0.37205. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.72030/0.37073. Took 0.05 sec\n",
            "Epoch 256, Loss(train/val) 0.71184/0.37001. Took 0.05 sec\n",
            "Epoch 257, Loss(train/val) 0.71337/0.36889. Took 0.05 sec\n",
            "Epoch 258, Loss(train/val) 0.70770/0.36807. Took 0.05 sec\n",
            "Epoch 259, Loss(train/val) 0.70305/0.36728. Took 0.05 sec\n",
            "Epoch 260, Loss(train/val) 0.69973/0.36625. Took 0.05 sec\n",
            "Epoch 261, Loss(train/val) 0.69779/0.36507. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.70264/0.36401. Took 0.05 sec\n",
            "Epoch 263, Loss(train/val) 0.69851/0.36267. Took 0.05 sec\n",
            "Epoch 264, Loss(train/val) 0.70441/0.36143. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.68275/0.36055. Took 0.05 sec\n",
            "Epoch 266, Loss(train/val) 0.68813/0.35993. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 0.69110/0.35962. Took 0.05 sec\n",
            "Epoch 268, Loss(train/val) 0.69840/0.35895. Took 0.05 sec\n",
            "Epoch 269, Loss(train/val) 0.68415/0.35795. Took 0.05 sec\n",
            "Epoch 270, Loss(train/val) 0.68755/0.35722. Took 0.05 sec\n",
            "Epoch 271, Loss(train/val) 0.68313/0.35592. Took 0.05 sec\n",
            "Epoch 272, Loss(train/val) 0.68962/0.35497. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 0.68367/0.35363. Took 0.05 sec\n",
            "Epoch 274, Loss(train/val) 0.68701/0.35272. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.68364/0.35247. Took 0.05 sec\n",
            "Epoch 276, Loss(train/val) 0.68255/0.35192. Took 0.05 sec\n",
            "Epoch 277, Loss(train/val) 0.67298/0.35163. Took 0.05 sec\n",
            "Epoch 278, Loss(train/val) 0.67859/0.35155. Took 0.05 sec\n",
            "Epoch 279, Loss(train/val) 0.67718/0.35037. Took 0.05 sec\n",
            "Epoch 280, Loss(train/val) 0.66542/0.34954. Took 0.05 sec\n",
            "Epoch 281, Loss(train/val) 0.67905/0.34864. Took 0.05 sec\n",
            "Epoch 282, Loss(train/val) 0.67398/0.34780. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 0.65851/0.34696. Took 0.05 sec\n",
            "Epoch 284, Loss(train/val) 0.67161/0.34626. Took 0.05 sec\n",
            "Epoch 285, Loss(train/val) 0.66490/0.34497. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.66894/0.34364. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.66319/0.34307. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.66955/0.34199. Took 0.05 sec\n",
            "Epoch 289, Loss(train/val) 0.65698/0.34087. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.66871/0.33986. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.66116/0.33863. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.65943/0.33754. Took 0.06 sec\n",
            "Epoch 293, Loss(train/val) 0.66364/0.33640. Took 0.05 sec\n",
            "Epoch 294, Loss(train/val) 0.66250/0.33613. Took 0.05 sec\n",
            "Epoch 295, Loss(train/val) 0.65898/0.33581. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 0.66012/0.33446. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.65083/0.33371. Took 0.05 sec\n",
            "Epoch 298, Loss(train/val) 0.64443/0.33266. Took 0.05 sec\n",
            "Epoch 299, Loss(train/val) 0.61649/0.33187. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.64047/0.33159. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.64204/0.33145. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.65170/0.33053. Took 0.05 sec\n",
            "Epoch 303, Loss(train/val) 0.64970/0.33019. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 0.63637/0.32945. Took 0.05 sec\n",
            "Epoch 305, Loss(train/val) 0.64871/0.32895. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 0.64652/0.32827. Took 0.05 sec\n",
            "Epoch 307, Loss(train/val) 0.64415/0.32786. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.63466/0.32656. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.64695/0.32597. Took 0.04 sec\n",
            "Epoch 310, Loss(train/val) 0.63935/0.32473. Took 0.05 sec\n",
            "Epoch 311, Loss(train/val) 0.63635/0.32383. Took 0.05 sec\n",
            "Epoch 312, Loss(train/val) 0.63628/0.32291. Took 0.06 sec\n",
            "Epoch 313, Loss(train/val) 0.62814/0.32167. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.64259/0.32138. Took 0.05 sec\n",
            "Epoch 315, Loss(train/val) 0.63345/0.32034. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.63179/0.31984. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.63217/0.31912. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.63627/0.31822. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.63240/0.31740. Took 0.05 sec\n",
            "Epoch 320, Loss(train/val) 0.63002/0.31667. Took 0.05 sec\n",
            "Epoch 321, Loss(train/val) 0.62910/0.31566. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.63094/0.31556. Took 0.05 sec\n",
            "Epoch 323, Loss(train/val) 0.62502/0.31476. Took 0.05 sec\n",
            "Epoch 324, Loss(train/val) 0.62896/0.31468. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.61934/0.31451. Took 0.05 sec\n",
            "Epoch 326, Loss(train/val) 0.61096/0.31324. Took 0.05 sec\n",
            "Epoch 327, Loss(train/val) 0.62645/0.31258. Took 0.05 sec\n",
            "Epoch 328, Loss(train/val) 0.61352/0.31221. Took 0.05 sec\n",
            "Epoch 329, Loss(train/val) 0.62372/0.31236. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.61620/0.31203. Took 0.04 sec\n",
            "Epoch 331, Loss(train/val) 0.62433/0.31046. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 0.61977/0.30961. Took 0.05 sec\n",
            "Epoch 333, Loss(train/val) 0.61010/0.30951. Took 0.05 sec\n",
            "Epoch 334, Loss(train/val) 0.60930/0.30903. Took 0.05 sec\n",
            "Epoch 335, Loss(train/val) 0.59932/0.30753. Took 0.05 sec\n",
            "Epoch 336, Loss(train/val) 0.61175/0.30672. Took 0.05 sec\n",
            "Epoch 337, Loss(train/val) 0.61276/0.30597. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.59934/0.30509. Took 0.05 sec\n",
            "Epoch 339, Loss(train/val) 0.60019/0.30472. Took 0.04 sec\n",
            "Epoch 340, Loss(train/val) 0.60576/0.30453. Took 0.05 sec\n",
            "Epoch 341, Loss(train/val) 0.60645/0.30421. Took 0.05 sec\n",
            "Epoch 342, Loss(train/val) 0.60146/0.30438. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.60952/0.30406. Took 0.05 sec\n",
            "Epoch 344, Loss(train/val) 0.60162/0.30360. Took 0.05 sec\n",
            "Epoch 345, Loss(train/val) 0.60244/0.30298. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.60675/0.30214. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 0.57805/0.30165. Took 0.05 sec\n",
            "Epoch 348, Loss(train/val) 0.60574/0.30155. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 0.59637/0.30186. Took 0.05 sec\n",
            "Epoch 350, Loss(train/val) 0.59380/0.30169. Took 0.05 sec\n",
            "Epoch 351, Loss(train/val) 0.58946/0.30072. Took 0.05 sec\n",
            "Epoch 352, Loss(train/val) 0.59518/0.30017. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 0.59179/0.29916. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 0.58889/0.29921. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 0.57459/0.29801. Took 0.07 sec\n",
            "Epoch 356, Loss(train/val) 0.58608/0.29720. Took 0.05 sec\n",
            "Epoch 357, Loss(train/val) 0.58810/0.29672. Took 0.05 sec\n",
            "Epoch 358, Loss(train/val) 0.59385/0.29634. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 0.57859/0.29597. Took 0.05 sec\n",
            "Epoch 360, Loss(train/val) 0.58316/0.29575. Took 0.06 sec\n",
            "Epoch 361, Loss(train/val) 0.59101/0.29576. Took 0.05 sec\n",
            "Epoch 362, Loss(train/val) 0.58468/0.29556. Took 0.05 sec\n",
            "Epoch 363, Loss(train/val) 0.57837/0.29665. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 0.58238/0.29627. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.58266/0.29646. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.58501/0.29636. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 0.58221/0.29566. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.58343/0.29463. Took 0.05 sec\n",
            "Epoch 369, Loss(train/val) 0.58082/0.29513. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.57248/0.29434. Took 0.05 sec\n",
            "Epoch 371, Loss(train/val) 0.58112/0.29498. Took 0.05 sec\n",
            "Epoch 372, Loss(train/val) 0.58110/0.29504. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 0.57612/0.29532. Took 0.05 sec\n",
            "Epoch 374, Loss(train/val) 0.57742/0.29439. Took 0.05 sec\n",
            "Epoch 375, Loss(train/val) 0.57171/0.29446. Took 0.06 sec\n",
            "Epoch 376, Loss(train/val) 0.56764/0.29305. Took 0.05 sec\n",
            "Epoch 377, Loss(train/val) 0.56683/0.29325. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.57695/0.29355. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 0.56240/0.29194. Took 0.05 sec\n",
            "Epoch 380, Loss(train/val) 0.57535/0.29092. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 0.56532/0.29008. Took 0.05 sec\n",
            "Epoch 382, Loss(train/val) 0.57061/0.29018. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.57227/0.29074. Took 0.05 sec\n",
            "Epoch 384, Loss(train/val) 0.56506/0.29071. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 0.56956/0.29059. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.57160/0.29003. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.56225/0.28964. Took 0.04 sec\n",
            "Epoch 388, Loss(train/val) 0.54442/0.28825. Took 0.05 sec\n",
            "Epoch 389, Loss(train/val) 0.56576/0.28913. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.55804/0.28827. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.56176/0.28761. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.55950/0.28824. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.54950/0.28764. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 0.55192/0.28768. Took 0.05 sec\n",
            "Epoch 395, Loss(train/val) 0.55969/0.28737. Took 0.05 sec\n",
            "Epoch 396, Loss(train/val) 0.56062/0.28655. Took 0.06 sec\n",
            "Epoch 397, Loss(train/val) 0.55807/0.28771. Took 0.05 sec\n",
            "Epoch 398, Loss(train/val) 0.56079/0.28752. Took 0.05 sec\n",
            "Epoch 399, Loss(train/val) 0.55834/0.28795. Took 0.05 sec\n",
            "Epoch 400, Loss(train/val) 0.55275/0.28705. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 0.53734/0.28680. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.53871/0.28582. Took 0.05 sec\n",
            "Epoch 403, Loss(train/val) 0.55490/0.28527. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 0.54191/0.28499. Took 0.05 sec\n",
            "Epoch 405, Loss(train/val) 0.54691/0.28465. Took 0.05 sec\n",
            "Epoch 406, Loss(train/val) 0.55081/0.28415. Took 0.05 sec\n",
            "Epoch 407, Loss(train/val) 0.54829/0.28425. Took 0.05 sec\n",
            "Epoch 408, Loss(train/val) 0.53904/0.28332. Took 0.05 sec\n",
            "Epoch 409, Loss(train/val) 0.54968/0.28298. Took 0.05 sec\n",
            "Epoch 410, Loss(train/val) 0.54882/0.28293. Took 0.05 sec\n",
            "Epoch 411, Loss(train/val) 0.54647/0.28350. Took 0.05 sec\n",
            "Epoch 412, Loss(train/val) 0.54597/0.28401. Took 0.05 sec\n",
            "Epoch 413, Loss(train/val) 0.54086/0.28427. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.53939/0.28464. Took 0.05 sec\n",
            "Epoch 415, Loss(train/val) 0.54830/0.28465. Took 0.05 sec\n",
            "Epoch 416, Loss(train/val) 0.53582/0.28479. Took 0.05 sec\n",
            "Epoch 417, Loss(train/val) 0.53670/0.28515. Took 0.05 sec\n",
            "Epoch 418, Loss(train/val) 0.54152/0.28538. Took 0.05 sec\n",
            "Epoch 419, Loss(train/val) 0.54437/0.28605. Took 0.05 sec\n",
            "Epoch 420, Loss(train/val) 0.54348/0.28641. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.53163/0.28545. Took 0.05 sec\n",
            "Epoch 422, Loss(train/val) 0.54738/0.28447. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.53512/0.28440. Took 0.05 sec\n",
            "Epoch 424, Loss(train/val) 0.53241/0.28561. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.53691/0.28552. Took 0.05 sec\n",
            "Epoch 426, Loss(train/val) 0.54040/0.28688. Took 0.05 sec\n",
            "Epoch 427, Loss(train/val) 0.53383/0.28742. Took 0.05 sec\n",
            "Epoch 428, Loss(train/val) 0.53168/0.28652. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.53080/0.28620. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.53825/0.28407. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.53656/0.28301. Took 0.05 sec\n",
            "Epoch 432, Loss(train/val) 0.53074/0.28310. Took 0.05 sec\n",
            "Epoch 433, Loss(train/val) 0.52168/0.28256. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.53418/0.28105. Took 0.05 sec\n",
            "Epoch 435, Loss(train/val) 0.51420/0.27832. Took 0.05 sec\n",
            "Epoch 436, Loss(train/val) 0.52708/0.27781. Took 0.05 sec\n",
            "Epoch 437, Loss(train/val) 0.53169/0.27696. Took 0.05 sec\n",
            "Epoch 438, Loss(train/val) 0.53375/0.27689. Took 0.05 sec\n",
            "Epoch 439, Loss(train/val) 0.52615/0.27763. Took 0.05 sec\n",
            "Epoch 440, Loss(train/val) 0.51844/0.27718. Took 0.05 sec\n",
            "Epoch 441, Loss(train/val) 0.50471/0.27654. Took 0.05 sec\n",
            "Epoch 442, Loss(train/val) 0.52515/0.27688. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 0.51986/0.27846. Took 0.05 sec\n",
            "Epoch 444, Loss(train/val) 0.52301/0.28085. Took 0.05 sec\n",
            "Epoch 445, Loss(train/val) 0.52537/0.28206. Took 0.05 sec\n",
            "Epoch 446, Loss(train/val) 0.50606/0.28026. Took 0.05 sec\n",
            "Epoch 447, Loss(train/val) 0.52847/0.28087. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 0.51104/0.27998. Took 0.05 sec\n",
            "Epoch 449, Loss(train/val) 0.52921/0.27957. Took 0.05 sec\n",
            "Epoch 450, Loss(train/val) 0.51331/0.27900. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 0.52360/0.27736. Took 0.04 sec\n",
            "Epoch 452, Loss(train/val) 0.51857/0.27829. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.51884/0.27811. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.52504/0.27725. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 0.51986/0.27689. Took 0.04 sec\n",
            "Epoch 456, Loss(train/val) 0.51031/0.27710. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.51511/0.27546. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.49721/0.27418. Took 0.06 sec\n",
            "Epoch 459, Loss(train/val) 0.50951/0.27403. Took 0.05 sec\n",
            "Epoch 460, Loss(train/val) 0.51456/0.27313. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.51329/0.27151. Took 0.05 sec\n",
            "Epoch 462, Loss(train/val) 0.50361/0.27102. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.52550/0.27142. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.51196/0.27242. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.51488/0.27246. Took 0.05 sec\n",
            "Epoch 466, Loss(train/val) 0.51426/0.27475. Took 0.05 sec\n",
            "Epoch 467, Loss(train/val) 0.51637/0.27535. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.50704/0.27447. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 0.51179/0.27618. Took 0.05 sec\n",
            "Epoch 470, Loss(train/val) 0.50341/0.27674. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 0.51512/0.27739. Took 0.05 sec\n",
            "Epoch 472, Loss(train/val) 0.51263/0.27798. Took 0.05 sec\n",
            "Epoch 473, Loss(train/val) 0.51697/0.28034. Took 0.05 sec\n",
            "Epoch 474, Loss(train/val) 0.51635/0.28123. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 0.51533/0.27986. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.48770/0.27583. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.51451/0.27612. Took 0.05 sec\n",
            "Epoch 478, Loss(train/val) 0.50543/0.27472. Took 0.05 sec\n",
            "Epoch 479, Loss(train/val) 0.50387/0.27408. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.50666/0.27530. Took 0.05 sec\n",
            "Epoch 481, Loss(train/val) 0.50319/0.27589. Took 0.05 sec\n",
            "Epoch 482, Loss(train/val) 0.49742/0.27498. Took 0.05 sec\n",
            "Epoch 483, Loss(train/val) 0.51012/0.27203. Took 0.05 sec\n",
            "Epoch 484, Loss(train/val) 0.50808/0.27271. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.49973/0.27368. Took 0.05 sec\n",
            "Epoch 486, Loss(train/val) 0.49339/0.27215. Took 0.04 sec\n",
            "Epoch 487, Loss(train/val) 0.50654/0.27282. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.50593/0.27190. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.49233/0.27017. Took 0.05 sec\n",
            "Epoch 490, Loss(train/val) 0.48627/0.26857. Took 0.05 sec\n",
            "Epoch 491, Loss(train/val) 0.50787/0.26652. Took 0.05 sec\n",
            "Epoch 492, Loss(train/val) 0.50580/0.26610. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.49674/0.26526. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.50124/0.26566. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.49470/0.26811. Took 0.04 sec\n",
            "Epoch 496, Loss(train/val) 0.49706/0.27031. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 0.50221/0.26945. Took 0.05 sec\n",
            "Epoch 498, Loss(train/val) 0.48659/0.26814. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.49898/0.26820. Took 0.05 sec\n",
            "Epoch 500, Loss(train/val) 0.49766/0.26787. Took 0.05 sec\n",
            "Epoch 501, Loss(train/val) 0.49895/0.26780. Took 0.05 sec\n",
            "Epoch 502, Loss(train/val) 0.50838/0.26791. Took 0.05 sec\n",
            "Epoch 503, Loss(train/val) 0.50065/0.26845. Took 0.05 sec\n",
            "Epoch 504, Loss(train/val) 0.48295/0.26875. Took 0.05 sec\n",
            "Epoch 505, Loss(train/val) 0.48877/0.26777. Took 0.05 sec\n",
            "Epoch 506, Loss(train/val) 0.49457/0.26849. Took 0.04 sec\n",
            "Epoch 507, Loss(train/val) 0.49492/0.27002. Took 0.05 sec\n",
            "Epoch 508, Loss(train/val) 0.49624/0.27056. Took 0.05 sec\n",
            "Epoch 509, Loss(train/val) 0.49276/0.26988. Took 0.05 sec\n",
            "Epoch 510, Loss(train/val) 0.49170/0.27143. Took 0.05 sec\n",
            "Epoch 511, Loss(train/val) 0.49480/0.26907. Took 0.05 sec\n",
            "Epoch 512, Loss(train/val) 0.49214/0.26828. Took 0.05 sec\n",
            "Epoch 513, Loss(train/val) 0.49282/0.26622. Took 0.06 sec\n",
            "Epoch 514, Loss(train/val) 0.49190/0.26703. Took 0.05 sec\n",
            "Epoch 515, Loss(train/val) 0.48237/0.26445. Took 0.04 sec\n",
            "Epoch 516, Loss(train/val) 0.49870/0.26409. Took 0.04 sec\n",
            "Epoch 517, Loss(train/val) 0.48516/0.26390. Took 0.04 sec\n",
            "Epoch 518, Loss(train/val) 0.48766/0.26454. Took 0.05 sec\n",
            "Epoch 519, Loss(train/val) 0.49442/0.26578. Took 0.05 sec\n",
            "Epoch 520, Loss(train/val) 0.48038/0.26643. Took 0.05 sec\n",
            "Epoch 521, Loss(train/val) 0.48966/0.26800. Took 0.05 sec\n",
            "Epoch 522, Loss(train/val) 0.48534/0.26806. Took 0.06 sec\n",
            "Epoch 523, Loss(train/val) 0.48484/0.26730. Took 0.05 sec\n",
            "Epoch 524, Loss(train/val) 0.48494/0.26672. Took 0.05 sec\n",
            "Epoch 525, Loss(train/val) 0.47518/0.26786. Took 0.04 sec\n",
            "Epoch 526, Loss(train/val) 0.48488/0.26523. Took 0.05 sec\n",
            "Epoch 527, Loss(train/val) 0.47930/0.26390. Took 0.05 sec\n",
            "Epoch 528, Loss(train/val) 0.48793/0.26398. Took 0.04 sec\n",
            "Epoch 529, Loss(train/val) 0.48312/0.26482. Took 0.05 sec\n",
            "Epoch 530, Loss(train/val) 0.47918/0.26361. Took 0.05 sec\n",
            "Epoch 531, Loss(train/val) 0.48214/0.26504. Took 0.04 sec\n",
            "Epoch 532, Loss(train/val) 0.47445/0.26475. Took 0.05 sec\n",
            "Epoch 533, Loss(train/val) 0.47966/0.26551. Took 0.05 sec\n",
            "Epoch 534, Loss(train/val) 0.48583/0.26430. Took 0.05 sec\n",
            "Epoch 535, Loss(train/val) 0.47668/0.26274. Took 0.05 sec\n",
            "Epoch 536, Loss(train/val) 0.47903/0.26154. Took 0.05 sec\n",
            "Epoch 537, Loss(train/val) 0.47669/0.26016. Took 0.05 sec\n",
            "Epoch 538, Loss(train/val) 0.47327/0.25825. Took 0.05 sec\n",
            "Epoch 539, Loss(train/val) 0.47614/0.25850. Took 0.05 sec\n",
            "Epoch 540, Loss(train/val) 0.48077/0.25979. Took 0.05 sec\n",
            "Epoch 541, Loss(train/val) 0.47905/0.26145. Took 0.05 sec\n",
            "Epoch 542, Loss(train/val) 0.48120/0.26223. Took 0.05 sec\n",
            "Epoch 543, Loss(train/val) 0.48285/0.26435. Took 0.05 sec\n",
            "Epoch 544, Loss(train/val) 0.47625/0.26460. Took 0.05 sec\n",
            "Epoch 545, Loss(train/val) 0.47395/0.26535. Took 0.05 sec\n",
            "Epoch 546, Loss(train/val) 0.47923/0.26464. Took 0.05 sec\n",
            "Epoch 547, Loss(train/val) 0.45695/0.26314. Took 0.05 sec\n",
            "Epoch 548, Loss(train/val) 0.47202/0.26105. Took 0.05 sec\n",
            "Epoch 549, Loss(train/val) 0.47729/0.26120. Took 0.05 sec\n",
            "Epoch 550, Loss(train/val) 0.47410/0.26197. Took 0.05 sec\n",
            "Epoch 551, Loss(train/val) 0.47860/0.26189. Took 0.05 sec\n",
            "Epoch 552, Loss(train/val) 0.47848/0.26310. Took 0.05 sec\n",
            "Epoch 553, Loss(train/val) 0.47631/0.26229. Took 0.05 sec\n",
            "Epoch 554, Loss(train/val) 0.47369/0.26051. Took 0.05 sec\n",
            "Epoch 555, Loss(train/val) 0.45895/0.26094. Took 0.05 sec\n",
            "Epoch 556, Loss(train/val) 0.47705/0.26079. Took 0.05 sec\n",
            "Epoch 557, Loss(train/val) 0.47293/0.26121. Took 0.05 sec\n",
            "Epoch 558, Loss(train/val) 0.46777/0.26168. Took 0.05 sec\n",
            "Epoch 559, Loss(train/val) 0.46191/0.26218. Took 0.05 sec\n",
            "Epoch 560, Loss(train/val) 0.46803/0.26178. Took 0.05 sec\n",
            "Epoch 561, Loss(train/val) 0.47578/0.26080. Took 0.05 sec\n",
            "Epoch 562, Loss(train/val) 0.46133/0.26126. Took 0.05 sec\n",
            "Epoch 563, Loss(train/val) 0.47015/0.26088. Took 0.05 sec\n",
            "Epoch 564, Loss(train/val) 0.46965/0.26095. Took 0.06 sec\n",
            "Epoch 565, Loss(train/val) 0.46954/0.25994. Took 0.06 sec\n",
            "Epoch 566, Loss(train/val) 0.46366/0.26104. Took 0.06 sec\n",
            "Epoch 567, Loss(train/val) 0.47022/0.26032. Took 0.05 sec\n",
            "Epoch 568, Loss(train/val) 0.46380/0.26005. Took 0.05 sec\n",
            "Epoch 569, Loss(train/val) 0.46766/0.26138. Took 0.05 sec\n",
            "Epoch 570, Loss(train/val) 0.47290/0.26066. Took 0.05 sec\n",
            "Epoch 571, Loss(train/val) 0.47336/0.26106. Took 0.05 sec\n",
            "Epoch 572, Loss(train/val) 0.46402/0.26085. Took 0.05 sec\n",
            "Epoch 573, Loss(train/val) 0.47323/0.26145. Took 0.04 sec\n",
            "Epoch 574, Loss(train/val) 0.45414/0.25903. Took 0.06 sec\n",
            "Epoch 575, Loss(train/val) 0.46456/0.25888. Took 0.05 sec\n",
            "Epoch 576, Loss(train/val) 0.46594/0.25980. Took 0.05 sec\n",
            "Epoch 577, Loss(train/val) 0.46537/0.25952. Took 0.05 sec\n",
            "Epoch 578, Loss(train/val) 0.46887/0.26021. Took 0.05 sec\n",
            "Epoch 579, Loss(train/val) 0.47115/0.25818. Took 0.05 sec\n",
            "Epoch 580, Loss(train/val) 0.46142/0.25838. Took 0.05 sec\n",
            "Epoch 581, Loss(train/val) 0.45663/0.25814. Took 0.05 sec\n",
            "Epoch 582, Loss(train/val) 0.45328/0.25596. Took 0.05 sec\n",
            "Epoch 583, Loss(train/val) 0.46460/0.25262. Took 0.05 sec\n",
            "Epoch 584, Loss(train/val) 0.45695/0.25014. Took 0.06 sec\n",
            "Epoch 585, Loss(train/val) 0.45962/0.24965. Took 0.05 sec\n",
            "Epoch 586, Loss(train/val) 0.46257/0.24928. Took 0.05 sec\n",
            "Epoch 587, Loss(train/val) 0.45946/0.25070. Took 0.05 sec\n",
            "Epoch 588, Loss(train/val) 0.45515/0.25283. Took 0.05 sec\n",
            "Epoch 589, Loss(train/val) 0.46334/0.25492. Took 0.05 sec\n",
            "Epoch 590, Loss(train/val) 0.45595/0.25584. Took 0.05 sec\n",
            "Epoch 591, Loss(train/val) 0.46440/0.25909. Took 0.05 sec\n",
            "Epoch 592, Loss(train/val) 0.46300/0.26112. Took 0.05 sec\n",
            "Epoch 593, Loss(train/val) 0.45920/0.26053. Took 0.05 sec\n",
            "Epoch 594, Loss(train/val) 0.44870/0.25905. Took 0.05 sec\n",
            "Epoch 595, Loss(train/val) 0.46601/0.25834. Took 0.05 sec\n",
            "Epoch 596, Loss(train/val) 0.45103/0.25803. Took 0.05 sec\n",
            "Epoch 597, Loss(train/val) 0.46059/0.25781. Took 0.05 sec\n",
            "Epoch 598, Loss(train/val) 0.45821/0.25724. Took 0.04 sec\n",
            "Epoch 599, Loss(train/val) 0.45479/0.25552. Took 0.05 sec\n",
            "Epoch 600, Loss(train/val) 0.44946/0.25541. Took 0.05 sec\n",
            "Epoch 601, Loss(train/val) 0.45328/0.25523. Took 0.05 sec\n",
            "Epoch 602, Loss(train/val) 0.45329/0.25503. Took 0.05 sec\n",
            "Epoch 603, Loss(train/val) 0.45396/0.25401. Took 0.05 sec\n",
            "Epoch 604, Loss(train/val) 0.45450/0.25367. Took 0.05 sec\n",
            "Epoch 605, Loss(train/val) 0.47136/0.25435. Took 0.06 sec\n",
            "Epoch 606, Loss(train/val) 0.43953/0.25551. Took 0.05 sec\n",
            "Epoch 607, Loss(train/val) 0.45965/0.25768. Took 0.05 sec\n",
            "Epoch 608, Loss(train/val) 0.44735/0.25736. Took 0.04 sec\n",
            "Epoch 609, Loss(train/val) 0.44667/0.25979. Took 0.05 sec\n",
            "Epoch 610, Loss(train/val) 0.44932/0.26139. Took 0.06 sec\n",
            "Epoch 611, Loss(train/val) 0.45384/0.25929. Took 0.05 sec\n",
            "Epoch 612, Loss(train/val) 0.43355/0.25784. Took 0.05 sec\n",
            "Epoch 613, Loss(train/val) 0.45676/0.25388. Took 0.05 sec\n",
            "Epoch 614, Loss(train/val) 0.45060/0.25577. Took 0.05 sec\n",
            "Epoch 615, Loss(train/val) 0.44570/0.25512. Took 0.05 sec\n",
            "Epoch 616, Loss(train/val) 0.45544/0.25592. Took 0.05 sec\n",
            "Epoch 617, Loss(train/val) 0.44598/0.25483. Took 0.05 sec\n",
            "Epoch 618, Loss(train/val) 0.45247/0.25400. Took 0.05 sec\n",
            "Epoch 619, Loss(train/val) 0.43193/0.25267. Took 0.05 sec\n",
            "Epoch 620, Loss(train/val) 0.44588/0.25302. Took 0.05 sec\n",
            "Epoch 621, Loss(train/val) 0.44627/0.25177. Took 0.05 sec\n",
            "Epoch 622, Loss(train/val) 0.44442/0.25134. Took 0.05 sec\n",
            "Epoch 623, Loss(train/val) 0.44396/0.25127. Took 0.05 sec\n",
            "Epoch 624, Loss(train/val) 0.44842/0.25168. Took 0.05 sec\n",
            "Epoch 625, Loss(train/val) 0.44407/0.25045. Took 0.05 sec\n",
            "Epoch 626, Loss(train/val) 0.43125/0.24983. Took 0.05 sec\n",
            "Epoch 627, Loss(train/val) 0.43880/0.24819. Took 0.05 sec\n",
            "Epoch 628, Loss(train/val) 0.44853/0.24928. Took 0.05 sec\n",
            "Epoch 629, Loss(train/val) 0.44605/0.24955. Took 0.05 sec\n",
            "Epoch 630, Loss(train/val) 0.43369/0.24911. Took 0.05 sec\n",
            "Epoch 631, Loss(train/val) 0.43623/0.25071. Took 0.05 sec\n",
            "Epoch 632, Loss(train/val) 0.44585/0.25202. Took 0.05 sec\n",
            "Epoch 633, Loss(train/val) 0.43711/0.25117. Took 0.05 sec\n",
            "Epoch 634, Loss(train/val) 0.43738/0.25072. Took 0.04 sec\n",
            "Epoch 635, Loss(train/val) 0.44792/0.25056. Took 0.05 sec\n",
            "Epoch 636, Loss(train/val) 0.44248/0.25013. Took 0.05 sec\n",
            "Epoch 637, Loss(train/val) 0.43042/0.25080. Took 0.05 sec\n",
            "Epoch 638, Loss(train/val) 0.45131/0.25021. Took 0.05 sec\n",
            "Epoch 639, Loss(train/val) 0.43758/0.25064. Took 0.05 sec\n",
            "Epoch 640, Loss(train/val) 0.44212/0.25331. Took 0.05 sec\n",
            "Epoch 641, Loss(train/val) 0.44062/0.25620. Took 0.05 sec\n",
            "Epoch 642, Loss(train/val) 0.43473/0.25885. Took 0.05 sec\n",
            "Epoch 643, Loss(train/val) 0.43034/0.26062. Took 0.05 sec\n",
            "Epoch 644, Loss(train/val) 0.44507/0.25964. Took 0.04 sec\n",
            "Epoch 645, Loss(train/val) 0.44151/0.25748. Took 0.05 sec\n",
            "Epoch 646, Loss(train/val) 0.43968/0.25595. Took 0.05 sec\n",
            "Epoch 647, Loss(train/val) 0.44101/0.25388. Took 0.05 sec\n",
            "Epoch 648, Loss(train/val) 0.43316/0.25188. Took 0.05 sec\n",
            "Epoch 649, Loss(train/val) 0.43715/0.25338. Took 0.05 sec\n",
            "Epoch 650, Loss(train/val) 0.43359/0.25290. Took 0.05 sec\n",
            "Epoch 651, Loss(train/val) 0.44337/0.25183. Took 0.05 sec\n",
            "Epoch 652, Loss(train/val) 0.44014/0.24890. Took 0.05 sec\n",
            "Epoch 653, Loss(train/val) 0.44882/0.24589. Took 0.05 sec\n",
            "Epoch 654, Loss(train/val) 0.43110/0.24382. Took 0.04 sec\n",
            "Epoch 655, Loss(train/val) 0.43557/0.24436. Took 0.05 sec\n",
            "Epoch 656, Loss(train/val) 0.43569/0.24478. Took 0.05 sec\n",
            "Epoch 657, Loss(train/val) 0.43170/0.24433. Took 0.05 sec\n",
            "Epoch 658, Loss(train/val) 0.43910/0.24591. Took 0.05 sec\n",
            "Epoch 659, Loss(train/val) 0.44862/0.24704. Took 0.05 sec\n",
            "Epoch 660, Loss(train/val) 0.44119/0.24957. Took 0.05 sec\n",
            "Epoch 661, Loss(train/val) 0.43492/0.25112. Took 0.05 sec\n",
            "Epoch 662, Loss(train/val) 0.43444/0.25268. Took 0.05 sec\n",
            "Epoch 663, Loss(train/val) 0.43752/0.25426. Took 0.05 sec\n",
            "Epoch 664, Loss(train/val) 0.42952/0.25406. Took 0.05 sec\n",
            "Epoch 665, Loss(train/val) 0.42367/0.25214. Took 0.06 sec\n",
            "Epoch 666, Loss(train/val) 0.42779/0.25032. Took 0.05 sec\n",
            "Epoch 667, Loss(train/val) 0.42819/0.24897. Took 0.05 sec\n",
            "Epoch 668, Loss(train/val) 0.43120/0.24664. Took 0.05 sec\n",
            "Epoch 669, Loss(train/val) 0.43644/0.24652. Took 0.05 sec\n",
            "Epoch 670, Loss(train/val) 0.42749/0.24624. Took 0.05 sec\n",
            "Epoch 671, Loss(train/val) 0.42942/0.24696. Took 0.05 sec\n",
            "Epoch 672, Loss(train/val) 0.42761/0.24935. Took 0.04 sec\n",
            "Epoch 673, Loss(train/val) 0.42454/0.24871. Took 0.05 sec\n",
            "Epoch 674, Loss(train/val) 0.42675/0.24836. Took 0.05 sec\n",
            "Epoch 675, Loss(train/val) 0.43050/0.24801. Took 0.05 sec\n",
            "Epoch 676, Loss(train/val) 0.42179/0.24716. Took 0.05 sec\n",
            "Epoch 677, Loss(train/val) 0.44288/0.24757. Took 0.05 sec\n",
            "Epoch 678, Loss(train/val) 0.42230/0.24750. Took 0.05 sec\n",
            "Epoch 679, Loss(train/val) 0.42689/0.24998. Took 0.05 sec\n",
            "Epoch 680, Loss(train/val) 0.43229/0.24982. Took 0.05 sec\n",
            "Epoch 681, Loss(train/val) 0.42865/0.24907. Took 0.05 sec\n",
            "Epoch 682, Loss(train/val) 0.42366/0.24934. Took 0.05 sec\n",
            "Epoch 683, Loss(train/val) 0.42739/0.25081. Took 0.05 sec\n",
            "Epoch 684, Loss(train/val) 0.43217/0.25243. Took 0.05 sec\n",
            "Epoch 685, Loss(train/val) 0.42523/0.25281. Took 0.05 sec\n",
            "Epoch 686, Loss(train/val) 0.41881/0.25059. Took 0.05 sec\n",
            "Epoch 687, Loss(train/val) 0.43122/0.25001. Took 0.05 sec\n",
            "Epoch 688, Loss(train/val) 0.41765/0.25039. Took 0.05 sec\n",
            "Epoch 689, Loss(train/val) 0.43520/0.24950. Took 0.07 sec\n",
            "Epoch 690, Loss(train/val) 0.42658/0.25109. Took 0.05 sec\n",
            "Epoch 691, Loss(train/val) 0.41576/0.24993. Took 0.05 sec\n",
            "Epoch 692, Loss(train/val) 0.41311/0.24920. Took 0.05 sec\n",
            "Epoch 693, Loss(train/val) 0.42553/0.24827. Took 0.06 sec\n",
            "Epoch 694, Loss(train/val) 0.42531/0.24822. Took 0.05 sec\n",
            "Epoch 695, Loss(train/val) 0.42990/0.24941. Took 0.05 sec\n",
            "Epoch 696, Loss(train/val) 0.42766/0.24664. Took 0.05 sec\n",
            "Epoch 697, Loss(train/val) 0.41954/0.24577. Took 0.06 sec\n",
            "Epoch 698, Loss(train/val) 0.42296/0.24489. Took 0.05 sec\n",
            "Epoch 699, Loss(train/val) 0.42977/0.24661. Took 0.05 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=700, exp_name='exp3_lr', hid_dim=16, input_dim=1, l2=1e-05, lr=0.0001, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.03763/0.40447. Took 0.05 sec\n",
            "Epoch 1, Loss(train/val) 1.03644/0.40231. Took 0.06 sec\n",
            "Epoch 2, Loss(train/val) 1.03575/0.40059. Took 0.05 sec\n",
            "Epoch 3, Loss(train/val) 1.04616/0.39919. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.04269/0.39805. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.04285/0.39710. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.03442/0.39631. Took 0.05 sec\n",
            "Epoch 7, Loss(train/val) 1.04259/0.39564. Took 0.05 sec\n",
            "Epoch 8, Loss(train/val) 1.04782/0.39510. Took 0.06 sec\n",
            "Epoch 9, Loss(train/val) 1.04345/0.39464. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.04623/0.39429. Took 0.04 sec\n",
            "Epoch 11, Loss(train/val) 1.04194/0.39398. Took 0.05 sec\n",
            "Epoch 12, Loss(train/val) 1.03342/0.39373. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 1.04315/0.39352. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.03907/0.39337. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.04186/0.39325. Took 0.05 sec\n",
            "Epoch 16, Loss(train/val) 1.04249/0.39317. Took 0.05 sec\n",
            "Epoch 17, Loss(train/val) 1.00456/0.39314. Took 0.05 sec\n",
            "Epoch 18, Loss(train/val) 1.04573/0.39318. Took 0.05 sec\n",
            "Epoch 19, Loss(train/val) 1.04248/0.39328. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 1.03736/0.39346. Took 0.05 sec\n",
            "Epoch 21, Loss(train/val) 1.03835/0.39374. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 1.04160/0.39416. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 1.03595/0.39462. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 1.03908/0.39513. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 1.03135/0.39597. Took 0.05 sec\n",
            "Epoch 26, Loss(train/val) 1.03792/0.39723. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.04277/0.39899. Took 0.05 sec\n",
            "Epoch 28, Loss(train/val) 1.04151/0.40142. Took 0.05 sec\n",
            "Epoch 29, Loss(train/val) 1.03360/0.40469. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 1.03904/0.40777. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 1.03773/0.40831. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 1.04142/0.40863. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 1.03948/0.40949. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 1.04071/0.41026. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 1.03590/0.41142. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 1.02934/0.41255. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.01645/0.41395. Took 0.05 sec\n",
            "Epoch 38, Loss(train/val) 1.03538/0.41550. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 1.02379/0.41733. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 1.03779/0.41912. Took 0.05 sec\n",
            "Epoch 41, Loss(train/val) 1.03219/0.42129. Took 0.05 sec\n",
            "Epoch 42, Loss(train/val) 1.02980/0.42317. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 1.03544/0.42445. Took 0.05 sec\n",
            "Epoch 44, Loss(train/val) 1.03406/0.42478. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 1.02882/0.42471. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 1.01839/0.42397. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 1.02830/0.42354. Took 0.05 sec\n",
            "Epoch 48, Loss(train/val) 1.02391/0.42330. Took 0.05 sec\n",
            "Epoch 49, Loss(train/val) 1.02574/0.42345. Took 0.06 sec\n",
            "Epoch 50, Loss(train/val) 1.00876/0.42411. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 1.01787/0.42521. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 1.00186/0.42671. Took 0.05 sec\n",
            "Epoch 53, Loss(train/val) 1.01333/0.42808. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 1.00730/0.42889. Took 0.05 sec\n",
            "Epoch 55, Loss(train/val) 1.00837/0.42972. Took 0.06 sec\n",
            "Epoch 56, Loss(train/val) 1.00564/0.42945. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 0.98980/0.42912. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 0.99164/0.42814. Took 0.05 sec\n",
            "Epoch 59, Loss(train/val) 0.98743/0.42717. Took 0.05 sec\n",
            "Epoch 60, Loss(train/val) 0.98071/0.42638. Took 0.05 sec\n",
            "Epoch 61, Loss(train/val) 0.97960/0.42635. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 0.95551/0.42590. Took 0.05 sec\n",
            "Epoch 63, Loss(train/val) 0.95442/0.42531. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 0.96055/0.42520. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 0.94961/0.42564. Took 0.05 sec\n",
            "Epoch 66, Loss(train/val) 0.93614/0.42614. Took 0.05 sec\n",
            "Epoch 67, Loss(train/val) 0.93033/0.42644. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 0.91968/0.42635. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 0.91062/0.42587. Took 0.05 sec\n",
            "Epoch 70, Loss(train/val) 0.90075/0.42486. Took 0.06 sec\n",
            "Epoch 71, Loss(train/val) 0.89555/0.42340. Took 0.05 sec\n",
            "Epoch 72, Loss(train/val) 0.87492/0.42133. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 0.87329/0.41874. Took 0.05 sec\n",
            "Epoch 74, Loss(train/val) 0.86674/0.41597. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 0.85427/0.41335. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 0.84374/0.41086. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 0.83958/0.40842. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 0.83623/0.40581. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 0.82316/0.40270. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 0.80862/0.39673. Took 0.05 sec\n",
            "Epoch 81, Loss(train/val) 0.81822/0.38255. Took 0.05 sec\n",
            "Epoch 82, Loss(train/val) 0.81426/0.36062. Took 0.04 sec\n",
            "Epoch 83, Loss(train/val) 0.79902/0.34850. Took 0.05 sec\n",
            "Epoch 84, Loss(train/val) 0.80847/0.35724. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 0.80321/0.39354. Took 0.05 sec\n",
            "Epoch 86, Loss(train/val) 0.79928/0.45094. Took 0.05 sec\n",
            "Epoch 87, Loss(train/val) 0.79471/0.52178. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 0.78602/0.59086. Took 0.06 sec\n",
            "Epoch 89, Loss(train/val) 0.78157/0.65545. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 0.78443/0.70213. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 0.77274/0.73947. Took 0.05 sec\n",
            "Epoch 92, Loss(train/val) 0.77804/0.75964. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 0.76119/0.75273. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 0.76582/0.74087. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 0.77186/0.71687. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 0.76273/0.67520. Took 0.05 sec\n",
            "Epoch 97, Loss(train/val) 0.74965/0.64311. Took 0.05 sec\n",
            "Epoch 98, Loss(train/val) 0.74644/0.59061. Took 0.05 sec\n",
            "Epoch 99, Loss(train/val) 0.73517/0.55530. Took 0.05 sec\n",
            "Epoch 100, Loss(train/val) 0.73561/0.51606. Took 0.05 sec\n",
            "Epoch 101, Loss(train/val) 0.74778/0.48907. Took 0.05 sec\n",
            "Epoch 102, Loss(train/val) 0.74193/0.47144. Took 0.05 sec\n",
            "Epoch 103, Loss(train/val) 0.73238/0.44657. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 0.73499/0.42171. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 0.73799/0.40293. Took 0.05 sec\n",
            "Epoch 106, Loss(train/val) 0.72417/0.37464. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 0.70823/0.35586. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 0.72185/0.34956. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 0.71527/0.34298. Took 0.05 sec\n",
            "Epoch 110, Loss(train/val) 0.70774/0.33665. Took 0.06 sec\n",
            "Epoch 111, Loss(train/val) 0.72052/0.32934. Took 0.05 sec\n",
            "Epoch 112, Loss(train/val) 0.70978/0.32312. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 0.71353/0.31545. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 0.70540/0.31111. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 0.70314/0.30687. Took 0.05 sec\n",
            "Epoch 116, Loss(train/val) 0.70387/0.30573. Took 0.05 sec\n",
            "Epoch 117, Loss(train/val) 0.69331/0.30496. Took 0.05 sec\n",
            "Epoch 118, Loss(train/val) 0.69575/0.30388. Took 0.05 sec\n",
            "Epoch 119, Loss(train/val) 0.69666/0.30250. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 0.69198/0.30194. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 0.66998/0.30154. Took 0.05 sec\n",
            "Epoch 122, Loss(train/val) 0.68713/0.30120. Took 0.05 sec\n",
            "Epoch 123, Loss(train/val) 0.68021/0.30111. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 0.67510/0.30116. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 0.67521/0.30179. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 0.66397/0.30216. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 0.67190/0.30300. Took 0.05 sec\n",
            "Epoch 128, Loss(train/val) 0.66749/0.30273. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 0.66338/0.30367. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 0.65436/0.30385. Took 0.06 sec\n",
            "Epoch 131, Loss(train/val) 0.64645/0.30474. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 0.66512/0.30329. Took 0.05 sec\n",
            "Epoch 133, Loss(train/val) 0.64866/0.30317. Took 0.06 sec\n",
            "Epoch 134, Loss(train/val) 0.63339/0.30267. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 0.65656/0.30093. Took 0.06 sec\n",
            "Epoch 136, Loss(train/val) 0.65051/0.30185. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 0.64673/0.30146. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 0.64065/0.30156. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 0.62504/0.30197. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 0.64137/0.30021. Took 0.05 sec\n",
            "Epoch 141, Loss(train/val) 0.64157/0.29952. Took 0.05 sec\n",
            "Epoch 142, Loss(train/val) 0.61819/0.29889. Took 0.04 sec\n",
            "Epoch 143, Loss(train/val) 0.63087/0.29774. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 0.63408/0.29849. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 0.63351/0.29842. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 0.62786/0.29716. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 0.60349/0.29581. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 0.61980/0.29521. Took 0.05 sec\n",
            "Epoch 149, Loss(train/val) 0.61302/0.29505. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 0.62507/0.29421. Took 0.04 sec\n",
            "Epoch 151, Loss(train/val) 0.61648/0.29337. Took 0.04 sec\n",
            "Epoch 152, Loss(train/val) 0.61339/0.29187. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 0.61439/0.29123. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 0.60766/0.29077. Took 0.06 sec\n",
            "Epoch 155, Loss(train/val) 0.61145/0.29013. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 0.60716/0.28963. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 0.60162/0.28890. Took 0.04 sec\n",
            "Epoch 158, Loss(train/val) 0.60478/0.28854. Took 0.05 sec\n",
            "Epoch 159, Loss(train/val) 0.60307/0.28800. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 0.59911/0.28802. Took 0.05 sec\n",
            "Epoch 161, Loss(train/val) 0.59390/0.28961. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 0.57674/0.29001. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 0.58681/0.28946. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 0.58434/0.29106. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 0.57170/0.28916. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.59230/0.28713. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 0.57894/0.28602. Took 0.05 sec\n",
            "Epoch 168, Loss(train/val) 0.58518/0.28327. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 0.56380/0.28261. Took 0.05 sec\n",
            "Epoch 170, Loss(train/val) 0.58096/0.28251. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 0.57950/0.28470. Took 0.05 sec\n",
            "Epoch 172, Loss(train/val) 0.57167/0.28497. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 0.57766/0.28584. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 0.56743/0.28415. Took 0.06 sec\n",
            "Epoch 175, Loss(train/val) 0.56773/0.28205. Took 0.06 sec\n",
            "Epoch 176, Loss(train/val) 0.57643/0.28185. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 0.56942/0.28039. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 0.57150/0.27888. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 0.56791/0.27863. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 0.56670/0.27958. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 0.55578/0.27950. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 0.56445/0.28035. Took 0.05 sec\n",
            "Epoch 183, Loss(train/val) 0.54903/0.27969. Took 0.05 sec\n",
            "Epoch 184, Loss(train/val) 0.56018/0.27854. Took 0.05 sec\n",
            "Epoch 185, Loss(train/val) 0.55140/0.27801. Took 0.05 sec\n",
            "Epoch 186, Loss(train/val) 0.54445/0.27630. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 0.54409/0.27486. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 0.55629/0.27455. Took 0.05 sec\n",
            "Epoch 189, Loss(train/val) 0.55057/0.27467. Took 0.05 sec\n",
            "Epoch 190, Loss(train/val) 0.53773/0.27553. Took 0.05 sec\n",
            "Epoch 191, Loss(train/val) 0.55432/0.27621. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 0.54480/0.27560. Took 0.05 sec\n",
            "Epoch 193, Loss(train/val) 0.53472/0.27400. Took 0.05 sec\n",
            "Epoch 194, Loss(train/val) 0.54435/0.27423. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 0.54479/0.27474. Took 0.05 sec\n",
            "Epoch 196, Loss(train/val) 0.54451/0.27603. Took 0.05 sec\n",
            "Epoch 197, Loss(train/val) 0.52785/0.27432. Took 0.05 sec\n",
            "Epoch 198, Loss(train/val) 0.53145/0.27342. Took 0.05 sec\n",
            "Epoch 199, Loss(train/val) 0.53704/0.27283. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 0.52247/0.27178. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 0.53412/0.27174. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 0.52545/0.27132. Took 0.05 sec\n",
            "Epoch 203, Loss(train/val) 0.53358/0.27110. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 0.53132/0.27147. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.53164/0.27130. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 0.52710/0.27089. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 0.52734/0.27155. Took 0.05 sec\n",
            "Epoch 208, Loss(train/val) 0.52787/0.27004. Took 0.05 sec\n",
            "Epoch 209, Loss(train/val) 0.52235/0.27049. Took 0.05 sec\n",
            "Epoch 210, Loss(train/val) 0.52612/0.27174. Took 0.05 sec\n",
            "Epoch 211, Loss(train/val) 0.52243/0.27173. Took 0.05 sec\n",
            "Epoch 212, Loss(train/val) 0.52164/0.27192. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 0.51747/0.26938. Took 0.05 sec\n",
            "Epoch 214, Loss(train/val) 0.50398/0.26830. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.50946/0.26797. Took 0.05 sec\n",
            "Epoch 216, Loss(train/val) 0.50802/0.26796. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 0.51206/0.26812. Took 0.05 sec\n",
            "Epoch 218, Loss(train/val) 0.51317/0.26822. Took 0.05 sec\n",
            "Epoch 219, Loss(train/val) 0.51821/0.26729. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 0.50857/0.26846. Took 0.05 sec\n",
            "Epoch 221, Loss(train/val) 0.51938/0.26956. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 0.51249/0.27070. Took 0.06 sec\n",
            "Epoch 223, Loss(train/val) 0.51606/0.27178. Took 0.05 sec\n",
            "Epoch 224, Loss(train/val) 0.52094/0.27858. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 0.50911/0.27811. Took 0.04 sec\n",
            "Epoch 226, Loss(train/val) 0.50185/0.27440. Took 0.05 sec\n",
            "Epoch 227, Loss(train/val) 0.50108/0.27062. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 0.50654/0.26908. Took 0.06 sec\n",
            "Epoch 229, Loss(train/val) 0.50405/0.27077. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 0.50418/0.26639. Took 0.04 sec\n",
            "Epoch 231, Loss(train/val) 0.50386/0.26433. Took 0.04 sec\n",
            "Epoch 232, Loss(train/val) 0.49475/0.26578. Took 0.05 sec\n",
            "Epoch 233, Loss(train/val) 0.50202/0.26649. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 0.49964/0.26633. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 0.49678/0.26428. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 0.49321/0.26364. Took 0.05 sec\n",
            "Epoch 237, Loss(train/val) 0.48761/0.26484. Took 0.06 sec\n",
            "Epoch 238, Loss(train/val) 0.49565/0.26449. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 0.49289/0.26672. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 0.48185/0.26323. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 0.48290/0.26318. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 0.48760/0.26398. Took 0.04 sec\n",
            "Epoch 243, Loss(train/val) 0.48226/0.26454. Took 0.05 sec\n",
            "Epoch 244, Loss(train/val) 0.49366/0.26399. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.47595/0.26327. Took 0.05 sec\n",
            "Epoch 246, Loss(train/val) 0.48906/0.26409. Took 0.04 sec\n",
            "Epoch 247, Loss(train/val) 0.49827/0.27034. Took 0.05 sec\n",
            "Epoch 248, Loss(train/val) 0.48548/0.26970. Took 0.05 sec\n",
            "Epoch 249, Loss(train/val) 0.48365/0.26540. Took 0.05 sec\n",
            "Epoch 250, Loss(train/val) 0.47364/0.26153. Took 0.05 sec\n",
            "Epoch 251, Loss(train/val) 0.46738/0.26122. Took 0.05 sec\n",
            "Epoch 252, Loss(train/val) 0.47783/0.26087. Took 0.05 sec\n",
            "Epoch 253, Loss(train/val) 0.48436/0.26064. Took 0.05 sec\n",
            "Epoch 254, Loss(train/val) 0.47568/0.26057. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.47915/0.26046. Took 0.05 sec\n",
            "Epoch 256, Loss(train/val) 0.47343/0.26060. Took 0.05 sec\n",
            "Epoch 257, Loss(train/val) 0.47610/0.26126. Took 0.05 sec\n",
            "Epoch 258, Loss(train/val) 0.47638/0.25974. Took 0.06 sec\n",
            "Epoch 259, Loss(train/val) 0.46710/0.25943. Took 0.05 sec\n",
            "Epoch 260, Loss(train/val) 0.46616/0.25928. Took 0.05 sec\n",
            "Epoch 261, Loss(train/val) 0.45769/0.25874. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.47160/0.25840. Took 0.05 sec\n",
            "Epoch 263, Loss(train/val) 0.47106/0.25939. Took 0.05 sec\n",
            "Epoch 264, Loss(train/val) 0.46569/0.25888. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.46749/0.25797. Took 0.05 sec\n",
            "Epoch 266, Loss(train/val) 0.44629/0.25845. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 0.46206/0.25753. Took 0.05 sec\n",
            "Epoch 268, Loss(train/val) 0.46709/0.25762. Took 0.05 sec\n",
            "Epoch 269, Loss(train/val) 0.46254/0.25919. Took 0.05 sec\n",
            "Epoch 270, Loss(train/val) 0.45846/0.26087. Took 0.05 sec\n",
            "Epoch 271, Loss(train/val) 0.46165/0.26047. Took 0.05 sec\n",
            "Epoch 272, Loss(train/val) 0.46048/0.25867. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 0.44325/0.25757. Took 0.05 sec\n",
            "Epoch 274, Loss(train/val) 0.45556/0.25637. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.45036/0.25578. Took 0.05 sec\n",
            "Epoch 276, Loss(train/val) 0.45599/0.25625. Took 0.05 sec\n",
            "Epoch 277, Loss(train/val) 0.45941/0.25599. Took 0.06 sec\n",
            "Epoch 278, Loss(train/val) 0.44952/0.25579. Took 0.05 sec\n",
            "Epoch 279, Loss(train/val) 0.44656/0.25716. Took 0.06 sec\n",
            "Epoch 280, Loss(train/val) 0.44620/0.25894. Took 0.05 sec\n",
            "Epoch 281, Loss(train/val) 0.45745/0.25762. Took 0.05 sec\n",
            "Epoch 282, Loss(train/val) 0.44732/0.25957. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 0.44121/0.26057. Took 0.05 sec\n",
            "Epoch 284, Loss(train/val) 0.45228/0.26054. Took 0.05 sec\n",
            "Epoch 285, Loss(train/val) 0.45022/0.26068. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.44345/0.25959. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.44938/0.25402. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.45644/0.25370. Took 0.05 sec\n",
            "Epoch 289, Loss(train/val) 0.44546/0.25530. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.45132/0.25534. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.43806/0.25930. Took 0.06 sec\n",
            "Epoch 292, Loss(train/val) 0.43719/0.25880. Took 0.05 sec\n",
            "Epoch 293, Loss(train/val) 0.44508/0.25817. Took 0.05 sec\n",
            "Epoch 294, Loss(train/val) 0.44337/0.25456. Took 0.05 sec\n",
            "Epoch 295, Loss(train/val) 0.44159/0.25243. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 0.44063/0.25299. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.43885/0.25297. Took 0.05 sec\n",
            "Epoch 298, Loss(train/val) 0.44348/0.25425. Took 0.05 sec\n",
            "Epoch 299, Loss(train/val) 0.43946/0.25428. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.44016/0.25283. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.44110/0.25252. Took 0.06 sec\n",
            "Epoch 302, Loss(train/val) 0.42422/0.25236. Took 0.05 sec\n",
            "Epoch 303, Loss(train/val) 0.42781/0.25181. Took 0.04 sec\n",
            "Epoch 304, Loss(train/val) 0.43054/0.25145. Took 0.05 sec\n",
            "Epoch 305, Loss(train/val) 0.43332/0.25175. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 0.43093/0.25134. Took 0.04 sec\n",
            "Epoch 307, Loss(train/val) 0.43968/0.25103. Took 0.04 sec\n",
            "Epoch 308, Loss(train/val) 0.43082/0.25163. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.42392/0.25102. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.42812/0.25368. Took 0.05 sec\n",
            "Epoch 311, Loss(train/val) 0.43365/0.25730. Took 0.04 sec\n",
            "Epoch 312, Loss(train/val) 0.42786/0.25593. Took 0.04 sec\n",
            "Epoch 313, Loss(train/val) 0.41909/0.25620. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.41465/0.25220. Took 0.04 sec\n",
            "Epoch 315, Loss(train/val) 0.43087/0.24997. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.41371/0.25390. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.42299/0.25584. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.42288/0.25215. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.42251/0.25724. Took 0.05 sec\n",
            "Epoch 320, Loss(train/val) 0.42098/0.25996. Took 0.05 sec\n",
            "Epoch 321, Loss(train/val) 0.42464/0.26015. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.41244/0.25700. Took 0.05 sec\n",
            "Epoch 323, Loss(train/val) 0.41309/0.25180. Took 0.05 sec\n",
            "Epoch 324, Loss(train/val) 0.41689/0.24911. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.41646/0.24931. Took 0.05 sec\n",
            "Epoch 326, Loss(train/val) 0.42240/0.24922. Took 0.05 sec\n",
            "Epoch 327, Loss(train/val) 0.41008/0.24875. Took 0.04 sec\n",
            "Epoch 328, Loss(train/val) 0.41252/0.24901. Took 0.05 sec\n",
            "Epoch 329, Loss(train/val) 0.41326/0.24847. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.41374/0.24841. Took 0.05 sec\n",
            "Epoch 331, Loss(train/val) 0.40807/0.24788. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 0.41370/0.24925. Took 0.20 sec\n",
            "Epoch 333, Loss(train/val) 0.41201/0.24905. Took 0.19 sec\n",
            "Epoch 334, Loss(train/val) 0.40544/0.24757. Took 0.18 sec\n",
            "Epoch 335, Loss(train/val) 0.41343/0.24739. Took 0.12 sec\n",
            "Epoch 336, Loss(train/val) 0.40240/0.24854. Took 0.13 sec\n",
            "Epoch 337, Loss(train/val) 0.41071/0.24870. Took 0.07 sec\n",
            "Epoch 338, Loss(train/val) 0.39944/0.24941. Took 0.04 sec\n",
            "Epoch 339, Loss(train/val) 0.41313/0.25028. Took 0.04 sec\n",
            "Epoch 340, Loss(train/val) 0.40362/0.24969. Took 0.05 sec\n",
            "Epoch 341, Loss(train/val) 0.41058/0.24729. Took 0.05 sec\n",
            "Epoch 342, Loss(train/val) 0.40954/0.24715. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.40781/0.24694. Took 0.05 sec\n",
            "Epoch 344, Loss(train/val) 0.40258/0.24709. Took 0.05 sec\n",
            "Epoch 345, Loss(train/val) 0.40607/0.24676. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.40652/0.24702. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 0.39903/0.24650. Took 0.05 sec\n",
            "Epoch 348, Loss(train/val) 0.39028/0.24686. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 0.39398/0.24789. Took 0.05 sec\n",
            "Epoch 350, Loss(train/val) 0.39948/0.24824. Took 0.05 sec\n",
            "Epoch 351, Loss(train/val) 0.40457/0.24991. Took 0.05 sec\n",
            "Epoch 352, Loss(train/val) 0.40138/0.24641. Took 0.04 sec\n",
            "Epoch 353, Loss(train/val) 0.39740/0.24752. Took 0.04 sec\n",
            "Epoch 354, Loss(train/val) 0.39748/0.24632. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 0.39122/0.24620. Took 0.05 sec\n",
            "Epoch 356, Loss(train/val) 0.39381/0.24603. Took 0.05 sec\n",
            "Epoch 357, Loss(train/val) 0.40006/0.24642. Took 0.04 sec\n",
            "Epoch 358, Loss(train/val) 0.40069/0.24734. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 0.39414/0.25179. Took 0.05 sec\n",
            "Epoch 360, Loss(train/val) 0.40987/0.25880. Took 0.05 sec\n",
            "Epoch 361, Loss(train/val) 0.39425/0.25929. Took 0.05 sec\n",
            "Epoch 362, Loss(train/val) 0.39535/0.25716. Took 0.04 sec\n",
            "Epoch 363, Loss(train/val) 0.39635/0.24970. Took 0.04 sec\n",
            "Epoch 364, Loss(train/val) 0.38824/0.24689. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.38154/0.24587. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.39258/0.24665. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 0.39543/0.24626. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.37979/0.24818. Took 0.05 sec\n",
            "Epoch 369, Loss(train/val) 0.38698/0.24899. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.38489/0.24598. Took 0.05 sec\n",
            "Epoch 371, Loss(train/val) 0.39298/0.24480. Took 0.05 sec\n",
            "Epoch 372, Loss(train/val) 0.39286/0.24453. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 0.38012/0.24713. Took 0.04 sec\n",
            "Epoch 374, Loss(train/val) 0.38792/0.24919. Took 0.05 sec\n",
            "Epoch 375, Loss(train/val) 0.39084/0.24761. Took 0.05 sec\n",
            "Epoch 376, Loss(train/val) 0.38611/0.24784. Took 0.04 sec\n",
            "Epoch 377, Loss(train/val) 0.39279/0.24563. Took 0.04 sec\n",
            "Epoch 378, Loss(train/val) 0.39283/0.24560. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 0.38251/0.24500. Took 0.04 sec\n",
            "Epoch 380, Loss(train/val) 0.39076/0.24503. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 0.39935/0.24506. Took 0.05 sec\n",
            "Epoch 382, Loss(train/val) 0.38508/0.24511. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.38018/0.25256. Took 0.05 sec\n",
            "Epoch 384, Loss(train/val) 0.39144/0.25313. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 0.38939/0.24955. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.38530/0.24824. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.38438/0.24743. Took 0.05 sec\n",
            "Epoch 388, Loss(train/val) 0.37784/0.24603. Took 0.05 sec\n",
            "Epoch 389, Loss(train/val) 0.37404/0.24622. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.38416/0.24582. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.38745/0.24814. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.37510/0.24783. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.37805/0.24501. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 0.37428/0.24607. Took 0.06 sec\n",
            "Epoch 395, Loss(train/val) 0.36978/0.24639. Took 0.05 sec\n",
            "Epoch 396, Loss(train/val) 0.37114/0.24520. Took 0.04 sec\n",
            "Epoch 397, Loss(train/val) 0.38050/0.24562. Took 0.05 sec\n",
            "Epoch 398, Loss(train/val) 0.37616/0.24627. Took 0.05 sec\n",
            "Epoch 399, Loss(train/val) 0.37293/0.24705. Took 0.05 sec\n",
            "Epoch 400, Loss(train/val) 0.36543/0.24821. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 0.37108/0.24790. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.37731/0.24576. Took 0.05 sec\n",
            "Epoch 403, Loss(train/val) 0.36855/0.24720. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 0.37632/0.25518. Took 0.05 sec\n",
            "Epoch 405, Loss(train/val) 0.36711/0.25656. Took 0.05 sec\n",
            "Epoch 406, Loss(train/val) 0.39550/0.25097. Took 0.05 sec\n",
            "Epoch 407, Loss(train/val) 0.36594/0.24562. Took 0.05 sec\n",
            "Epoch 408, Loss(train/val) 0.37323/0.24427. Took 0.04 sec\n",
            "Epoch 409, Loss(train/val) 0.36822/0.24332. Took 0.05 sec\n",
            "Epoch 410, Loss(train/val) 0.36306/0.24612. Took 0.06 sec\n",
            "Epoch 411, Loss(train/val) 0.37309/0.24836. Took 0.05 sec\n",
            "Epoch 412, Loss(train/val) 0.37306/0.24497. Took 0.05 sec\n",
            "Epoch 413, Loss(train/val) 0.36966/0.24432. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.36757/0.24609. Took 0.05 sec\n",
            "Epoch 415, Loss(train/val) 0.36829/0.24723. Took 0.06 sec\n",
            "Epoch 416, Loss(train/val) 0.36099/0.24938. Took 0.04 sec\n",
            "Epoch 417, Loss(train/val) 0.37095/0.24589. Took 0.04 sec\n",
            "Epoch 418, Loss(train/val) 0.36953/0.24330. Took 0.05 sec\n",
            "Epoch 419, Loss(train/val) 0.36175/0.24375. Took 0.04 sec\n",
            "Epoch 420, Loss(train/val) 0.38105/0.24345. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.36855/0.24338. Took 0.05 sec\n",
            "Epoch 422, Loss(train/val) 0.36684/0.24353. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.35960/0.24308. Took 0.05 sec\n",
            "Epoch 424, Loss(train/val) 0.36332/0.24273. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.36626/0.24251. Took 0.05 sec\n",
            "Epoch 426, Loss(train/val) 0.35989/0.25204. Took 0.05 sec\n",
            "Epoch 427, Loss(train/val) 0.37143/0.25054. Took 0.04 sec\n",
            "Epoch 428, Loss(train/val) 0.36533/0.24741. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.36370/0.24213. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.36497/0.24369. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.36180/0.24143. Took 0.05 sec\n",
            "Epoch 432, Loss(train/val) 0.36106/0.24151. Took 0.05 sec\n",
            "Epoch 433, Loss(train/val) 0.36204/0.24152. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.36867/0.24194. Took 0.05 sec\n",
            "Epoch 435, Loss(train/val) 0.36392/0.24211. Took 0.05 sec\n",
            "Epoch 436, Loss(train/val) 0.36081/0.24200. Took 0.05 sec\n",
            "Epoch 437, Loss(train/val) 0.35839/0.24161. Took 0.04 sec\n",
            "Epoch 438, Loss(train/val) 0.35571/0.24205. Took 0.04 sec\n",
            "Epoch 439, Loss(train/val) 0.36157/0.24379. Took 0.05 sec\n",
            "Epoch 440, Loss(train/val) 0.37245/0.24577. Took 0.05 sec\n",
            "Epoch 441, Loss(train/val) 0.34997/0.24202. Took 0.05 sec\n",
            "Epoch 442, Loss(train/val) 0.36017/0.24230. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 0.36791/0.24111. Took 0.05 sec\n",
            "Epoch 444, Loss(train/val) 0.35680/0.24217. Took 0.05 sec\n",
            "Epoch 445, Loss(train/val) 0.35477/0.24316. Took 0.05 sec\n",
            "Epoch 446, Loss(train/val) 0.34503/0.24157. Took 0.05 sec\n",
            "Epoch 447, Loss(train/val) 0.35986/0.24351. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 0.36060/0.24537. Took 0.05 sec\n",
            "Epoch 449, Loss(train/val) 0.35233/0.24736. Took 0.05 sec\n",
            "Epoch 450, Loss(train/val) 0.35659/0.24704. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 0.35965/0.24723. Took 0.05 sec\n",
            "Epoch 452, Loss(train/val) 0.34897/0.24049. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.34623/0.23969. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.35636/0.24151. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 0.36944/0.24971. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.35590/0.25242. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.35678/0.24659. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.36459/0.24222. Took 0.05 sec\n",
            "Epoch 459, Loss(train/val) 0.35318/0.23903. Took 0.05 sec\n",
            "Epoch 460, Loss(train/val) 0.34681/0.23911. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.35365/0.24196. Took 0.04 sec\n",
            "Epoch 462, Loss(train/val) 0.35982/0.24286. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.35760/0.24428. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.35651/0.24371. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.35867/0.24149. Took 0.05 sec\n",
            "Epoch 466, Loss(train/val) 0.34920/0.24363. Took 0.04 sec\n",
            "Epoch 467, Loss(train/val) 0.34681/0.26273. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.35171/0.25984. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 0.34849/0.24569. Took 0.05 sec\n",
            "Epoch 470, Loss(train/val) 0.36440/0.23813. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 0.34819/0.23856. Took 0.05 sec\n",
            "Epoch 472, Loss(train/val) 0.34801/0.23718. Took 0.05 sec\n",
            "Epoch 473, Loss(train/val) 0.35035/0.23810. Took 0.05 sec\n",
            "Epoch 474, Loss(train/val) 0.37072/0.23868. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 0.34126/0.23976. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.36490/0.23952. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.33630/0.24668. Took 0.05 sec\n",
            "Epoch 478, Loss(train/val) 0.34859/0.24971. Took 0.05 sec\n",
            "Epoch 479, Loss(train/val) 0.36604/0.26284. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.34962/0.24951. Took 0.05 sec\n",
            "Epoch 481, Loss(train/val) 0.34299/0.24958. Took 0.04 sec\n",
            "Epoch 482, Loss(train/val) 0.34963/0.24438. Took 0.04 sec\n",
            "Epoch 483, Loss(train/val) 0.34544/0.24087. Took 0.04 sec\n",
            "Epoch 484, Loss(train/val) 0.37059/0.23674. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.34189/0.23665. Took 0.05 sec\n",
            "Epoch 486, Loss(train/val) 0.34876/0.24641. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.34157/0.24630. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.34724/0.25222. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.35022/0.24888. Took 0.05 sec\n",
            "Epoch 490, Loss(train/val) 0.34686/0.23823. Took 0.05 sec\n",
            "Epoch 491, Loss(train/val) 0.33819/0.23263. Took 0.05 sec\n",
            "Epoch 492, Loss(train/val) 0.35059/0.23567. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.34456/0.24058. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.34114/0.23821. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.34778/0.24044. Took 0.05 sec\n",
            "Epoch 496, Loss(train/val) 0.34326/0.23441. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 0.34242/0.23083. Took 0.05 sec\n",
            "Epoch 498, Loss(train/val) 0.34114/0.23001. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.34216/0.23215. Took 0.05 sec\n",
            "Epoch 500, Loss(train/val) 0.35171/0.22910. Took 0.06 sec\n",
            "Epoch 501, Loss(train/val) 0.33344/0.23186. Took 0.05 sec\n",
            "Epoch 502, Loss(train/val) 0.32453/0.23455. Took 0.05 sec\n",
            "Epoch 503, Loss(train/val) 0.34151/0.23907. Took 0.05 sec\n",
            "Epoch 504, Loss(train/val) 0.33894/0.24219. Took 0.05 sec\n",
            "Epoch 505, Loss(train/val) 0.34532/0.24044. Took 0.06 sec\n",
            "Epoch 506, Loss(train/val) 0.34252/0.23689. Took 0.05 sec\n",
            "Epoch 507, Loss(train/val) 0.33755/0.23195. Took 0.05 sec\n",
            "Epoch 508, Loss(train/val) 0.35525/0.23758. Took 0.05 sec\n",
            "Epoch 509, Loss(train/val) 0.33314/0.22802. Took 0.04 sec\n",
            "Epoch 510, Loss(train/val) 0.33422/0.22956. Took 0.05 sec\n",
            "Epoch 511, Loss(train/val) 0.32947/0.23265. Took 0.05 sec\n",
            "Epoch 512, Loss(train/val) 0.34749/0.22988. Took 0.06 sec\n",
            "Epoch 513, Loss(train/val) 0.32977/0.23664. Took 0.04 sec\n",
            "Epoch 514, Loss(train/val) 0.33248/0.25050. Took 0.04 sec\n",
            "Epoch 515, Loss(train/val) 0.34244/0.24717. Took 0.06 sec\n",
            "Epoch 516, Loss(train/val) 0.33655/0.24364. Took 0.05 sec\n",
            "Epoch 517, Loss(train/val) 0.33500/0.23830. Took 0.04 sec\n",
            "Epoch 518, Loss(train/val) 0.35751/0.24295. Took 0.05 sec\n",
            "Epoch 519, Loss(train/val) 0.33265/0.23135. Took 0.05 sec\n",
            "Epoch 520, Loss(train/val) 0.33799/0.22966. Took 0.05 sec\n",
            "Epoch 521, Loss(train/val) 0.34159/0.22572. Took 0.05 sec\n",
            "Epoch 522, Loss(train/val) 0.33307/0.22967. Took 0.04 sec\n",
            "Epoch 523, Loss(train/val) 0.33015/0.23208. Took 0.04 sec\n",
            "Epoch 524, Loss(train/val) 0.33298/0.23163. Took 0.05 sec\n",
            "Epoch 525, Loss(train/val) 0.34199/0.22697. Took 0.05 sec\n",
            "Epoch 526, Loss(train/val) 0.34087/0.22599. Took 0.05 sec\n",
            "Epoch 527, Loss(train/val) 0.33211/0.23988. Took 0.05 sec\n",
            "Epoch 528, Loss(train/val) 0.33592/0.24441. Took 0.05 sec\n",
            "Epoch 529, Loss(train/val) 0.32259/0.23535. Took 0.05 sec\n",
            "Epoch 530, Loss(train/val) 0.32524/0.22892. Took 0.05 sec\n",
            "Epoch 531, Loss(train/val) 0.32507/0.22649. Took 0.05 sec\n",
            "Epoch 532, Loss(train/val) 0.33185/0.22578. Took 0.05 sec\n",
            "Epoch 533, Loss(train/val) 0.33127/0.22467. Took 0.05 sec\n",
            "Epoch 534, Loss(train/val) 0.31718/0.23782. Took 0.04 sec\n",
            "Epoch 535, Loss(train/val) 0.33412/0.23626. Took 0.05 sec\n",
            "Epoch 536, Loss(train/val) 0.33602/0.22716. Took 0.05 sec\n",
            "Epoch 537, Loss(train/val) 0.32565/0.22432. Took 0.05 sec\n",
            "Epoch 538, Loss(train/val) 0.32289/0.22340. Took 0.05 sec\n",
            "Epoch 539, Loss(train/val) 0.32180/0.22455. Took 0.05 sec\n",
            "Epoch 540, Loss(train/val) 0.32525/0.22420. Took 0.05 sec\n",
            "Epoch 541, Loss(train/val) 0.33209/0.23020. Took 0.05 sec\n",
            "Epoch 542, Loss(train/val) 0.31463/0.23703. Took 0.05 sec\n",
            "Epoch 543, Loss(train/val) 0.32312/0.24164. Took 0.04 sec\n",
            "Epoch 544, Loss(train/val) 0.32423/0.23155. Took 0.04 sec\n",
            "Epoch 545, Loss(train/val) 0.32559/0.22404. Took 0.05 sec\n",
            "Epoch 546, Loss(train/val) 0.33910/0.23433. Took 0.05 sec\n",
            "Epoch 547, Loss(train/val) 0.32801/0.23859. Took 0.05 sec\n",
            "Epoch 548, Loss(train/val) 0.32533/0.22798. Took 0.05 sec\n",
            "Epoch 549, Loss(train/val) 0.31956/0.22041. Took 0.05 sec\n",
            "Epoch 550, Loss(train/val) 0.31651/0.21866. Took 0.05 sec\n",
            "Epoch 551, Loss(train/val) 0.31859/0.22053. Took 0.04 sec\n",
            "Epoch 552, Loss(train/val) 0.32215/0.22910. Took 0.05 sec\n",
            "Epoch 553, Loss(train/val) 0.32762/0.23416. Took 0.05 sec\n",
            "Epoch 554, Loss(train/val) 0.31819/0.25347. Took 0.05 sec\n",
            "Epoch 555, Loss(train/val) 0.31738/0.23819. Took 0.05 sec\n",
            "Epoch 556, Loss(train/val) 0.31917/0.22325. Took 0.04 sec\n",
            "Epoch 557, Loss(train/val) 0.32485/0.21709. Took 0.05 sec\n",
            "Epoch 558, Loss(train/val) 0.32507/0.23185. Took 0.05 sec\n",
            "Epoch 559, Loss(train/val) 0.31566/0.24052. Took 0.04 sec\n",
            "Epoch 560, Loss(train/val) 0.31694/0.24316. Took 0.05 sec\n",
            "Epoch 561, Loss(train/val) 0.30826/0.22837. Took 0.05 sec\n",
            "Epoch 562, Loss(train/val) 0.30832/0.22871. Took 0.05 sec\n",
            "Epoch 563, Loss(train/val) 0.30456/0.21936. Took 0.05 sec\n",
            "Epoch 564, Loss(train/val) 0.31135/0.21789. Took 0.05 sec\n",
            "Epoch 565, Loss(train/val) 0.31245/0.21782. Took 0.06 sec\n",
            "Epoch 566, Loss(train/val) 0.32331/0.22344. Took 0.05 sec\n",
            "Epoch 567, Loss(train/val) 0.32741/0.21290. Took 0.05 sec\n",
            "Epoch 568, Loss(train/val) 0.30979/0.22837. Took 0.05 sec\n",
            "Epoch 569, Loss(train/val) 0.30183/0.23427. Took 0.05 sec\n",
            "Epoch 570, Loss(train/val) 0.30947/0.23750. Took 0.05 sec\n",
            "Epoch 571, Loss(train/val) 0.30817/0.21993. Took 0.05 sec\n",
            "Epoch 572, Loss(train/val) 0.30889/0.22338. Took 0.05 sec\n",
            "Epoch 573, Loss(train/val) 0.31858/0.21450. Took 0.05 sec\n",
            "Epoch 574, Loss(train/val) 0.30572/0.22232. Took 0.05 sec\n",
            "Epoch 575, Loss(train/val) 0.30396/0.22886. Took 0.05 sec\n",
            "Epoch 576, Loss(train/val) 0.29952/0.24079. Took 0.04 sec\n",
            "Epoch 577, Loss(train/val) 0.29444/0.22588. Took 0.05 sec\n",
            "Epoch 578, Loss(train/val) 0.30688/0.21725. Took 0.05 sec\n",
            "Epoch 579, Loss(train/val) 0.29985/0.21403. Took 0.05 sec\n",
            "Epoch 580, Loss(train/val) 0.31677/0.22317. Took 0.05 sec\n",
            "Epoch 581, Loss(train/val) 0.29293/0.21500. Took 0.05 sec\n",
            "Epoch 582, Loss(train/val) 0.30631/0.21167. Took 0.05 sec\n",
            "Epoch 583, Loss(train/val) 0.30391/0.20824. Took 0.04 sec\n",
            "Epoch 584, Loss(train/val) 0.29422/0.21348. Took 0.04 sec\n",
            "Epoch 585, Loss(train/val) 0.30172/0.22528. Took 0.06 sec\n",
            "Epoch 586, Loss(train/val) 0.29349/0.22661. Took 0.04 sec\n",
            "Epoch 587, Loss(train/val) 0.32065/0.22826. Took 0.05 sec\n",
            "Epoch 588, Loss(train/val) 0.29273/0.22159. Took 0.05 sec\n",
            "Epoch 589, Loss(train/val) 0.29657/0.22406. Took 0.05 sec\n",
            "Epoch 590, Loss(train/val) 0.30610/0.21090. Took 0.05 sec\n",
            "Epoch 591, Loss(train/val) 0.30547/0.21053. Took 0.05 sec\n",
            "Epoch 592, Loss(train/val) 0.29610/0.22690. Took 0.05 sec\n",
            "Epoch 593, Loss(train/val) 0.30381/0.21564. Took 0.05 sec\n",
            "Epoch 594, Loss(train/val) 0.29390/0.21752. Took 0.05 sec\n",
            "Epoch 595, Loss(train/val) 0.29211/0.22042. Took 0.05 sec\n",
            "Epoch 596, Loss(train/val) 0.29489/0.21753. Took 0.05 sec\n",
            "Epoch 597, Loss(train/val) 0.29193/0.21619. Took 0.04 sec\n",
            "Epoch 598, Loss(train/val) 0.29786/0.20688. Took 0.04 sec\n",
            "Epoch 599, Loss(train/val) 0.29480/0.21330. Took 0.05 sec\n",
            "Epoch 600, Loss(train/val) 0.28359/0.21898. Took 0.05 sec\n",
            "Epoch 601, Loss(train/val) 0.28686/0.21031. Took 0.05 sec\n",
            "Epoch 602, Loss(train/val) 0.28581/0.20621. Took 0.04 sec\n",
            "Epoch 603, Loss(train/val) 0.28261/0.20965. Took 0.05 sec\n",
            "Epoch 604, Loss(train/val) 0.28720/0.22704. Took 0.05 sec\n",
            "Epoch 605, Loss(train/val) 0.27910/0.22068. Took 0.05 sec\n",
            "Epoch 606, Loss(train/val) 0.28177/0.20459. Took 0.05 sec\n",
            "Epoch 607, Loss(train/val) 0.28151/0.20061. Took 0.04 sec\n",
            "Epoch 608, Loss(train/val) 0.28226/0.20282. Took 0.05 sec\n",
            "Epoch 609, Loss(train/val) 0.27610/0.20918. Took 0.05 sec\n",
            "Epoch 610, Loss(train/val) 0.28270/0.21231. Took 0.06 sec\n",
            "Epoch 611, Loss(train/val) 0.28446/0.20671. Took 0.05 sec\n",
            "Epoch 612, Loss(train/val) 0.28841/0.19984. Took 0.05 sec\n",
            "Epoch 613, Loss(train/val) 0.28134/0.20068. Took 0.05 sec\n",
            "Epoch 614, Loss(train/val) 0.28653/0.20452. Took 0.05 sec\n",
            "Epoch 615, Loss(train/val) 0.28418/0.22121. Took 0.05 sec\n",
            "Epoch 616, Loss(train/val) 0.27535/0.21450. Took 0.05 sec\n",
            "Epoch 617, Loss(train/val) 0.27438/0.20100. Took 0.05 sec\n",
            "Epoch 618, Loss(train/val) 0.28011/0.19276. Took 0.05 sec\n",
            "Epoch 619, Loss(train/val) 0.26882/0.19959. Took 0.05 sec\n",
            "Epoch 620, Loss(train/val) 0.27725/0.20457. Took 0.05 sec\n",
            "Epoch 621, Loss(train/val) 0.26892/0.20550. Took 0.05 sec\n",
            "Epoch 622, Loss(train/val) 0.28493/0.19520. Took 0.05 sec\n",
            "Epoch 623, Loss(train/val) 0.27313/0.19104. Took 0.05 sec\n",
            "Epoch 624, Loss(train/val) 0.28373/0.20142. Took 0.05 sec\n",
            "Epoch 625, Loss(train/val) 0.28397/0.20256. Took 0.05 sec\n",
            "Epoch 626, Loss(train/val) 0.27477/0.20782. Took 0.05 sec\n",
            "Epoch 627, Loss(train/val) 0.26983/0.19592. Took 0.05 sec\n",
            "Epoch 628, Loss(train/val) 0.27693/0.19491. Took 0.04 sec\n",
            "Epoch 629, Loss(train/val) 0.28948/0.19292. Took 0.05 sec\n",
            "Epoch 630, Loss(train/val) 0.28571/0.18951. Took 0.05 sec\n",
            "Epoch 631, Loss(train/val) 0.27259/0.19716. Took 0.05 sec\n",
            "Epoch 632, Loss(train/val) 0.27446/0.19337. Took 0.05 sec\n",
            "Epoch 633, Loss(train/val) 0.28249/0.19330. Took 0.05 sec\n",
            "Epoch 634, Loss(train/val) 0.28602/0.19265. Took 0.05 sec\n",
            "Epoch 635, Loss(train/val) 0.26423/0.19845. Took 0.05 sec\n",
            "Epoch 636, Loss(train/val) 0.26931/0.20519. Took 0.05 sec\n",
            "Epoch 637, Loss(train/val) 0.27966/0.19652. Took 0.05 sec\n",
            "Epoch 638, Loss(train/val) 0.27981/0.18722. Took 0.05 sec\n",
            "Epoch 639, Loss(train/val) 0.26963/0.19239. Took 0.04 sec\n",
            "Epoch 640, Loss(train/val) 0.26334/0.19581. Took 0.05 sec\n",
            "Epoch 641, Loss(train/val) 0.26212/0.19456. Took 0.05 sec\n",
            "Epoch 642, Loss(train/val) 0.27095/0.18963. Took 0.04 sec\n",
            "Epoch 643, Loss(train/val) 0.26802/0.18610. Took 0.04 sec\n",
            "Epoch 644, Loss(train/val) 0.28032/0.19327. Took 0.05 sec\n",
            "Epoch 645, Loss(train/val) 0.27911/0.19747. Took 0.05 sec\n",
            "Epoch 646, Loss(train/val) 0.27489/0.20017. Took 0.05 sec\n",
            "Epoch 647, Loss(train/val) 0.26245/0.19435. Took 0.04 sec\n",
            "Epoch 648, Loss(train/val) 0.26612/0.18844. Took 0.05 sec\n",
            "Epoch 649, Loss(train/val) 0.27079/0.18937. Took 0.05 sec\n",
            "Epoch 650, Loss(train/val) 0.25272/0.18991. Took 0.05 sec\n",
            "Epoch 651, Loss(train/val) 0.27373/0.18831. Took 0.05 sec\n",
            "Epoch 652, Loss(train/val) 0.26109/0.18870. Took 0.05 sec\n",
            "Epoch 653, Loss(train/val) 0.26016/0.18690. Took 0.05 sec\n",
            "Epoch 654, Loss(train/val) 0.27855/0.18892. Took 0.04 sec\n",
            "Epoch 655, Loss(train/val) 0.25836/0.19063. Took 0.05 sec\n",
            "Epoch 656, Loss(train/val) 0.25811/0.18817. Took 0.05 sec\n",
            "Epoch 657, Loss(train/val) 0.25679/0.18875. Took 0.05 sec\n",
            "Epoch 658, Loss(train/val) 0.27466/0.18585. Took 0.05 sec\n",
            "Epoch 659, Loss(train/val) 0.26638/0.18758. Took 0.05 sec\n",
            "Epoch 660, Loss(train/val) 0.27235/0.18746. Took 0.05 sec\n",
            "Epoch 661, Loss(train/val) 0.26317/0.18473. Took 0.05 sec\n",
            "Epoch 662, Loss(train/val) 0.26319/0.18573. Took 0.05 sec\n",
            "Epoch 663, Loss(train/val) 0.25556/0.19132. Took 0.05 sec\n",
            "Epoch 664, Loss(train/val) 0.26885/0.18963. Took 0.05 sec\n",
            "Epoch 665, Loss(train/val) 0.25687/0.18460. Took 0.05 sec\n",
            "Epoch 666, Loss(train/val) 0.28292/0.18319. Took 0.05 sec\n",
            "Epoch 667, Loss(train/val) 0.26818/0.18361. Took 0.05 sec\n",
            "Epoch 668, Loss(train/val) 0.26598/0.18638. Took 0.05 sec\n",
            "Epoch 669, Loss(train/val) 0.25829/0.18636. Took 0.05 sec\n",
            "Epoch 670, Loss(train/val) 0.25541/0.18792. Took 0.06 sec\n",
            "Epoch 671, Loss(train/val) 0.25845/0.18917. Took 0.05 sec\n",
            "Epoch 672, Loss(train/val) 0.25779/0.18541. Took 0.04 sec\n",
            "Epoch 673, Loss(train/val) 0.26661/0.18480. Took 0.04 sec\n",
            "Epoch 674, Loss(train/val) 0.25972/0.18197. Took 0.04 sec\n",
            "Epoch 675, Loss(train/val) 0.28823/0.18470. Took 0.05 sec\n",
            "Epoch 676, Loss(train/val) 0.26309/0.18806. Took 0.05 sec\n",
            "Epoch 677, Loss(train/val) 0.27134/0.18595. Took 0.04 sec\n",
            "Epoch 678, Loss(train/val) 0.25790/0.18414. Took 0.05 sec\n",
            "Epoch 679, Loss(train/val) 0.25784/0.18440. Took 0.04 sec\n",
            "Epoch 680, Loss(train/val) 0.27666/0.18416. Took 0.05 sec\n",
            "Epoch 681, Loss(train/val) 0.26239/0.18602. Took 0.05 sec\n",
            "Epoch 682, Loss(train/val) 0.25851/0.18227. Took 0.05 sec\n",
            "Epoch 683, Loss(train/val) 0.26225/0.18396. Took 0.05 sec\n",
            "Epoch 684, Loss(train/val) 0.26783/0.18448. Took 0.05 sec\n",
            "Epoch 685, Loss(train/val) 0.25945/0.18441. Took 0.05 sec\n",
            "Epoch 686, Loss(train/val) 0.26338/0.18362. Took 0.05 sec\n",
            "Epoch 687, Loss(train/val) 0.29154/0.18410. Took 0.05 sec\n",
            "Epoch 688, Loss(train/val) 0.29815/0.18326. Took 0.05 sec\n",
            "Epoch 689, Loss(train/val) 0.25073/0.18444. Took 0.05 sec\n",
            "Epoch 690, Loss(train/val) 0.26757/0.18458. Took 0.05 sec\n",
            "Epoch 691, Loss(train/val) 0.27015/0.18329. Took 0.05 sec\n",
            "Epoch 692, Loss(train/val) 0.27320/0.18118. Took 0.05 sec\n",
            "Epoch 693, Loss(train/val) 0.25850/0.18031. Took 0.05 sec\n",
            "Epoch 694, Loss(train/val) 0.27115/0.18117. Took 0.05 sec\n",
            "Epoch 695, Loss(train/val) 0.25363/0.18366. Took 0.05 sec\n",
            "Epoch 696, Loss(train/val) 0.25443/0.18233. Took 0.05 sec\n",
            "Epoch 697, Loss(train/val) 0.24761/0.18206. Took 0.05 sec\n",
            "Epoch 698, Loss(train/val) 0.25980/0.18633. Took 0.04 sec\n",
            "Epoch 699, Loss(train/val) 0.25258/0.18597. Took 0.05 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=700, exp_name='exp3_lr', hid_dim=16, input_dim=1, l2=1e-05, lr=0.0003, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.05864/0.38132. Took 0.04 sec\n",
            "Epoch 1, Loss(train/val) 1.05264/0.38172. Took 0.05 sec\n",
            "Epoch 2, Loss(train/val) 1.04750/0.38211. Took 0.04 sec\n",
            "Epoch 3, Loss(train/val) 1.05116/0.38247. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.05471/0.38278. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.05079/0.38306. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.04991/0.38328. Took 0.05 sec\n",
            "Epoch 7, Loss(train/val) 1.04603/0.38343. Took 0.05 sec\n",
            "Epoch 8, Loss(train/val) 1.04939/0.38357. Took 0.05 sec\n",
            "Epoch 9, Loss(train/val) 1.05248/0.38372. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.04125/0.38378. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.04735/0.38378. Took 0.04 sec\n",
            "Epoch 12, Loss(train/val) 1.04488/0.38371. Took 0.06 sec\n",
            "Epoch 13, Loss(train/val) 1.05270/0.38350. Took 0.06 sec\n",
            "Epoch 14, Loss(train/val) 1.04281/0.38311. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.04124/0.38245. Took 0.04 sec\n",
            "Epoch 16, Loss(train/val) 1.04192/0.38144. Took 0.05 sec\n",
            "Epoch 17, Loss(train/val) 1.03947/0.38022. Took 0.05 sec\n",
            "Epoch 18, Loss(train/val) 1.03141/0.38051. Took 0.05 sec\n",
            "Epoch 19, Loss(train/val) 1.03408/0.38108. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 1.02361/0.38211. Took 0.05 sec\n",
            "Epoch 21, Loss(train/val) 1.02086/0.38367. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 1.00286/0.38579. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 0.99412/0.38860. Took 0.06 sec\n",
            "Epoch 24, Loss(train/val) 0.98327/0.39281. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 0.97709/0.39997. Took 0.05 sec\n",
            "Epoch 26, Loss(train/val) 0.94492/0.41152. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 0.92579/0.43239. Took 0.05 sec\n",
            "Epoch 28, Loss(train/val) 0.87171/0.46257. Took 0.06 sec\n",
            "Epoch 29, Loss(train/val) 0.84190/0.50257. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 0.79157/0.55093. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 0.73756/0.60292. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 0.68586/0.65003. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 0.64883/0.68220. Took 0.06 sec\n",
            "Epoch 34, Loss(train/val) 0.61292/0.68223. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 0.58384/0.63575. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 0.57096/0.55964. Took 0.04 sec\n",
            "Epoch 37, Loss(train/val) 0.55761/0.51490. Took 0.04 sec\n",
            "Epoch 38, Loss(train/val) 0.54869/0.56608. Took 0.06 sec\n",
            "Epoch 39, Loss(train/val) 0.53680/0.69200. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 0.53032/0.83733. Took 0.05 sec\n",
            "Epoch 41, Loss(train/val) 0.52019/0.97763. Took 0.05 sec\n",
            "Epoch 42, Loss(train/val) 0.49894/1.07153. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 0.50008/1.07828. Took 0.05 sec\n",
            "Epoch 44, Loss(train/val) 0.49729/1.08646. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 0.48913/0.99036. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 0.48409/0.79368. Took 0.04 sec\n",
            "Epoch 47, Loss(train/val) 0.48840/0.65544. Took 0.05 sec\n",
            "Epoch 48, Loss(train/val) 0.48208/0.56686. Took 0.05 sec\n",
            "Epoch 49, Loss(train/val) 0.47630/0.44255. Took 0.04 sec\n",
            "Epoch 50, Loss(train/val) 0.45111/0.36980. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 0.45935/0.32467. Took 0.04 sec\n",
            "Epoch 52, Loss(train/val) 0.45931/0.29413. Took 0.04 sec\n",
            "Epoch 53, Loss(train/val) 0.45938/0.28742. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 0.45195/0.28081. Took 0.06 sec\n",
            "Epoch 55, Loss(train/val) 0.45194/0.26344. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 0.43893/0.26191. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 0.44373/0.26777. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 0.42528/0.27381. Took 0.05 sec\n",
            "Epoch 59, Loss(train/val) 0.44562/0.27632. Took 0.05 sec\n",
            "Epoch 60, Loss(train/val) 0.42697/0.27716. Took 0.05 sec\n",
            "Epoch 61, Loss(train/val) 0.43772/0.28547. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 0.43138/0.29415. Took 0.05 sec\n",
            "Epoch 63, Loss(train/val) 0.42736/0.29505. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 0.42482/0.28662. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 0.41840/0.29346. Took 0.05 sec\n",
            "Epoch 66, Loss(train/val) 0.42094/0.30357. Took 0.05 sec\n",
            "Epoch 67, Loss(train/val) 0.42255/0.27557. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 0.40712/0.25052. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 0.41584/0.24881. Took 0.05 sec\n",
            "Epoch 70, Loss(train/val) 0.41160/0.24937. Took 0.05 sec\n",
            "Epoch 71, Loss(train/val) 0.41081/0.24947. Took 0.04 sec\n",
            "Epoch 72, Loss(train/val) 0.40606/0.25578. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 0.40632/0.26367. Took 0.04 sec\n",
            "Epoch 74, Loss(train/val) 0.40466/0.27136. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 0.40109/0.27477. Took 0.06 sec\n",
            "Epoch 76, Loss(train/val) 0.38865/0.28532. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 0.39632/0.27231. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 0.40204/0.26575. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 0.39659/0.26387. Took 0.04 sec\n",
            "Epoch 80, Loss(train/val) 0.39144/0.25863. Took 0.05 sec\n",
            "Epoch 81, Loss(train/val) 0.39516/0.25221. Took 0.05 sec\n",
            "Epoch 82, Loss(train/val) 0.39186/0.25137. Took 0.05 sec\n",
            "Epoch 83, Loss(train/val) 0.38962/0.24730. Took 0.05 sec\n",
            "Epoch 84, Loss(train/val) 0.38336/0.24741. Took 0.04 sec\n",
            "Epoch 85, Loss(train/val) 0.40136/0.25004. Took 0.05 sec\n",
            "Epoch 86, Loss(train/val) 0.36583/0.25497. Took 0.05 sec\n",
            "Epoch 87, Loss(train/val) 0.38525/0.26484. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 0.38071/0.25720. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 0.37914/0.25896. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 0.37952/0.25761. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 0.38130/0.26155. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 0.36963/0.25602. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 0.37003/0.24944. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 0.37496/0.24916. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 0.37262/0.24959. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 0.37447/0.25976. Took 0.05 sec\n",
            "Epoch 97, Loss(train/val) 0.39032/0.28246. Took 0.06 sec\n",
            "Epoch 98, Loss(train/val) 0.36934/0.26639. Took 0.04 sec\n",
            "Epoch 99, Loss(train/val) 0.36647/0.27812. Took 0.04 sec\n",
            "Epoch 100, Loss(train/val) 0.37051/0.27321. Took 0.04 sec\n",
            "Epoch 101, Loss(train/val) 0.37850/0.26526. Took 0.05 sec\n",
            "Epoch 102, Loss(train/val) 0.38102/0.26109. Took 0.05 sec\n",
            "Epoch 103, Loss(train/val) 0.37098/0.25728. Took 0.04 sec\n",
            "Epoch 104, Loss(train/val) 0.36819/0.25259. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 0.37967/0.24730. Took 0.05 sec\n",
            "Epoch 106, Loss(train/val) 0.36726/0.25042. Took 0.04 sec\n",
            "Epoch 107, Loss(train/val) 0.37088/0.25055. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 0.36060/0.25179. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 0.36194/0.25302. Took 0.05 sec\n",
            "Epoch 110, Loss(train/val) 0.36655/0.25043. Took 0.05 sec\n",
            "Epoch 111, Loss(train/val) 0.36684/0.24616. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 0.35818/0.24409. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 0.38262/0.25636. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 0.35724/0.27477. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 0.36467/0.30286. Took 0.05 sec\n",
            "Epoch 116, Loss(train/val) 0.36154/0.30241. Took 0.05 sec\n",
            "Epoch 117, Loss(train/val) 0.35872/0.27768. Took 0.05 sec\n",
            "Epoch 118, Loss(train/val) 0.36016/0.24645. Took 0.05 sec\n",
            "Epoch 119, Loss(train/val) 0.35591/0.24335. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 0.39036/0.24938. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 0.35893/0.25847. Took 0.05 sec\n",
            "Epoch 122, Loss(train/val) 0.35310/0.24571. Took 0.05 sec\n",
            "Epoch 123, Loss(train/val) 0.36799/0.24679. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 0.35983/0.24258. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 0.36131/0.24598. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 0.35211/0.24236. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 0.36026/0.24386. Took 0.05 sec\n",
            "Epoch 128, Loss(train/val) 0.35682/0.24251. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 0.35299/0.24798. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 0.35258/0.24986. Took 0.05 sec\n",
            "Epoch 131, Loss(train/val) 0.35003/0.24757. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 0.36175/0.25364. Took 0.05 sec\n",
            "Epoch 133, Loss(train/val) 0.35857/0.27291. Took 0.05 sec\n",
            "Epoch 134, Loss(train/val) 0.36510/0.24780. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 0.38763/0.23993. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 0.35718/0.24178. Took 0.04 sec\n",
            "Epoch 137, Loss(train/val) 0.35004/0.24196. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 0.36091/0.23926. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 0.35508/0.24438. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 0.35426/0.24118. Took 0.06 sec\n",
            "Epoch 141, Loss(train/val) 0.35815/0.23846. Took 0.05 sec\n",
            "Epoch 142, Loss(train/val) 0.35944/0.23963. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 0.37499/0.24201. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 0.35121/0.23759. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 0.35863/0.24088. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 0.35378/0.24319. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 0.35107/0.23751. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 0.34759/0.23954. Took 0.05 sec\n",
            "Epoch 149, Loss(train/val) 0.35430/0.24209. Took 0.04 sec\n",
            "Epoch 150, Loss(train/val) 0.35330/0.23729. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 0.34103/0.24789. Took 0.06 sec\n",
            "Epoch 152, Loss(train/val) 0.34966/0.24537. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 0.34009/0.25142. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 0.34869/0.26558. Took 0.04 sec\n",
            "Epoch 155, Loss(train/val) 0.35024/0.24568. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 0.35170/0.23491. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 0.34372/0.23705. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 0.35056/0.24980. Took 0.05 sec\n",
            "Epoch 159, Loss(train/val) 0.35505/0.24984. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 0.35187/0.25594. Took 0.05 sec\n",
            "Epoch 161, Loss(train/val) 0.34657/0.27088. Took 0.06 sec\n",
            "Epoch 162, Loss(train/val) 0.33906/0.24772. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 0.34157/0.26856. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 0.34456/0.25337. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 0.34496/0.23237. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.34623/0.23214. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 0.34038/0.23237. Took 0.05 sec\n",
            "Epoch 168, Loss(train/val) 0.34283/0.23626. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 0.34000/0.23294. Took 0.05 sec\n",
            "Epoch 170, Loss(train/val) 0.33598/0.23628. Took 0.04 sec\n",
            "Epoch 171, Loss(train/val) 0.34137/0.24321. Took 0.06 sec\n",
            "Epoch 172, Loss(train/val) 0.34744/0.23621. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 0.34730/0.23908. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 0.33872/0.24242. Took 0.05 sec\n",
            "Epoch 175, Loss(train/val) 0.33749/0.23962. Took 0.04 sec\n",
            "Epoch 176, Loss(train/val) 0.33908/0.22928. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 0.33993/0.22951. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 0.33516/0.23298. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 0.33771/0.25149. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 0.34500/0.27682. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 0.33799/0.27538. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 0.33867/0.24443. Took 0.05 sec\n",
            "Epoch 183, Loss(train/val) 0.33637/0.24358. Took 0.05 sec\n",
            "Epoch 184, Loss(train/val) 0.33250/0.23849. Took 0.04 sec\n",
            "Epoch 185, Loss(train/val) 0.33579/0.23220. Took 0.05 sec\n",
            "Epoch 186, Loss(train/val) 0.33682/0.24271. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 0.34951/0.22593. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 0.33098/0.22916. Took 0.05 sec\n",
            "Epoch 189, Loss(train/val) 0.33434/0.23077. Took 0.04 sec\n",
            "Epoch 190, Loss(train/val) 0.33118/0.22732. Took 0.05 sec\n",
            "Epoch 191, Loss(train/val) 0.33285/0.22877. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 0.32025/0.22885. Took 0.05 sec\n",
            "Epoch 193, Loss(train/val) 0.33771/0.23107. Took 0.04 sec\n",
            "Epoch 194, Loss(train/val) 0.33241/0.25435. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 0.33365/0.24060. Took 0.05 sec\n",
            "Epoch 196, Loss(train/val) 0.33880/0.28395. Took 0.05 sec\n",
            "Epoch 197, Loss(train/val) 0.33342/0.23653. Took 0.05 sec\n",
            "Epoch 198, Loss(train/val) 0.31905/0.23331. Took 0.05 sec\n",
            "Epoch 199, Loss(train/val) 0.32353/0.22497. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 0.32323/0.23531. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 0.32868/0.23902. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 0.32750/0.22688. Took 0.05 sec\n",
            "Epoch 203, Loss(train/val) 0.31179/0.23724. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 0.32179/0.24843. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.32785/0.24000. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 0.31815/0.22737. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 0.32747/0.21856. Took 0.05 sec\n",
            "Epoch 208, Loss(train/val) 0.31721/0.22002. Took 0.05 sec\n",
            "Epoch 209, Loss(train/val) 0.32453/0.25309. Took 0.04 sec\n",
            "Epoch 210, Loss(train/val) 0.32111/0.22915. Took 0.05 sec\n",
            "Epoch 211, Loss(train/val) 0.31208/0.21806. Took 0.05 sec\n",
            "Epoch 212, Loss(train/val) 0.30772/0.24655. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 0.31084/0.26098. Took 0.05 sec\n",
            "Epoch 214, Loss(train/val) 0.31316/0.25684. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.30916/0.30404. Took 0.05 sec\n",
            "Epoch 216, Loss(train/val) 0.30028/0.28843. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 0.30043/0.28139. Took 0.05 sec\n",
            "Epoch 218, Loss(train/val) 0.29926/0.26077. Took 0.04 sec\n",
            "Epoch 219, Loss(train/val) 0.29377/0.23178. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 0.28972/0.23584. Took 0.05 sec\n",
            "Epoch 221, Loss(train/val) 0.29488/0.26385. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 0.29869/0.31414. Took 0.04 sec\n",
            "Epoch 223, Loss(train/val) 0.30457/0.38291. Took 0.04 sec\n",
            "Epoch 224, Loss(train/val) 0.29173/0.30168. Took 0.04 sec\n",
            "Epoch 225, Loss(train/val) 0.31652/0.28755. Took 0.05 sec\n",
            "Epoch 226, Loss(train/val) 0.29467/0.23522. Took 0.05 sec\n",
            "Epoch 227, Loss(train/val) 0.29125/0.28039. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 0.27853/0.46388. Took 0.05 sec\n",
            "Epoch 229, Loss(train/val) 0.27972/0.51244. Took 0.04 sec\n",
            "Epoch 230, Loss(train/val) 0.29287/0.39375. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 0.29716/0.36261. Took 0.05 sec\n",
            "Epoch 232, Loss(train/val) 0.27499/0.37489. Took 0.05 sec\n",
            "Epoch 233, Loss(train/val) 0.27436/0.36132. Took 0.04 sec\n",
            "Epoch 234, Loss(train/val) 0.28413/0.32525. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 0.27482/0.35526. Took 0.04 sec\n",
            "Epoch 236, Loss(train/val) 0.27814/0.41430. Took 0.05 sec\n",
            "Epoch 237, Loss(train/val) 0.26912/0.51553. Took 0.05 sec\n",
            "Epoch 238, Loss(train/val) 0.26571/0.51499. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 0.27057/0.43408. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 0.27724/0.39287. Took 0.04 sec\n",
            "Epoch 241, Loss(train/val) 0.27056/0.54173. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 0.26326/0.51469. Took 0.05 sec\n",
            "Epoch 243, Loss(train/val) 0.25592/0.38675. Took 0.04 sec\n",
            "Epoch 244, Loss(train/val) 0.26083/0.34768. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.25739/0.35764. Took 0.05 sec\n",
            "Epoch 246, Loss(train/val) 0.25694/0.33371. Took 0.05 sec\n",
            "Epoch 247, Loss(train/val) 0.26202/0.25001. Took 0.06 sec\n",
            "Epoch 248, Loss(train/val) 0.25026/0.24498. Took 0.05 sec\n",
            "Epoch 249, Loss(train/val) 0.25712/0.26637. Took 0.05 sec\n",
            "Epoch 250, Loss(train/val) 0.25657/0.38452. Took 0.06 sec\n",
            "Epoch 251, Loss(train/val) 0.26216/0.27390. Took 0.05 sec\n",
            "Epoch 252, Loss(train/val) 0.26047/0.25763. Took 0.05 sec\n",
            "Epoch 253, Loss(train/val) 0.25883/0.24671. Took 0.05 sec\n",
            "Epoch 254, Loss(train/val) 0.26262/0.25192. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.25268/0.19696. Took 0.05 sec\n",
            "Epoch 256, Loss(train/val) 0.25342/0.23137. Took 0.04 sec\n",
            "Epoch 257, Loss(train/val) 0.25575/0.32008. Took 0.05 sec\n",
            "Epoch 258, Loss(train/val) 0.25723/0.33680. Took 0.05 sec\n",
            "Epoch 259, Loss(train/val) 0.25372/0.32422. Took 0.05 sec\n",
            "Epoch 260, Loss(train/val) 0.24971/0.24762. Took 0.05 sec\n",
            "Epoch 261, Loss(train/val) 0.26208/0.18592. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.24548/0.19672. Took 0.04 sec\n",
            "Epoch 263, Loss(train/val) 0.26040/0.21838. Took 0.05 sec\n",
            "Epoch 264, Loss(train/val) 0.24257/0.21506. Took 0.04 sec\n",
            "Epoch 265, Loss(train/val) 0.27225/0.20623. Took 0.05 sec\n",
            "Epoch 266, Loss(train/val) 0.25199/0.24010. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 0.26124/0.22682. Took 0.04 sec\n",
            "Epoch 268, Loss(train/val) 0.24273/0.24481. Took 0.05 sec\n",
            "Epoch 269, Loss(train/val) 0.26004/0.22566. Took 0.05 sec\n",
            "Epoch 270, Loss(train/val) 0.25483/0.22037. Took 0.05 sec\n",
            "Epoch 271, Loss(train/val) 0.24630/0.20427. Took 0.05 sec\n",
            "Epoch 272, Loss(train/val) 0.25656/0.18145. Took 0.04 sec\n",
            "Epoch 273, Loss(train/val) 0.25396/0.17707. Took 0.05 sec\n",
            "Epoch 274, Loss(train/val) 0.23505/0.17820. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.24117/0.17888. Took 0.05 sec\n",
            "Epoch 276, Loss(train/val) 0.24655/0.18038. Took 0.04 sec\n",
            "Epoch 277, Loss(train/val) 0.25959/0.18385. Took 0.05 sec\n",
            "Epoch 278, Loss(train/val) 0.25250/0.16863. Took 0.05 sec\n",
            "Epoch 279, Loss(train/val) 0.24163/0.16715. Took 0.05 sec\n",
            "Epoch 280, Loss(train/val) 0.23595/0.16897. Took 0.05 sec\n",
            "Epoch 281, Loss(train/val) 0.24106/0.17719. Took 0.05 sec\n",
            "Epoch 282, Loss(train/val) 0.24435/0.19313. Took 0.04 sec\n",
            "Epoch 283, Loss(train/val) 0.23638/0.20397. Took 0.05 sec\n",
            "Epoch 284, Loss(train/val) 0.23661/0.17800. Took 0.05 sec\n",
            "Epoch 285, Loss(train/val) 0.24115/0.17826. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.23722/0.18147. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.24211/0.18495. Took 0.04 sec\n",
            "Epoch 288, Loss(train/val) 0.23577/0.18209. Took 0.04 sec\n",
            "Epoch 289, Loss(train/val) 0.24060/0.16540. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.25018/0.16692. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.23274/0.17051. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.25440/0.17714. Took 0.05 sec\n",
            "Epoch 293, Loss(train/val) 0.23421/0.19395. Took 0.04 sec\n",
            "Epoch 294, Loss(train/val) 0.23909/0.19264. Took 0.05 sec\n",
            "Epoch 295, Loss(train/val) 0.26094/0.16752. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 0.23586/0.16299. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.23626/0.16341. Took 0.05 sec\n",
            "Epoch 298, Loss(train/val) 0.24287/0.16288. Took 0.04 sec\n",
            "Epoch 299, Loss(train/val) 0.24059/0.16698. Took 0.04 sec\n",
            "Epoch 300, Loss(train/val) 0.23717/0.17402. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.23051/0.18349. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.24016/0.17666. Took 0.04 sec\n",
            "Epoch 303, Loss(train/val) 0.23491/0.16514. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 0.23054/0.17403. Took 0.05 sec\n",
            "Epoch 305, Loss(train/val) 0.23080/0.17137. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 0.27124/0.16149. Took 0.05 sec\n",
            "Epoch 307, Loss(train/val) 0.23748/0.16464. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.23783/0.18041. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.23972/0.17940. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.24083/0.16134. Took 0.06 sec\n",
            "Epoch 311, Loss(train/val) 0.24297/0.16336. Took 0.05 sec\n",
            "Epoch 312, Loss(train/val) 0.22936/0.16380. Took 0.04 sec\n",
            "Epoch 313, Loss(train/val) 0.23102/0.16074. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.24656/0.16673. Took 0.04 sec\n",
            "Epoch 315, Loss(train/val) 0.23276/0.16481. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.23854/0.15978. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.22668/0.16241. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.24950/0.17378. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.23158/0.16012. Took 0.05 sec\n",
            "Epoch 320, Loss(train/val) 0.23745/0.16068. Took 0.05 sec\n",
            "Epoch 321, Loss(train/val) 0.24912/0.16342. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.22988/0.16083. Took 0.04 sec\n",
            "Epoch 323, Loss(train/val) 0.23048/0.16231. Took 0.05 sec\n",
            "Epoch 324, Loss(train/val) 0.22632/0.16197. Took 0.04 sec\n",
            "Epoch 325, Loss(train/val) 0.23160/0.16869. Took 0.05 sec\n",
            "Epoch 326, Loss(train/val) 0.23736/0.18235. Took 0.05 sec\n",
            "Epoch 327, Loss(train/val) 0.22882/0.18008. Took 0.05 sec\n",
            "Epoch 328, Loss(train/val) 0.22692/0.16574. Took 0.05 sec\n",
            "Epoch 329, Loss(train/val) 0.24575/0.16593. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.22682/0.17295. Took 0.05 sec\n",
            "Epoch 331, Loss(train/val) 0.23211/0.16283. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 0.24605/0.15894. Took 0.06 sec\n",
            "Epoch 333, Loss(train/val) 0.23959/0.15896. Took 0.05 sec\n",
            "Epoch 334, Loss(train/val) 0.23492/0.15999. Took 0.05 sec\n",
            "Epoch 335, Loss(train/val) 0.23322/0.16211. Took 0.05 sec\n",
            "Epoch 336, Loss(train/val) 0.22916/0.16311. Took 0.05 sec\n",
            "Epoch 337, Loss(train/val) 0.23463/0.15944. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.22994/0.15978. Took 0.05 sec\n",
            "Epoch 339, Loss(train/val) 0.23287/0.15854. Took 0.05 sec\n",
            "Epoch 340, Loss(train/val) 0.25583/0.16516. Took 0.05 sec\n",
            "Epoch 341, Loss(train/val) 0.23938/0.15803. Took 0.05 sec\n",
            "Epoch 342, Loss(train/val) 0.22685/0.15794. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.24006/0.15991. Took 0.05 sec\n",
            "Epoch 344, Loss(train/val) 0.23461/0.15777. Took 0.04 sec\n",
            "Epoch 345, Loss(train/val) 0.22605/0.16110. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.23198/0.16751. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 0.22651/0.16081. Took 0.05 sec\n",
            "Epoch 348, Loss(train/val) 0.24487/0.16424. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 0.22923/0.16208. Took 0.05 sec\n",
            "Epoch 350, Loss(train/val) 0.23846/0.17535. Took 0.05 sec\n",
            "Epoch 351, Loss(train/val) 0.22416/0.17123. Took 0.04 sec\n",
            "Epoch 352, Loss(train/val) 0.23315/0.15882. Took 0.04 sec\n",
            "Epoch 353, Loss(train/val) 0.22964/0.15983. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 0.22831/0.16072. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 0.23635/0.15881. Took 0.05 sec\n",
            "Epoch 356, Loss(train/val) 0.22805/0.16243. Took 0.05 sec\n",
            "Epoch 357, Loss(train/val) 0.23244/0.15994. Took 0.04 sec\n",
            "Epoch 358, Loss(train/val) 0.22709/0.15903. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 0.22908/0.15735. Took 0.04 sec\n",
            "Epoch 360, Loss(train/val) 0.23956/0.15686. Took 0.05 sec\n",
            "Epoch 361, Loss(train/val) 0.23169/0.15645. Took 0.05 sec\n",
            "Epoch 362, Loss(train/val) 0.23620/0.16083. Took 0.05 sec\n",
            "Epoch 363, Loss(train/val) 0.22534/0.16641. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 0.23225/0.16709. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.23109/0.16285. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.22473/0.16343. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 0.22663/0.16334. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.25076/0.16755. Took 0.05 sec\n",
            "Epoch 369, Loss(train/val) 0.22670/0.15975. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.23495/0.16017. Took 0.05 sec\n",
            "Epoch 371, Loss(train/val) 0.25498/0.17399. Took 0.05 sec\n",
            "Epoch 372, Loss(train/val) 0.24738/0.15823. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 0.23439/0.15762. Took 0.05 sec\n",
            "Epoch 374, Loss(train/val) 0.23090/0.15684. Took 0.05 sec\n",
            "Epoch 375, Loss(train/val) 0.23196/0.15958. Took 0.06 sec\n",
            "Epoch 376, Loss(train/val) 0.23026/0.16293. Took 0.05 sec\n",
            "Epoch 377, Loss(train/val) 0.22493/0.16023. Took 0.04 sec\n",
            "Epoch 378, Loss(train/val) 0.22704/0.15660. Took 0.04 sec\n",
            "Epoch 379, Loss(train/val) 0.24843/0.15555. Took 0.05 sec\n",
            "Epoch 380, Loss(train/val) 0.22298/0.15528. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 0.22965/0.15714. Took 0.04 sec\n",
            "Epoch 382, Loss(train/val) 0.23062/0.15698. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.23428/0.15819. Took 0.05 sec\n",
            "Epoch 384, Loss(train/val) 0.27216/0.19353. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 0.24140/0.18599. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.22722/0.16831. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.23567/0.17604. Took 0.04 sec\n",
            "Epoch 388, Loss(train/val) 0.23324/0.15851. Took 0.05 sec\n",
            "Epoch 389, Loss(train/val) 0.22184/0.15621. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.23207/0.15676. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.22483/0.16184. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.22977/0.15816. Took 0.04 sec\n",
            "Epoch 393, Loss(train/val) 0.23580/0.15446. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 0.23292/0.15574. Took 0.05 sec\n",
            "Epoch 395, Loss(train/val) 0.23905/0.15926. Took 0.06 sec\n",
            "Epoch 396, Loss(train/val) 0.22507/0.15987. Took 0.05 sec\n",
            "Epoch 397, Loss(train/val) 0.23042/0.18954. Took 0.04 sec\n",
            "Epoch 398, Loss(train/val) 0.22247/0.17715. Took 0.05 sec\n",
            "Epoch 399, Loss(train/val) 0.22895/0.17780. Took 0.05 sec\n",
            "Epoch 400, Loss(train/val) 0.22553/0.17148. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 0.23624/0.15680. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.22466/0.15588. Took 0.04 sec\n",
            "Epoch 403, Loss(train/val) 0.22937/0.15931. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 0.22624/0.15910. Took 0.05 sec\n",
            "Epoch 405, Loss(train/val) 0.22585/0.15789. Took 0.06 sec\n",
            "Epoch 406, Loss(train/val) 0.25073/0.15464. Took 0.05 sec\n",
            "Epoch 407, Loss(train/val) 0.22723/0.15619. Took 0.05 sec\n",
            "Epoch 408, Loss(train/val) 0.23597/0.15522. Took 0.05 sec\n",
            "Epoch 409, Loss(train/val) 0.24243/0.15529. Took 0.05 sec\n",
            "Epoch 410, Loss(train/val) 0.22529/0.16624. Took 0.05 sec\n",
            "Epoch 411, Loss(train/val) 0.22864/0.17670. Took 0.05 sec\n",
            "Epoch 412, Loss(train/val) 0.23126/0.17364. Took 0.05 sec\n",
            "Epoch 413, Loss(train/val) 0.22645/0.18577. Took 0.04 sec\n",
            "Epoch 414, Loss(train/val) 0.21750/0.18756. Took 0.04 sec\n",
            "Epoch 415, Loss(train/val) 0.23923/0.16926. Took 0.05 sec\n",
            "Epoch 416, Loss(train/val) 0.23180/0.15553. Took 0.04 sec\n",
            "Epoch 417, Loss(train/val) 0.23915/0.15527. Took 0.05 sec\n",
            "Epoch 418, Loss(train/val) 0.23147/0.15640. Took 0.05 sec\n",
            "Epoch 419, Loss(train/val) 0.21690/0.16089. Took 0.04 sec\n",
            "Epoch 420, Loss(train/val) 0.23055/0.15742. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.23110/0.15452. Took 0.05 sec\n",
            "Epoch 422, Loss(train/val) 0.22187/0.15688. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.23796/0.15487. Took 0.04 sec\n",
            "Epoch 424, Loss(train/val) 0.24997/0.15430. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.23088/0.15525. Took 0.06 sec\n",
            "Epoch 426, Loss(train/val) 0.22780/0.15446. Took 0.05 sec\n",
            "Epoch 427, Loss(train/val) 0.23746/0.16207. Took 0.05 sec\n",
            "Epoch 428, Loss(train/val) 0.22843/0.18412. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.23748/0.19666. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.22947/0.20307. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.23024/0.21039. Took 0.05 sec\n",
            "Epoch 432, Loss(train/val) 0.22094/0.17950. Took 0.05 sec\n",
            "Epoch 433, Loss(train/val) 0.24442/0.15425. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.23177/0.16129. Took 0.05 sec\n",
            "Epoch 435, Loss(train/val) 0.23176/0.16703. Took 0.05 sec\n",
            "Epoch 436, Loss(train/val) 0.25943/0.16338. Took 0.05 sec\n",
            "Epoch 437, Loss(train/val) 0.24250/0.15915. Took 0.05 sec\n",
            "Epoch 438, Loss(train/val) 0.22330/0.15589. Took 0.05 sec\n",
            "Epoch 439, Loss(train/val) 0.23259/0.15293. Took 0.05 sec\n",
            "Epoch 440, Loss(train/val) 0.22753/0.15307. Took 0.05 sec\n",
            "Epoch 441, Loss(train/val) 0.23270/0.15646. Took 0.05 sec\n",
            "Epoch 442, Loss(train/val) 0.22602/0.15685. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 0.22300/0.15718. Took 0.05 sec\n",
            "Epoch 444, Loss(train/val) 0.24815/0.15523. Took 0.05 sec\n",
            "Epoch 445, Loss(train/val) 0.22703/0.17312. Took 0.05 sec\n",
            "Epoch 446, Loss(train/val) 0.22150/0.16588. Took 0.05 sec\n",
            "Epoch 447, Loss(train/val) 0.23008/0.18013. Took 0.04 sec\n",
            "Epoch 448, Loss(train/val) 0.22600/0.18670. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 0.22998/0.19680. Took 0.05 sec\n",
            "Epoch 450, Loss(train/val) 0.22458/0.18099. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 0.22541/0.16110. Took 0.05 sec\n",
            "Epoch 452, Loss(train/val) 0.25512/0.15823. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.24550/0.15814. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.22676/0.15310. Took 0.04 sec\n",
            "Epoch 455, Loss(train/val) 0.23578/0.15389. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.23272/0.15642. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.24006/0.16305. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.22881/0.17402. Took 0.05 sec\n",
            "Epoch 459, Loss(train/val) 0.22093/0.16153. Took 0.05 sec\n",
            "Epoch 460, Loss(train/val) 0.22975/0.15708. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.22833/0.15580. Took 0.05 sec\n",
            "Epoch 462, Loss(train/val) 0.22242/0.15958. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.23997/0.16243. Took 0.04 sec\n",
            "Epoch 464, Loss(train/val) 0.22444/0.16385. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.24086/0.15429. Took 0.05 sec\n",
            "Epoch 466, Loss(train/val) 0.22289/0.16119. Took 0.05 sec\n",
            "Epoch 467, Loss(train/val) 0.23876/0.17161. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.23489/0.16443. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 0.23189/0.15801. Took 0.04 sec\n",
            "Epoch 470, Loss(train/val) 0.22408/0.15919. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 0.21730/0.15608. Took 0.05 sec\n",
            "Epoch 472, Loss(train/val) 0.22390/0.15465. Took 0.05 sec\n",
            "Epoch 473, Loss(train/val) 0.22597/0.15603. Took 0.05 sec\n",
            "Epoch 474, Loss(train/val) 0.22278/0.16023. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 0.23300/0.15659. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.22668/0.16153. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.23033/0.15839. Took 0.05 sec\n",
            "Epoch 478, Loss(train/val) 0.23479/0.15590. Took 0.05 sec\n",
            "Epoch 479, Loss(train/val) 0.22658/0.15495. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.23689/0.15335. Took 0.05 sec\n",
            "Epoch 481, Loss(train/val) 0.22849/0.15309. Took 0.05 sec\n",
            "Epoch 482, Loss(train/val) 0.21523/0.15416. Took 0.04 sec\n",
            "Epoch 483, Loss(train/val) 0.23117/0.15863. Took 0.05 sec\n",
            "Epoch 484, Loss(train/val) 0.22806/0.15969. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.23697/0.17057. Took 0.06 sec\n",
            "Epoch 486, Loss(train/val) 0.22506/0.17927. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.22930/0.18812. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.23468/0.15951. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.22886/0.15258. Took 0.04 sec\n",
            "Epoch 490, Loss(train/val) 0.23486/0.15224. Took 0.05 sec\n",
            "Epoch 491, Loss(train/val) 0.22822/0.15695. Took 0.05 sec\n",
            "Epoch 492, Loss(train/val) 0.23396/0.16264. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.22345/0.15548. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.23872/0.16607. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.22556/0.17252. Took 0.05 sec\n",
            "Epoch 496, Loss(train/val) 0.22523/0.16695. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 0.22624/0.15877. Took 0.05 sec\n",
            "Epoch 498, Loss(train/val) 0.22067/0.15262. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.22714/0.15662. Took 0.05 sec\n",
            "Epoch 500, Loss(train/val) 0.23057/0.16688. Took 0.05 sec\n",
            "Epoch 501, Loss(train/val) 0.23579/0.16723. Took 0.05 sec\n",
            "Epoch 502, Loss(train/val) 0.23020/0.15990. Took 0.05 sec\n",
            "Epoch 503, Loss(train/val) 0.23821/0.16299. Took 0.05 sec\n",
            "Epoch 504, Loss(train/val) 0.24518/0.15470. Took 0.05 sec\n",
            "Epoch 505, Loss(train/val) 0.22988/0.17382. Took 0.05 sec\n",
            "Epoch 506, Loss(train/val) 0.22628/0.16831. Took 0.05 sec\n",
            "Epoch 507, Loss(train/val) 0.22359/0.16935. Took 0.05 sec\n",
            "Epoch 508, Loss(train/val) 0.21862/0.15664. Took 0.05 sec\n",
            "Epoch 509, Loss(train/val) 0.22192/0.15489. Took 0.05 sec\n",
            "Epoch 510, Loss(train/val) 0.22425/0.15797. Took 0.06 sec\n",
            "Epoch 511, Loss(train/val) 0.22317/0.15253. Took 0.05 sec\n",
            "Epoch 512, Loss(train/val) 0.22657/0.15438. Took 0.05 sec\n",
            "Epoch 513, Loss(train/val) 0.24747/0.15408. Took 0.05 sec\n",
            "Epoch 514, Loss(train/val) 0.22495/0.15440. Took 0.05 sec\n",
            "Epoch 515, Loss(train/val) 0.22239/0.15381. Took 0.05 sec\n",
            "Epoch 516, Loss(train/val) 0.22382/0.15227. Took 0.05 sec\n",
            "Epoch 517, Loss(train/val) 0.22784/0.15186. Took 0.05 sec\n",
            "Epoch 518, Loss(train/val) 0.22508/0.15202. Took 0.05 sec\n",
            "Epoch 519, Loss(train/val) 0.21987/0.16746. Took 0.06 sec\n",
            "Epoch 520, Loss(train/val) 0.23201/0.18368. Took 0.05 sec\n",
            "Epoch 521, Loss(train/val) 0.21915/0.20746. Took 0.05 sec\n",
            "Epoch 522, Loss(train/val) 0.22826/0.20723. Took 0.06 sec\n",
            "Epoch 523, Loss(train/val) 0.23364/0.18894. Took 0.06 sec\n",
            "Epoch 524, Loss(train/val) 0.22663/0.15732. Took 0.05 sec\n",
            "Epoch 525, Loss(train/val) 0.22289/0.15182. Took 0.05 sec\n",
            "Epoch 526, Loss(train/val) 0.22393/0.16022. Took 0.05 sec\n",
            "Epoch 527, Loss(train/val) 0.21903/0.15792. Took 0.06 sec\n",
            "Epoch 528, Loss(train/val) 0.22592/0.15363. Took 0.06 sec\n",
            "Epoch 529, Loss(train/val) 0.22204/0.15225. Took 0.05 sec\n",
            "Epoch 530, Loss(train/val) 0.22873/0.15993. Took 0.05 sec\n",
            "Epoch 531, Loss(train/val) 0.21738/0.16010. Took 0.05 sec\n",
            "Epoch 532, Loss(train/val) 0.23803/0.15293. Took 0.05 sec\n",
            "Epoch 533, Loss(train/val) 0.22240/0.16251. Took 0.05 sec\n",
            "Epoch 534, Loss(train/val) 0.23744/0.16640. Took 0.05 sec\n",
            "Epoch 535, Loss(train/val) 0.22908/0.18043. Took 0.05 sec\n",
            "Epoch 536, Loss(train/val) 0.23056/0.18322. Took 0.06 sec\n",
            "Epoch 537, Loss(train/val) 0.22810/0.18030. Took 0.05 sec\n",
            "Epoch 538, Loss(train/val) 0.23507/0.17557. Took 0.05 sec\n",
            "Epoch 539, Loss(train/val) 0.23426/0.16626. Took 0.05 sec\n",
            "Epoch 540, Loss(train/val) 0.22295/0.15806. Took 0.05 sec\n",
            "Epoch 541, Loss(train/val) 0.21993/0.15528. Took 0.06 sec\n",
            "Epoch 542, Loss(train/val) 0.22657/0.15872. Took 0.05 sec\n",
            "Epoch 543, Loss(train/val) 0.22224/0.16695. Took 0.05 sec\n",
            "Epoch 544, Loss(train/val) 0.22912/0.16289. Took 0.05 sec\n",
            "Epoch 545, Loss(train/val) 0.23874/0.17985. Took 0.05 sec\n",
            "Epoch 546, Loss(train/val) 0.23118/0.17070. Took 0.06 sec\n",
            "Epoch 547, Loss(train/val) 0.23475/0.15223. Took 0.05 sec\n",
            "Epoch 548, Loss(train/val) 0.22500/0.15421. Took 0.05 sec\n",
            "Epoch 549, Loss(train/val) 0.22161/0.16711. Took 0.05 sec\n",
            "Epoch 550, Loss(train/val) 0.22244/0.18643. Took 0.05 sec\n",
            "Epoch 551, Loss(train/val) 0.22335/0.18850. Took 0.05 sec\n",
            "Epoch 552, Loss(train/val) 0.22867/0.17442. Took 0.05 sec\n",
            "Epoch 553, Loss(train/val) 0.23363/0.15967. Took 0.05 sec\n",
            "Epoch 554, Loss(train/val) 0.22457/0.15731. Took 0.05 sec\n",
            "Epoch 555, Loss(train/val) 0.22895/0.16142. Took 0.05 sec\n",
            "Epoch 556, Loss(train/val) 0.22331/0.15390. Took 0.06 sec\n",
            "Epoch 557, Loss(train/val) 0.23715/0.15250. Took 0.05 sec\n",
            "Epoch 558, Loss(train/val) 0.23278/0.15596. Took 0.05 sec\n",
            "Epoch 559, Loss(train/val) 0.22978/0.15402. Took 0.05 sec\n",
            "Epoch 560, Loss(train/val) 0.22198/0.15299. Took 0.05 sec\n",
            "Epoch 561, Loss(train/val) 0.23014/0.15479. Took 0.05 sec\n",
            "Epoch 562, Loss(train/val) 0.22439/0.15718. Took 0.06 sec\n",
            "Epoch 563, Loss(train/val) 0.23988/0.17717. Took 0.05 sec\n",
            "Epoch 564, Loss(train/val) 0.22388/0.19057. Took 0.05 sec\n",
            "Epoch 565, Loss(train/val) 0.22777/0.19565. Took 0.05 sec\n",
            "Epoch 566, Loss(train/val) 0.22500/0.17508. Took 0.05 sec\n",
            "Epoch 567, Loss(train/val) 0.22887/0.16888. Took 0.05 sec\n",
            "Epoch 568, Loss(train/val) 0.23926/0.15120. Took 0.05 sec\n",
            "Epoch 569, Loss(train/val) 0.21638/0.15984. Took 0.04 sec\n",
            "Epoch 570, Loss(train/val) 0.22349/0.16541. Took 0.05 sec\n",
            "Epoch 571, Loss(train/val) 0.22566/0.15825. Took 0.05 sec\n",
            "Epoch 572, Loss(train/val) 0.23003/0.15219. Took 0.05 sec\n",
            "Epoch 573, Loss(train/val) 0.21931/0.15478. Took 0.05 sec\n",
            "Epoch 574, Loss(train/val) 0.23591/0.15168. Took 0.05 sec\n",
            "Epoch 575, Loss(train/val) 0.22212/0.15259. Took 0.05 sec\n",
            "Epoch 576, Loss(train/val) 0.23360/0.16220. Took 0.05 sec\n",
            "Epoch 577, Loss(train/val) 0.21420/0.16566. Took 0.05 sec\n",
            "Epoch 578, Loss(train/val) 0.22223/0.16074. Took 0.05 sec\n",
            "Epoch 579, Loss(train/val) 0.22555/0.15235. Took 0.05 sec\n",
            "Epoch 580, Loss(train/val) 0.21968/0.15114. Took 0.04 sec\n",
            "Epoch 581, Loss(train/val) 0.25325/0.15224. Took 0.05 sec\n",
            "Epoch 582, Loss(train/val) 0.21939/0.16771. Took 0.05 sec\n",
            "Epoch 583, Loss(train/val) 0.21758/0.17735. Took 0.05 sec\n",
            "Epoch 584, Loss(train/val) 0.23124/0.18623. Took 0.04 sec\n",
            "Epoch 585, Loss(train/val) 0.22491/0.19275. Took 0.05 sec\n",
            "Epoch 586, Loss(train/val) 0.21989/0.17897. Took 0.05 sec\n",
            "Epoch 587, Loss(train/val) 0.22903/0.15199. Took 0.05 sec\n",
            "Epoch 588, Loss(train/val) 0.24959/0.16178. Took 0.05 sec\n",
            "Epoch 589, Loss(train/val) 0.23205/0.15677. Took 0.05 sec\n",
            "Epoch 590, Loss(train/val) 0.22528/0.17162. Took 0.05 sec\n",
            "Epoch 591, Loss(train/val) 0.22954/0.15596. Took 0.05 sec\n",
            "Epoch 592, Loss(train/val) 0.22727/0.15362. Took 0.05 sec\n",
            "Epoch 593, Loss(train/val) 0.22703/0.15325. Took 0.05 sec\n",
            "Epoch 594, Loss(train/val) 0.22536/0.15238. Took 0.05 sec\n",
            "Epoch 595, Loss(train/val) 0.23104/0.15559. Took 0.05 sec\n",
            "Epoch 596, Loss(train/val) 0.21746/0.15581. Took 0.05 sec\n",
            "Epoch 597, Loss(train/val) 0.22888/0.16074. Took 0.05 sec\n",
            "Epoch 598, Loss(train/val) 0.23643/0.15822. Took 0.05 sec\n",
            "Epoch 599, Loss(train/val) 0.24153/0.16356. Took 0.04 sec\n",
            "Epoch 600, Loss(train/val) 0.23273/0.15464. Took 0.05 sec\n",
            "Epoch 601, Loss(train/val) 0.21951/0.16450. Took 0.05 sec\n",
            "Epoch 602, Loss(train/val) 0.22549/0.17140. Took 0.05 sec\n",
            "Epoch 603, Loss(train/val) 0.22865/0.16812. Took 0.05 sec\n",
            "Epoch 604, Loss(train/val) 0.23005/0.15125. Took 0.06 sec\n",
            "Epoch 605, Loss(train/val) 0.21938/0.15378. Took 0.05 sec\n",
            "Epoch 606, Loss(train/val) 0.22289/0.15915. Took 0.05 sec\n",
            "Epoch 607, Loss(train/val) 0.21906/0.16669. Took 0.05 sec\n",
            "Epoch 608, Loss(train/val) 0.22272/0.16838. Took 0.05 sec\n",
            "Epoch 609, Loss(train/val) 0.22240/0.16841. Took 0.05 sec\n",
            "Epoch 610, Loss(train/val) 0.21930/0.17499. Took 0.04 sec\n",
            "Epoch 611, Loss(train/val) 0.22069/0.16625. Took 0.05 sec\n",
            "Epoch 612, Loss(train/val) 0.22467/0.15398. Took 0.05 sec\n",
            "Epoch 613, Loss(train/val) 0.21923/0.15402. Took 0.05 sec\n",
            "Epoch 614, Loss(train/val) 0.22560/0.15475. Took 0.05 sec\n",
            "Epoch 615, Loss(train/val) 0.22941/0.15329. Took 0.05 sec\n",
            "Epoch 616, Loss(train/val) 0.22771/0.15670. Took 0.05 sec\n",
            "Epoch 617, Loss(train/val) 0.22527/0.16004. Took 0.04 sec\n",
            "Epoch 618, Loss(train/val) 0.21496/0.16311. Took 0.04 sec\n",
            "Epoch 619, Loss(train/val) 0.22225/0.16365. Took 0.04 sec\n",
            "Epoch 620, Loss(train/val) 0.21907/0.15738. Took 0.04 sec\n",
            "Epoch 621, Loss(train/val) 0.22007/0.15276. Took 0.05 sec\n",
            "Epoch 622, Loss(train/val) 0.21872/0.15190. Took 0.05 sec\n",
            "Epoch 623, Loss(train/val) 0.23519/0.15157. Took 0.05 sec\n",
            "Epoch 624, Loss(train/val) 0.22300/0.15722. Took 0.04 sec\n",
            "Epoch 625, Loss(train/val) 0.22543/0.16031. Took 0.05 sec\n",
            "Epoch 626, Loss(train/val) 0.22747/0.15464. Took 0.06 sec\n",
            "Epoch 627, Loss(train/val) 0.22636/0.15813. Took 0.05 sec\n",
            "Epoch 628, Loss(train/val) 0.22276/0.15478. Took 0.04 sec\n",
            "Epoch 629, Loss(train/val) 0.22143/0.15457. Took 0.05 sec\n",
            "Epoch 630, Loss(train/val) 0.23338/0.15276. Took 0.05 sec\n",
            "Epoch 631, Loss(train/val) 0.23787/0.16296. Took 0.05 sec\n",
            "Epoch 632, Loss(train/val) 0.22425/0.17120. Took 0.04 sec\n",
            "Epoch 633, Loss(train/val) 0.22760/0.16850. Took 0.05 sec\n",
            "Epoch 634, Loss(train/val) 0.24561/0.15533. Took 0.05 sec\n",
            "Epoch 635, Loss(train/val) 0.21810/0.15160. Took 0.05 sec\n",
            "Epoch 636, Loss(train/val) 0.25014/0.15285. Took 0.05 sec\n",
            "Epoch 637, Loss(train/val) 0.22569/0.15266. Took 0.05 sec\n",
            "Epoch 638, Loss(train/val) 0.22143/0.15305. Took 0.04 sec\n",
            "Epoch 639, Loss(train/val) 0.22630/0.15346. Took 0.05 sec\n",
            "Epoch 640, Loss(train/val) 0.21677/0.15579. Took 0.04 sec\n",
            "Epoch 641, Loss(train/val) 0.21808/0.15333. Took 0.05 sec\n",
            "Epoch 642, Loss(train/val) 0.24062/0.15535. Took 0.04 sec\n",
            "Epoch 643, Loss(train/val) 0.21330/0.15385. Took 0.04 sec\n",
            "Epoch 644, Loss(train/val) 0.22068/0.15499. Took 0.04 sec\n",
            "Epoch 645, Loss(train/val) 0.22296/0.15399. Took 0.04 sec\n",
            "Epoch 646, Loss(train/val) 0.22225/0.15555. Took 0.05 sec\n",
            "Epoch 647, Loss(train/val) 0.22915/0.15846. Took 0.05 sec\n",
            "Epoch 648, Loss(train/val) 0.22628/0.15389. Took 0.05 sec\n",
            "Epoch 649, Loss(train/val) 0.23939/0.15176. Took 0.05 sec\n",
            "Epoch 650, Loss(train/val) 0.22807/0.15100. Took 0.05 sec\n",
            "Epoch 651, Loss(train/val) 0.22001/0.15354. Took 0.05 sec\n",
            "Epoch 652, Loss(train/val) 0.22926/0.16242. Took 0.05 sec\n",
            "Epoch 653, Loss(train/val) 0.22143/0.16918. Took 0.04 sec\n",
            "Epoch 654, Loss(train/val) 0.22464/0.16706. Took 0.05 sec\n",
            "Epoch 655, Loss(train/val) 0.22367/0.15277. Took 0.05 sec\n",
            "Epoch 656, Loss(train/val) 0.21900/0.15654. Took 0.05 sec\n",
            "Epoch 657, Loss(train/val) 0.22518/0.16648. Took 0.05 sec\n",
            "Epoch 658, Loss(train/val) 0.23450/0.18268. Took 0.05 sec\n",
            "Epoch 659, Loss(train/val) 0.22906/0.17339. Took 0.05 sec\n",
            "Epoch 660, Loss(train/val) 0.22224/0.17953. Took 0.05 sec\n",
            "Epoch 661, Loss(train/val) 0.22483/0.16109. Took 0.05 sec\n",
            "Epoch 662, Loss(train/val) 0.22051/0.15344. Took 0.05 sec\n",
            "Epoch 663, Loss(train/val) 0.22191/0.15544. Took 0.04 sec\n",
            "Epoch 664, Loss(train/val) 0.22151/0.15319. Took 0.04 sec\n",
            "Epoch 665, Loss(train/val) 0.21589/0.15259. Took 0.05 sec\n",
            "Epoch 666, Loss(train/val) 0.21988/0.15095. Took 0.05 sec\n",
            "Epoch 667, Loss(train/val) 0.22566/0.15122. Took 0.05 sec\n",
            "Epoch 668, Loss(train/val) 0.22869/0.15112. Took 0.05 sec\n",
            "Epoch 669, Loss(train/val) 0.21675/0.15418. Took 0.05 sec\n",
            "Epoch 670, Loss(train/val) 0.22789/0.16835. Took 0.05 sec\n",
            "Epoch 671, Loss(train/val) 0.21845/0.16825. Took 0.05 sec\n",
            "Epoch 672, Loss(train/val) 0.21820/0.16052. Took 0.05 sec\n",
            "Epoch 673, Loss(train/val) 0.23288/0.15375. Took 0.05 sec\n",
            "Epoch 674, Loss(train/val) 0.23285/0.16353. Took 0.05 sec\n",
            "Epoch 675, Loss(train/val) 0.24051/0.15547. Took 0.05 sec\n",
            "Epoch 676, Loss(train/val) 0.22164/0.15556. Took 0.05 sec\n",
            "Epoch 677, Loss(train/val) 0.22620/0.16440. Took 0.05 sec\n",
            "Epoch 678, Loss(train/val) 0.22030/0.16178. Took 0.05 sec\n",
            "Epoch 679, Loss(train/val) 0.21845/0.15567. Took 0.05 sec\n",
            "Epoch 680, Loss(train/val) 0.21792/0.16670. Took 0.04 sec\n",
            "Epoch 681, Loss(train/val) 0.22950/0.18040. Took 0.05 sec\n",
            "Epoch 682, Loss(train/val) 0.22926/0.16860. Took 0.04 sec\n",
            "Epoch 683, Loss(train/val) 0.22965/0.15458. Took 0.05 sec\n",
            "Epoch 684, Loss(train/val) 0.22182/0.15171. Took 0.05 sec\n",
            "Epoch 685, Loss(train/val) 0.21726/0.15184. Took 0.04 sec\n",
            "Epoch 686, Loss(train/val) 0.22049/0.15282. Took 0.05 sec\n",
            "Epoch 687, Loss(train/val) 0.21332/0.15378. Took 0.04 sec\n",
            "Epoch 688, Loss(train/val) 0.21453/0.15129. Took 0.05 sec\n",
            "Epoch 689, Loss(train/val) 0.21736/0.15097. Took 0.05 sec\n",
            "Epoch 690, Loss(train/val) 0.21739/0.15594. Took 0.06 sec\n",
            "Epoch 691, Loss(train/val) 0.23203/0.16799. Took 0.05 sec\n",
            "Epoch 692, Loss(train/val) 0.22463/0.15578. Took 0.05 sec\n",
            "Epoch 693, Loss(train/val) 0.24649/0.15781. Took 0.05 sec\n",
            "Epoch 694, Loss(train/val) 0.23010/0.16008. Took 0.05 sec\n",
            "Epoch 695, Loss(train/val) 0.21583/0.17117. Took 0.05 sec\n",
            "Epoch 696, Loss(train/val) 0.22451/0.17962. Took 0.05 sec\n",
            "Epoch 697, Loss(train/val) 0.22449/0.16855. Took 0.05 sec\n",
            "Epoch 698, Loss(train/val) 0.22590/0.16192. Took 0.05 sec\n",
            "Epoch 699, Loss(train/val) 0.21843/0.15894. Took 0.05 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var1 = 'lr'\n",
        "df = load_exp_result('exp3_lr')\n",
        "\n",
        "plot_loss_variation(var1, df, sharey=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "9uEcoUpnytpc",
        "outputId": "42c1c59d-2635-4ef2-f703-e5fd140911ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 923.375x216 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5YAAADXCAYAAABh584qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxU9bn48c8zWVgDhF0MCSJI2aoSJOBuXapoq4J1iVbt4nLb/lrb3l4ptdbaDW837a1XXGrVXkVtKWoVFJdqqOwglV0gJoQtkBAgEMgy8/z+OGeSmckkmSQzmZnkeb9eeWXOOt/AOTPn+S7PV1QVY4wxxhhjjDGmrTzxLoAxxhhjjDHGmORmgaUxxhhjjDHGmHaxwNIYY4wxxhhjTLtYYGmMMcYYY4wxpl0ssDTGGGOMMcYY0y4WWBpjjDHGGGOMaRcLLI0xxnQIEVkkIre18dgiEbkk2mVKRCJyu4j8q4ltI0RERSS1o8tljDHGNMcCS2OMMU0SkaMBPz4ROR6wfHNrzqWqV6jqs7Eqa6IQkZNFpE5ETg2zbYGI/CYe5TLGGGNiyQJLY4wxTVLV3v4fYCfwhYB1z/v3sxa0Bqq6G3gX+HLgehHpD0wHOn1wbYwxpuuxwNIYY0yriciFIrJLRO4VkX3An0UkU0ReF5EDIlLhvs4KOOZ9Efm6+/p2EfmXiPzG3fdTEbkiwvfuJiIPi8ge9+dhEenmbhvovu8hETkoIktExONuu1dEdotIpYhsFZGLw5w7T0T2iUhKwLprReRj9/UUEVktIkdEpFREftdEMZ8lJLAEbgQ2qep6EZklIjvcsmwSkWsj+dvDlHeYiLzm/q3bReSOgG1hyyoi3UXk/0Sk3P13WiUiQ9ry/sYYY4yfBZbGGGPaaijQH8gB7sT5Tvmzu5wNHAf+2MzxecBWYCDw38CfREQieN8fAVOBM4DTgSnAfe627wO7gEHAEGA2oCIyBvgWcJaqZgCfB4pCT6yqK4BjwOcCVucDL7ivHwEeUdU+wKnAy02UcQEwUETODVj3ZRpaK3cA5wF9gZ8C/yciJ0Xwt4d6EefvHQZcB/xSRPxlb6qst7nvOxwYANyN839ljDHGtJkFlsYYY9rKB/xEVatV9biqlqvqfFWtUtVK4BfABc0cX6yqT6qqFyfgOgknGGzJzcCDqrpfVQ/gBGb+1sFa9zw5qlqrqktUVQEv0A0YJyJpqlqkqjuaOP884CYAEcnA6b46L+D8o0RkoKoeVdXl4U6gqseBvwK3uucZDeTiBqiq+ldV3aOqPlV9CdiGEyBHTESGA+cA96rqCVVdBzzlf89mylqLE1COUlWvqq5R1SOteW9jjDEmlAWWSUZEjsbgnJ8RkWUiUi0i/9nGc/zQ7Ya1VUQ+H7C+SETWi8g6EVkdvVIb0zYxuoeuFpGP/dd5SCtVJMeLiPzBvYc+FpFJAdu87nnXichr0S57Ox1Q1RP+BRHpKSKPi0ixiBwBCoB+gd1KQ+zzv1DVKvdl7wjedxhQHLBc7K4D+DWwHVgsIoUiMss9/3bgHuABYL+IvCgiwwjvBWCG2712BrBWVf3v9zXgNGCL24X0qmbK+SzwJRHpjhP4vqWq+wFE5Fb3//SQiBwCJuC03LbGMOCgG8T7FQMnt1DWvwBvAS+6XYn/W0TSWnqzGN07TV77Ifvlut8l2939xV3fX0TeFpFt7u/Mls4rIm+6/+6vR/vvMaY5neEeEpEcEVnrfn5tFJG7o/03meRlgWUnIO1PmnEQ+DbQpkyFIjIOZ+zQeOBy4H9DHiQvUtUzVHVyO8tpTExE4R56FzhdVc8AvorTatQaVwCj3Z87gccCth13758zVPWL7SxntGnI8veBMUCe2/3yfHd9JN1bW2MPTndbv2x3HapaqarfV9WRwBeB74k7llJVX1DVc91jFXgo3MlVdRNOgHYFwd1gUdVtqnoTMNg9/m8i0quJcv4L5/P1auAW3G6wIpIDPInTNXeAqvYDNtD6f6c9QH9xWlUD/y12N1dWtyX3p6o6DjgbuIqGVs5WicK909y1H+gx4I6AfS93188C3lXV0Tj34awIzvtrGo9/NSYukvAe2gtMc7/v8oBZzVTSmS7GAsskJU7ijCVuC8am9pzL7U62Cqd7VOj73CIiK92aqcebaHm4GnjR7Q73KU5rQau6dBnT0aJ8Dx11u1sC9CIg4BKRH7itRR+LyE+bOMXVwHPqWI7TyteW8XbxloEzVu+QOBlQfxKj95kH3Ccig0RkIHA/8H8AInKViIxya+MP43SB9YnIGBH5nNsKecItp6+Z93gB+A5OcPxX/0r3M3GQqvqAQ+7qsOdxr4nncIK6fsA/3E3+a+SAe86v4LRYtoqqlgBLgV+Jk5DnszitlP5/i7BlFZGLRGSi+3l+BOezv7l/iyDRvHeI4Np3l/uo6vKAf9NrAo73j1t9NmR92POq6rtAYCuvMR0qme8hVa1R1Wp3n25YLGEC2MWQ3CYB31HV00I3iMhL0tB9LvAn4lppERkL3ACc49ZMeXHGNoU6GSgJWN5FQ1csxemStkZE7oz0vY3pIFG7h8TJHLoFeAOn1RIRuQyntncKTqKZXBE5P8zhzd1D3cXpXrtcRK5pfGhCeRjoAZQBy4E3Y/Q+PwdWAx8D64G17jpw/r3fAY4Cy4D/VdV/4jwAzXHLtg+nFe+HzbzHPJzxoe+palnA+suBjW6XtkeAG93xlE15DqcV8SX/w5jbIvpbt3ylwETgw4j+8sZuAkbgtF4uwBnz+k4LZR0K/A0nqNwMfIDTPbY1onXvNHftB+6zq4l9hqjqXvf1PhrG6EZyXmPiKWnvIREZLk6m7BLgIVXd09Ifa7oGm3csua10WwgbUdUbonD+i3GSTaxyu+L3APa38hznqupuERkMvC0iW1S1IAplMyYaonYPqeoCYIEbOP4MuAS4zP35yN2tN07g05p7IMe9h0YC74nI+maSzsSUqo4IeP0+kBWyfQ9wYchhjwdsvzDg9TPAMyHHN9kVNOS9T+B03/92mP1+D/w+zPqPaUVPClXdSZjKV1W9JdJzuPt/2sR5foST3TbcMc8Q8m8TsK2IgC6zqroLpytruH3DllVV59GQjKitYv3902qqqiIS2j3bmESVtPeQ21vis24X2FdE5G+qWhr7EppEZ4FlcjvW1AYReQlnrFOo36nqcxGeX4BnVTWoVl+c+db8Xdy+jjOeZ3jALlk0jPHx/94vIgtwHuwssDSJIur3kKoWiMhIt4umAL9S1ccD9xGRb+KMdQEn42gk91ChiLwPnIkzVYUx8RSte6fJaz9kn6wm9il1u+ftdbv77Q84pqXzGhNPSX8PqeoeEdmAM3XS35r6e0zXYYFlJxWl2q53gVdF5PduYNgfyPC3zPh3EpHjwAviTL49DKdFZqWb0MKjqpXu68uAB6NQLmNirjX3kIiMAna4tb2TcLpdluNk3vyZiDyvqkdF5GSgVlUfBR4NOP414Fsi8iJOMoTD7pd8JlClqtVuoHoOznyPxiSsVn7/hL32Q863V0SOiMhUYAVOoqH/CTj+NpxuzrcBr0Z6XmMSVSLfQyKSBZSr6nH3O+pcwvQSMV2TBZYGERmKM16pD05ih3uAcaq6SUTuwxkj6cFJ8PBNgtP8o6obReRlnAHodcA3VdUrIkNwugaCc629oKqxGnNlTDzNBG4VkVqcpDA3uAkSFrtjlZe598FRnOygoV3KF+K0XG4HqoCvuOvHAo+LiA+nO+Ucd3yeMZ1FU9c+IrLOHd8P8A2c7sE9gEXuDzgPwy+LyNdwvpuuj+C8S4DPAL1FZBfwNVV9KxZ/nDEdoKPvobHAb90uswL8RlXXx+QvM0lHVG04gjHGGGOMMcaYtrOssMYYY4wxxhhj2sUCS2OMMcYYY4wx7WKBpTHGGGOMMcaYdrHA0hhjjDHGGGNMuyRdYHn55ZcrYD/2k6w/cWf3kP0k+U9CsPvIfpL8J+7sHrKfJP8xYSRdYFlWVhbvIhiT1OweMqb97D4ypn3sHjKm80m6wNIYY4wxxhhjTGKxwNIYY4wxxhhjTLtYYGmMMcYYY4wxpl1S412AeFtTXMHywnKmjhxAbk5mm4/P7JlORVVN0HnCnbut79fechoTDYHX+4Y9hxFgxqSsoGvSrlXT1fnvgcrjtWzce4QrJpxEfl52vItlTOdTshKKlsCI82D4lHiXxpgur1MFlk8W7OC5ZcX4gD7dUklP9TBt5ABWFZezu+IEpw3JYFdFFYeP1yEe6Jaawt5Dx/G5uZ1GDOjJaUMyADhUVUN1nY++PdLYebCK7P49UWBAr3Q27D7M8TofqSLsPFjVKDVURvdU+vdMp+RgFT4gPUWYPvEkFm/cR1Wtr36/nP49OVZTR+WJOlI8woRhfbjmzCw27DnM9tJKDh6rYeSg3pyo9VKwzRnkLsCAjHT69Ujnq+ecwpihGcxZtJkd+4+S0T2Nfj3TuOGsbPLzsoMebpYVltMt1UO/nukMyujG+GF9GwXCxqwprmDOos1s2XuEFI+QmuJxLjpXWWVNo2NeXLWTiz8zBIBDx2pYWVwBQKoHvn7uyPprb/SQDDK6pbJx7xHGn9SHjB5p9uBtOp01xRXc/NRyTgR81i/ZVsYv39jE8P49qfX66N8rndFDMhpVyhhjWqFkJTwzHby1kNINbn/dgktj4kxUkytj7uTJk3X16tWN1r+wYiezF6yPQ4kSU480D8cDHmyaM7BXOv17p3PwWA0+hetzs8ge0Iun/1XIgWPVnKjx4VMlLcVDdv8eZPfvxaCMbmR0S+WdzaUgwlfPOSUomLWAtUnS8i6x1dQ9tKa4gi/NXVpf0dLRTurTjT/enGvXjWlJ3O8haPo+evSf2/n1W1sjPs+owb3rPz+N6UBxv4+auoci9vp3YfXTASf8Klz1+/YXzJjIxP0eSkSdpsVy0Ya98S5CQok0qAQoO1ZD2bGGlqi5BYVh96v1etmy7yhb9h1ttG32gvX8auEmKqu9gHO3DcvswfiT+nDXBaeydV8lizbstZapBLa8sDxuQSXA3iPVzHxsKZk9UzlrxADuuuBUCzJN0tm853Cr9t++/yizF6xnZ/kxZk0fG6NSGdMJHd3f/LIxpsN1msDyigknsWSbzYkUT/6gEpyZY3dXHGd3xXEWbyqtX79kWxk/eW0DAGkpHvp0S6XG66vvxjtt5AAyeqRZi6dLRJ4GrgL2q+qEMNsFeASYDlQBt6vq2ra819SRAxDiP+tvRVUdizeVsnhTKVNGZHLvFWPtWjBJY/3uI206bm5BIZeOH2rXujFEOFa/54Dg5d6DYl8wY0yzYhZYduQDMUB+Xja/WbyVg8cajwEziaXWq+5vL1U1TjB6sKqW4oPw710Ntf05/XtQ6/UhHicA7aJjk54B/gg818T2K4DR7k8e8Jj7u9VyczL5xbUTE6pL+cqiCmY+tpTBGencc8kYa+02Ce+KCUOb7PXRkrkf7ODJWydHuUTGJJc1xRXc8PhS6tyOVyMG9OScUQMbf+9PuA7WPuu8TkmH0/M7vrDGmCCxnG7kGeDyZrYHPhDfifNA3C7X52a19xQxZx2yI1d88Dh7Dlezu+I4m/dVsv3AMVYWVfD8ip1c//gy1rhJYjozVS0ADjazy9XAc+pYDvQTkZPa+n75edncnIDB2/7KGmYvWM/0Rwq6xP+7iR4ReVpE9ovIhia2i4j8QUS2i8jHIjKpPe83a/pY7j5/JEP7dGPKiExmnjks4mM/2mnXtjHz1+6qDyoBisqrwn/vDzuj4fWt/7DEPcYkgJi1WKpqgYiMaGaX+gdiYLmI9BORk1S1zYMl/eNTnltWFJR9NdCoQb0oOlhFndtqlp7q4YEvjGfDnsN8VFzB5n2VQfunCNxx3kiOVNdRsHU/uw6daHROcX8yeqZyuKquyfJdNm4Id11wKuB8cPqnagCnpnrTnsPsDnN+gLvPd8pQVlnNoaoaVhZ17QcQr0+Z+dhSeqZ5SEnxMHZoRlftMnkyUBKwvMtd1+b7aMakLOav3UVtnQ8RGD+sL6cM7EX5sRqumHASY4ZmNLp+56/dRVllNUB9YqcnlhQGjdkM7GbbLdVDdV3k44D9Nu2t5PrHl/LyXWd3xf9r0zbP0EGt/n6zpo+t/z7asPsw8z/aQ7+eaVw50anzeX7FzrDHlR2tYc7CzTbW0nRpTVXAe33K915axwf/dZGzQhuG35CVG/NyGWNaFs8xllF/IIaGL/TAqTae+ten+FRJT/Xw0HWnA8GBXbg5+Jqal/Lmp5ZTU+s8cF88dggXjhkctN+a4or6h2z/A3a46RRCH4qfvHUya4oruO6xpSiQmiJMGt6P6jpf/fQhgdYUVzD3gx18euAoIwf15q4LTuXva3eFfWARIMUjfP3cU9hRdoz9R04wbeQAnllWRK37cN+3RxqjBvcGCApaxw7NIKt/T97bsh9vmMwu/Xqkcuh408F0rFXV+qDWV99l0h+8W9ARTETuxOkZQHZ28y2SuTmZPP/1qc2ObwldF26fS8cPbRSABp4zdE7M7aWVrCqqaHGMp9cH9y1Yz6J7zm9hT2PiU8kZaMs+Z8zloapa5q/dxf1Xjad7mofqWl/Ya/1xG2tpurgZk7KarHwpPljVUPniC6ic9NVBSloHldAY05SkSN7Tmodiv9yczPov5kvHD230kNzUl3bgceG2RfLA3dYHguWF5YiAKqhPuWDMYL550agmyxJuLI6/pSnFI3xp8vBm56sM9++ypriCm55cTm2dj7RUDz+/dmJQEBAaqE85ZUBQcp6M7qkcq66LW3ZRf9KX3ukppKelcH1uVmev/d8NDA9YznLXBVHVJ4AnwEnx3tJJ23MdN3eOwOVw29cUV/DgPzYGjbUNZ/O+Su558SMevvHMdpXRGGJUyem373BDL5TaOh8VVTX13yOVx2sbjcdUnO8CCyxNV5Wbk8nd549scqzy8yuKne/1wBZLX/wquI0xDeIZWEb0QAytfygOFY2H5FicK9TUkQNIT/XUB3VTRw5o+aCQsrUU+IbuH+7Bf94djc/RVKAO8P4nB+rL/MxXnDEO/lbb9z85gNfrBLpeVbxuBWOaR/CpIgJt6BHZoqM1XqjxMregkHc3l3LNpKzOmmn2NeBbIvIiTve9w9FqaYmH3JxM7v/CeG54Yll9d/WmvLJuD4AFl6bDtKWSc9qpA+metj3ocz30szf0AbryeG30Cm1MEvJXCIcLLiurvawpriA3MyCw9No9Y0wiEKf3T4xO7nQ/er2JrLBXAt/CyQqbB/xBVVsced3uCXUTXEQpthNMc2UO3LZ1X2V9xtH0FKdFFeCFlTtRdbrs5gzoSVF5VZPvNTSjG/vcsXytJQK/uGZivDOLtip/k4jMAy4EBgKlwE+ANABVnetmV/4jTqKsKuArqtrsDZIM95C/S/n20soWxxOfP3ogz32tXUPiTHJpdQ60Fr6LHgfeV9V57vJW4MKWKmhacx+19Ll+zkPvsbvieP3yqEG9eOf7F0Z0bmPaKOL7KFZZ/iO5h9YUVzBn0WZWhXwP5Odl88vPZcLvxzsr/nO7TTdiOprl4wwjltON1D8Qi8guQh6IgYU4H0LbcR+IY1WWZBLLFtFYaan7sH/b8sJyPAI+dQbhD+vXg6kjB9R3301L9XDn+afy4Osb65fvv8pJrBQ4HvaFFTu5/9UN1LWyv60qzF6wnve37k+acZiqelML2xX4ZgcVp8MEXjcvrNjJj19ZT1MNmAXbynhhxc54VxiY5BXzVv+WPtdP7ts9KLAsLDvmtMgkwWeU6RKeoYMTYPnl5mTy17vP5vq5S4MqGQWCu79aV1hjEkIss8J2yQdi07RwXX3Ddd8dMzSj2dr9/LzsoH3e3riPxwsKW0z64rd4Uynvf3KAeXdMtQe3JOD//25u7OXv3t5qgaUJKxkqOUcPyQh6aFa1cZYmccQ7ARbANWdmBd0j44f1BV9A91cLLI1JCEmRvMd0Dk2NAQ2tzY+k1TZwn9yczPpxn9tKK3l3cymV1d5mj6+p83Hb0yu4JS+nsyf36RRaGntp0zSYpiRDJeeMSVm8tLqk/tpOSZFWj7E3Jo5imgALoKKqpv61+JcDh3LtXgP9hjc+0BjToTzxLoDpWnJzMvnmRaOiXhPvP+/DN57J3ReOwhNBz/ej1U5ynwt//c/gSZdNQsrNyeSlO6dx6bghYbc/v6K4g0tkTHTk5mTy4Bcbhq7FMPWBMXElIneKyGoRWX3gwIGIj8vsmV7/WnETXO1Z17DDgjuhZGUUS2qMaQsLLE2n4+9yG+nFXVRexfWPL7PgMgn4p9n55bUTG22rrPbyQhNznxmT6N7fur/+tdenPLRocxxLY0yrtCrLv6pOVtXJgwZFnmwnsMUS4MklhRxc+/eGFd46KFrSiiIbY2LBAkvT6fi73H7/82P45bUTGTW4d4vHeH3K3X9ZbcFlksjPyw4bXP7u7a1xKI0x7Vd65ETQ8qriCvs8MsniNeBWcUwlBgmwpo4cQEpAT6TT+YQ+RYsbVng8MOK8aL6lMaYNLLA0nZK/a2x+XjYPzfws6aktX+oHjtYw87GlzFloLQXJID8vm5P7dQ9aV3a0xlotTVK64azg5FP+BD7GxJubAGsZMEZEdonI10TkbhG5291lIVCIkwDrSeAb0S5Dbk4mF49tGAYx1bMZDwG5FE6bDsNbnLHOGBNjFliaTi83J5N5d0zlB58fw6hBvVrcf25BoQWXSeKbF41utO7pfzWeUNuYRJefl801ZwwLWhc4rsyYeFHVm1T1JFVNU9UsVf2Tqs51syqjjm+q6qmqOrGluZTb6q4LTq1/vVrGgyetYeNpn4/FWxpjWskCS9Ml+Fswp0SYaXFuQaF1Q0sCY4ZmNFq348Ax+78zSalnt+BE7Rv2hJ9ex5iuKDcnk1MG9uTUQb24945b8Uz/74aNgy0juDGJwAJL06XMnJRFeoogQHqKMDZMYOJ33WNLuefFjzqucKbVwnUVVGD+2l0dXxhj2qmssrrZZWO6uu6pKdTWuWmTB41p2GCplI1JCDaPpelScnMymXfntPq5NAFuemIZNWHmRlTglXV7eHdzKT+cPo78vOxG+5j4mjpyAOkp0uj/b3tpZZxKZEzbDcroFu8iGJOw1hRXsLW0Ep/CzU8t59UrlfrQUpufu9oY0zGsxdJ0OYFzafoDzeaCxspqL7MXrOfWP63owFKaSPj///r3Ch6LVlh2LE4lMqbtZkzKCsp8+d7W/dat2xjX8sJyfG4dYm2djy17DjVsVF98CmWMCWKBpenycnMy+eW1E8NOXxGoYFuZJfVJQLk5mfznZWOC1pUdrbH/K5N0cnMyOXf0wPrlOq9at25jXFNHDqh/aE3xCOOG9mzYaIGlMQnBAktjXPl52dzcQnfXZ5cVWQtCAsrPy6Z/r7SgdW9u3Ben0hjTdt1SU4KWbZylMQH8Lfoi4AsIJn3WFdaYRGCBpTEBZkzKonta07fF8Vof181danMlJqAJw/oGLWf379nEnsYkrtBxljbu0hjH8sLy+hw9Xq+PT/ZaV1hjEo0FlsYEyM3J5PmvTyU/LztorFMgVZi9YL0FlwkmNP3S4eO1cSmHMe0xY1JWfaNMaoowY1JWXMtjTKKYOnIAKR7n7khL9TBmSGBXWGuxNCYRWGBpTAj/mMsbpzTfLfa+V9Zbt9gEcsWEk4KW1+8+bP8/xhjTSeTmZPLlaTkAPHHLZEYN7NGw0VosjUkIFlga04QZ7pyXTfEp3PHsKu58brUFMAkgPy+b0YN71y/7FOYssgQ+JrnMX7urvvW9zqvM/WBHXMtjTCL5jDv39KghvYPHVdo8lsYkBAssjWmCfyqL07P6NrnPwapaFm8q5cYnlllwmQBCHy1WFVXY/4tJKqFVWe9tsSlHjPHrme5Mv15VUxccWFryHmMSggWWxjQjNyeT+78wvtmEPgC1XuXe+R/bA2CcjRzYq9E6a/ExycSZy7IhvPT5lOWF5XEskTGJo2e6kzX5WLU3eFyldYU1JiFYYGlMC/wJfX7w+TGMOymjyf227z/Kl+YuteAyju664NRG6z49cDQOJTGmbXJzMrnjvFPqlxXI7JkevwIZk0D8LZYvrNxJ0YEjDRsseY8xCcECS2MikJuTyTcvGsXPrpnY7H4+hfsWrO+gUplQuTmZjB0aHPz372UP5Sa5HKmuC1resOdwnEpiTGIpOXgMgJdXlfBkwbaGDdZiaUxCsMDSmFbIzclk/n+cTU4zcyRu3ldpU5HE0aSczKDlftbaY5JMWWV10PL20so4lcSYxLKl1OmBogBe6wprTKKxwNKYVsrNyeSD/7qIX17bdOvlog17O7BEJtCMSVmkBHyyvbul1Lonm6QyKKNb0PKanYfsGjYGmHpKf8BJcpWeEpCuzZL3GJMQLLA0po3y87L55bUTG2VxhMZzKpqOk5uTyRnD+9Uve33wkE07YpJIaAIfryXwMQaAvJEDADhlUC+uHD+4YYNNN2JMQkiNdwGMSWb5edmMGZrB/LW72F5aSXWdjxvOyiY/LzveRevSDlfVBi37px3JDekma0wiys3J5Aunn8Qr6/bUr9tm3WGNYes+J2HPpweOsfHwv5nsbx6x5D3GJAQLLI1pp9ycTAtYEszIQb3ZfuBY/bICywvL7f+pixKRy4FHgBTgKVWdE7I9G3gW6OfuM0tVF3Z4QQOUH6sJWn513R6+PG2EXcOmS1tV5HQJP1M+IV8WN2wo3x6nEhljAllXWGMSmIhcLiJbRWS7iMwKsz1bRP4pIh+JyMciMj0e5Uw0d11waqMPt8rjtWH3NZ2biKQAjwJXAOOAm0RkXMhu9wEvq+qZwI3A/3ZsKRsbf1KfoGUF5q/dFZ/CGJMgprpdYad6NpNCQCvlga1xKpExJpAFlsYkqGR9IE4EuTmZTB4R3LLzeEGhJUDpmqYA21W1UFVrgBeBq0P2UcAfyfUF9hBnGT3SGq37cFtZHEpiTOJUcubmZNK3RyplA88CT0Cnu/6N5zA2xnQ8CyyNSVxJ+UCcKKrrgtPPW4tPl3UyUBKwvMtdF+gB4BYR2QUsBP5fuBOJyJ0islpEVh84cCAWZa3nb5kJVBRo2OcAACAASURBVHywyipHTIdLtErO3t3S8J48Bc+ZNzeszMyJ1dsZY1rBAktjEldSPhAnihvOapxAKXR+QGNcNwHPqGoWMB34i4g0+n5U1SdUdbKqTh40aFBMC5Sbk8k1ZwxrtN4qR0wcJFQlZ/c0DyfqvJARkH3d5rE0JiHENLBMlK4TxnRiCfdAnCjy87IbPZjbnJZd0m5geMBylrsu0NeAlwFUdRnQHRjYIaVrxsM3nsnQPsFzWn5k16/peFGr5IwGr0/ZsvcIew81JGhj29tQsjJWb2mMiVDMAstE6zphTBJK2gfiRDF6SEbQstdnLT5d0CpgtIicIiLpON81r4XssxO4GEBExuLcRwnRtD+kT/eg5c37Kq1yxCSiiCo529t7Zk1xBcUHq9hx4BivfbSzYcMnb8KzX7Tg0pg4i2WLZUJ1nTAmCSX1A3EiCJcJVsLsZzovVa0DvgW8BWzGqczcKCIPisgX3d2+D9whIv8G5gG3qybGjOvhunQ/tGhzHEpiurCoVXK2t/fM8sJy6u9MX2D3VwVvDRQtafU5jTHRE8t5LMN1ncgL2ecBYLGI/D+gF3BJDMtjTFJR1ToR8T8QpwBP+x+IgdWq+hrOA/GTIvJdnIqahHkgTgQb9x5ptC6jm03f29W4c1IuDFl3f8DrTcA5HV2uSOTnZfPfb25h9ImPOcuzleW+cawqOo01xRU2p6XpKPWVnDgB5Y1Afsg+/krOZ2JZyTl15AA8Aj6FNE9gYCmQkg4jzov2WxpjWiHeT1j+rhO/FZFpOF0nJqgGj8IWkTuBOwGysxvX3hrTWSXzA3EiuGLCSSwJmaJhbkEh2QN6kZ9nnyUmOZzXo5D/0Z/hU6gllZtq7mN54RgLLE2HSKRKztycTKac0p9tpUe5csxg2CTO2426GC64F4ZPifZbGmNaIZZdYROm64QxpmvKz8tm1ODejda/tGpnmL2NSUz/r/9qADwC6dQxI2UJ20or41wq05Wo6kJVPU1VT1XVX7jr7neDSlR1k6qeo6qnq+oZqro4VmU5qW8PenVLZUjvNKeVEmDEuRZUGpMAYhlY2vgwY0zcffWcUxqtW7/7sCVAMUnjtIHdG617Zd0e5iy0sZam6+me5uFErRd83obA0qYbMSYhxCywTPaECcaYziE/L5vLxg0JWudTmPvBjjiVyJhWSguecuSI9gDg8YJCqyAxXU631BQnsFQvpKQ5K30WWBqTCGI6xtLGhxljEsGFYwazeFNp0Lq3N5VaAhSTHPZtCFoc7ykGrzOQ7e9rd9k1bLqUiqoajtV4KTtynIH+wNJaLI1JCLHsCmuMMQmhoqom7Po5Nm2DSQYjgutfF3kbxpJ9YmMtTReypriCNz7ei9enFGzdR42mOBsssDQmIVhgaYzp9KaOHEBKmAksVxVVWFdCk/h6D61/WYeHT7QhL96qogpeWGHJqEzXsLywHK/PGTEl6uW4VwBxusUaY+LOAktjTKeXm5PJjVPCTy9ik82bhLd7Tf1Lj8BUT/A1O3vBeqsgMV3C1JEDSPE4tYSp4qNbWhp4UqzF0pgEYYGlMaZLmDEpi1RP42bLldZqaRKdJ63htaSw3De20S6WjMp0Bbk5mXz9XCfT91nZfemengbicTLEGmPizgJLY0yXkJuTyYNXTwi7bf7aXR1cGmMiVLISPn6xfnHj0C+yVk9rtNvSHWUdWSpj4ua0oRkAZHTzOK2V4rEWS2MShAWWxpguIz8vm7vPH9lo/cKP91qrpUlMRUvAV1e/mNWtmvQwA4aPVXu59U8rOrJkxsRFt1QnYY/PWweS4vxYYGk6IRHpJyLfiHc5WsMCS2NMlzJr+liy+gVPOH/oeC3XzV1qSVBM4hlxntMq4+qfdRrz7pzGpSFzswIUbCuzChLT6XVPcx5d5Xg5VJWBqgWWprPqB1hgaYwxiewbF41utE4VfvzqBnswN4ll+BQ445aG5f2byPVs48lbJ3P+6IGNdrexlqazKzlYxX+lvECv0tVwtBTqquDInngXy5hYmAOcKiLrROSvInKNf4OIPC8iV4vI7SLyqoi8LyLbROQnAfvcIiIr3eMfF5GUsO8SRRZYGmO6nPy8bAb1Tm+03utTezA3iaf34IbXn7wFz34RSlby3NfySA35Fn9nU6lVjphOa01xBdsW/ZH/SH0dNGDDkd1xK5MxMTQL2KGqZwB/BG4HEJG+wNnAG+5+U4CZwGeBL4nIZBEZC9wAnOMe7wVujnWBLbA0xnRJ3710TNj1b28qZfojBfZwbhKHt9pJUAKAgrcGipawpriCupAegIq1WprOa3lhOVfKUkRAJCC27N24a7gxnYmqfgCMFpFBwE3AfFX1D8B/W1XLVfU48HfgXOBiIBdYJSLr3OXGSSaizAJLY0yXlJ+XzVg3u2CoTXsruf7xpRZcmsRQsdNJUAKAQEo6jDiP5YXlYXd/Z1OpjRc2ndLUkQNYphODV6b2hF6D4lMgYzrWc8AtwFeApwPWa8h+CgjwrKqe4f6MUdUHYl1ACyyNMV3WmTmZTW7z+uDb89ZacGniq2QlbH4NfLXO8uCxcNtrMHwKU0cOqE9kEkiB+xast2vXdDq5OZmcdcWt9cvSfxSkdYe965x7xZjOpRIIrAF/BrgHQFU3Bay/VET6i0gP4BrgQ+Bd4DoRGQzgbs+JdYEjCixF5Dsi0kccfxKRtSJyWawLZ4wxsTRzUlajMWqBdh86wXWPWbZYE0dFS0ADJn8/3hAs5uZk8vzXpzJqcO9Gh/lwgktjOpszPNvqXx+THs49seej+rHHxnQWqloOfCgiG0Tk16paCmwG/hyy60pgPvAxThfZ1W7geR+wWEQ+Bt4GTop1mSNtsfyqqh4BLgMygS/jZCoyxpiklZuTyUt3nU1+XjZDM7qF3UeB2db6Y+JlxHkB3WCByr1BD9C5OZk8NPOzNJ7ZEjbvq2TOws0dU05jOkLJSnq//V/1i4fLdqP+XoDu2GNjOhNVzVfVCar6AxHpCYwG5oXstktVL1LV0ar604BjX3K7wX5WVXNVdXmsyxtpYOn/zpoO/EVVNwasM8aYpJWbk8kvr53Il88e0ex+1i3WxMXwKZBzDqT1bFgX8gCdm5PJXeeHz8kwt6DQrlvTeRQtaegWDqRqHfWPo+7YY2M6IxG5BKe18n9U9XC8y9OUSAPLNSKyGCewfEtEMnB62hhjTKcwdeQAUj1N15ftPnSCmY8ttRYg0/G69YbeQ2nuAfrS8UObrO2ds8iuWdNJjDgvIEMyDJBKarsPgMHj6sceG9MZqeo7qpqjqg+HrH9GVb8Vr3KFijSw/BrOXCpnqWoVkIaTkcgYYzqF3JxMHrx6Aikt9MWYW1BowaXpWHXV0DMThk6AfjlhH6CXF5YjTVy7q4oqrNXSdA7Dp+A5paFSxSNKeooH+o+0oNKYBBBpYDkN2Kqqh0TkFpzBoAnbDGuMMW2Rn5fNy3c7Yy6njGg6Y6wFl8lFRC4Xka0isl1EZjWxz/UisklENorICx1dxmZ5a5xWyl6DodfAsA/QU0cOIL2ZTFTfe2ldLEtoTMc5eXLDa/FA9z7grW16f2NMh4k0sHwMqBKR04HvAztw5lIxxphOxT/m8uW7zya7f88m95tbUMg1f/xXB5bMtIWIpACPAlcA44CbRGRcyD6jgR8C56jqeNx07gnDH1im9YDaE2F38WeIPT2rb9jtxQeruPVPK2JZStOJJVLlzM7ahmt8mXcsRz29g8ZdGmPiJ9LAsk5VFbga+KOqPkrwvCrGGNPp3HhWVrPb1+06zBkPLrbpSBLbFGC7qhaqag3wIs53WaA7gEdVtQJAVfd3cBmbV1cNqd0gtTvUHW9yt9ycTO7/wnhSm+jPXbCtzK5V02qJVjlzaGfDNDoHtC9HqhV8dbF6O2NMK0QaWFaKyA9xphl5Q0Q8OOMsjTGm07ppSstzCR+qqmX2gvX2wJ64TgZKApZ3uesCnQacJiIfishyEbm8w0oXiROH4dBOOFwCRw80O1dfbk4mN0we3uT2OYs223hL01qJUzlTspIJe/9ev9hPjpHRswd4LbA0xk9E+onIN9pw3EIR6dee9440sLwBqMaZz3IfkAX8uj1vbIxpWSJ1P+qKtpYeqX/d0oflg//YyKP/3G4P7ckpFWdusAuBm4Anw325isidIrJaRFYfOHCgY0pWshIqiuDAFihZATWVLU4EP2NSFt3Twl+xR07UccMTy+w6Na0RtcqZdt9DRUvwqLd+cfLAWiewtK6wxgTqBzQKLEUktbmDVHW6qh5qzxtHFFi6weTzQF8RuQo4oao2xtKYGEq07kddUeDDt4iT3OesJpL6nKjz8eu3tvKluTYlSYLZDQQ24WW56wLtAl5T1VpV/RT4BCfQDKKqT6jqZFWdPGjQoJgVOEjREvBPAO/XwkTw/vGWP/j8GM4fPbDR9jqvct+C9WGONKbNIqqcafc9NOI88KTUL3q7ZUJKmiXvMUltxKw3po2Y9cYPR8x6Y1qUTjkHOFVE1onIKhFZIiKvAZsAROQVEVnjNkjc6T9IRIpEZKCIjBCRzSLypLvPYhHpEckbNxu5BrzR9TgtlO/jTKT1PyLyA1X9Wyv/UGNM5Oq7HwGIiL/70aaAfRJ7bFiSmzpyIN3TtlNb5yMt1cPMSVnk5mRyye8+YPv+o2GP8amT2Cd7QC/y87I7uMQmjFXAaBE5BSegvBHID9nnFZyH4T+LyECc1pfCDi1lU8JN+B7BRPC5OZnk5mTyzYtGcdbP3+bA0Zqg7Zv3VXLrn1bw3Nfyolla0zlFWjmzQlVrgU9FxF85syqqJRk+hYqsz5G5czEAH+72Mm2E0s/GWJoENGLWGw8DZ7SwWx/gdJzGPt+IWW/8GzjSzP7riuZc2VIjwixggqqeISIXAm+4y5+627+qqgfdYHGViMxX1fKQc4wGblLVO0TkZWAm8H8tvG/EXWF/hDOH5W2qeivOA++PIzzWGNM2yT82LMn5W36+d9kYnv/6VHJznNbKr55zSovH/uHdT6y7YQJQ1TrgW8BbwGbgZVXdKCIPisgX3d3eAspFZBPwT+AHYb5k42P4FOjWx5li4TNXOetu/mur5uwbMbBX2PUF28osU6yJRH3ljIik41TOvBayzys4rZXEunKmtLohxceplFBTXgKV+5rtHm5MAutHQzwm7nK0rQwIKgG+LSL/BpbjVBo16qEDfKqq/nmq1gAjInmjiFosAU9IS0g5kQelxpjYCex+lAUUiMjE0D7ybleHOwGys60VrTX8LT+B/C2RTxTsoKi8Kuxx+45U86W5S/n5NROt5TLOVHUhsDBk3f0BrxX4nvuTgASyzoLMEbDldRgyvlVHnzYkg1VF4Ss5CraVcc+LH/HwjWdGoZymM1LVOhHxV86kAE/7K2eA1ar6mrvtMrdyxksMK2d6SEO319GyG465C89+EW57rVWVLsbEUgQti7jdX9/FSYpaC9xcNOfKZVEuiv8uwW3BvASYpqpVIvI+0D3MMdUBr71ARF1hIw0O3xSRt0TkdhG5HadJdWELxxhj2ie5x4Z1cvl52fz2+jNoZk56fAqzF6xn1OyF3PPiRx1XONO5eGsgJRUq9zrLxa175pgxKavJKUgAXlm3x8YFm2ap6kJVPU1VT1XVX7jr7neDStTxPVUdp6oTVfXFWJWl55GGhlAJvKxbGHtsTCJyg8iLgfuBi6MUVFbS9LSQfYEKN6j8DDA1Cu9XL9LkPT8AngA+6/48oar3RrMgxphGEqr7kWksNyeTl+46m0vHDaFnetMfp3U+5ZV1e5j4kzdtWhLTer5aOFYGyx91lud/pVXd/vxTkDQdWjrjgi24NAmvZCUDq3bUL6oC6qa3imDssTGJqGjOlcuK5lz5q2i1VLq9BT4UkQ00nsXjTSBVRDbjJPlZHo339Iu0KyyqOh+YH803N8Y0LdG6H5nwcnMyefLWySzdXkb+U82PV6us9jJ7wXqeKNjBb68/o1EXW2Ma8fmcyd+P7AGvO82Ct9ZpmWlFl78Zk7KYv3YXtXU+fNoozyxgSadMEihaggRcvQps8uYwOr2UbsnSDbZkpXP/jjgvOcprkpKqhiap86+vxpltINy2Ee7LMmBCwPrfRPq+zbZYikiliBwJ81MpIs1lLPIfb3PwGdMOidT9yDTvo5JDzbYIBSoqr+JLc5dach/TMv/8fP1HOt1hATxprW6ZCUxE9YtrJzb55T97wXpruTSJa8R5EPBJu8WXzWrG4JW05AjSSlbCn6fDuw+2OB+tMcmo2cBSVTNUtU+YnwxV7dPcsTYHnzGmK5k6cgDdmpiUPhyfwnWPLWX6IwUWYJqmeQMCy8t+4by+7Odteoj2Tz+Sn5fNJeOGNLnf3IJCpv3yHbsuTeIZPgX6ZuFzg8sSBlNHCml441ywCBUtaagssjGhphOKZWbX+jn4VLUG8M/BF8jm4DPGdAr+FqH8vOz6RCkeQpJLhFBg095KZj621MZemvC87vyTRf+C1G7O68wR7T7tXReciqeZa3Pvker66/LRf263INMklFqPk8RSEepIdbqLJ4PAngYpre95YEyii3iMZRuEm4MvdCbm0wBE5EOcMWQPqOqboSeyqRKMMcnAPzXJzElZLC8sZ+rIAWzdV8mPFqwPO6Yt0OwF63n4na3MODOLjB5pTB05wMZgmoauctsWw6fvO6/Xvww9+rWr619uTiY/v2Yi972yHl8zF+fsBesBSE/1MO+OqXZNmvirPU4taXTjOKfIHo5qdzya4IGlf1xljwE4XXkVTs9Pju67xrRCLAPLSN+/xTn4VPUJnKy0TJ48uaXnM2OMiavAuS/9v3/86ga8zT3BA/sra5hb4CT1TfUID149wRKpdHU7/UkCFercLnTr/wabX2/3nH35edmMGZrBt+etZfehE83uW1Pn4+9rd1lgaeKv7gQ1Kb3Bd4TRspuRKftIweekiG2ui0i8lKyEZ78AdSH32Jqn4aTTYfLtcSmWMbEQy66wUZuDzxhjkll+XjYv3zWtVUFinU8tkYqBYWe4Lzzg8X9la9TGZ+XmZPKHmyaREsHTwEurSqzLtom/mip6dkvFp+AR8OBz1vvHIyeaoiVQVx1+2+ZXO7YsxsRYLANLm4PPGGNcuTmZnNyvR8SZY/3mFhQy/ZECG+vWVQ0e7/wefw18/lfuSonqnH25OZm8fNfZ5Odlc1kzSX38lR33vPiRXYsmPoqXAj66H92FCNSp4HMfZX+7aEN8y9aUEeeBNPG4PTY09YgxHU9EjkbrXDHrCmtz8BljTDB/5tiaWh/uvN4R2bS3sn6smwhcffowRg/JsHGYXYE/ec/4a2Dc1fDmLOh7Mpz7/aiOzwrsvn3FwwVs3lfZ5L6vrNsDQJpHePGuaXYNmo5T+H79Sy/Ch74J7NBhfDX1LV5dW8z3vzApfmVryvApMPoy+GRR8PpzvmPdYE2nE9Mxlqq6EFgYsu7+gNcKfM/9McaYTs2fOXZ5YTmVx2vrx1O2hmrDg333NA/Pf90SqnRq/sAyJd0Zq6VeOLTTCTCHjItJ8o+fXzuR6+cuxdtCzUetT/nOvLU8ctMkuwZNxzg5130h+CSdR+pmMsHzKQBHj5/ghRU7E3Nceo9+jdeddnnHl8Mkpgf6TsPpwfk+Dxxe1sLeLRKROUCJqj7qLj8A1AEXAZlAGnCfqka9L3a8k/cYY0yXEtgylD2gF4s27GX8SX04Ul3Hh9vKKD5YFfG5TtT6uO6xpaSnerhiwlAevvHMWBXbxIt/GoWUtOAxlf4xljEILHNzMnn57rNZXlhOZs90fvPWFg5WhR+/tuvQCWY+tpS7zx/JrOljo14WY4IM+ozze+xVvNZtBmuXd2MszrjfFLw8/a/CxAwsw02H4kuSuTdN2z3Q92HgjBb26gOcjjM80ccDff8NHGlm/3U8cPieFs75EvAw8Ki7fD3weeAPqnrEHX64XERecxv5osYCS2OMiZP8vOxGD0EvrNjJnEWbOXIisvT5ClTX+Xhl3R7e2riPXt1T6dcjnUs+M9imLekM/C2WnpA57zwpMZ0DL7ACJD8vmwt//U+Kypuu9JhbUMi+IyescsPElj+zqieNCSf3QaimlhQA0vAmZlZYgMrSxuvUAksDQD8act6Iu9xcYNkiVf1IRAaLyDBgEFAB7AN+LyLnAz6caSGHuOujxgJLY4xJIP4pIG54fBl1LUxPEup4rY/jtTWUVdawfb8zFj89RZh3p42DS1qBXWHBeXBWhVangWqf315/BjMfW9rsPv4u2hZcmpjZtcr5vfEVPrN1ET898w+s+7cTWKaKl2F9u8excE0oWQnF/2q83losO7+WWxb93WDfxemeWgvcHI3usMBfgeuAoTgtmDfjBJm5qlorIkVA1G+YWGaFNcYY0wa5OZk8ePUEUqIQO9R4lS//aQXXz13Kjxast0yeycbr7wqb7nR99fda8tVFZbqRSOXmZDL/P84mp3/PZvd7Zd0eLvz1P/nRgvWWydhEnz+wxAfeGm4dVsKQzN4ApFFHwbYy7nnxo/iVL5yiJaC+xuvDrTNdjxNEXgzcD1wcpaASnGDyRpzg8q9AX2C/G1ReBORE6X2CWIulMcYkoPy8bDbuOczzUZg3sKrGy8qiClYWVfDiqhJ+dvWExByHZBor3ej8PrDFnbYgxelCF8XpRiKVm5PJB/91EXc+t5rFm8J07XMVlVdRVN5w3VqruYma/qOc3+Kpvwe6vf0UeGCiFLJDT+aVdXuYcsqAxPmM8083EhpIWoul8XOCyWgFlAC4M3FkALtVda+IPA/8Q0TWA6uBLdF8Pz9rsTTGmAQ1Y1IW3dM8eIAUgVGDe7f7nF53LsLpjxSwpriCNcUV1qqUqEpWwvu/dF6/8V3n9ykXAAKX/yomiXsicdcFp5KeGvnjQ41X+fa8tXaNmfbLcOdZzfsPuM2ZGv0bnvkAzEl7iknyCQD3vZJAvTOGT4HhU92FgPsmXEIfY6JIVSeq6kXu6zJVneau+4qqjlXVIndb+x8uXNZiaYwxCSpwepKpIwcAcPNTy6mp9eHxCFd99qT6cW2ttWlvZaMxc2OHZjApJ5MZk7KsdSkRFC0Br5uN1VsH/34BigoAdacbGR+X4DI3J5N5d0xl/tpdvLSqBG8EY4F3u9ljLxs3hLsuONWuL9M2e//t/B55gXPtL/mtk7QHSKWOqZ7NrPWehk9h/tpdiXOd9ezvvgi4Vw5sgbFXxaU4xsSKBZbGGJPAArNzAkGB5vLCcjwCrczx06TN+yrZvK+SeSt3cu6ogew8WMUZw/sxekhGo+yya4or6suRMA9vnc2I88CTCr5aZ7oRpKH7nLc2ZtONRMJ/Xc6clMXvFm/lwx3lER23eFMpizeVWoBpWq9kJayY67x++Va47R8w4jwkJQ28NXhJYbmvYcqb9zaVwrUT41TYpgR8WH/wEJxyftzuYWNiwQJLY4xJIqGBZnqqh9o6HykewavUtx4JQY8wreJTKNhWBlA/xUR6qocHvjCeiqoaMnum89N/bKTW6yM91cPzX59qAUJbvP0T+PhlJ/Nr1UGcDPA44yh7DYQLZ8O0b8CHj8B1f3bWffQXJ6hMSe3wMZbh5OZk8r3LxrCilVmMF28q5d0t+228r4lc0ZLGFSvnfR8u+wUs+gE/r7uFtXpa/e77Kqu59U8reO5reXEqcIBw06B46+JaOWRMLFhgaYwxSSpcV1n/pPb+AHDDnsO8vLqEOm/7mjVr6nzMXrAeCA5aa2p9LC8st8CytZ74HOxZE36beuFoKbz+HRg/01k3fIoTWF7yU3hrNoy6pOPK2gJ/FuP7X93QquDS61N+5F5TFlwmNhG5HHgESAGeUtU5Tew3E/gbcJaqro5qIUac1zDdjiegYmWo0yr5qZ7U6JCCbWWsKa6I/+eThBmT7PEkROWQMdFkgaUxxiSx0BbMcA9QMydlMfeDHby3ZX9E4+FaEngGHzBvRTGZPdMtOIhUc0FlqC1vOL9T0pzf6W6OhS2LYPt7TgKTBGjx8M+/GlqxseCjXawqajqJigKzF6znwX9s5LNZfbn3irHxDwJMEBFJAR4FLgV2AatE5DVV3RSyXwbwHWBFzArjn24n8FPIvTd6eLz1jf6BbvvTCmZfOY6Kqpo4dt0P02KZPS0h7l1joskCS2OM6eRyczJ58tbJ9eMi/Q/8W/YeobK6/Snvdx06wewF6/nLsiIm5WQyfljfoIe4NcUVzF+7C4G4JAZKiNYWvz9OgbKtke/vPeH8Tkl3fh/wH+vM45dIXelCKznACTjveG41bzczPQnAiTofK4sqmPnYUgZnpHPPJWPqA1Ubxxt3U4DtqloIICIvAlcDm0L2+xnwEPCDmJSiaAn1AaUvoBupx3mUvfeyUVA0pNG1drTGy+wF6xGgW1qcuu6H6wqbdVbHlsGYDmCBpTHGdBGBD/75edk8+s/t/Hbx1qgn//FLEec9VxVV1Lcv/HXNLubd4aTe74igIS6tLS/cADtXwOhLYeaTDev/eBaUfdK2c+5ZBznTnGyYyx911nlSkqIr3d0XnMp7m0uJtDf2/sqa+m7X4IzvnXfH1PpKCgs2O9zJQEnA8i4gaOCiiEwChqvqGyISm8DyxOGG1+qDHk73f39gOWpgd548fzLXz13KyjCt5ApU1/p4+J1PuOeS05pMRgYx+GyqOth4XclyJyFRglQMGRMNFlgaY0wXNXXkANJTPdTU+vDhdNbyCJw2JIMjJ2rZfehEu87vVRo94NXU+Zj7wQ4KPjnQUcl/Ora15aVb4ZM3ndfrX3Z+WpLWCz5zZUMQOmcEnAh5MF76P05gmd4rYGWYVpAElJuTyct3n838tbv4qLgiqPIhEjV1Pm770wq6p6dQfrQGaNzyZAFn/IiIB/gdcHsE+94J3AmQnd1M1/nVz8A7DzS+DwLtc6ce8XcTd6fmufeKsY2mUvJT09XSZQAAIABJREFUYMm2MpbtKOfBqycwZmgGL63ayd/W7AIg1SMozvjfqH02lax0phYJVbwUnr4crvwdTL69fe9hTIKwwNIYY7qowOQ//nFxgQ/mL6zYyY9f3RCVcZmBAruqnaj1ccezq5g8on+spp+IWmtLRA/FRQWtK90598ClPw1ed8kDTuKeQJ8sch5QSwIaVH3Jk1UysLX8hRU7eWnVTkoOVnGwqjai44/WeDla09Bt+0Stj3vnf8xDMz9LrdfLzU+tRDWKwYAJtBsYHrCc5a7zywAmAO+L0+VzKPCaiHwxtEu5qj4BPAEwefLk8B8sq59pfP2H5R7utljiqwOca+3mvGyeX7GzySPrfMrsBetJEcGrDcWo9WpQYrJwrZutUrISnrnS6bYe9k/wwsLvw5BxSXEfG9MSCyyNMa1XshLe+B4c+AR69HOmRUiUGte3fwKr/+w05uR+pfFDuwkSblycX2BCFn8Xsflrd7Hw4z0cOl4XtTIcrKqtn99wUO90vnvpmA5LBNSa1paIHopHXRpZK6UnDab/Jvx9M/l2WP+S06LR8OZOEFnf9VWccZdJ0BU2VH5edv3/7wsrdvK7t7dSdrSJB+9mbN9/lC/NXUpWZs/6yo/aOh/z1+6y1svoWgWMFpFTcALKG4F8/0ZVPQwM9C+LyPvAf7Z5nPLmVyPb73S3CP4Wyy2vQ/+RMHwKMyZl8dfVJdS00P86MKgESPFAnZsAyAf8a1sZq4oOtr2yomhJ00Gln/qSpoLImJZYYGmMaVCysuHhNdyX3Pw7YMN8p5bVzz8tQkQ1zAAC3fs5rTLRDkZDs21++DDs/RhuXRDd9+lCwmWdnTkpi+vnLo14zFxrHDjaML4uSsFl1FpbIjLzSdi5HA433VoCwFcWNv8gOfGG4MDSP73C8CnQe6hzD150X9I/jPqDzDkLN/N/K4o52spkUj6FnQer6pdTPMLLq0rwWetl1KhqnYh8C3gLJwHW06q6UUQeBFar6mtRfcOxV8OO95rfZ+L1Ddf+vg3O782vw7Z34LbXyM2Zwrw7pzH3gx0tJo4KVBeSVVZxumK3eUqlSCp+krSCyJhwLLA0xjhKVsKzX4C69o2ra5k642ZaCkYlBU65IPKg8Llrw0/hUPie04ppLZdRk5uTyc+umcj9bjfZGMSXPP3hp9EKLDu2tQXgu+udSpiNC8AX2NXTA5kjYMbjLQeEx8sJmjF00i3OMSUr4dh+p5XjzVmdpgvdrOljuXT8UG5+ajknasPMGRGhwBaq6lpnPO+gjG6UHalmUJ9uzJiUBQQnZwnMlhzfKSkSl6ouBBaGrLu/iX0vbNeb+Sscw42xTO0OeXcHf57v9n/ua1CmZH827DkLNzO3oLDNxfEprCgsB2j9tdHkvRlwb980r1Pcw8aABZbGJLQOnSahaEkHBJWtoF4nKHygb/vP9eHDTnIU+/KOmnDzFlYer+WdzaXsOHCsPtj0CPTvlc7h47XUtqaJU6MTrnZ4a4vfzCeDM8K21ojzICW1PiFJfbe/oiVOUAkJN91Ie4WO+d2w5zDzVuxsc8WFQqPWqpdWl+DzKaqQliJ89ZxTeGJJYVBm5O6pHp6/w1o642ry7ZH3aMmZBkugqe7hGT3S2l2cgm1lFGwrA2Ds0AyG9+/JoIzwFRWRCbjgfG2vSDEm0VhgaUyC6vBpEjp7V5yXb4Pvb453KTqVcOMzZ00fG3beyjXFFdz05HJqQvuaNeGr546MWjk7tLUlWoZPgct+DovudZZLNznrRpwHeABf0kw30hqh19SEYX358Svr8arTxtOnZyqHq9o+vrcuoHKjxqthW7KqQ7o+WotmghvuTF/EqIvhgnsbVbRMHTmA7mkeqt2W8CEZ3Sg/Vk1bG8YDp1V6fsVOZ4pKbcccmRv+BqMvaVthjEkwFlgak7g6dpqE4VNg5OecVsJIeP5/e/cdH1d1LXr8t2ZGcrfcwL1iwAUDLhiMAySUAI4fIVQbkgAhEOcm9+EktwAhQDr3vRsCL40SwCmYjoEYfMGYEgO2ZbmAcTfGstxtWZa76n5/7DNNGkkzmnLOGa3v5zOgc+ZoZs9Ya+ass/deOwR9zoD9m5svCe8Vh3bY4bI63zLrEiWc4wd355nbor1R767fw5odlQmXNLlkVO+cFe/xtCPl0Z/Dw8Z7jyLS25GhXl0va1hAKnyR4p45q1JetiRZBpizYjuHjtVwsKqWF0rKqI2pFtq+tQmEyo5w8Z5BkxL23sf2hMdeFJj57ApeWbkj7acPh+HxmnqmP76Ib507lDunjEz+AQ7vhYW/abq2gVI+oomlUt6V+0WpvzkncYGesEAIThgJUx9s/RdgMuuTpWvMdXBkX+MkefM79vWlM0RRtVpswhlOHJeVVvDAvLWUlFZErvrPuOAkN5vpHeH1MMNW/BX6nk4ksayvscO8p83OedNyKVEBqXkzz0977lxzNu05zKY9hxPel5FlKFTmBJzEsr7pnuxEF7semjaWiUN78uQHm9l7pCqtnvCw6lrbC/5sSRn/cemIxBfIQh2g9lh0+7MF8Nnbdv9Nr2lyqXxNE0ulfCori1JD+nPDWpLs3JmExU+aE4DOJ8QvffKrAVDdoFcjvBSEJpeeMH5wd16Yca4ucJ9I0QDY9Ul0u0tf7IDQGOuc9S3b4MlouODPS8u3sWn3Iapq6xnaqxOvrtyRlYJSYfXAwo37+GjTPqZNHJTGPDuVEYEAILDlg5RjIXbpm/AQ/n2HqjihSztG9yvib4u2tKpn/MDRGu6es4riz8t5qOGdsUklELlQVHs8r+ZMq7ZJjM+G0kyYMMGUlLS+WJ9SLpOWD3EOFJkE3G+MudTZvgvAGPNrZ7sI+AwIX1bvA+wHml0moU3FUHMLbY+5TpNLf0o6hrIpJ3FUVgxPXW57YgIFdokSgCcuxaY34cbcAlMbnb62WbFzfEf3K2LOim0s3ZK9ERLhP0gR/LTEietxlLEYKiuGJy4BxFaNzXCv3+wlW3lu6Vaqa+vZUXmMyhTX8N3S3hbeqkcItHTJY/JMrWDuH67HkBdpj6VS3pX7ZRLyzYSbofTDxAvWr3reDpfVOZfKqwZOhFvmNV5bdvAk+3cddnivO+3zqIbDHm84exCzl2zlnjmryEb9zXCqYIydZ/e9p5dxzrCelB+p5vLT+up84WzbstD5wWSlUnJsrybYCxePvP8ZK7ZWUHG0mrpm/qjGyYbohjFUE6IgYGzBn/oE001iRygo5UOaWCrlUa4tk5Bvwr2SiZLLze/A7yfC94tz2yalkjVwYuOT5A6e7w3znHARoNiezPASOQ2XG0nXroNVkaIwCzfu45UV2zi5d5fIc+pw2Qwbch6RzqNgIXToAf/8bxh6fssJZlkxfDzb/v4Z05NKSMPrY4bNXrKVe15Z1ehvaLys5wehFyPbArxQdwHDho1g0pBu8O4vGz/4yK+2+PxKeZkmlkp5mC+XSfCiqx9PXMwHYN96+PUg+PqLOrdF+UPnE+O3j/mgKrMHJCrgAkTmaYYTzk93VLJp9yGWbqnIyFzN4i0VFMcMxdWqshk2cCJ06WOHjA/7Esz9AWCiw8eb+lwvK4anpkTn8a94Gm6em/L3QOxFiw837qN0/1HGyQaeK/w5IYnvzlxVP4SyE77BpBM2Nn6gL/04+bU7lfIoTSyVUm3DN+fY5UYSJZdVlXaOzrALdWis8r4zpsOyv0QrN5d+aP+2ewwmlZ4XZTWVcM5espV7X/2UWqcrql0oQIeCAEeq66ipa33KebymngfmreWFGee2+jFUjLJiOLQLMPEjU+pr4LV/hSt+lzgetiyMLw6XxjDa2L+h2Uu2snPuawQTDLw+LbCFe/65meEVu7im4Z0jr0j5eZXymqwmliJyGfAwdhjfn40xDzRx3NXAi8BZOj9MKZU14eVUEg2LBZt03t8NxlyrhX2Udw2cCF37Q+XW6L7N70B45Y2SJ+3/23eHi+/XXpBWSrSGJtg5dtc98hFp5JYs3VLBzGdX8NC0sRlpa5uuqvzxbGiqb3nvOvjLFdGCPrFDX/ucARKMXqAJhJxhta206W0oW8oNwy9iy3mnw4fPxt0tAtcF3+PluvP4x+pjXFPY4Pfrqlv/3Ep5RNYSSxEJAn8ALsGuv7dURF4zxqxpcFwX4A5gSbbaopRSEVc/DoMnw7z/gLqqBAc4V703vKnDY5V3FXZs+ZjjFbYq8lv3wJd/oQlmKyTqzRw/uDvPzzg3sszJsq0HqGvFJM1XVu5g4tCeaRf3WVZawbTHFlFbZygIBXjmtjY2zLal4lW1x5xkEnjysphEsgBOGAF7VtttCdj/lxU3LpiVaP/SJ2D1y3DatdB7FPz9anvchw8z5MzpmAQ1Qwuo46rgQt6oP7vRfWu3l/POuk3NXhzI1AWENn0hQmVVNnssJwKbjDGbAUTkWeCrwJoGx/0c+C8g/cXdlVIqGeG1NH87Jr7XJ1Z4eKyWf1dedPZ3m15Kp6HqQ/bYuXdAqCN0H2R/XxPNVotNOGNP0uev3sUj/9zcwm9Hhdc6LD9Szei+XenSoYBDx2pYtLmc3l3b850LTmoxyfi47EBkaG51bT0vL9/WtpKFhnOOEymZBTs/iSaVYIfBhpPK8PaKv8PKp22J32Ch7ekEm5gu/5ut5BosgHP+BT50lvjZ8gGMmBp9nLpqbA+q0FRP6lB2Ntp3/ysrWVp3tMkla5aVVnDD44upqatPa1mbZaUVTH9sMbX16T2OUolkM7HsD5TFbG8D4i7RiMg4YKAx5nUR8U9i+dJt8OlL8R9QKvtC7eHsGXqSrzLnB6uaHxoL9uTh84Vwe4K5mUq5ZcLNsObVxHOGm1N71A4PDCeanXvDF+9uOslsqvdGRcQmmeH/v7JyO7sOJhoR0VhsBdl4lby1ZjfDT+zMxSNOpEuHAs4Z1hOAl5Zv4/mlZdTVGwINesb8tTp5BpwxHZbNAtPcYjIGti9r+bG2l9giQGATxI9nw8pnoPZ49Ji66sbfGYdiEsVgIZxxA7JxQaMLlzUEebnuPG4Pzm301IH6GuqdJWt+9o/V3Pu/RsddvHjo7Q1U1drXWFNbz+LN5a1KCBdvLqe6LrXH0R5OlSzXiveISAB4ELg5iWNvB24HGDQoiSEjJbPg7fvtMCCVP2qP25P8Dx+2ld+0yIrKhKsft7emCvsA7FgG9xdB7zEw9UE9wVbe8M05MP8+KH4cao4CYidypXLR8/DuaJIZLIRRV0bnF5cVw1OX2xPtQMiuqal/+y26c8pI7pwyMrLe4fw1u9N6vE17DrNpz2EgcR9Y7FzPoMBp/YrSej7fGTgReo2AvQ0HxKXIGNi91tkQGw9IfFIZ1rEnHNwR3R5yXjRxvek1WPd6wtEwz9d9keXmlITrqY6UUhYxGoCPt1Vy7SMfcft5w1i36yDvbYi/6FAQCkQuMsQq/ryc4s/3M+mkXnEJ4LLSirjKx4kep2HyGN7u3rGQe1/9lHpjKAwFuHfqaFeWzWlNcqsJce6JMdm5tiUik4D7jTGXOtt3ARhjfu1sFwGfAYedX+kD7AeuaK6Az4QJE0xJSRN3l8yC/7nLXpFVbVTAzj0669te7dlMMOsit5qNobaurBj+fo0dBtscrR7rJtdjCHwQR1n5PgzA4HNgwES7kPvIr+pw2iSEE8wVWyuoqq3nWHUttc11rqUpFBSeu31SSyfSrsdRRmPo8QuT65FMRZ8x0K6rrbrcSIMUP7YI0OSZ0WGyDVxVdT/LzSn8IPg8dxS8EnfftroevGfG8nLdeSw3pzTbtF99bQwrtu6n/HA1oWAgsv8t5yJGYVC4dsJArho3AIBpjy2KDJcOBsDpsOSl754bSSJjh9neO3U0P/3Hamrq6hGiFy/E+Y8xEAwIP//qaS3OEY5NUFubkC4rreD6RxdRV28IBYXrnNfW3OPMXrKVn7z6KcZJiJMd8ptCMup6DHlRNhPLELABuAjYDiwFbjDGrG7i+PeAf2upKmyTH0Qls5Kfb6LaGLFzImKvxLvaGHd5/oTYCx670PZSNifYDi7/P3pinXuuxxD4KI4iPZpHsvccoQ7Qazh0G2znu7XrCp++bIumnHlj4ot8JbNg7avuJ6fhdvQ5Hdp3zcmQ32WlFVzzp4+yOmR14hBbYKgZrsdRRmPIJ+eAsy9fxd1zVvFQ6HdcGVoUd1/4dLyGINOqf9JicpmMADDsxM6RHu+GZpw/jC4dCvi47EAkKQXo3C7I4arkRj70796B0X278p0LTgJotCbs80vLIkv2ALQPBXjaKTA1e8lW5n26k8tP6xupwBxOQLt3LOTTHZWRP9Snl8T3AIcC9jmuP2tQo+R29pKt3D1nVdy+MwYUxQ0vhsZJ5Aeb9nHTE8UYkkpGXY8hL8paYgkgIlOAh7DLjTxpjPmliPwMKDHGvNbg2PdIJ7H829fgM50DpZJQ2NnNHk3XP4h8c0LstpJZ8Ma/xa9zlkiw0BZy8GYPeT5yPYbAp3FUMgsW/xEOlOV4ZE8Aug2AL/zI9v6snhMfV1MfTpxcLnkMlv8Fug+ByXdEE76G8z5bm6R+9Ad46+74faEO0aUpUpXCfNRlpRXcM2cVa3cdSv15kiDAi05vVDOHuCrjMRT+O9i+DI63MOrELbfOZ/aOPtT/YyY3BhcgCf4VjIGn6y7intpbc9++HBDg9AFFtAsFKN6SmSlrM84fxiWj+7B4czmVR6t5fOHnCS/cFAYD3H/F6EjiGl6nNgCMGVDEgaM1lO63n40B4EeXnsr3vjS8uZeiGshqYpkNnuixlCAMvUCHwuVSWTG8/kPYtarlY1PRfRhc9Wgu5w25/kHkyxNiNzU397IhnYOZC67HEORJHHmyHoHYZR8EW4EzVmEnW2xof0zV1aJB8XPZegyDDt2hx0lQvtGu9xlOSsuK7RDFnZ9AQQeoPgIHtzduQs+TYeh5tihMOJZbSl7Liu16iXVVdjRDkslpeO7bM0u2ZrwHs4VeS9fjKGsxNP++Joeius65cLFu10GGv34dQeoa/UMYA2/WTWBG7Q9daaKKmnH+MO6cMrKpu12PIS/Kn8QSYr4kK6F9kS4Mna+yUZU3dxVnXf8gyosTYje0VD02lgThtKu9MPQ6H7keQ5CHcRS+eLd7TfxnqwTt/9tiFXQJwGnXwM6PYd/66P5hF8LQL0R7JsuK4b1fx4+a6jEMzr3Drm+YRC9mOMHcd8hWkn177W5asTRmfPNpttfS9TjKagzNvw/WvgYjr4DuQ6PDncs3wqFd9t+jqtKugXmgNPMXrZsiQbjwx3Dej+zfzTPT4Wh8YR5joJYA11ffm5HhsKr1vB5DXpRfiaVSJbPg3V/Bkb2QsO5aC8InEtlLCFz/INIYSkMTJwLJCUDnE5pf2kElw/UYgjYYRyWzYOFv4PAuZzmGFKvP5iWBboOgclsz74Xz5xpqn9IQ23DBn1XbDiS9bEki/970UL6U4khELgMexk5t+rMx5oEG9/8Q+DZQC+wFvmWMKW3uMT0VQ+He7EO7oF2RHU5bc8T2mgeCUF9Pq84p4gQg1KA3e/Z1sOHNRkd6aTjsONnAOYG1LK4f2SYT3RvOHsSvvjYm0V2e+C7yGk0sVf6LDKP9lJRW+MrOsEbXP4g0hjJg/n2w+BGoS1CGPhUShHadYfwt8b3lXils4k2uxxBoHEUkSjgxdjioBLJbNMhXxMZy0cCUCgQtK63gxj8vprrGJjV9u3egf1F7dh+sovxIFcNP6Mzhqlo27zvSqIezMBTgmduaLD6SdByJSBBbjPES7JrkS4Hpxpg1Mcd8CVhijDkqIt8FvmiMub65x/VVDIXnz+5ZZ3umj+1vYd3MGO26wIRbExeHevk78MmzjX7FAGu6nE9N37HMO3wSCw4NpiAY4ODxGhAhJBKZC5hN0wIL+EXBUwiGagq4sfrutJPLaYEFTAkW80bdRJ6tvyhDLc2eG88exC81sUyaJpaqbYn0aKawrtiY6zLZg+n6B5HGUAaVFcOc78TP+cq0Lv3ggv+EY+WJT0jb3gL2rscQaBwlrWHBoEDIzn+sr4FAAZRvaoM9nwJTH7LDZcuWwLALWhwm29LyB7FLOoQrabawHEMqiWWzy8clOH4s8HtjzOTmHtfXMRT7ubt7jb24UnvMzvltWFG8qeJUAH+9Eja/m/g+CRK5SJOgtzs8fHrT7kPsP1JNj06FdOtYyIGj1Ww/cCySgB46XkNh0M5bPmdYT45U17FmRyU7K483uhjRsSDAUecixjjZwPOFPyUk9qBaIzxYex2L60cm1YNZEJTIEidh0wIL+HXBE5Htu2pubTK59EJPaUFQeLbppXs88V3kNZpYqrZr/n2w6I9QX53EwQLDvpSJgk2ufxBpDGVJpnoxk9G+O4y/yc4hik1qJ89sC9VpXY8h0DjKmrJi+Hg2bH7fTmnoeQpUH7a1E+qqoeYYBAK2iM/RCluAp10nWxTlyF5smU2Buhq7zJSpc3pSPSwQgtFXwcTbPFlITkSuAS4zxnzb2f4GcLYx5vtNHP97YJcx5hfNPW7exlDssNqx32w6qSwrhicvTa7ns/swW4QqUyNYyorZvvItFtWNomv7EF12L6b7qAsZcdbFkYT1y5sf4IJDc6MrdkqQ0kk/Z+Ci+wiYOkywkEeHPsS2TqdFlhZ5cdk26urqKXCW6gBYvLmcQ8dqWL3zID+t/AlDDy6JPObawMn8786/4bR+XSk/Us34wEb6HSihc7fefHnrbwiaGmop4PqqH7PcnMI42cAVRZvpd+YlbGw3ikPHanh77W52HjzOkZjlUc4cUMSo/kWsKK2Iq7zcv3sHurYLsaPyGMeq6+nRqYDaekPlsZq4JHhIz45MHt4rYxdn2hJNLJVKqRdTYMy16fRguv5BpDGUA3/9Gmx+j/Tn5LRCuGr10C9Ah56w6jmo2GJ73vMj6Uw5hvJ+fphKXrin6fhB2PUJdOxlk9jjldC1H3Tta5dEcUMgCFMezNXw96wkliLydeD7wAXGmEaTQ0XkduB2gEGDBo0vLW02zPLbwt/Agl+Q+Hsiks7FGzE1fukdSH3USlkxPHGJ/TkQIjJfOtwzun25nfdZVxUfCyO+YqcIve98fEoALrzHFiJyLCut4PMV7zK5bgl9J17duD0NV3AIFMCU/7Yjcjr0hHn/bi8KicQl3HtOvZF/driYq1feilBv23rz3LjHb6pnv9H+Jt6v2DU1G66L2QTXz+e8SBNLpWIlu7SEBOHcf23NibrrH0QaQy7x2tIOwUIYdaX9edN8GH6JX6rYplp0ROeHqdQ07DXt0g+OltuT72M5iN9ep8Kplyeek5c5GR8KKyIXA7/DJpV7WnrcNh9D4SVqao+TUv0HCUK/M21vaO9R8NTltshQsABufj1aqbipZHPuTCh5KtEDQ//xsL2Jf5PJM20vbOyc0IbDfMuKnfbU2qTxljcaJ8HhpDbR8zf1Poz4CtTVwsaYQkcTvgVTfxt93GSS661LYNaUxu9X6WKbRA89zx6XXKLu+vmcF2liqVQiySaYvU6F7xen8siufxBpDHlE+OR1/ZtwaCfxX6jNfMHmnNgv4FFXwpF9UPoBdB1g139d93q0pH/Diywls2DFX6FL38ZX2NNuUAoH6/wwlUmRtTBX2aG51Ufs8FyTreG2Aeg/tvkhla2TSmIZwl6cuQjYjr04c4MxZnXMMWOBF7E9mxuTeVyNIRIvVZOKjr3iq5SP+IpNAGd9JTocPJw8fb4Qti6yy+esm5v6c0mw8XzofuNg3E3RGgAfPwMlT0bv7z8eLnsAVr8CG+ZBlz5Q+lErXmiARj27g8+FMdfbnt/KMrsvtvpywyJ4ZcW2WFJFzPSRXqdCz+Gw/vXGrzFRYtzgHWnFC8l7mlgq1ZRINdkk1rfq3DvZZSRc/yDSGPKRuHmbCb5YvSS2F7/hcCewJzvQOBFNvQJuqomlzg9T2Re+kBJqb7e3Lkq+amiyeo+Bs74Nu1YCAmdMT+eCTapxNAV4CDuc/EljzC9F5GdAiTHmNRF5GxgD7HR+Zasx5ormHlNjyNHansumFHaB6ui8QooG2R73SIXmLH2XSBCKBth1Qd3Ufzx0GwKrX4rumzzTFhGrS6amRozBk21ymZjr53NepImlUsl46TZY9XzLx7VcQdb1DyKNIZ+LDKk9YNdEO3uGXQDcS8NsW02g84ktXaTJWmKp88NUxoSH5nXoCZvegnVvYJOGTI9GCD+e2HlpHXpAx57Q6+SWRgrod5GXhEewlMzCO6NV8ogEWn+h59b5TcWR6zHkRZpYKpWKxy5sXEq8oeaTS9c/iDSG8li4l333GudLVCAYAglBbRXgo2Udmi7Rn5WhsDo/TGVV7BwwsEnE3g3ZLRTU/FA+/S7yorJi+NtV8T2Oyl0TbrHLAzXmegx5UcjtBijlK7e/k3iYX6xVz9vhE7qwvcq1gRNhxgctHxebgIpAQSe7pIOX1hNc8sdMxdBS4GQRGYqdHzYNuCH2AGd+2KPYns0Wk0qlUjZwYnyCF/65rBjevq+V885aUF9jk9m2sb5tfhg4Eb7xMjw1xf77KQ/Q/DEVmlgqlaoJN9tKbM3Nv1z7qiaWyruSTUAh8TDwQKFd/1WCtlckG2t3ZmgwjTGmVkS+D7xJdH7Y6tj5YcD/BToDL4gIJDE/TKmMGDgRbpkXLQpUVpLk0lfJkGgPqfKPgRNtT3PssjiV26Hic2jX1Y4+0R7N3AiE7FxmlTRNLJVqjdgT80QVZEd+NfdtUiobrn7cru/3yfPQYwhc/NPGPSCRQleriRSFCK+neXA77FvfzBOIM/+lQW/pOf+SsZdgjHkDeKPBvntjfr44Y0+mVGsMnAjTZtufY+dn7lpph8zuXmOTDOpJ+qpLZqsxq1xq2MPdUMksW4ym9jj0GQMFHW1l2doqu9RHqBA694H9m+20iPZdoUN3OLQ7WiSosEt0ikSoox21IgLAxCCNAAAIXUlEQVSdTrSVjo8fhICz3ahyeYzCTna6RVVl5t8HNw0+N/H3nWqWzrFUKhPCH/IicPZ3M1Z4JBs0hlTOvXQbrJ0L7bvYwjy9RyVeJyy5CrGuxxBoHCkXxSae4YXlj5XDnnU2uSjsCF/4UUujZlyPI40hHwkXFwpXI969JvFndfgzPLwMSp/To+ux7l5jlwapOgjdBtre1yP7ooWmINpLu36ePZ865TIo3xhd3ifUzlaeBZsg9zjJPk/HXrD/M3tMbTV06mUT6T1rbG+vBCEQsElwz+G2DRVboaA9DDjLruNc8mco/8w+5tQHk0koXY8hL9LEUqnccv2DSGNI+ZzrMQQaR8r3XI8jjSHlc67HkBcF3G6AUkoppZRSSil/08RSKaWUUkoppVRaNLFUSimllFJKKZUW382xFJG9QGkzh/QC9uWoOZnkx3b7sc3gbrv3GWMuc+m5AY0hD9J2p8b1GIK8jSM/thm03a3hehzlaQyBtjuX2nQMeZHvEsuWiEiJMWaC2+1IlR/b7cc2g3/bnSt+fX+03bnl13bnih/fHz+2GbTd+cqv74+2O3f82OZ8p0NhlVJKKaWUUkqlRRNLpZRSSimllFJpycfE8jG3G9BKfmy3H9sM/m13rvj1/dF255Zf250rfnx//Nhm0HbnK7++P9ru3PFjm/Na3s2xVEoppZRSSimVW/nYY6mUUkoppZRSKofyKrEUkctEZL2IbBKRO91uT5iIDBSRd0VkjYisFpE7nP09RGS+iGx0/t/d2S8i8v+c1/GJiIxzuf1BEVkhInOd7aEissRp33MiUujsb+dsb3LuH+Jim7uJyIsisk5E1orIJL+8327SGMpa+zWG2givxhD4O440htoWr8aRn2PIaY/GkcqqvEksRSQI/AG4HBgFTBeRUe62KqIW+JExZhRwDvA9p213AguMMScDC5xtsK/hZOd2O/Cn3Dc5zh3A2pjt/wJ+a4wZDlQAtzr7bwUqnP2/dY5zy8PA/xhjRgBnYNvvl/fbFRpDWaUx1AZ4PIbA33GkMdRGeDyO/BxDoHGkss0Ykxc3YBLwZsz2XcBdbreriba+ClwCrAf6Ovv6Auudnx8FpsccHznOhbYOwAbthcBcQLCL0YYavu/Am8Ak5+eQc5y40OYi4POGz+2H99vNm8ZQ1tqqMdRGbn6KIad9vogjjaG2dfNTHPklhpzn1jjSW9ZvedNjCfQHymK2tzn7PMUZTjAWWAL0NsbsdO7aBfR2fvbSa3kI+A+g3tnuCRwwxtQ627Fti7Tbub/SOT7XhgJ7gaecIR9/FpFO+OP9dpMv3geNoZzQGGod37wPPosjjaG2xRfvhc9iCDSOVA7kU2LpeSLSGXgJmGmMORh7n7GXVjxVoldEpgJ7jDHL3G5LikLAOOBPxpixwBGiwyQAb77fqmUaQzmjMZTH/BRHGkPKi/wUQ6BxpHInnxLL7cDAmO0Bzj5PEJEC7IfQ08aYl53du0Wkr3N/X2CPs98rr2UycIWIbAGexQ6feBjoJiKhBG2LtNu5vwgoz2WDHduAbcaYJc72i9gPJq+/327z9PugMZRTGkOt4/n3wYdxpDHU9nj6vfBhDIHGkcqRfEoslwInOxWuCoFpwGsutwmwVaqAJ4C1xpgHY+56DbjJ+fkm7Fj98P5vOtWtzgEqY7r8c8YYc5cxZoAxZgj2/XzHGHMj8C5wTRPtDr+ea5zjc34VyRizCygTkVOdXRcBa/D4++0BGkMZpjHU5ng2hsCfcaQx1CZ5No78GEOgcZTLNrd5bk/yzOQNmAJsAD4Dfux2e2La9QVsN/0nwErnNgU7Xn0BsBF4G+jhHC/YimifAauACR54DV8E5jo/DwOKgU3AC0A7Z397Z3uTc/8wF9t7JlDivOevAN399H67+L5pDGXvNWgMtYGbV2PIaZuv40hjqO3cvBpHfo8hp00aR3rL2k2cfwillFJKKaWUUqpV8mkorFJKKaWUUkopF2hiqZRSSimllFIqLZpYKqWUUkoppZRKiyaWSimllFJKKaXSoomlUkoppZRSSqm0aGKpUiIiXxSRuW63Qyk/0zhSKj0aQ0qlR2NIZYMmlkoppZRSSiml0qKJZZ4Ska+LSLGIrBSRR0UkKCKHReS3IrJaRBaIyAnOsWeKyGIR+URE5ohId2f/cBF5W0Q+FpHlInKS8/CdReRFEVknIk+LiLj2QpXKIo0jpdKjMaRUejSGlJ9oYpmHRGQkcD0w2RhzJlAH3Ah0AkqMMaOB94H7nF/5K/CfxpjTgVUx+58G/mCMOQM4F9jp7B8LzARGAcOAyVl/UUrlmMaRUunRGFIqPRpDym9CbjdAZcVFwHhgqXPxqQOwB6gHnnOO+TvwsogUAd2MMe87+/8CvCAiXYD+xpg5AMaY4wDO4xUbY7Y52yuBIcAH2X9ZSuWUxpFS6dEYUio9GkPKVzSxzE8C/MUYc1fcTpGfNDjOtPLxq2J+rkP/jlR+0jhSKj0aQ0qlR2NI+YoOhc1PC4BrROREABHpISKDsf/e1zjH3AB8YIypBCpE5Dxn/zeA940xh4BtInKl8xjtRKRjTl+FUu7SOFIqPRpDSqVHY0j5il6ZyEPGmDUicg/wlogEgBrge8ARYKJz3x7suH2Am4BHnA+azcAtzv5vAI+KyM+cx7g2hy9DKVdpHCmVHo0hpdKjMaT8Roxpbe+58hsROWyM6ex2O5TyM40jpdKjMaRUejSGlFfpUFillFJKKaWUUmnRHkullFJKKaWUUmnRHkullFJKKaWUUmnRxFIppZRSSimlVFo0sVRKKaWUUkoplRZNLJVSSimllFJKpUUTS6WUUkoppZRSadHEUimllFJKKaVUWv4/IXp8pjRcnkMAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 666\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp3_lr_deep\"\n",
        "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "args.batch_size = 193\n",
        "args.x_frames = 4\n",
        "args.y_frames = 4\n",
        "trainset = MyDataset(args.x_frames, args.y_frames, mydata, 0, int(0.6*len(mydata)))\n",
        "valset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.6*len(mydata)), int(0.8*len(mydata)))\n",
        "testset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.8*len(mydata)), int(len(mydata)))\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
        "\n",
        "args.input_dim = 1\n",
        "args.hid_dim = 16\n",
        "args.n_layers = 8\n",
        "\n",
        "args.l2 = 0.00001\n",
        "args.dropout = 0.0\n",
        "args.use_bn = True\n",
        "\n",
        "args.optim = 'Adam'\n",
        "args.lr = 0.0001\n",
        "args.epoch = 1000\n",
        "\n",
        "name_var1 = 'lr'\n",
        "list_var1 = [0.00002, 0.00003, 0.00004, 0.00005]\n",
        "\n",
        "for var1 in list_var1:\n",
        "    setattr(args, name_var1, var1)\n",
        "    print(args)\n",
        "                \n",
        "    setting, result = experiment(partition, deepcopy(args))\n",
        "    save_exp_result(setting, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69D6Ofx_u_2-",
        "outputId": "9a63adaf-fa84-447d-b320-0f8ca8474438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=1000, exp_name='exp3_lr_deep', hid_dim=16, input_dim=1, l2=1e-05, lr=2e-05, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.03470/0.38242. Took 0.11 sec\n",
            "Epoch 1, Loss(train/val) 1.03300/0.38343. Took 0.10 sec\n",
            "Epoch 2, Loss(train/val) 1.02343/0.38429. Took 0.08 sec\n",
            "Epoch 3, Loss(train/val) 1.03086/0.38506. Took 0.08 sec\n",
            "Epoch 4, Loss(train/val) 1.02083/0.38571. Took 0.08 sec\n",
            "Epoch 5, Loss(train/val) 1.03312/0.38628. Took 0.08 sec\n",
            "Epoch 6, Loss(train/val) 1.02409/0.38678. Took 0.09 sec\n",
            "Epoch 7, Loss(train/val) 1.02079/0.38721. Took 0.06 sec\n",
            "Epoch 8, Loss(train/val) 1.03286/0.38759. Took 0.05 sec\n",
            "Epoch 9, Loss(train/val) 1.02378/0.38792. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.02055/0.38821. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.03130/0.38847. Took 0.04 sec\n",
            "Epoch 12, Loss(train/val) 1.02566/0.38870. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 1.03085/0.38890. Took 0.04 sec\n",
            "Epoch 14, Loss(train/val) 1.02524/0.38910. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.01693/0.38928. Took 0.05 sec\n",
            "Epoch 16, Loss(train/val) 1.02198/0.38945. Took 0.05 sec\n",
            "Epoch 17, Loss(train/val) 1.02854/0.38961. Took 0.05 sec\n",
            "Epoch 18, Loss(train/val) 1.02217/0.38978. Took 0.05 sec\n",
            "Epoch 19, Loss(train/val) 1.02482/0.38994. Took 0.04 sec\n",
            "Epoch 20, Loss(train/val) 1.02006/0.39010. Took 0.04 sec\n",
            "Epoch 21, Loss(train/val) 1.01759/0.39027. Took 0.04 sec\n",
            "Epoch 22, Loss(train/val) 1.03319/0.39043. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 1.02715/0.39055. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 1.01344/0.39068. Took 0.04 sec\n",
            "Epoch 25, Loss(train/val) 1.03435/0.39083. Took 0.05 sec\n",
            "Epoch 26, Loss(train/val) 1.02493/0.39099. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.03132/0.39116. Took 0.06 sec\n",
            "Epoch 28, Loss(train/val) 1.03020/0.39136. Took 0.04 sec\n",
            "Epoch 29, Loss(train/val) 1.02394/0.39156. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 1.02552/0.39179. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 1.02845/0.39204. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 1.02788/0.39231. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 1.02887/0.39259. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 1.03171/0.39288. Took 0.04 sec\n",
            "Epoch 35, Loss(train/val) 1.02406/0.39316. Took 0.04 sec\n",
            "Epoch 36, Loss(train/val) 1.03230/0.39336. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.02824/0.39314. Took 0.05 sec\n",
            "Epoch 38, Loss(train/val) 1.02557/0.39296. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 1.02417/0.39280. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 1.03465/0.39267. Took 0.05 sec\n",
            "Epoch 41, Loss(train/val) 1.03181/0.39256. Took 0.05 sec\n",
            "Epoch 42, Loss(train/val) 1.02626/0.39249. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 1.01547/0.39242. Took 0.05 sec\n",
            "Epoch 44, Loss(train/val) 1.03276/0.39238. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 1.03147/0.39232. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 1.02787/0.39230. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 1.03190/0.39229. Took 0.05 sec\n",
            "Epoch 48, Loss(train/val) 1.01608/0.39230. Took 0.06 sec\n",
            "Epoch 49, Loss(train/val) 1.02631/0.39232. Took 0.06 sec\n",
            "Epoch 50, Loss(train/val) 1.03066/0.39232. Took 0.04 sec\n",
            "Epoch 51, Loss(train/val) 1.02800/0.39233. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 1.02506/0.39236. Took 0.05 sec\n",
            "Epoch 53, Loss(train/val) 1.02503/0.39240. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 1.02438/0.39241. Took 0.05 sec\n",
            "Epoch 55, Loss(train/val) 1.02727/0.39240. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 1.03272/0.39242. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 1.03028/0.39246. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 1.03487/0.39250. Took 0.05 sec\n",
            "Epoch 59, Loss(train/val) 1.03251/0.39255. Took 0.05 sec\n",
            "Epoch 60, Loss(train/val) 1.02271/0.39260. Took 0.05 sec\n",
            "Epoch 61, Loss(train/val) 1.02985/0.39261. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 1.02880/0.39261. Took 0.05 sec\n",
            "Epoch 63, Loss(train/val) 1.03238/0.39263. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 1.03020/0.39268. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 1.02228/0.39273. Took 0.05 sec\n",
            "Epoch 66, Loss(train/val) 1.02688/0.39275. Took 0.05 sec\n",
            "Epoch 67, Loss(train/val) 1.02802/0.39273. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 1.02453/0.39275. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 1.03012/0.39279. Took 0.06 sec\n",
            "Epoch 70, Loss(train/val) 1.02071/0.39284. Took 0.05 sec\n",
            "Epoch 71, Loss(train/val) 1.02148/0.39286. Took 0.05 sec\n",
            "Epoch 72, Loss(train/val) 1.01630/0.39287. Took 0.04 sec\n",
            "Epoch 73, Loss(train/val) 1.02919/0.39287. Took 0.04 sec\n",
            "Epoch 74, Loss(train/val) 1.02934/0.39286. Took 0.04 sec\n",
            "Epoch 75, Loss(train/val) 1.01959/0.39288. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 1.02833/0.39290. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 1.03094/0.39292. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 1.02732/0.39290. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 1.02279/0.39289. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 1.02500/0.39290. Took 0.05 sec\n",
            "Epoch 81, Loss(train/val) 1.02952/0.39289. Took 0.05 sec\n",
            "Epoch 82, Loss(train/val) 1.02689/0.39288. Took 0.05 sec\n",
            "Epoch 83, Loss(train/val) 1.01799/0.39289. Took 0.05 sec\n",
            "Epoch 84, Loss(train/val) 1.02120/0.39288. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 1.03050/0.39288. Took 0.05 sec\n",
            "Epoch 86, Loss(train/val) 1.02665/0.39286. Took 0.05 sec\n",
            "Epoch 87, Loss(train/val) 1.02850/0.39287. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 1.02144/0.39286. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 1.02567/0.39282. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 1.03003/0.39282. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 1.03029/0.39282. Took 0.06 sec\n",
            "Epoch 92, Loss(train/val) 1.02813/0.39281. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 1.03166/0.39276. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 1.02342/0.39267. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 1.02510/0.39265. Took 0.06 sec\n",
            "Epoch 96, Loss(train/val) 1.02762/0.39265. Took 0.05 sec\n",
            "Epoch 97, Loss(train/val) 1.02397/0.39267. Took 0.04 sec\n",
            "Epoch 98, Loss(train/val) 1.02802/0.39265. Took 0.05 sec\n",
            "Epoch 99, Loss(train/val) 1.02688/0.39260. Took 0.05 sec\n",
            "Epoch 100, Loss(train/val) 1.03225/0.39253. Took 0.05 sec\n",
            "Epoch 101, Loss(train/val) 1.03192/0.39251. Took 0.05 sec\n",
            "Epoch 102, Loss(train/val) 1.02114/0.39249. Took 0.04 sec\n",
            "Epoch 103, Loss(train/val) 1.02169/0.39248. Took 0.04 sec\n",
            "Epoch 104, Loss(train/val) 1.02127/0.39247. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 1.02792/0.39239. Took 0.05 sec\n",
            "Epoch 106, Loss(train/val) 1.02736/0.39232. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 1.02959/0.39224. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 1.03099/0.39222. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 1.01115/0.39223. Took 0.05 sec\n",
            "Epoch 110, Loss(train/val) 1.01976/0.39225. Took 0.05 sec\n",
            "Epoch 111, Loss(train/val) 1.02116/0.39219. Took 0.05 sec\n",
            "Epoch 112, Loss(train/val) 1.02890/0.39209. Took 0.06 sec\n",
            "Epoch 113, Loss(train/val) 1.02547/0.39206. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 1.03101/0.39206. Took 0.04 sec\n",
            "Epoch 115, Loss(train/val) 1.01300/0.39204. Took 0.05 sec\n",
            "Epoch 116, Loss(train/val) 1.02627/0.39197. Took 0.05 sec\n",
            "Epoch 117, Loss(train/val) 1.03282/0.39196. Took 0.04 sec\n",
            "Epoch 118, Loss(train/val) 1.01503/0.39190. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 1.01935/0.39185. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 1.02401/0.39175. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 1.03025/0.39171. Took 0.05 sec\n",
            "Epoch 122, Loss(train/val) 1.02323/0.39174. Took 0.05 sec\n",
            "Epoch 123, Loss(train/val) 1.02325/0.39179. Took 0.04 sec\n",
            "Epoch 124, Loss(train/val) 1.03263/0.39184. Took 0.04 sec\n",
            "Epoch 125, Loss(train/val) 1.02455/0.39161. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 1.02512/0.39153. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 1.01347/0.39153. Took 0.06 sec\n",
            "Epoch 128, Loss(train/val) 1.03522/0.39155. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 1.01803/0.39149. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 1.02836/0.39134. Took 0.05 sec\n",
            "Epoch 131, Loss(train/val) 1.02610/0.39100. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 1.02813/0.39078. Took 0.05 sec\n",
            "Epoch 133, Loss(train/val) 1.02319/0.39062. Took 0.05 sec\n",
            "Epoch 134, Loss(train/val) 1.03314/0.39058. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 1.02262/0.39053. Took 0.05 sec\n",
            "Epoch 136, Loss(train/val) 1.01891/0.39053. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 1.03358/0.39053. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 1.02155/0.39040. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 1.02567/0.39015. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 1.03123/0.38975. Took 0.05 sec\n",
            "Epoch 141, Loss(train/val) 1.01917/0.38955. Took 0.05 sec\n",
            "Epoch 142, Loss(train/val) 1.02664/0.38947. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 1.02958/0.38946. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 1.03161/0.38944. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 1.02831/0.38941. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 1.01471/0.38931. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 1.01637/0.38905. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 1.02513/0.38881. Took 0.05 sec\n",
            "Epoch 149, Loss(train/val) 1.02740/0.38867. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 1.02911/0.38854. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 1.01478/0.38848. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 1.01864/0.38847. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 1.02960/0.38835. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 1.02339/0.38816. Took 0.05 sec\n",
            "Epoch 155, Loss(train/val) 1.01759/0.38802. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 1.02500/0.38799. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 1.02596/0.38801. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 1.03238/0.38804. Took 0.05 sec\n",
            "Epoch 159, Loss(train/val) 1.02852/0.38804. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 1.02174/0.38805. Took 0.05 sec\n",
            "Epoch 161, Loss(train/val) 1.01588/0.38798. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 1.02549/0.38793. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 1.01423/0.38782. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 1.02608/0.38772. Took 0.04 sec\n",
            "Epoch 165, Loss(train/val) 1.02304/0.38758. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 1.02360/0.38742. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 1.02426/0.38732. Took 0.04 sec\n",
            "Epoch 168, Loss(train/val) 1.02167/0.38727. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 1.02803/0.38726. Took 0.04 sec\n",
            "Epoch 170, Loss(train/val) 1.01902/0.38730. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 1.02746/0.38730. Took 0.05 sec\n",
            "Epoch 172, Loss(train/val) 1.03013/0.38726. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 1.02754/0.38721. Took 0.04 sec\n",
            "Epoch 174, Loss(train/val) 1.02561/0.38727. Took 0.05 sec\n",
            "Epoch 175, Loss(train/val) 1.02085/0.38733. Took 0.05 sec\n",
            "Epoch 176, Loss(train/val) 1.03024/0.38730. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 1.02982/0.38731. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 1.03068/0.38739. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 1.02818/0.38752. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 1.02047/0.38781. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 1.02258/0.38818. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 1.01459/0.38859. Took 0.05 sec\n",
            "Epoch 183, Loss(train/val) 1.02683/0.38898. Took 0.05 sec\n",
            "Epoch 184, Loss(train/val) 1.02353/0.38939. Took 0.04 sec\n",
            "Epoch 185, Loss(train/val) 1.02699/0.38978. Took 0.05 sec\n",
            "Epoch 186, Loss(train/val) 1.02251/0.39021. Took 0.04 sec\n",
            "Epoch 187, Loss(train/val) 1.02677/0.39068. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 1.02175/0.39114. Took 0.04 sec\n",
            "Epoch 189, Loss(train/val) 1.02284/0.39152. Took 0.05 sec\n",
            "Epoch 190, Loss(train/val) 1.01654/0.39187. Took 0.05 sec\n",
            "Epoch 191, Loss(train/val) 1.02358/0.39220. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 1.02639/0.39244. Took 0.04 sec\n",
            "Epoch 193, Loss(train/val) 1.02125/0.39259. Took 0.05 sec\n",
            "Epoch 194, Loss(train/val) 1.02088/0.39270. Took 0.07 sec\n",
            "Epoch 195, Loss(train/val) 1.02325/0.39277. Took 0.05 sec\n",
            "Epoch 196, Loss(train/val) 1.02685/0.39280. Took 0.05 sec\n",
            "Epoch 197, Loss(train/val) 1.02459/0.39285. Took 0.05 sec\n",
            "Epoch 198, Loss(train/val) 1.01841/0.39290. Took 0.04 sec\n",
            "Epoch 199, Loss(train/val) 1.02267/0.39298. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 1.02330/0.39303. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 1.01568/0.39304. Took 0.04 sec\n",
            "Epoch 202, Loss(train/val) 1.01899/0.39303. Took 0.05 sec\n",
            "Epoch 203, Loss(train/val) 1.02509/0.39301. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 1.02285/0.39302. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 1.01689/0.39297. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 1.01530/0.39298. Took 0.04 sec\n",
            "Epoch 207, Loss(train/val) 1.01395/0.39301. Took 0.06 sec\n",
            "Epoch 208, Loss(train/val) 1.01975/0.39304. Took 0.05 sec\n",
            "Epoch 209, Loss(train/val) 1.01772/0.39309. Took 0.05 sec\n",
            "Epoch 210, Loss(train/val) 1.01824/0.39312. Took 0.05 sec\n",
            "Epoch 211, Loss(train/val) 1.01387/0.39314. Took 0.05 sec\n",
            "Epoch 212, Loss(train/val) 1.02027/0.39317. Took 0.06 sec\n",
            "Epoch 213, Loss(train/val) 1.01135/0.39321. Took 0.05 sec\n",
            "Epoch 214, Loss(train/val) 1.00819/0.39324. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.99291/0.39330. Took 0.06 sec\n",
            "Epoch 216, Loss(train/val) 1.01835/0.39334. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 1.01568/0.39338. Took 0.06 sec\n",
            "Epoch 218, Loss(train/val) 1.01903/0.39342. Took 0.05 sec\n",
            "Epoch 219, Loss(train/val) 1.01257/0.39346. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 1.01054/0.39351. Took 0.05 sec\n",
            "Epoch 221, Loss(train/val) 1.01891/0.39355. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 1.01349/0.39360. Took 0.06 sec\n",
            "Epoch 223, Loss(train/val) 1.00211/0.39366. Took 0.05 sec\n",
            "Epoch 224, Loss(train/val) 1.00965/0.39372. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 1.01950/0.39380. Took 0.05 sec\n",
            "Epoch 226, Loss(train/val) 1.01738/0.39388. Took 0.06 sec\n",
            "Epoch 227, Loss(train/val) 1.00443/0.39398. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 1.00209/0.39407. Took 0.06 sec\n",
            "Epoch 229, Loss(train/val) 1.01273/0.39418. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 1.01553/0.39430. Took 0.06 sec\n",
            "Epoch 231, Loss(train/val) 1.01349/0.39442. Took 0.05 sec\n",
            "Epoch 232, Loss(train/val) 1.01230/0.39454. Took 0.05 sec\n",
            "Epoch 233, Loss(train/val) 1.01458/0.39468. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 1.01555/0.39482. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 1.01657/0.39495. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 1.01492/0.39510. Took 0.05 sec\n",
            "Epoch 237, Loss(train/val) 1.01124/0.39525. Took 0.05 sec\n",
            "Epoch 238, Loss(train/val) 1.01243/0.39542. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 1.01636/0.39558. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 1.00460/0.39573. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 1.01847/0.39588. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 1.00742/0.39603. Took 0.05 sec\n",
            "Epoch 243, Loss(train/val) 1.01231/0.39619. Took 0.05 sec\n",
            "Epoch 244, Loss(train/val) 1.00689/0.39634. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 1.00295/0.39649. Took 0.05 sec\n",
            "Epoch 246, Loss(train/val) 1.00990/0.39665. Took 0.04 sec\n",
            "Epoch 247, Loss(train/val) 1.01081/0.39681. Took 0.05 sec\n",
            "Epoch 248, Loss(train/val) 1.01242/0.39697. Took 0.04 sec\n",
            "Epoch 249, Loss(train/val) 1.00427/0.39713. Took 0.05 sec\n",
            "Epoch 250, Loss(train/val) 0.99855/0.39729. Took 0.05 sec\n",
            "Epoch 251, Loss(train/val) 1.00326/0.39744. Took 0.05 sec\n",
            "Epoch 252, Loss(train/val) 1.00978/0.39760. Took 0.06 sec\n",
            "Epoch 253, Loss(train/val) 1.00795/0.39775. Took 0.07 sec\n",
            "Epoch 254, Loss(train/val) 1.00077/0.39790. Took 0.07 sec\n",
            "Epoch 255, Loss(train/val) 1.00425/0.39807. Took 0.07 sec\n",
            "Epoch 256, Loss(train/val) 1.00032/0.39817. Took 0.07 sec\n",
            "Epoch 257, Loss(train/val) 1.00296/0.39806. Took 0.08 sec\n",
            "Epoch 258, Loss(train/val) 0.99823/0.39768. Took 0.07 sec\n",
            "Epoch 259, Loss(train/val) 0.99183/0.39723. Took 0.07 sec\n",
            "Epoch 260, Loss(train/val) 0.99596/0.39680. Took 0.08 sec\n",
            "Epoch 261, Loss(train/val) 0.99083/0.39634. Took 0.07 sec\n",
            "Epoch 262, Loss(train/val) 0.99049/0.39595. Took 0.07 sec\n",
            "Epoch 263, Loss(train/val) 1.00037/0.39564. Took 0.07 sec\n",
            "Epoch 264, Loss(train/val) 0.99318/0.39549. Took 0.08 sec\n",
            "Epoch 265, Loss(train/val) 0.99894/0.39546. Took 0.08 sec\n",
            "Epoch 266, Loss(train/val) 0.98345/0.39541. Took 0.08 sec\n",
            "Epoch 267, Loss(train/val) 0.99473/0.39546. Took 0.08 sec\n",
            "Epoch 268, Loss(train/val) 0.99363/0.39550. Took 0.08 sec\n",
            "Epoch 269, Loss(train/val) 0.99699/0.39563. Took 0.08 sec\n",
            "Epoch 270, Loss(train/val) 0.98667/0.39575. Took 0.09 sec\n",
            "Epoch 271, Loss(train/val) 0.98953/0.39589. Took 0.07 sec\n",
            "Epoch 272, Loss(train/val) 0.99492/0.39608. Took 0.08 sec\n",
            "Epoch 273, Loss(train/val) 0.99196/0.39629. Took 0.08 sec\n",
            "Epoch 274, Loss(train/val) 0.98497/0.39651. Took 0.07 sec\n",
            "Epoch 275, Loss(train/val) 0.98636/0.39676. Took 0.08 sec\n",
            "Epoch 276, Loss(train/val) 0.98745/0.39697. Took 0.07 sec\n",
            "Epoch 277, Loss(train/val) 0.98219/0.39713. Took 0.08 sec\n",
            "Epoch 278, Loss(train/val) 0.98549/0.39726. Took 0.08 sec\n",
            "Epoch 279, Loss(train/val) 0.97295/0.39728. Took 0.08 sec\n",
            "Epoch 280, Loss(train/val) 0.97533/0.39728. Took 0.07 sec\n",
            "Epoch 281, Loss(train/val) 0.98066/0.39729. Took 0.08 sec\n",
            "Epoch 282, Loss(train/val) 0.97889/0.39721. Took 0.07 sec\n",
            "Epoch 283, Loss(train/val) 0.95711/0.39720. Took 0.05 sec\n",
            "Epoch 284, Loss(train/val) 0.96655/0.39694. Took 0.05 sec\n",
            "Epoch 285, Loss(train/val) 0.97412/0.39675. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.97180/0.39660. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.96874/0.39649. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.96695/0.39640. Took 0.05 sec\n",
            "Epoch 289, Loss(train/val) 0.96219/0.39610. Took 0.04 sec\n",
            "Epoch 290, Loss(train/val) 0.95810/0.39549. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.95886/0.39478. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.94584/0.39405. Took 0.05 sec\n",
            "Epoch 293, Loss(train/val) 0.94646/0.39343. Took 0.05 sec\n",
            "Epoch 294, Loss(train/val) 0.95128/0.39294. Took 0.05 sec\n",
            "Epoch 295, Loss(train/val) 0.95259/0.39251. Took 0.06 sec\n",
            "Epoch 296, Loss(train/val) 0.95124/0.39187. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.94952/0.39136. Took 0.05 sec\n",
            "Epoch 298, Loss(train/val) 0.95381/0.39078. Took 0.05 sec\n",
            "Epoch 299, Loss(train/val) 0.94509/0.39017. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.94000/0.38899. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.93990/0.38808. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.93618/0.38727. Took 0.04 sec\n",
            "Epoch 303, Loss(train/val) 0.92132/0.38664. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 0.93631/0.38591. Took 0.05 sec\n",
            "Epoch 305, Loss(train/val) 0.92721/0.38478. Took 0.06 sec\n",
            "Epoch 306, Loss(train/val) 0.92079/0.38406. Took 0.05 sec\n",
            "Epoch 307, Loss(train/val) 0.92253/0.38292. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.92536/0.38244. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.91434/0.38140. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.92065/0.38066. Took 0.05 sec\n",
            "Epoch 311, Loss(train/val) 0.91458/0.37998. Took 0.05 sec\n",
            "Epoch 312, Loss(train/val) 0.90838/0.37962. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 0.90762/0.37893. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.90726/0.37798. Took 0.05 sec\n",
            "Epoch 315, Loss(train/val) 0.90832/0.37655. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.89206/0.37608. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.89445/0.37412. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.88411/0.37306. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.88986/0.37181. Took 0.05 sec\n",
            "Epoch 320, Loss(train/val) 0.89533/0.37007. Took 0.05 sec\n",
            "Epoch 321, Loss(train/val) 0.88874/0.36898. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.88784/0.36747. Took 0.05 sec\n",
            "Epoch 323, Loss(train/val) 0.87837/0.36629. Took 0.04 sec\n",
            "Epoch 324, Loss(train/val) 0.87649/0.36506. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.87805/0.36452. Took 0.05 sec\n",
            "Epoch 326, Loss(train/val) 0.87874/0.36345. Took 0.05 sec\n",
            "Epoch 327, Loss(train/val) 0.87197/0.36264. Took 0.05 sec\n",
            "Epoch 328, Loss(train/val) 0.87141/0.36177. Took 0.04 sec\n",
            "Epoch 329, Loss(train/val) 0.87002/0.35999. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.86579/0.35871. Took 0.05 sec\n",
            "Epoch 331, Loss(train/val) 0.85876/0.35709. Took 0.04 sec\n",
            "Epoch 332, Loss(train/val) 0.84999/0.35492. Took 0.05 sec\n",
            "Epoch 333, Loss(train/val) 0.85648/0.35330. Took 0.04 sec\n",
            "Epoch 334, Loss(train/val) 0.85399/0.35089. Took 0.04 sec\n",
            "Epoch 335, Loss(train/val) 0.84609/0.34978. Took 0.05 sec\n",
            "Epoch 336, Loss(train/val) 0.85018/0.34744. Took 0.05 sec\n",
            "Epoch 337, Loss(train/val) 0.85000/0.34566. Took 0.04 sec\n",
            "Epoch 338, Loss(train/val) 0.84675/0.34394. Took 0.04 sec\n",
            "Epoch 339, Loss(train/val) 0.83869/0.34144. Took 0.04 sec\n",
            "Epoch 340, Loss(train/val) 0.83973/0.33953. Took 0.05 sec\n",
            "Epoch 341, Loss(train/val) 0.84164/0.33850. Took 0.05 sec\n",
            "Epoch 342, Loss(train/val) 0.82819/0.33737. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.83200/0.33484. Took 0.04 sec\n",
            "Epoch 344, Loss(train/val) 0.83133/0.33338. Took 0.04 sec\n",
            "Epoch 345, Loss(train/val) 0.83037/0.33126. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.83011/0.32931. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 0.82501/0.32823. Took 0.05 sec\n",
            "Epoch 348, Loss(train/val) 0.82812/0.32595. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 0.82466/0.32439. Took 0.04 sec\n",
            "Epoch 350, Loss(train/val) 0.83021/0.32169. Took 0.05 sec\n",
            "Epoch 351, Loss(train/val) 0.82108/0.32021. Took 0.05 sec\n",
            "Epoch 352, Loss(train/val) 0.81866/0.31862. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 0.81880/0.31471. Took 0.04 sec\n",
            "Epoch 354, Loss(train/val) 0.80682/0.31394. Took 0.04 sec\n",
            "Epoch 355, Loss(train/val) 0.80874/0.31252. Took 0.05 sec\n",
            "Epoch 356, Loss(train/val) 0.81150/0.31107. Took 0.04 sec\n",
            "Epoch 357, Loss(train/val) 0.80848/0.30957. Took 0.05 sec\n",
            "Epoch 358, Loss(train/val) 0.80818/0.30746. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 0.80181/0.30556. Took 0.05 sec\n",
            "Epoch 360, Loss(train/val) 0.79573/0.30362. Took 0.05 sec\n",
            "Epoch 361, Loss(train/val) 0.80783/0.30190. Took 0.05 sec\n",
            "Epoch 362, Loss(train/val) 0.79709/0.29996. Took 0.04 sec\n",
            "Epoch 363, Loss(train/val) 0.80356/0.29978. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 0.80329/0.29748. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.80344/0.29703. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.80327/0.29638. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 0.79808/0.29522. Took 0.04 sec\n",
            "Epoch 368, Loss(train/val) 0.79657/0.29462. Took 0.05 sec\n",
            "Epoch 369, Loss(train/val) 0.79189/0.29425. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.78808/0.29436. Took 0.06 sec\n",
            "Epoch 371, Loss(train/val) 0.79444/0.29331. Took 0.05 sec\n",
            "Epoch 372, Loss(train/val) 0.79479/0.29122. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 0.79324/0.28942. Took 0.04 sec\n",
            "Epoch 374, Loss(train/val) 0.79624/0.28776. Took 0.04 sec\n",
            "Epoch 375, Loss(train/val) 0.79432/0.28683. Took 0.05 sec\n",
            "Epoch 376, Loss(train/val) 0.78697/0.28531. Took 0.05 sec\n",
            "Epoch 377, Loss(train/val) 0.79095/0.28339. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.78756/0.28301. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 0.79445/0.28215. Took 0.05 sec\n",
            "Epoch 380, Loss(train/val) 0.78928/0.28167. Took 0.06 sec\n",
            "Epoch 381, Loss(train/val) 0.79173/0.27959. Took 0.05 sec\n",
            "Epoch 382, Loss(train/val) 0.77690/0.27860. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.78160/0.27779. Took 0.04 sec\n",
            "Epoch 384, Loss(train/val) 0.78793/0.27618. Took 0.04 sec\n",
            "Epoch 385, Loss(train/val) 0.78235/0.27492. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.77760/0.27478. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.77259/0.27353. Took 0.04 sec\n",
            "Epoch 388, Loss(train/val) 0.78262/0.27315. Took 0.05 sec\n",
            "Epoch 389, Loss(train/val) 0.77842/0.27149. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.77728/0.27036. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.77420/0.26956. Took 0.04 sec\n",
            "Epoch 392, Loss(train/val) 0.77223/0.26745. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.77215/0.26665. Took 0.04 sec\n",
            "Epoch 394, Loss(train/val) 0.77627/0.26627. Took 0.05 sec\n",
            "Epoch 395, Loss(train/val) 0.78004/0.26613. Took 0.05 sec\n",
            "Epoch 396, Loss(train/val) 0.76953/0.26541. Took 0.05 sec\n",
            "Epoch 397, Loss(train/val) 0.77606/0.26596. Took 0.04 sec\n",
            "Epoch 398, Loss(train/val) 0.76659/0.26596. Took 0.04 sec\n",
            "Epoch 399, Loss(train/val) 0.77123/0.26575. Took 0.04 sec\n",
            "Epoch 400, Loss(train/val) 0.76955/0.26544. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 0.76843/0.26378. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.77229/0.26253. Took 0.04 sec\n",
            "Epoch 403, Loss(train/val) 0.77362/0.26263. Took 0.04 sec\n",
            "Epoch 404, Loss(train/val) 0.77552/0.26137. Took 0.04 sec\n",
            "Epoch 405, Loss(train/val) 0.76968/0.26125. Took 0.05 sec\n",
            "Epoch 406, Loss(train/val) 0.76962/0.26125. Took 0.05 sec\n",
            "Epoch 407, Loss(train/val) 0.77345/0.26103. Took 0.04 sec\n",
            "Epoch 408, Loss(train/val) 0.76931/0.26094. Took 0.05 sec\n",
            "Epoch 409, Loss(train/val) 0.76694/0.25953. Took 0.04 sec\n",
            "Epoch 410, Loss(train/val) 0.75666/0.26000. Took 0.05 sec\n",
            "Epoch 411, Loss(train/val) 0.75390/0.25962. Took 0.05 sec\n",
            "Epoch 412, Loss(train/val) 0.76400/0.25911. Took 0.04 sec\n",
            "Epoch 413, Loss(train/val) 0.76652/0.25874. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.76309/0.25952. Took 0.06 sec\n",
            "Epoch 415, Loss(train/val) 0.76895/0.25852. Took 0.05 sec\n",
            "Epoch 416, Loss(train/val) 0.75938/0.25803. Took 0.05 sec\n",
            "Epoch 417, Loss(train/val) 0.76575/0.25773. Took 0.04 sec\n",
            "Epoch 418, Loss(train/val) 0.76660/0.25778. Took 0.04 sec\n",
            "Epoch 419, Loss(train/val) 0.76329/0.25733. Took 0.04 sec\n",
            "Epoch 420, Loss(train/val) 0.76299/0.25687. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.76596/0.25676. Took 0.05 sec\n",
            "Epoch 422, Loss(train/val) 0.76645/0.25645. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.76088/0.25565. Took 0.04 sec\n",
            "Epoch 424, Loss(train/val) 0.76870/0.25509. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.75688/0.25491. Took 0.05 sec\n",
            "Epoch 426, Loss(train/val) 0.75898/0.25450. Took 0.05 sec\n",
            "Epoch 427, Loss(train/val) 0.76283/0.25473. Took 0.04 sec\n",
            "Epoch 428, Loss(train/val) 0.75242/0.25427. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.76396/0.25464. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.75869/0.25363. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.75374/0.25263. Took 0.04 sec\n",
            "Epoch 432, Loss(train/val) 0.75345/0.25253. Took 0.05 sec\n",
            "Epoch 433, Loss(train/val) 0.75995/0.25211. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.75609/0.25255. Took 0.04 sec\n",
            "Epoch 435, Loss(train/val) 0.76318/0.25144. Took 0.05 sec\n",
            "Epoch 436, Loss(train/val) 0.76439/0.25123. Took 0.05 sec\n",
            "Epoch 437, Loss(train/val) 0.75601/0.25139. Took 0.04 sec\n",
            "Epoch 438, Loss(train/val) 0.75896/0.25158. Took 0.04 sec\n",
            "Epoch 439, Loss(train/val) 0.75589/0.25165. Took 0.05 sec\n",
            "Epoch 440, Loss(train/val) 0.75581/0.25162. Took 0.05 sec\n",
            "Epoch 441, Loss(train/val) 0.75097/0.25077. Took 0.04 sec\n",
            "Epoch 442, Loss(train/val) 0.75103/0.25092. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 0.75756/0.25043. Took 0.05 sec\n",
            "Epoch 444, Loss(train/val) 0.75511/0.25047. Took 0.05 sec\n",
            "Epoch 445, Loss(train/val) 0.75910/0.25045. Took 0.05 sec\n",
            "Epoch 446, Loss(train/val) 0.75341/0.25044. Took 0.05 sec\n",
            "Epoch 447, Loss(train/val) 0.75656/0.25019. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 0.75400/0.25032. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 0.75214/0.25029. Took 0.04 sec\n",
            "Epoch 450, Loss(train/val) 0.75701/0.25053. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 0.76023/0.25109. Took 0.05 sec\n",
            "Epoch 452, Loss(train/val) 0.75049/0.25163. Took 0.04 sec\n",
            "Epoch 453, Loss(train/val) 0.74794/0.25059. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.75242/0.25048. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 0.76957/0.25011. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.74887/0.24966. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.75484/0.24941. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.75262/0.24920. Took 0.05 sec\n",
            "Epoch 459, Loss(train/val) 0.74623/0.24928. Took 0.05 sec\n",
            "Epoch 460, Loss(train/val) 0.74972/0.24886. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.75950/0.24874. Took 0.05 sec\n",
            "Epoch 462, Loss(train/val) 0.74753/0.24873. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.75320/0.24877. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.75097/0.24922. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.74246/0.24964. Took 0.05 sec\n",
            "Epoch 466, Loss(train/val) 0.75177/0.25000. Took 0.05 sec\n",
            "Epoch 467, Loss(train/val) 0.73361/0.25033. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.75073/0.25054. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 0.74905/0.25038. Took 0.04 sec\n",
            "Epoch 470, Loss(train/val) 0.74941/0.25040. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 0.75541/0.25041. Took 0.06 sec\n",
            "Epoch 472, Loss(train/val) 0.74372/0.25059. Took 0.06 sec\n",
            "Epoch 473, Loss(train/val) 0.74497/0.25037. Took 0.05 sec\n",
            "Epoch 474, Loss(train/val) 0.74580/0.25000. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 0.74852/0.24992. Took 0.04 sec\n",
            "Epoch 476, Loss(train/val) 0.74538/0.24989. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.74538/0.24995. Took 0.04 sec\n",
            "Epoch 478, Loss(train/val) 0.75202/0.24983. Took 0.04 sec\n",
            "Epoch 479, Loss(train/val) 0.74323/0.25008. Took 0.06 sec\n",
            "Epoch 480, Loss(train/val) 0.74790/0.24999. Took 0.05 sec\n",
            "Epoch 481, Loss(train/val) 0.74259/0.24946. Took 0.04 sec\n",
            "Epoch 482, Loss(train/val) 0.74025/0.24921. Took 0.04 sec\n",
            "Epoch 483, Loss(train/val) 0.74533/0.24925. Took 0.04 sec\n",
            "Epoch 484, Loss(train/val) 0.75106/0.24920. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.75832/0.24899. Took 0.05 sec\n",
            "Epoch 486, Loss(train/val) 0.74410/0.24903. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.74669/0.24931. Took 0.04 sec\n",
            "Epoch 488, Loss(train/val) 0.74499/0.24931. Took 0.04 sec\n",
            "Epoch 489, Loss(train/val) 0.74393/0.24931. Took 0.05 sec\n",
            "Epoch 490, Loss(train/val) 0.74274/0.24928. Took 0.04 sec\n",
            "Epoch 491, Loss(train/val) 0.73957/0.24929. Took 0.04 sec\n",
            "Epoch 492, Loss(train/val) 0.74817/0.24988. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.74463/0.24946. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.74518/0.24949. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.74471/0.24956. Took 0.04 sec\n",
            "Epoch 496, Loss(train/val) 0.74413/0.24986. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 0.74937/0.24969. Took 0.04 sec\n",
            "Epoch 498, Loss(train/val) 0.74238/0.24988. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.73872/0.24946. Took 0.05 sec\n",
            "Epoch 500, Loss(train/val) 0.73849/0.24950. Took 0.05 sec\n",
            "Epoch 501, Loss(train/val) 0.74480/0.24957. Took 0.05 sec\n",
            "Epoch 502, Loss(train/val) 0.74102/0.24957. Took 0.05 sec\n",
            "Epoch 503, Loss(train/val) 0.73735/0.24950. Took 0.04 sec\n",
            "Epoch 504, Loss(train/val) 0.74471/0.24930. Took 0.05 sec\n",
            "Epoch 505, Loss(train/val) 0.74230/0.24935. Took 0.04 sec\n",
            "Epoch 506, Loss(train/val) 0.74467/0.24942. Took 0.05 sec\n",
            "Epoch 507, Loss(train/val) 0.74462/0.24967. Took 0.05 sec\n",
            "Epoch 508, Loss(train/val) 0.73427/0.24964. Took 0.05 sec\n",
            "Epoch 509, Loss(train/val) 0.73495/0.24980. Took 0.05 sec\n",
            "Epoch 510, Loss(train/val) 0.73492/0.24993. Took 0.04 sec\n",
            "Epoch 511, Loss(train/val) 0.74107/0.24993. Took 0.05 sec\n",
            "Epoch 512, Loss(train/val) 0.73689/0.24967. Took 0.04 sec\n",
            "Epoch 513, Loss(train/val) 0.74479/0.24965. Took 0.05 sec\n",
            "Epoch 514, Loss(train/val) 0.74295/0.24968. Took 0.05 sec\n",
            "Epoch 515, Loss(train/val) 0.73389/0.24963. Took 0.05 sec\n",
            "Epoch 516, Loss(train/val) 0.74296/0.24958. Took 0.04 sec\n",
            "Epoch 517, Loss(train/val) 0.73760/0.24962. Took 0.05 sec\n",
            "Epoch 518, Loss(train/val) 0.73612/0.24959. Took 0.05 sec\n",
            "Epoch 519, Loss(train/val) 0.74181/0.24955. Took 0.05 sec\n",
            "Epoch 520, Loss(train/val) 0.74051/0.24944. Took 0.05 sec\n",
            "Epoch 521, Loss(train/val) 0.74161/0.24954. Took 0.05 sec\n",
            "Epoch 522, Loss(train/val) 0.73800/0.24971. Took 0.05 sec\n",
            "Epoch 523, Loss(train/val) 0.73924/0.24953. Took 0.04 sec\n",
            "Epoch 524, Loss(train/val) 0.74048/0.24953. Took 0.05 sec\n",
            "Epoch 525, Loss(train/val) 0.74447/0.24951. Took 0.05 sec\n",
            "Epoch 526, Loss(train/val) 0.73572/0.24954. Took 0.05 sec\n",
            "Epoch 527, Loss(train/val) 0.73719/0.24950. Took 0.05 sec\n",
            "Epoch 528, Loss(train/val) 0.73730/0.24920. Took 0.05 sec\n",
            "Epoch 529, Loss(train/val) 0.73991/0.24910. Took 0.05 sec\n",
            "Epoch 530, Loss(train/val) 0.74036/0.24909. Took 0.05 sec\n",
            "Epoch 531, Loss(train/val) 0.73296/0.24920. Took 0.05 sec\n",
            "Epoch 532, Loss(train/val) 0.73362/0.24957. Took 0.05 sec\n",
            "Epoch 533, Loss(train/val) 0.73175/0.24955. Took 0.05 sec\n",
            "Epoch 534, Loss(train/val) 0.73555/0.24953. Took 0.05 sec\n",
            "Epoch 535, Loss(train/val) 0.73514/0.24964. Took 0.05 sec\n",
            "Epoch 536, Loss(train/val) 0.73786/0.24980. Took 0.05 sec\n",
            "Epoch 537, Loss(train/val) 0.73644/0.24983. Took 0.05 sec\n",
            "Epoch 538, Loss(train/val) 0.72936/0.24993. Took 0.05 sec\n",
            "Epoch 539, Loss(train/val) 0.73597/0.25000. Took 0.05 sec\n",
            "Epoch 540, Loss(train/val) 0.73220/0.24999. Took 0.04 sec\n",
            "Epoch 541, Loss(train/val) 0.73161/0.25008. Took 0.04 sec\n",
            "Epoch 542, Loss(train/val) 0.73195/0.24974. Took 0.04 sec\n",
            "Epoch 543, Loss(train/val) 0.72930/0.24991. Took 0.05 sec\n",
            "Epoch 544, Loss(train/val) 0.73462/0.25002. Took 0.05 sec\n",
            "Epoch 545, Loss(train/val) 0.72979/0.25017. Took 0.05 sec\n",
            "Epoch 546, Loss(train/val) 0.73542/0.25002. Took 0.04 sec\n",
            "Epoch 547, Loss(train/val) 0.73268/0.24995. Took 0.04 sec\n",
            "Epoch 548, Loss(train/val) 0.73755/0.24986. Took 0.05 sec\n",
            "Epoch 549, Loss(train/val) 0.73799/0.24983. Took 0.05 sec\n",
            "Epoch 550, Loss(train/val) 0.73680/0.24988. Took 0.04 sec\n",
            "Epoch 551, Loss(train/val) 0.73078/0.25002. Took 0.05 sec\n",
            "Epoch 552, Loss(train/val) 0.73092/0.25005. Took 0.05 sec\n",
            "Epoch 553, Loss(train/val) 0.73739/0.24996. Took 0.05 sec\n",
            "Epoch 554, Loss(train/val) 0.73221/0.24990. Took 0.05 sec\n",
            "Epoch 555, Loss(train/val) 0.72805/0.25011. Took 0.05 sec\n",
            "Epoch 556, Loss(train/val) 0.73225/0.25036. Took 0.04 sec\n",
            "Epoch 557, Loss(train/val) 0.72810/0.25038. Took 0.04 sec\n",
            "Epoch 558, Loss(train/val) 0.73177/0.25024. Took 0.04 sec\n",
            "Epoch 559, Loss(train/val) 0.72612/0.25024. Took 0.05 sec\n",
            "Epoch 560, Loss(train/val) 0.72743/0.25027. Took 0.05 sec\n",
            "Epoch 561, Loss(train/val) 0.73336/0.25006. Took 0.04 sec\n",
            "Epoch 562, Loss(train/val) 0.73229/0.24978. Took 0.04 sec\n",
            "Epoch 563, Loss(train/val) 0.72022/0.24985. Took 0.05 sec\n",
            "Epoch 564, Loss(train/val) 0.72565/0.24987. Took 0.05 sec\n",
            "Epoch 565, Loss(train/val) 0.73738/0.24979. Took 0.05 sec\n",
            "Epoch 566, Loss(train/val) 0.72600/0.24978. Took 0.05 sec\n",
            "Epoch 567, Loss(train/val) 0.72389/0.25008. Took 0.04 sec\n",
            "Epoch 568, Loss(train/val) 0.72734/0.25008. Took 0.04 sec\n",
            "Epoch 569, Loss(train/val) 0.73193/0.25011. Took 0.05 sec\n",
            "Epoch 570, Loss(train/val) 0.73066/0.25007. Took 0.05 sec\n",
            "Epoch 571, Loss(train/val) 0.72862/0.24993. Took 0.04 sec\n",
            "Epoch 572, Loss(train/val) 0.73129/0.24989. Took 0.05 sec\n",
            "Epoch 573, Loss(train/val) 0.71795/0.25009. Took 0.04 sec\n",
            "Epoch 574, Loss(train/val) 0.73914/0.24987. Took 0.05 sec\n",
            "Epoch 575, Loss(train/val) 0.73409/0.24976. Took 0.05 sec\n",
            "Epoch 576, Loss(train/val) 0.72889/0.24982. Took 0.04 sec\n",
            "Epoch 577, Loss(train/val) 0.72849/0.24973. Took 0.05 sec\n",
            "Epoch 578, Loss(train/val) 0.72303/0.24971. Took 0.04 sec\n",
            "Epoch 579, Loss(train/val) 0.72975/0.24958. Took 0.05 sec\n",
            "Epoch 580, Loss(train/val) 0.71983/0.24975. Took 0.05 sec\n",
            "Epoch 581, Loss(train/val) 0.72927/0.24987. Took 0.05 sec\n",
            "Epoch 582, Loss(train/val) 0.72203/0.24995. Took 0.05 sec\n",
            "Epoch 583, Loss(train/val) 0.72736/0.24986. Took 0.05 sec\n",
            "Epoch 584, Loss(train/val) 0.72174/0.24987. Took 0.05 sec\n",
            "Epoch 585, Loss(train/val) 0.72519/0.24994. Took 0.05 sec\n",
            "Epoch 586, Loss(train/val) 0.73001/0.24981. Took 0.05 sec\n",
            "Epoch 587, Loss(train/val) 0.72428/0.24985. Took 0.05 sec\n",
            "Epoch 588, Loss(train/val) 0.72840/0.24976. Took 0.04 sec\n",
            "Epoch 589, Loss(train/val) 0.72522/0.24999. Took 0.05 sec\n",
            "Epoch 590, Loss(train/val) 0.72251/0.24989. Took 0.05 sec\n",
            "Epoch 591, Loss(train/val) 0.72381/0.24997. Took 0.05 sec\n",
            "Epoch 592, Loss(train/val) 0.72592/0.24988. Took 0.04 sec\n",
            "Epoch 593, Loss(train/val) 0.72880/0.24986. Took 0.05 sec\n",
            "Epoch 594, Loss(train/val) 0.72435/0.24975. Took 0.05 sec\n",
            "Epoch 595, Loss(train/val) 0.72513/0.24985. Took 0.05 sec\n",
            "Epoch 596, Loss(train/val) 0.72607/0.24983. Took 0.04 sec\n",
            "Epoch 597, Loss(train/val) 0.72917/0.24973. Took 0.05 sec\n",
            "Epoch 598, Loss(train/val) 0.72584/0.24982. Took 0.05 sec\n",
            "Epoch 599, Loss(train/val) 0.72854/0.24972. Took 0.05 sec\n",
            "Epoch 600, Loss(train/val) 0.72147/0.24982. Took 0.05 sec\n",
            "Epoch 601, Loss(train/val) 0.72130/0.24982. Took 0.05 sec\n",
            "Epoch 602, Loss(train/val) 0.71027/0.24991. Took 0.05 sec\n",
            "Epoch 603, Loss(train/val) 0.72081/0.24990. Took 0.04 sec\n",
            "Epoch 604, Loss(train/val) 0.72226/0.24993. Took 0.05 sec\n",
            "Epoch 605, Loss(train/val) 0.72480/0.25018. Took 0.05 sec\n",
            "Epoch 606, Loss(train/val) 0.72531/0.24998. Took 0.05 sec\n",
            "Epoch 607, Loss(train/val) 0.72073/0.24983. Took 0.05 sec\n",
            "Epoch 608, Loss(train/val) 0.72524/0.24985. Took 0.04 sec\n",
            "Epoch 609, Loss(train/val) 0.72533/0.25011. Took 0.06 sec\n",
            "Epoch 610, Loss(train/val) 0.71360/0.25049. Took 0.04 sec\n",
            "Epoch 611, Loss(train/val) 0.71710/0.25070. Took 0.04 sec\n",
            "Epoch 612, Loss(train/val) 0.71991/0.25058. Took 0.04 sec\n",
            "Epoch 613, Loss(train/val) 0.72006/0.25023. Took 0.05 sec\n",
            "Epoch 614, Loss(train/val) 0.72219/0.25015. Took 0.05 sec\n",
            "Epoch 615, Loss(train/val) 0.71586/0.25012. Took 0.05 sec\n",
            "Epoch 616, Loss(train/val) 0.72433/0.24991. Took 0.05 sec\n",
            "Epoch 617, Loss(train/val) 0.72207/0.24993. Took 0.05 sec\n",
            "Epoch 618, Loss(train/val) 0.71581/0.25000. Took 0.05 sec\n",
            "Epoch 619, Loss(train/val) 0.71370/0.25024. Took 0.05 sec\n",
            "Epoch 620, Loss(train/val) 0.71917/0.25020. Took 0.05 sec\n",
            "Epoch 621, Loss(train/val) 0.72439/0.24994. Took 0.05 sec\n",
            "Epoch 622, Loss(train/val) 0.71973/0.24982. Took 0.04 sec\n",
            "Epoch 623, Loss(train/val) 0.71871/0.24999. Took 0.04 sec\n",
            "Epoch 624, Loss(train/val) 0.71739/0.24998. Took 0.05 sec\n",
            "Epoch 625, Loss(train/val) 0.72063/0.24994. Took 0.05 sec\n",
            "Epoch 626, Loss(train/val) 0.71667/0.24996. Took 0.05 sec\n",
            "Epoch 627, Loss(train/val) 0.72590/0.24994. Took 0.04 sec\n",
            "Epoch 628, Loss(train/val) 0.71935/0.24993. Took 0.05 sec\n",
            "Epoch 629, Loss(train/val) 0.72319/0.25003. Took 0.06 sec\n",
            "Epoch 630, Loss(train/val) 0.71097/0.25013. Took 0.05 sec\n",
            "Epoch 631, Loss(train/val) 0.72436/0.25001. Took 0.04 sec\n",
            "Epoch 632, Loss(train/val) 0.71216/0.24985. Took 0.04 sec\n",
            "Epoch 633, Loss(train/val) 0.71735/0.24982. Took 0.04 sec\n",
            "Epoch 634, Loss(train/val) 0.71510/0.24992. Took 0.05 sec\n",
            "Epoch 635, Loss(train/val) 0.71211/0.25011. Took 0.05 sec\n",
            "Epoch 636, Loss(train/val) 0.71683/0.25004. Took 0.05 sec\n",
            "Epoch 637, Loss(train/val) 0.71382/0.24993. Took 0.05 sec\n",
            "Epoch 638, Loss(train/val) 0.71000/0.24993. Took 0.05 sec\n",
            "Epoch 639, Loss(train/val) 0.71413/0.25000. Took 0.05 sec\n",
            "Epoch 640, Loss(train/val) 0.71344/0.25011. Took 0.04 sec\n",
            "Epoch 641, Loss(train/val) 0.71434/0.25016. Took 0.04 sec\n",
            "Epoch 642, Loss(train/val) 0.71347/0.25022. Took 0.04 sec\n",
            "Epoch 643, Loss(train/val) 0.71787/0.25031. Took 0.04 sec\n",
            "Epoch 644, Loss(train/val) 0.72044/0.25034. Took 0.05 sec\n",
            "Epoch 645, Loss(train/val) 0.72140/0.25028. Took 0.05 sec\n",
            "Epoch 646, Loss(train/val) 0.71519/0.25054. Took 0.04 sec\n",
            "Epoch 647, Loss(train/val) 0.71951/0.25033. Took 0.04 sec\n",
            "Epoch 648, Loss(train/val) 0.71668/0.25014. Took 0.05 sec\n",
            "Epoch 649, Loss(train/val) 0.71077/0.24990. Took 0.05 sec\n",
            "Epoch 650, Loss(train/val) 0.71695/0.24974. Took 0.05 sec\n",
            "Epoch 651, Loss(train/val) 0.71507/0.24986. Took 0.05 sec\n",
            "Epoch 652, Loss(train/val) 0.71242/0.24998. Took 0.05 sec\n",
            "Epoch 653, Loss(train/val) 0.71245/0.24996. Took 0.05 sec\n",
            "Epoch 654, Loss(train/val) 0.72101/0.24976. Took 0.05 sec\n",
            "Epoch 655, Loss(train/val) 0.71904/0.24978. Took 0.05 sec\n",
            "Epoch 656, Loss(train/val) 0.71508/0.24980. Took 0.05 sec\n",
            "Epoch 657, Loss(train/val) 0.71117/0.24984. Took 0.05 sec\n",
            "Epoch 658, Loss(train/val) 0.70947/0.24989. Took 0.05 sec\n",
            "Epoch 659, Loss(train/val) 0.70955/0.24988. Took 0.05 sec\n",
            "Epoch 660, Loss(train/val) 0.71176/0.24988. Took 0.04 sec\n",
            "Epoch 661, Loss(train/val) 0.71075/0.24985. Took 0.04 sec\n",
            "Epoch 662, Loss(train/val) 0.71071/0.24981. Took 0.05 sec\n",
            "Epoch 663, Loss(train/val) 0.71191/0.24996. Took 0.04 sec\n",
            "Epoch 664, Loss(train/val) 0.71375/0.25021. Took 0.05 sec\n",
            "Epoch 665, Loss(train/val) 0.71199/0.25003. Took 0.05 sec\n",
            "Epoch 666, Loss(train/val) 0.70929/0.25041. Took 0.05 sec\n",
            "Epoch 667, Loss(train/val) 0.71504/0.25008. Took 0.05 sec\n",
            "Epoch 668, Loss(train/val) 0.71205/0.25008. Took 0.04 sec\n",
            "Epoch 669, Loss(train/val) 0.70681/0.25015. Took 0.05 sec\n",
            "Epoch 670, Loss(train/val) 0.70775/0.25001. Took 0.05 sec\n",
            "Epoch 671, Loss(train/val) 0.70985/0.24996. Took 0.05 sec\n",
            "Epoch 672, Loss(train/val) 0.70649/0.25018. Took 0.04 sec\n",
            "Epoch 673, Loss(train/val) 0.71619/0.25006. Took 0.04 sec\n",
            "Epoch 674, Loss(train/val) 0.70608/0.25035. Took 0.06 sec\n",
            "Epoch 675, Loss(train/val) 0.70604/0.25018. Took 0.05 sec\n",
            "Epoch 676, Loss(train/val) 0.71461/0.25011. Took 0.05 sec\n",
            "Epoch 677, Loss(train/val) 0.70711/0.25006. Took 0.04 sec\n",
            "Epoch 678, Loss(train/val) 0.70892/0.24998. Took 0.04 sec\n",
            "Epoch 679, Loss(train/val) 0.70970/0.24997. Took 0.05 sec\n",
            "Epoch 680, Loss(train/val) 0.70877/0.25012. Took 0.05 sec\n",
            "Epoch 681, Loss(train/val) 0.70831/0.25016. Took 0.05 sec\n",
            "Epoch 682, Loss(train/val) 0.70843/0.25024. Took 0.05 sec\n",
            "Epoch 683, Loss(train/val) 0.70277/0.25027. Took 0.05 sec\n",
            "Epoch 684, Loss(train/val) 0.70936/0.25020. Took 0.05 sec\n",
            "Epoch 685, Loss(train/val) 0.71153/0.25037. Took 0.05 sec\n",
            "Epoch 686, Loss(train/val) 0.71087/0.25074. Took 0.04 sec\n",
            "Epoch 687, Loss(train/val) 0.70378/0.25043. Took 0.05 sec\n",
            "Epoch 688, Loss(train/val) 0.70828/0.25008. Took 0.04 sec\n",
            "Epoch 689, Loss(train/val) 0.70556/0.25027. Took 0.05 sec\n",
            "Epoch 690, Loss(train/val) 0.70973/0.25016. Took 0.04 sec\n",
            "Epoch 691, Loss(train/val) 0.71269/0.25003. Took 0.05 sec\n",
            "Epoch 692, Loss(train/val) 0.70796/0.24984. Took 0.06 sec\n",
            "Epoch 693, Loss(train/val) 0.69854/0.24996. Took 0.05 sec\n",
            "Epoch 694, Loss(train/val) 0.70657/0.25012. Took 0.05 sec\n",
            "Epoch 695, Loss(train/val) 0.71523/0.25023. Took 0.06 sec\n",
            "Epoch 696, Loss(train/val) 0.70767/0.25036. Took 0.05 sec\n",
            "Epoch 697, Loss(train/val) 0.71210/0.25035. Took 0.05 sec\n",
            "Epoch 698, Loss(train/val) 0.70552/0.25034. Took 0.05 sec\n",
            "Epoch 699, Loss(train/val) 0.70306/0.25033. Took 0.05 sec\n",
            "Epoch 700, Loss(train/val) 0.70784/0.25012. Took 0.05 sec\n",
            "Epoch 701, Loss(train/val) 0.70553/0.25012. Took 0.04 sec\n",
            "Epoch 702, Loss(train/val) 0.70747/0.25031. Took 0.04 sec\n",
            "Epoch 703, Loss(train/val) 0.70407/0.25019. Took 0.05 sec\n",
            "Epoch 704, Loss(train/val) 0.70882/0.25033. Took 0.05 sec\n",
            "Epoch 705, Loss(train/val) 0.70662/0.24996. Took 0.05 sec\n",
            "Epoch 706, Loss(train/val) 0.70801/0.24982. Took 0.04 sec\n",
            "Epoch 707, Loss(train/val) 0.70394/0.24988. Took 0.05 sec\n",
            "Epoch 708, Loss(train/val) 0.70289/0.24990. Took 0.05 sec\n",
            "Epoch 709, Loss(train/val) 0.70443/0.25002. Took 0.05 sec\n",
            "Epoch 710, Loss(train/val) 0.70534/0.24987. Took 0.05 sec\n",
            "Epoch 711, Loss(train/val) 0.69408/0.25021. Took 0.05 sec\n",
            "Epoch 712, Loss(train/val) 0.70544/0.25017. Took 0.05 sec\n",
            "Epoch 713, Loss(train/val) 0.70303/0.24984. Took 0.05 sec\n",
            "Epoch 714, Loss(train/val) 0.70271/0.24981. Took 0.05 sec\n",
            "Epoch 715, Loss(train/val) 0.70773/0.24982. Took 0.05 sec\n",
            "Epoch 716, Loss(train/val) 0.70561/0.24965. Took 0.05 sec\n",
            "Epoch 717, Loss(train/val) 0.70270/0.24972. Took 0.05 sec\n",
            "Epoch 718, Loss(train/val) 0.69809/0.24979. Took 0.05 sec\n",
            "Epoch 719, Loss(train/val) 0.69375/0.24986. Took 0.05 sec\n",
            "Epoch 720, Loss(train/val) 0.70646/0.24962. Took 0.05 sec\n",
            "Epoch 721, Loss(train/val) 0.70478/0.24981. Took 0.04 sec\n",
            "Epoch 722, Loss(train/val) 0.70333/0.24973. Took 0.04 sec\n",
            "Epoch 723, Loss(train/val) 0.69624/0.24987. Took 0.05 sec\n",
            "Epoch 724, Loss(train/val) 0.70364/0.25038. Took 0.05 sec\n",
            "Epoch 725, Loss(train/val) 0.69429/0.25054. Took 0.05 sec\n",
            "Epoch 726, Loss(train/val) 0.69764/0.25048. Took 0.05 sec\n",
            "Epoch 727, Loss(train/val) 0.70743/0.25007. Took 0.04 sec\n",
            "Epoch 728, Loss(train/val) 0.70184/0.25000. Took 0.05 sec\n",
            "Epoch 729, Loss(train/val) 0.70540/0.24992. Took 0.05 sec\n",
            "Epoch 730, Loss(train/val) 0.70661/0.24994. Took 0.04 sec\n",
            "Epoch 731, Loss(train/val) 0.70466/0.24989. Took 0.04 sec\n",
            "Epoch 732, Loss(train/val) 0.70589/0.24975. Took 0.04 sec\n",
            "Epoch 733, Loss(train/val) 0.70508/0.25017. Took 0.05 sec\n",
            "Epoch 734, Loss(train/val) 0.69888/0.25021. Took 0.05 sec\n",
            "Epoch 735, Loss(train/val) 0.70533/0.25026. Took 0.04 sec\n",
            "Epoch 736, Loss(train/val) 0.69524/0.25067. Took 0.04 sec\n",
            "Epoch 737, Loss(train/val) 0.69846/0.25097. Took 0.04 sec\n",
            "Epoch 738, Loss(train/val) 0.69692/0.25050. Took 0.05 sec\n",
            "Epoch 739, Loss(train/val) 0.69826/0.25096. Took 0.05 sec\n",
            "Epoch 740, Loss(train/val) 0.70420/0.25045. Took 0.05 sec\n",
            "Epoch 741, Loss(train/val) 0.69671/0.25028. Took 0.05 sec\n",
            "Epoch 742, Loss(train/val) 0.70623/0.25002. Took 0.04 sec\n",
            "Epoch 743, Loss(train/val) 0.69852/0.25007. Took 0.04 sec\n",
            "Epoch 744, Loss(train/val) 0.70543/0.24972. Took 0.05 sec\n",
            "Epoch 745, Loss(train/val) 0.69988/0.24967. Took 0.05 sec\n",
            "Epoch 746, Loss(train/val) 0.69757/0.24967. Took 0.04 sec\n",
            "Epoch 747, Loss(train/val) 0.69826/0.24994. Took 0.04 sec\n",
            "Epoch 748, Loss(train/val) 0.70217/0.25015. Took 0.05 sec\n",
            "Epoch 749, Loss(train/val) 0.69831/0.25036. Took 0.05 sec\n",
            "Epoch 750, Loss(train/val) 0.70024/0.25027. Took 0.04 sec\n",
            "Epoch 751, Loss(train/val) 0.69337/0.25067. Took 0.05 sec\n",
            "Epoch 752, Loss(train/val) 0.69719/0.25075. Took 0.05 sec\n",
            "Epoch 753, Loss(train/val) 0.69212/0.25145. Took 0.05 sec\n",
            "Epoch 754, Loss(train/val) 0.70047/0.25139. Took 0.05 sec\n",
            "Epoch 755, Loss(train/val) 0.69283/0.25087. Took 0.04 sec\n",
            "Epoch 756, Loss(train/val) 0.70228/0.25057. Took 0.04 sec\n",
            "Epoch 757, Loss(train/val) 0.69339/0.25040. Took 0.04 sec\n",
            "Epoch 758, Loss(train/val) 0.69612/0.25047. Took 0.04 sec\n",
            "Epoch 759, Loss(train/val) 0.68813/0.25080. Took 0.05 sec\n",
            "Epoch 760, Loss(train/val) 0.69397/0.25082. Took 0.05 sec\n",
            "Epoch 761, Loss(train/val) 0.69189/0.25072. Took 0.05 sec\n",
            "Epoch 762, Loss(train/val) 0.69504/0.25087. Took 0.04 sec\n",
            "Epoch 763, Loss(train/val) 0.69509/0.25106. Took 0.05 sec\n",
            "Epoch 764, Loss(train/val) 0.69549/0.25080. Took 0.05 sec\n",
            "Epoch 765, Loss(train/val) 0.69881/0.25070. Took 0.05 sec\n",
            "Epoch 766, Loss(train/val) 0.69882/0.25103. Took 0.05 sec\n",
            "Epoch 767, Loss(train/val) 0.69995/0.25092. Took 0.05 sec\n",
            "Epoch 768, Loss(train/val) 0.71550/0.25035. Took 0.04 sec\n",
            "Epoch 769, Loss(train/val) 0.70300/0.25035. Took 0.05 sec\n",
            "Epoch 770, Loss(train/val) 0.70985/0.25018. Took 0.04 sec\n",
            "Epoch 771, Loss(train/val) 0.69393/0.25021. Took 0.04 sec\n",
            "Epoch 772, Loss(train/val) 0.69769/0.25013. Took 0.05 sec\n",
            "Epoch 773, Loss(train/val) 0.69840/0.25014. Took 0.05 sec\n",
            "Epoch 774, Loss(train/val) 0.69843/0.25026. Took 0.05 sec\n",
            "Epoch 775, Loss(train/val) 0.69719/0.24982. Took 0.05 sec\n",
            "Epoch 776, Loss(train/val) 0.70052/0.24971. Took 0.05 sec\n",
            "Epoch 777, Loss(train/val) 0.69327/0.24969. Took 0.05 sec\n",
            "Epoch 778, Loss(train/val) 0.69474/0.24980. Took 0.05 sec\n",
            "Epoch 779, Loss(train/val) 0.69145/0.24990. Took 0.05 sec\n",
            "Epoch 780, Loss(train/val) 0.68911/0.25033. Took 0.05 sec\n",
            "Epoch 781, Loss(train/val) 0.69587/0.25016. Took 0.05 sec\n",
            "Epoch 782, Loss(train/val) 0.69414/0.25018. Took 0.05 sec\n",
            "Epoch 783, Loss(train/val) 0.69835/0.24999. Took 0.05 sec\n",
            "Epoch 784, Loss(train/val) 0.69266/0.25010. Took 0.06 sec\n",
            "Epoch 785, Loss(train/val) 0.69431/0.25017. Took 0.04 sec\n",
            "Epoch 786, Loss(train/val) 0.68801/0.25016. Took 0.05 sec\n",
            "Epoch 787, Loss(train/val) 0.69002/0.25005. Took 0.05 sec\n",
            "Epoch 788, Loss(train/val) 0.69689/0.25015. Took 0.04 sec\n",
            "Epoch 789, Loss(train/val) 0.69355/0.25017. Took 0.05 sec\n",
            "Epoch 790, Loss(train/val) 0.69731/0.25017. Took 0.05 sec\n",
            "Epoch 791, Loss(train/val) 0.68632/0.25052. Took 0.05 sec\n",
            "Epoch 792, Loss(train/val) 0.69198/0.25069. Took 0.05 sec\n",
            "Epoch 793, Loss(train/val) 0.69383/0.25039. Took 0.05 sec\n",
            "Epoch 794, Loss(train/val) 0.68779/0.25066. Took 0.05 sec\n",
            "Epoch 795, Loss(train/val) 0.68574/0.25059. Took 0.05 sec\n",
            "Epoch 796, Loss(train/val) 0.68995/0.25052. Took 0.04 sec\n",
            "Epoch 797, Loss(train/val) 0.68859/0.25061. Took 0.05 sec\n",
            "Epoch 798, Loss(train/val) 0.68968/0.25057. Took 0.05 sec\n",
            "Epoch 799, Loss(train/val) 0.69054/0.25056. Took 0.05 sec\n",
            "Epoch 800, Loss(train/val) 0.68786/0.25127. Took 0.04 sec\n",
            "Epoch 801, Loss(train/val) 0.69208/0.25077. Took 0.04 sec\n",
            "Epoch 802, Loss(train/val) 0.69101/0.25063. Took 0.04 sec\n",
            "Epoch 803, Loss(train/val) 0.68952/0.25058. Took 0.05 sec\n",
            "Epoch 804, Loss(train/val) 0.69221/0.25059. Took 0.05 sec\n",
            "Epoch 805, Loss(train/val) 0.69184/0.25037. Took 0.05 sec\n",
            "Epoch 806, Loss(train/val) 0.68907/0.25051. Took 0.05 sec\n",
            "Epoch 807, Loss(train/val) 0.69099/0.25039. Took 0.05 sec\n",
            "Epoch 808, Loss(train/val) 0.68874/0.25023. Took 0.05 sec\n",
            "Epoch 809, Loss(train/val) 0.68874/0.25037. Took 0.05 sec\n",
            "Epoch 810, Loss(train/val) 0.68595/0.25009. Took 0.05 sec\n",
            "Epoch 811, Loss(train/val) 0.68633/0.25016. Took 0.05 sec\n",
            "Epoch 812, Loss(train/val) 0.69305/0.25043. Took 0.05 sec\n",
            "Epoch 813, Loss(train/val) 0.68139/0.25058. Took 0.05 sec\n",
            "Epoch 814, Loss(train/val) 0.68980/0.25031. Took 0.05 sec\n",
            "Epoch 815, Loss(train/val) 0.68748/0.25015. Took 0.04 sec\n",
            "Epoch 816, Loss(train/val) 0.68722/0.24984. Took 0.04 sec\n",
            "Epoch 817, Loss(train/val) 0.69104/0.24976. Took 0.05 sec\n",
            "Epoch 818, Loss(train/val) 0.67986/0.24985. Took 0.04 sec\n",
            "Epoch 819, Loss(train/val) 0.68901/0.24986. Took 0.05 sec\n",
            "Epoch 820, Loss(train/val) 0.68856/0.25003. Took 0.05 sec\n",
            "Epoch 821, Loss(train/val) 0.68804/0.25000. Took 0.05 sec\n",
            "Epoch 822, Loss(train/val) 0.68987/0.25023. Took 0.05 sec\n",
            "Epoch 823, Loss(train/val) 0.69065/0.25015. Took 0.05 sec\n",
            "Epoch 824, Loss(train/val) 0.68851/0.25017. Took 0.06 sec\n",
            "Epoch 825, Loss(train/val) 0.68550/0.25040. Took 0.05 sec\n",
            "Epoch 826, Loss(train/val) 0.67961/0.25022. Took 0.04 sec\n",
            "Epoch 827, Loss(train/val) 0.68575/0.25027. Took 0.04 sec\n",
            "Epoch 828, Loss(train/val) 0.68604/0.25042. Took 0.05 sec\n",
            "Epoch 829, Loss(train/val) 0.68307/0.25065. Took 0.05 sec\n",
            "Epoch 830, Loss(train/val) 0.68331/0.25053. Took 0.05 sec\n",
            "Epoch 831, Loss(train/val) 0.68193/0.25033. Took 0.05 sec\n",
            "Epoch 832, Loss(train/val) 0.68573/0.25013. Took 0.04 sec\n",
            "Epoch 833, Loss(train/val) 0.68497/0.25008. Took 0.04 sec\n",
            "Epoch 834, Loss(train/val) 0.68799/0.24975. Took 0.05 sec\n",
            "Epoch 835, Loss(train/val) 0.69050/0.24973. Took 0.05 sec\n",
            "Epoch 836, Loss(train/val) 0.68770/0.24973. Took 0.05 sec\n",
            "Epoch 837, Loss(train/val) 0.68727/0.24973. Took 0.04 sec\n",
            "Epoch 838, Loss(train/val) 0.69204/0.24967. Took 0.04 sec\n",
            "Epoch 839, Loss(train/val) 0.69306/0.24976. Took 0.05 sec\n",
            "Epoch 840, Loss(train/val) 0.68544/0.24955. Took 0.05 sec\n",
            "Epoch 841, Loss(train/val) 0.69329/0.24932. Took 0.05 sec\n",
            "Epoch 842, Loss(train/val) 0.68665/0.24922. Took 0.04 sec\n",
            "Epoch 843, Loss(train/val) 0.68005/0.24934. Took 0.04 sec\n",
            "Epoch 844, Loss(train/val) 0.68343/0.24922. Took 0.05 sec\n",
            "Epoch 845, Loss(train/val) 0.68043/0.24937. Took 0.04 sec\n",
            "Epoch 846, Loss(train/val) 0.68588/0.25000. Took 0.05 sec\n",
            "Epoch 847, Loss(train/val) 0.68589/0.24997. Took 0.04 sec\n",
            "Epoch 848, Loss(train/val) 0.67915/0.24996. Took 0.05 sec\n",
            "Epoch 849, Loss(train/val) 0.68055/0.24980. Took 0.05 sec\n",
            "Epoch 850, Loss(train/val) 0.67943/0.24995. Took 0.05 sec\n",
            "Epoch 851, Loss(train/val) 0.68469/0.25011. Took 0.04 sec\n",
            "Epoch 852, Loss(train/val) 0.68882/0.24990. Took 0.05 sec\n",
            "Epoch 853, Loss(train/val) 0.67807/0.24962. Took 0.04 sec\n",
            "Epoch 854, Loss(train/val) 0.68880/0.24968. Took 0.05 sec\n",
            "Epoch 855, Loss(train/val) 0.68789/0.24949. Took 0.04 sec\n",
            "Epoch 856, Loss(train/val) 0.67953/0.24951. Took 0.05 sec\n",
            "Epoch 857, Loss(train/val) 0.67987/0.24967. Took 0.05 sec\n",
            "Epoch 858, Loss(train/val) 0.67996/0.24984. Took 0.04 sec\n",
            "Epoch 859, Loss(train/val) 0.68168/0.24969. Took 0.05 sec\n",
            "Epoch 860, Loss(train/val) 0.67320/0.25004. Took 0.05 sec\n",
            "Epoch 861, Loss(train/val) 0.68816/0.24982. Took 0.05 sec\n",
            "Epoch 862, Loss(train/val) 0.68360/0.25013. Took 0.05 sec\n",
            "Epoch 863, Loss(train/val) 0.68136/0.24964. Took 0.04 sec\n",
            "Epoch 864, Loss(train/val) 0.68145/0.25000. Took 0.05 sec\n",
            "Epoch 865, Loss(train/val) 0.68306/0.25006. Took 0.04 sec\n",
            "Epoch 866, Loss(train/val) 0.67678/0.25003. Took 0.04 sec\n",
            "Epoch 867, Loss(train/val) 0.67707/0.25003. Took 0.06 sec\n",
            "Epoch 868, Loss(train/val) 0.67565/0.24993. Took 0.05 sec\n",
            "Epoch 869, Loss(train/val) 0.68200/0.24949. Took 0.05 sec\n",
            "Epoch 870, Loss(train/val) 0.68142/0.24965. Took 0.04 sec\n",
            "Epoch 871, Loss(train/val) 0.68293/0.24967. Took 0.05 sec\n",
            "Epoch 872, Loss(train/val) 0.67656/0.24948. Took 0.04 sec\n",
            "Epoch 873, Loss(train/val) 0.68768/0.24946. Took 0.04 sec\n",
            "Epoch 874, Loss(train/val) 0.68058/0.24942. Took 0.05 sec\n",
            "Epoch 875, Loss(train/val) 0.67330/0.24962. Took 0.05 sec\n",
            "Epoch 876, Loss(train/val) 0.68197/0.24945. Took 0.05 sec\n",
            "Epoch 877, Loss(train/val) 0.67741/0.24945. Took 0.05 sec\n",
            "Epoch 878, Loss(train/val) 0.67990/0.24927. Took 0.05 sec\n",
            "Epoch 879, Loss(train/val) 0.68090/0.24941. Took 0.05 sec\n",
            "Epoch 880, Loss(train/val) 0.68000/0.24934. Took 0.04 sec\n",
            "Epoch 881, Loss(train/val) 0.67989/0.24938. Took 0.05 sec\n",
            "Epoch 882, Loss(train/val) 0.68928/0.24968. Took 0.05 sec\n",
            "Epoch 883, Loss(train/val) 0.67725/0.25004. Took 0.06 sec\n",
            "Epoch 884, Loss(train/val) 0.67533/0.24986. Took 0.05 sec\n",
            "Epoch 885, Loss(train/val) 0.67402/0.24981. Took 0.05 sec\n",
            "Epoch 886, Loss(train/val) 0.67461/0.25008. Took 0.05 sec\n",
            "Epoch 887, Loss(train/val) 0.67779/0.25029. Took 0.05 sec\n",
            "Epoch 888, Loss(train/val) 0.67737/0.24975. Took 0.05 sec\n",
            "Epoch 889, Loss(train/val) 0.68332/0.24962. Took 0.06 sec\n",
            "Epoch 890, Loss(train/val) 0.67815/0.24988. Took 0.05 sec\n",
            "Epoch 891, Loss(train/val) 0.67640/0.24981. Took 0.05 sec\n",
            "Epoch 892, Loss(train/val) 0.67979/0.24962. Took 0.04 sec\n",
            "Epoch 893, Loss(train/val) 0.67967/0.24954. Took 0.05 sec\n",
            "Epoch 894, Loss(train/val) 0.67621/0.24994. Took 0.05 sec\n",
            "Epoch 895, Loss(train/val) 0.68162/0.24946. Took 0.05 sec\n",
            "Epoch 896, Loss(train/val) 0.67815/0.24929. Took 0.05 sec\n",
            "Epoch 897, Loss(train/val) 0.66810/0.24948. Took 0.05 sec\n",
            "Epoch 898, Loss(train/val) 0.67177/0.24944. Took 0.05 sec\n",
            "Epoch 899, Loss(train/val) 0.67475/0.24902. Took 0.05 sec\n",
            "Epoch 900, Loss(train/val) 0.67974/0.24904. Took 0.05 sec\n",
            "Epoch 901, Loss(train/val) 0.67546/0.24899. Took 0.04 sec\n",
            "Epoch 902, Loss(train/val) 0.67631/0.24898. Took 0.04 sec\n",
            "Epoch 903, Loss(train/val) 0.67343/0.24924. Took 0.04 sec\n",
            "Epoch 904, Loss(train/val) 0.67719/0.24928. Took 0.05 sec\n",
            "Epoch 905, Loss(train/val) 0.67613/0.24937. Took 0.05 sec\n",
            "Epoch 906, Loss(train/val) 0.67153/0.24942. Took 0.05 sec\n",
            "Epoch 907, Loss(train/val) 0.67836/0.24940. Took 0.05 sec\n",
            "Epoch 908, Loss(train/val) 0.67606/0.24932. Took 0.05 sec\n",
            "Epoch 909, Loss(train/val) 0.67174/0.24916. Took 0.05 sec\n",
            "Epoch 910, Loss(train/val) 0.67034/0.24921. Took 0.05 sec\n",
            "Epoch 911, Loss(train/val) 0.67314/0.24913. Took 0.05 sec\n",
            "Epoch 912, Loss(train/val) 0.67891/0.24901. Took 0.05 sec\n",
            "Epoch 913, Loss(train/val) 0.67292/0.24891. Took 0.05 sec\n",
            "Epoch 914, Loss(train/val) 0.67049/0.24897. Took 0.06 sec\n",
            "Epoch 915, Loss(train/val) 0.67171/0.24915. Took 0.04 sec\n",
            "Epoch 916, Loss(train/val) 0.67251/0.24936. Took 0.05 sec\n",
            "Epoch 917, Loss(train/val) 0.67591/0.24922. Took 0.04 sec\n",
            "Epoch 918, Loss(train/val) 0.67366/0.24941. Took 0.05 sec\n",
            "Epoch 919, Loss(train/val) 0.67419/0.24952. Took 0.05 sec\n",
            "Epoch 920, Loss(train/val) 0.66477/0.24951. Took 0.04 sec\n",
            "Epoch 921, Loss(train/val) 0.67384/0.24938. Took 0.04 sec\n",
            "Epoch 922, Loss(train/val) 0.67143/0.24952. Took 0.05 sec\n",
            "Epoch 923, Loss(train/val) 0.66696/0.24939. Took 0.05 sec\n",
            "Epoch 924, Loss(train/val) 0.67531/0.24950. Took 0.05 sec\n",
            "Epoch 925, Loss(train/val) 0.66938/0.24923. Took 0.05 sec\n",
            "Epoch 926, Loss(train/val) 0.66469/0.24963. Took 0.04 sec\n",
            "Epoch 927, Loss(train/val) 0.67266/0.24960. Took 0.05 sec\n",
            "Epoch 928, Loss(train/val) 0.67701/0.24956. Took 0.05 sec\n",
            "Epoch 929, Loss(train/val) 0.67237/0.24993. Took 0.05 sec\n",
            "Epoch 930, Loss(train/val) 0.66984/0.24966. Took 0.05 sec\n",
            "Epoch 931, Loss(train/val) 0.66720/0.24935. Took 0.05 sec\n",
            "Epoch 932, Loss(train/val) 0.67479/0.24916. Took 0.05 sec\n",
            "Epoch 933, Loss(train/val) 0.65968/0.24994. Took 0.04 sec\n",
            "Epoch 934, Loss(train/val) 0.67888/0.24963. Took 0.05 sec\n",
            "Epoch 935, Loss(train/val) 0.66398/0.25008. Took 0.04 sec\n",
            "Epoch 936, Loss(train/val) 0.66507/0.24991. Took 0.04 sec\n",
            "Epoch 937, Loss(train/val) 0.67523/0.24934. Took 0.05 sec\n",
            "Epoch 938, Loss(train/val) 0.67251/0.24932. Took 0.05 sec\n",
            "Epoch 939, Loss(train/val) 0.66917/0.24938. Took 0.05 sec\n",
            "Epoch 940, Loss(train/val) 0.66891/0.24960. Took 0.05 sec\n",
            "Epoch 941, Loss(train/val) 0.66569/0.24947. Took 0.05 sec\n",
            "Epoch 942, Loss(train/val) 0.66642/0.24929. Took 0.05 sec\n",
            "Epoch 943, Loss(train/val) 0.66707/0.24968. Took 0.05 sec\n",
            "Epoch 944, Loss(train/val) 0.67073/0.24959. Took 0.05 sec\n",
            "Epoch 945, Loss(train/val) 0.67283/0.24944. Took 0.05 sec\n",
            "Epoch 946, Loss(train/val) 0.66729/0.24956. Took 0.05 sec\n",
            "Epoch 947, Loss(train/val) 0.66973/0.24924. Took 0.04 sec\n",
            "Epoch 948, Loss(train/val) 0.65902/0.24916. Took 0.04 sec\n",
            "Epoch 949, Loss(train/val) 0.66690/0.24903. Took 0.05 sec\n",
            "Epoch 950, Loss(train/val) 0.66491/0.24897. Took 0.04 sec\n",
            "Epoch 951, Loss(train/val) 0.66023/0.24937. Took 0.04 sec\n",
            "Epoch 952, Loss(train/val) 0.66065/0.24966. Took 0.04 sec\n",
            "Epoch 953, Loss(train/val) 0.66875/0.24940. Took 0.04 sec\n",
            "Epoch 954, Loss(train/val) 0.67181/0.24923. Took 0.06 sec\n",
            "Epoch 955, Loss(train/val) 0.66896/0.24906. Took 0.04 sec\n",
            "Epoch 956, Loss(train/val) 0.67127/0.24907. Took 0.04 sec\n",
            "Epoch 957, Loss(train/val) 0.66550/0.24895. Took 0.05 sec\n",
            "Epoch 958, Loss(train/val) 0.68318/0.24914. Took 0.05 sec\n",
            "Epoch 959, Loss(train/val) 0.67023/0.24928. Took 0.05 sec\n",
            "Epoch 960, Loss(train/val) 0.66422/0.24987. Took 0.04 sec\n",
            "Epoch 961, Loss(train/val) 0.66690/0.24944. Took 0.05 sec\n",
            "Epoch 962, Loss(train/val) 0.66146/0.24966. Took 0.05 sec\n",
            "Epoch 963, Loss(train/val) 0.66475/0.24940. Took 0.04 sec\n",
            "Epoch 964, Loss(train/val) 0.66613/0.24907. Took 0.05 sec\n",
            "Epoch 965, Loss(train/val) 0.66120/0.24955. Took 0.04 sec\n",
            "Epoch 966, Loss(train/val) 0.66816/0.24947. Took 0.04 sec\n",
            "Epoch 967, Loss(train/val) 0.66951/0.24945. Took 0.04 sec\n",
            "Epoch 968, Loss(train/val) 0.66696/0.24929. Took 0.04 sec\n",
            "Epoch 969, Loss(train/val) 0.66333/0.24951. Took 0.05 sec\n",
            "Epoch 970, Loss(train/val) 0.65739/0.24921. Took 0.04 sec\n",
            "Epoch 971, Loss(train/val) 0.66313/0.24914. Took 0.04 sec\n",
            "Epoch 972, Loss(train/val) 0.66566/0.24922. Took 0.05 sec\n",
            "Epoch 973, Loss(train/val) 0.65772/0.24932. Took 0.05 sec\n",
            "Epoch 974, Loss(train/val) 0.66394/0.24937. Took 0.05 sec\n",
            "Epoch 975, Loss(train/val) 0.66174/0.24941. Took 0.04 sec\n",
            "Epoch 976, Loss(train/val) 0.66422/0.24937. Took 0.06 sec\n",
            "Epoch 977, Loss(train/val) 0.65956/0.24929. Took 0.04 sec\n",
            "Epoch 978, Loss(train/val) 0.65820/0.24911. Took 0.04 sec\n",
            "Epoch 979, Loss(train/val) 0.66447/0.24872. Took 0.05 sec\n",
            "Epoch 980, Loss(train/val) 0.66303/0.24905. Took 0.05 sec\n",
            "Epoch 981, Loss(train/val) 0.66462/0.24889. Took 0.04 sec\n",
            "Epoch 982, Loss(train/val) 0.66841/0.24911. Took 0.04 sec\n",
            "Epoch 983, Loss(train/val) 0.66360/0.24947. Took 0.05 sec\n",
            "Epoch 984, Loss(train/val) 0.66341/0.24970. Took 0.05 sec\n",
            "Epoch 985, Loss(train/val) 0.66407/0.24922. Took 0.04 sec\n",
            "Epoch 986, Loss(train/val) 0.65958/0.24898. Took 0.05 sec\n",
            "Epoch 987, Loss(train/val) 0.65620/0.24912. Took 0.05 sec\n",
            "Epoch 988, Loss(train/val) 0.66217/0.24951. Took 0.05 sec\n",
            "Epoch 989, Loss(train/val) 0.66328/0.24959. Took 0.05 sec\n",
            "Epoch 990, Loss(train/val) 0.65859/0.24947. Took 0.04 sec\n",
            "Epoch 991, Loss(train/val) 0.65710/0.24967. Took 0.04 sec\n",
            "Epoch 992, Loss(train/val) 0.66419/0.24996. Took 0.04 sec\n",
            "Epoch 993, Loss(train/val) 0.66556/0.24981. Took 0.05 sec\n",
            "Epoch 994, Loss(train/val) 0.65795/0.24964. Took 0.05 sec\n",
            "Epoch 995, Loss(train/val) 0.66206/0.24980. Took 0.05 sec\n",
            "Epoch 996, Loss(train/val) 0.66086/0.25014. Took 0.05 sec\n",
            "Epoch 997, Loss(train/val) 0.65925/0.24970. Took 0.05 sec\n",
            "Epoch 998, Loss(train/val) 0.66009/0.24998. Took 0.04 sec\n",
            "Epoch 999, Loss(train/val) 0.65908/0.24958. Took 0.05 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=1000, exp_name='exp3_lr_deep', hid_dim=16, input_dim=1, l2=1e-05, lr=3e-05, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.09494/0.38534. Took 0.05 sec\n",
            "Epoch 1, Loss(train/val) 1.09652/0.38531. Took 0.05 sec\n",
            "Epoch 2, Loss(train/val) 1.10771/0.38528. Took 0.05 sec\n",
            "Epoch 3, Loss(train/val) 1.09566/0.38525. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.09859/0.38522. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.10690/0.38518. Took 0.04 sec\n",
            "Epoch 6, Loss(train/val) 1.10181/0.38515. Took 0.05 sec\n",
            "Epoch 7, Loss(train/val) 1.09740/0.38511. Took 0.04 sec\n",
            "Epoch 8, Loss(train/val) 1.09606/0.38508. Took 0.04 sec\n",
            "Epoch 9, Loss(train/val) 1.10299/0.38504. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.09750/0.38501. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.10113/0.38498. Took 0.05 sec\n",
            "Epoch 12, Loss(train/val) 1.10029/0.38495. Took 0.04 sec\n",
            "Epoch 13, Loss(train/val) 1.09435/0.38493. Took 0.04 sec\n",
            "Epoch 14, Loss(train/val) 1.10234/0.38491. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.10525/0.38489. Took 0.05 sec\n",
            "Epoch 16, Loss(train/val) 1.10160/0.38489. Took 0.04 sec\n",
            "Epoch 17, Loss(train/val) 1.09929/0.38489. Took 0.04 sec\n",
            "Epoch 18, Loss(train/val) 1.10436/0.38490. Took 0.04 sec\n",
            "Epoch 19, Loss(train/val) 1.09687/0.38493. Took 0.06 sec\n",
            "Epoch 20, Loss(train/val) 1.09697/0.38497. Took 0.04 sec\n",
            "Epoch 21, Loss(train/val) 1.09530/0.38503. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 1.09981/0.38511. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 1.09469/0.38521. Took 0.04 sec\n",
            "Epoch 24, Loss(train/val) 1.10087/0.38535. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 1.08745/0.38553. Took 0.04 sec\n",
            "Epoch 26, Loss(train/val) 1.08739/0.38574. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.10630/0.38600. Took 0.05 sec\n",
            "Epoch 28, Loss(train/val) 1.09619/0.38632. Took 0.05 sec\n",
            "Epoch 29, Loss(train/val) 1.09657/0.38669. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 1.09369/0.38714. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 1.09907/0.38767. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 1.10291/0.38828. Took 0.04 sec\n",
            "Epoch 33, Loss(train/val) 1.09320/0.38894. Took 0.04 sec\n",
            "Epoch 34, Loss(train/val) 1.09624/0.38963. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 1.08592/0.39033. Took 0.04 sec\n",
            "Epoch 36, Loss(train/val) 1.09725/0.39102. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.09445/0.39168. Took 0.05 sec\n",
            "Epoch 38, Loss(train/val) 1.09037/0.39224. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 1.09664/0.39273. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 1.09802/0.39317. Took 0.06 sec\n",
            "Epoch 41, Loss(train/val) 1.09944/0.39357. Took 0.05 sec\n",
            "Epoch 42, Loss(train/val) 1.09244/0.39392. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 1.09183/0.39420. Took 0.05 sec\n",
            "Epoch 44, Loss(train/val) 1.09603/0.39440. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 1.09313/0.39456. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 1.10388/0.39473. Took 0.04 sec\n",
            "Epoch 47, Loss(train/val) 1.08370/0.39486. Took 0.05 sec\n",
            "Epoch 48, Loss(train/val) 1.08926/0.39494. Took 0.06 sec\n",
            "Epoch 49, Loss(train/val) 1.09801/0.39501. Took 0.05 sec\n",
            "Epoch 50, Loss(train/val) 1.09081/0.39506. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 1.09372/0.39509. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 1.09061/0.39516. Took 0.05 sec\n",
            "Epoch 53, Loss(train/val) 1.09014/0.39521. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 1.09676/0.39524. Took 0.05 sec\n",
            "Epoch 55, Loss(train/val) 1.09057/0.39526. Took 0.04 sec\n",
            "Epoch 56, Loss(train/val) 1.09174/0.39530. Took 0.04 sec\n",
            "Epoch 57, Loss(train/val) 1.09097/0.39531. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 1.09374/0.39537. Took 0.05 sec\n",
            "Epoch 59, Loss(train/val) 1.08364/0.39551. Took 0.05 sec\n",
            "Epoch 60, Loss(train/val) 1.09083/0.39568. Took 0.05 sec\n",
            "Epoch 61, Loss(train/val) 1.09717/0.39588. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 1.09374/0.39609. Took 0.04 sec\n",
            "Epoch 63, Loss(train/val) 1.09311/0.39622. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 1.09742/0.39641. Took 0.04 sec\n",
            "Epoch 65, Loss(train/val) 1.10270/0.39653. Took 0.05 sec\n",
            "Epoch 66, Loss(train/val) 1.08914/0.39671. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 1.09675/0.39695. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 1.09845/0.39714. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 1.09490/0.39732. Took 0.04 sec\n",
            "Epoch 70, Loss(train/val) 1.08320/0.39748. Took 0.04 sec\n",
            "Epoch 71, Loss(train/val) 1.08730/0.39762. Took 0.05 sec\n",
            "Epoch 72, Loss(train/val) 1.08959/0.39777. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 1.09342/0.39789. Took 0.05 sec\n",
            "Epoch 74, Loss(train/val) 1.09582/0.39805. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 1.09444/0.39814. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 1.08899/0.39825. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 1.08161/0.39831. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 1.09798/0.39845. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 1.09121/0.39859. Took 0.04 sec\n",
            "Epoch 80, Loss(train/val) 1.09947/0.39869. Took 0.04 sec\n",
            "Epoch 81, Loss(train/val) 1.09299/0.39881. Took 0.05 sec\n",
            "Epoch 82, Loss(train/val) 1.09190/0.39892. Took 0.05 sec\n",
            "Epoch 83, Loss(train/val) 1.09398/0.39899. Took 0.06 sec\n",
            "Epoch 84, Loss(train/val) 1.08866/0.39908. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 1.09164/0.39915. Took 0.05 sec\n",
            "Epoch 86, Loss(train/val) 1.09231/0.39922. Took 0.05 sec\n",
            "Epoch 87, Loss(train/val) 1.08933/0.39928. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 1.08502/0.39932. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 1.08860/0.39933. Took 0.04 sec\n",
            "Epoch 90, Loss(train/val) 1.09014/0.39935. Took 0.04 sec\n",
            "Epoch 91, Loss(train/val) 1.09441/0.39937. Took 0.05 sec\n",
            "Epoch 92, Loss(train/val) 1.09034/0.39935. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 1.09159/0.39933. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 1.08805/0.39927. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 1.09120/0.39924. Took 0.04 sec\n",
            "Epoch 96, Loss(train/val) 1.09838/0.39925. Took 0.04 sec\n",
            "Epoch 97, Loss(train/val) 1.09170/0.39924. Took 0.05 sec\n",
            "Epoch 98, Loss(train/val) 1.09623/0.39921. Took 0.05 sec\n",
            "Epoch 99, Loss(train/val) 1.09396/0.39923. Took 0.05 sec\n",
            "Epoch 100, Loss(train/val) 1.09111/0.39928. Took 0.04 sec\n",
            "Epoch 101, Loss(train/val) 1.08294/0.39923. Took 0.04 sec\n",
            "Epoch 102, Loss(train/val) 1.07870/0.39934. Took 0.04 sec\n",
            "Epoch 103, Loss(train/val) 1.09526/0.39952. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 1.07956/0.39971. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 1.09143/0.39994. Took 0.05 sec\n",
            "Epoch 106, Loss(train/val) 1.09216/0.40016. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 1.08630/0.40034. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 1.09228/0.40057. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 1.09226/0.40083. Took 0.05 sec\n",
            "Epoch 110, Loss(train/val) 1.09170/0.40106. Took 0.05 sec\n",
            "Epoch 111, Loss(train/val) 1.08503/0.40130. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 1.08895/0.40151. Took 0.04 sec\n",
            "Epoch 113, Loss(train/val) 1.08729/0.40163. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 1.09253/0.40182. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 1.09563/0.40194. Took 0.05 sec\n",
            "Epoch 116, Loss(train/val) 1.08528/0.40206. Took 0.04 sec\n",
            "Epoch 117, Loss(train/val) 1.08801/0.40218. Took 0.04 sec\n",
            "Epoch 118, Loss(train/val) 1.07855/0.40227. Took 0.05 sec\n",
            "Epoch 119, Loss(train/val) 1.08242/0.40241. Took 0.04 sec\n",
            "Epoch 120, Loss(train/val) 1.09585/0.40248. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 1.08377/0.40256. Took 0.04 sec\n",
            "Epoch 122, Loss(train/val) 1.09617/0.40260. Took 0.05 sec\n",
            "Epoch 123, Loss(train/val) 1.08718/0.40260. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 1.08379/0.40259. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 1.08736/0.40252. Took 0.04 sec\n",
            "Epoch 126, Loss(train/val) 1.08036/0.40249. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 1.08607/0.40242. Took 0.05 sec\n",
            "Epoch 128, Loss(train/val) 1.09101/0.40239. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 1.08538/0.40235. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 1.09356/0.40237. Took 0.05 sec\n",
            "Epoch 131, Loss(train/val) 1.08290/0.40232. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 1.08113/0.40229. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 1.08044/0.40228. Took 0.06 sec\n",
            "Epoch 134, Loss(train/val) 1.08797/0.40228. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 1.07862/0.40226. Took 0.05 sec\n",
            "Epoch 136, Loss(train/val) 1.07772/0.40224. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 1.08538/0.40224. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 1.09171/0.40225. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 1.07675/0.40225. Took 0.04 sec\n",
            "Epoch 140, Loss(train/val) 1.08293/0.40226. Took 0.05 sec\n",
            "Epoch 141, Loss(train/val) 1.07706/0.40222. Took 0.05 sec\n",
            "Epoch 142, Loss(train/val) 1.08376/0.40216. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 1.07364/0.40209. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 1.07296/0.40193. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 1.07833/0.40183. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 1.06693/0.40164. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 1.07959/0.40149. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 1.06681/0.40124. Took 0.06 sec\n",
            "Epoch 149, Loss(train/val) 1.07999/0.40093. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 1.07028/0.40047. Took 0.04 sec\n",
            "Epoch 151, Loss(train/val) 1.07151/0.39977. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 1.06755/0.39879. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 1.08071/0.39746. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 1.07663/0.39583. Took 0.05 sec\n",
            "Epoch 155, Loss(train/val) 1.07334/0.39407. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 1.06951/0.39228. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 1.07360/0.39046. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 1.05791/0.38878. Took 0.05 sec\n",
            "Epoch 159, Loss(train/val) 1.07012/0.38714. Took 0.04 sec\n",
            "Epoch 160, Loss(train/val) 1.07194/0.38570. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 1.07157/0.38442. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 1.06334/0.38327. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 1.06625/0.38223. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 1.06170/0.38130. Took 0.04 sec\n",
            "Epoch 165, Loss(train/val) 1.06545/0.38030. Took 0.04 sec\n",
            "Epoch 166, Loss(train/val) 1.06623/0.37938. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 1.06873/0.37849. Took 0.05 sec\n",
            "Epoch 168, Loss(train/val) 1.06629/0.37762. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 1.05990/0.37687. Took 0.05 sec\n",
            "Epoch 170, Loss(train/val) 1.05606/0.37617. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 1.06447/0.37556. Took 0.05 sec\n",
            "Epoch 172, Loss(train/val) 1.06249/0.37501. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 1.05869/0.37465. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 1.05219/0.37463. Took 0.05 sec\n",
            "Epoch 175, Loss(train/val) 1.06392/0.37502. Took 0.04 sec\n",
            "Epoch 176, Loss(train/val) 1.04629/0.37569. Took 0.04 sec\n",
            "Epoch 177, Loss(train/val) 1.06156/0.37646. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 1.05586/0.37756. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 1.05356/0.37884. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 1.04891/0.38002. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 1.04968/0.38114. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 1.04876/0.38201. Took 0.05 sec\n",
            "Epoch 183, Loss(train/val) 1.05199/0.38275. Took 0.05 sec\n",
            "Epoch 184, Loss(train/val) 1.05059/0.38325. Took 0.05 sec\n",
            "Epoch 185, Loss(train/val) 1.05359/0.38360. Took 0.05 sec\n",
            "Epoch 186, Loss(train/val) 1.04410/0.38375. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 1.05089/0.38366. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 1.04561/0.38346. Took 0.05 sec\n",
            "Epoch 189, Loss(train/val) 1.04646/0.38311. Took 0.05 sec\n",
            "Epoch 190, Loss(train/val) 1.03780/0.38268. Took 0.04 sec\n",
            "Epoch 191, Loss(train/val) 1.04836/0.38211. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 1.04009/0.38148. Took 0.05 sec\n",
            "Epoch 193, Loss(train/val) 1.03936/0.38073. Took 0.05 sec\n",
            "Epoch 194, Loss(train/val) 1.04005/0.37996. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 1.03272/0.37906. Took 0.05 sec\n",
            "Epoch 196, Loss(train/val) 1.04151/0.37815. Took 0.04 sec\n",
            "Epoch 197, Loss(train/val) 1.04304/0.37720. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 1.03121/0.37631. Took 0.05 sec\n",
            "Epoch 199, Loss(train/val) 1.03912/0.37540. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 1.02968/0.37448. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 1.03398/0.37358. Took 0.04 sec\n",
            "Epoch 202, Loss(train/val) 1.03432/0.37266. Took 0.04 sec\n",
            "Epoch 203, Loss(train/val) 1.03482/0.37176. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 1.02634/0.37085. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 1.02422/0.37000. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 1.02504/0.36918. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 1.02646/0.36829. Took 0.05 sec\n",
            "Epoch 208, Loss(train/val) 1.02596/0.36744. Took 0.05 sec\n",
            "Epoch 209, Loss(train/val) 1.02056/0.36658. Took 0.04 sec\n",
            "Epoch 210, Loss(train/val) 1.01976/0.36577. Took 0.04 sec\n",
            "Epoch 211, Loss(train/val) 1.02222/0.36497. Took 0.04 sec\n",
            "Epoch 212, Loss(train/val) 1.01790/0.36418. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 1.01794/0.36334. Took 0.05 sec\n",
            "Epoch 214, Loss(train/val) 1.02147/0.36260. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 1.01676/0.36177. Took 0.05 sec\n",
            "Epoch 216, Loss(train/val) 1.01604/0.36095. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 1.01179/0.36014. Took 0.05 sec\n",
            "Epoch 218, Loss(train/val) 1.00909/0.35933. Took 0.05 sec\n",
            "Epoch 219, Loss(train/val) 1.00565/0.35856. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 1.00506/0.35777. Took 0.04 sec\n",
            "Epoch 221, Loss(train/val) 1.00119/0.35704. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 1.00527/0.35627. Took 0.05 sec\n",
            "Epoch 223, Loss(train/val) 1.00729/0.35546. Took 0.05 sec\n",
            "Epoch 224, Loss(train/val) 1.00901/0.35472. Took 0.04 sec\n",
            "Epoch 225, Loss(train/val) 0.99453/0.35400. Took 0.04 sec\n",
            "Epoch 226, Loss(train/val) 1.00528/0.35327. Took 0.05 sec\n",
            "Epoch 227, Loss(train/val) 1.00197/0.35262. Took 0.04 sec\n",
            "Epoch 228, Loss(train/val) 1.00212/0.35206. Took 0.05 sec\n",
            "Epoch 229, Loss(train/val) 0.99751/0.35153. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 0.99163/0.35082. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 0.99209/0.35023. Took 0.05 sec\n",
            "Epoch 232, Loss(train/val) 0.99541/0.34959. Took 0.05 sec\n",
            "Epoch 233, Loss(train/val) 0.99390/0.34899. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 0.99505/0.34845. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 0.99091/0.34787. Took 0.04 sec\n",
            "Epoch 236, Loss(train/val) 0.99477/0.34732. Took 0.05 sec\n",
            "Epoch 237, Loss(train/val) 0.98121/0.34663. Took 0.05 sec\n",
            "Epoch 238, Loss(train/val) 0.99131/0.34608. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 0.99484/0.34568. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 0.98294/0.34525. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 0.98988/0.34489. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 0.98885/0.34441. Took 0.05 sec\n",
            "Epoch 243, Loss(train/val) 0.98453/0.34405. Took 0.05 sec\n",
            "Epoch 244, Loss(train/val) 0.98730/0.34364. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.98718/0.34322. Took 0.04 sec\n",
            "Epoch 246, Loss(train/val) 0.98874/0.34280. Took 0.05 sec\n",
            "Epoch 247, Loss(train/val) 0.98495/0.34232. Took 0.05 sec\n",
            "Epoch 248, Loss(train/val) 0.98637/0.34183. Took 0.05 sec\n",
            "Epoch 249, Loss(train/val) 0.97920/0.34143. Took 0.05 sec\n",
            "Epoch 250, Loss(train/val) 0.97985/0.34105. Took 0.06 sec\n",
            "Epoch 251, Loss(train/val) 0.98658/0.34068. Took 0.05 sec\n",
            "Epoch 252, Loss(train/val) 0.98776/0.34040. Took 0.05 sec\n",
            "Epoch 253, Loss(train/val) 0.98312/0.34004. Took 0.05 sec\n",
            "Epoch 254, Loss(train/val) 0.98712/0.33971. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.98098/0.33936. Took 0.06 sec\n",
            "Epoch 256, Loss(train/val) 0.98267/0.33907. Took 0.05 sec\n",
            "Epoch 257, Loss(train/val) 0.98087/0.33877. Took 0.05 sec\n",
            "Epoch 258, Loss(train/val) 0.98094/0.33854. Took 0.04 sec\n",
            "Epoch 259, Loss(train/val) 0.97241/0.33828. Took 0.04 sec\n",
            "Epoch 260, Loss(train/val) 0.98466/0.33807. Took 0.04 sec\n",
            "Epoch 261, Loss(train/val) 0.97593/0.33787. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.97703/0.33768. Took 0.05 sec\n",
            "Epoch 263, Loss(train/val) 0.97775/0.33748. Took 0.04 sec\n",
            "Epoch 264, Loss(train/val) 0.97824/0.33732. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.97662/0.33719. Took 0.04 sec\n",
            "Epoch 266, Loss(train/val) 0.96143/0.33702. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 0.97619/0.33695. Took 0.05 sec\n",
            "Epoch 268, Loss(train/val) 0.97021/0.33685. Took 0.05 sec\n",
            "Epoch 269, Loss(train/val) 0.97133/0.33672. Took 0.05 sec\n",
            "Epoch 270, Loss(train/val) 0.97074/0.33665. Took 0.05 sec\n",
            "Epoch 271, Loss(train/val) 0.96995/0.33649. Took 0.05 sec\n",
            "Epoch 272, Loss(train/val) 0.96991/0.33642. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 0.97736/0.33634. Took 0.05 sec\n",
            "Epoch 274, Loss(train/val) 0.96894/0.33622. Took 0.04 sec\n",
            "Epoch 275, Loss(train/val) 0.96962/0.33615. Took 0.04 sec\n",
            "Epoch 276, Loss(train/val) 0.96696/0.33598. Took 0.06 sec\n",
            "Epoch 277, Loss(train/val) 0.96859/0.33599. Took 0.04 sec\n",
            "Epoch 278, Loss(train/val) 0.96732/0.33585. Took 0.05 sec\n",
            "Epoch 279, Loss(train/val) 0.96966/0.33584. Took 0.04 sec\n",
            "Epoch 280, Loss(train/val) 0.96959/0.33566. Took 0.04 sec\n",
            "Epoch 281, Loss(train/val) 0.96086/0.33563. Took 0.05 sec\n",
            "Epoch 282, Loss(train/val) 0.96435/0.33585. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 0.96992/0.33608. Took 0.04 sec\n",
            "Epoch 284, Loss(train/val) 0.97177/0.33629. Took 0.04 sec\n",
            "Epoch 285, Loss(train/val) 0.96291/0.33635. Took 0.04 sec\n",
            "Epoch 286, Loss(train/val) 0.96850/0.33644. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.96961/0.33640. Took 0.04 sec\n",
            "Epoch 288, Loss(train/val) 0.96302/0.33609. Took 0.04 sec\n",
            "Epoch 289, Loss(train/val) 0.96587/0.33584. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.96306/0.33619. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.96641/0.33594. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.96184/0.33571. Took 0.04 sec\n",
            "Epoch 293, Loss(train/val) 0.97079/0.33574. Took 0.05 sec\n",
            "Epoch 294, Loss(train/val) 0.95830/0.33610. Took 0.04 sec\n",
            "Epoch 295, Loss(train/val) 0.95957/0.33606. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 0.96553/0.33614. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.96862/0.33576. Took 0.04 sec\n",
            "Epoch 298, Loss(train/val) 0.96104/0.33565. Took 0.06 sec\n",
            "Epoch 299, Loss(train/val) 0.95412/0.33593. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.96042/0.33583. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.95752/0.33584. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.95469/0.33585. Took 0.05 sec\n",
            "Epoch 303, Loss(train/val) 0.96775/0.33554. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 0.96085/0.33521. Took 0.04 sec\n",
            "Epoch 305, Loss(train/val) 0.96310/0.33462. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 0.94556/0.33457. Took 0.05 sec\n",
            "Epoch 307, Loss(train/val) 0.96078/0.33416. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.95511/0.33412. Took 0.04 sec\n",
            "Epoch 309, Loss(train/val) 0.94844/0.33400. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.95791/0.33393. Took 0.05 sec\n",
            "Epoch 311, Loss(train/val) 0.95846/0.33404. Took 0.05 sec\n",
            "Epoch 312, Loss(train/val) 0.95406/0.33407. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 0.95928/0.33467. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.94930/0.33407. Took 0.05 sec\n",
            "Epoch 315, Loss(train/val) 0.94394/0.33452. Took 0.04 sec\n",
            "Epoch 316, Loss(train/val) 0.95665/0.33413. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.95900/0.33420. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.95145/0.33432. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.94655/0.33483. Took 0.05 sec\n",
            "Epoch 320, Loss(train/val) 0.94322/0.33480. Took 0.05 sec\n",
            "Epoch 321, Loss(train/val) 0.94897/0.33493. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.94803/0.33547. Took 0.04 sec\n",
            "Epoch 323, Loss(train/val) 0.94591/0.33551. Took 0.04 sec\n",
            "Epoch 324, Loss(train/val) 0.95764/0.33513. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.95531/0.33516. Took 0.05 sec\n",
            "Epoch 326, Loss(train/val) 0.94623/0.33475. Took 0.05 sec\n",
            "Epoch 327, Loss(train/val) 0.95256/0.33515. Took 0.05 sec\n",
            "Epoch 328, Loss(train/val) 0.94515/0.33464. Took 0.05 sec\n",
            "Epoch 329, Loss(train/val) 0.94621/0.33465. Took 0.04 sec\n",
            "Epoch 330, Loss(train/val) 0.94656/0.33481. Took 0.05 sec\n",
            "Epoch 331, Loss(train/val) 0.94395/0.33417. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 0.95095/0.33345. Took 0.05 sec\n",
            "Epoch 333, Loss(train/val) 0.95074/0.33331. Took 0.05 sec\n",
            "Epoch 334, Loss(train/val) 0.94653/0.33328. Took 0.05 sec\n",
            "Epoch 335, Loss(train/val) 0.94226/0.33317. Took 0.05 sec\n",
            "Epoch 336, Loss(train/val) 0.94735/0.33259. Took 0.05 sec\n",
            "Epoch 337, Loss(train/val) 0.93852/0.33204. Took 0.04 sec\n",
            "Epoch 338, Loss(train/val) 0.95200/0.33165. Took 0.05 sec\n",
            "Epoch 339, Loss(train/val) 0.94670/0.33124. Took 0.05 sec\n",
            "Epoch 340, Loss(train/val) 0.93561/0.33025. Took 0.04 sec\n",
            "Epoch 341, Loss(train/val) 0.94234/0.32981. Took 0.06 sec\n",
            "Epoch 342, Loss(train/val) 0.94133/0.32936. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.94076/0.32915. Took 0.04 sec\n",
            "Epoch 344, Loss(train/val) 0.95316/0.32918. Took 0.04 sec\n",
            "Epoch 345, Loss(train/val) 0.94735/0.32899. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.94789/0.32873. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 0.95374/0.32854. Took 0.04 sec\n",
            "Epoch 348, Loss(train/val) 0.94230/0.32860. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 0.94665/0.32829. Took 0.04 sec\n",
            "Epoch 350, Loss(train/val) 0.94442/0.32804. Took 0.05 sec\n",
            "Epoch 351, Loss(train/val) 0.93814/0.32793. Took 0.05 sec\n",
            "Epoch 352, Loss(train/val) 0.95128/0.32788. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 0.95035/0.32808. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 0.94505/0.32827. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 0.94629/0.32788. Took 0.06 sec\n",
            "Epoch 356, Loss(train/val) 0.94473/0.32766. Took 0.05 sec\n",
            "Epoch 357, Loss(train/val) 0.94717/0.32773. Took 0.05 sec\n",
            "Epoch 358, Loss(train/val) 0.94301/0.32745. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 0.94342/0.32716. Took 0.05 sec\n",
            "Epoch 360, Loss(train/val) 0.94086/0.32690. Took 0.05 sec\n",
            "Epoch 361, Loss(train/val) 0.93121/0.32680. Took 0.05 sec\n",
            "Epoch 362, Loss(train/val) 0.93577/0.32661. Took 0.07 sec\n",
            "Epoch 363, Loss(train/val) 0.94078/0.32648. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 0.94466/0.32646. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.93764/0.32643. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.94095/0.32644. Took 0.04 sec\n",
            "Epoch 367, Loss(train/val) 0.93938/0.32647. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.94471/0.32676. Took 0.05 sec\n",
            "Epoch 369, Loss(train/val) 0.94540/0.32667. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.93909/0.32647. Took 0.05 sec\n",
            "Epoch 371, Loss(train/val) 0.93772/0.32622. Took 0.04 sec\n",
            "Epoch 372, Loss(train/val) 0.93216/0.32587. Took 0.04 sec\n",
            "Epoch 373, Loss(train/val) 0.93073/0.32586. Took 0.04 sec\n",
            "Epoch 374, Loss(train/val) 0.93682/0.32595. Took 0.05 sec\n",
            "Epoch 375, Loss(train/val) 0.93980/0.32590. Took 0.05 sec\n",
            "Epoch 376, Loss(train/val) 0.93996/0.32584. Took 0.05 sec\n",
            "Epoch 377, Loss(train/val) 0.94202/0.32585. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.93685/0.32596. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 0.93178/0.32557. Took 0.05 sec\n",
            "Epoch 380, Loss(train/val) 0.94053/0.32562. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 0.93792/0.32562. Took 0.05 sec\n",
            "Epoch 382, Loss(train/val) 0.93930/0.32555. Took 0.04 sec\n",
            "Epoch 383, Loss(train/val) 0.93435/0.32533. Took 0.05 sec\n",
            "Epoch 384, Loss(train/val) 0.93216/0.32499. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 0.93851/0.32491. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.93130/0.32525. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.93376/0.32525. Took 0.04 sec\n",
            "Epoch 388, Loss(train/val) 0.93288/0.32540. Took 0.04 sec\n",
            "Epoch 389, Loss(train/val) 0.93462/0.32553. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.93601/0.32483. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.93025/0.32500. Took 0.04 sec\n",
            "Epoch 392, Loss(train/val) 0.92774/0.32511. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.93142/0.32514. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 0.93561/0.32512. Took 0.05 sec\n",
            "Epoch 395, Loss(train/val) 0.93226/0.32435. Took 0.04 sec\n",
            "Epoch 396, Loss(train/val) 0.93607/0.32466. Took 0.05 sec\n",
            "Epoch 397, Loss(train/val) 0.92710/0.32461. Took 0.05 sec\n",
            "Epoch 398, Loss(train/val) 0.93256/0.32481. Took 0.04 sec\n",
            "Epoch 399, Loss(train/val) 0.93360/0.32433. Took 0.05 sec\n",
            "Epoch 400, Loss(train/val) 0.93589/0.32419. Took 0.04 sec\n",
            "Epoch 401, Loss(train/val) 0.92943/0.32421. Took 0.04 sec\n",
            "Epoch 402, Loss(train/val) 0.93257/0.32485. Took 0.05 sec\n",
            "Epoch 403, Loss(train/val) 0.93004/0.32534. Took 0.04 sec\n",
            "Epoch 404, Loss(train/val) 0.92769/0.32511. Took 0.05 sec\n",
            "Epoch 405, Loss(train/val) 0.92841/0.32525. Took 0.05 sec\n",
            "Epoch 406, Loss(train/val) 0.92729/0.32487. Took 0.04 sec\n",
            "Epoch 407, Loss(train/val) 0.93177/0.32375. Took 0.04 sec\n",
            "Epoch 408, Loss(train/val) 0.91977/0.32309. Took 0.05 sec\n",
            "Epoch 409, Loss(train/val) 0.92913/0.32275. Took 0.05 sec\n",
            "Epoch 410, Loss(train/val) 0.92992/0.32273. Took 0.05 sec\n",
            "Epoch 411, Loss(train/val) 0.92431/0.32281. Took 0.04 sec\n",
            "Epoch 412, Loss(train/val) 0.93204/0.32337. Took 0.05 sec\n",
            "Epoch 413, Loss(train/val) 0.93101/0.32367. Took 0.04 sec\n",
            "Epoch 414, Loss(train/val) 0.92343/0.32353. Took 0.05 sec\n",
            "Epoch 415, Loss(train/val) 0.93247/0.32354. Took 0.05 sec\n",
            "Epoch 416, Loss(train/val) 0.92585/0.32333. Took 0.05 sec\n",
            "Epoch 417, Loss(train/val) 0.92734/0.32290. Took 0.04 sec\n",
            "Epoch 418, Loss(train/val) 0.92629/0.32302. Took 0.05 sec\n",
            "Epoch 419, Loss(train/val) 0.93288/0.32372. Took 0.05 sec\n",
            "Epoch 420, Loss(train/val) 0.91935/0.32399. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.93172/0.32362. Took 0.05 sec\n",
            "Epoch 422, Loss(train/val) 0.92938/0.32332. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.92323/0.32290. Took 0.05 sec\n",
            "Epoch 424, Loss(train/val) 0.92904/0.32275. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.92596/0.32245. Took 0.04 sec\n",
            "Epoch 426, Loss(train/val) 0.92638/0.32190. Took 0.05 sec\n",
            "Epoch 427, Loss(train/val) 0.92517/0.32160. Took 0.05 sec\n",
            "Epoch 428, Loss(train/val) 0.92818/0.32138. Took 0.04 sec\n",
            "Epoch 429, Loss(train/val) 0.92062/0.32168. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.93137/0.32160. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.92729/0.32193. Took 0.04 sec\n",
            "Epoch 432, Loss(train/val) 0.92325/0.32173. Took 0.05 sec\n",
            "Epoch 433, Loss(train/val) 0.92773/0.32165. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.92733/0.32145. Took 0.05 sec\n",
            "Epoch 435, Loss(train/val) 0.92187/0.32097. Took 0.05 sec\n",
            "Epoch 436, Loss(train/val) 0.92148/0.32089. Took 0.05 sec\n",
            "Epoch 437, Loss(train/val) 0.92354/0.32097. Took 0.05 sec\n",
            "Epoch 438, Loss(train/val) 0.92584/0.32080. Took 0.04 sec\n",
            "Epoch 439, Loss(train/val) 0.92261/0.32074. Took 0.05 sec\n",
            "Epoch 440, Loss(train/val) 0.92589/0.32076. Took 0.05 sec\n",
            "Epoch 441, Loss(train/val) 0.91634/0.32058. Took 0.04 sec\n",
            "Epoch 442, Loss(train/val) 0.91974/0.32058. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 0.91676/0.32058. Took 0.05 sec\n",
            "Epoch 444, Loss(train/val) 0.92448/0.32093. Took 0.05 sec\n",
            "Epoch 445, Loss(train/val) 0.92593/0.32122. Took 0.04 sec\n",
            "Epoch 446, Loss(train/val) 0.92351/0.32107. Took 0.04 sec\n",
            "Epoch 447, Loss(train/val) 0.92118/0.32050. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 0.92170/0.32031. Took 0.06 sec\n",
            "Epoch 449, Loss(train/val) 0.91867/0.32040. Took 0.06 sec\n",
            "Epoch 450, Loss(train/val) 0.91723/0.32035. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 0.91689/0.32016. Took 0.04 sec\n",
            "Epoch 452, Loss(train/val) 0.92576/0.32025. Took 0.04 sec\n",
            "Epoch 453, Loss(train/val) 0.91191/0.32067. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.92162/0.32076. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 0.91195/0.32075. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.92177/0.32095. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.92014/0.32041. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.91511/0.32014. Took 0.05 sec\n",
            "Epoch 459, Loss(train/val) 0.91387/0.32006. Took 0.05 sec\n",
            "Epoch 460, Loss(train/val) 0.91964/0.31998. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.92303/0.31987. Took 0.06 sec\n",
            "Epoch 462, Loss(train/val) 0.91384/0.31981. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.91730/0.31979. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.92020/0.31967. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.91686/0.31961. Took 0.05 sec\n",
            "Epoch 466, Loss(train/val) 0.91448/0.31975. Took 0.05 sec\n",
            "Epoch 467, Loss(train/val) 0.91981/0.31918. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.91974/0.31900. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 0.91020/0.31891. Took 0.06 sec\n",
            "Epoch 470, Loss(train/val) 0.91876/0.31890. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 0.91620/0.31888. Took 0.05 sec\n",
            "Epoch 472, Loss(train/val) 0.91305/0.31881. Took 0.05 sec\n",
            "Epoch 473, Loss(train/val) 0.91018/0.31896. Took 0.05 sec\n",
            "Epoch 474, Loss(train/val) 0.91225/0.31869. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 0.91672/0.31858. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.91059/0.31870. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.91140/0.31876. Took 0.05 sec\n",
            "Epoch 478, Loss(train/val) 0.91340/0.31813. Took 0.06 sec\n",
            "Epoch 479, Loss(train/val) 0.91697/0.31780. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.91000/0.31800. Took 0.05 sec\n",
            "Epoch 481, Loss(train/val) 0.91450/0.31799. Took 0.06 sec\n",
            "Epoch 482, Loss(train/val) 0.91683/0.31733. Took 0.05 sec\n",
            "Epoch 483, Loss(train/val) 0.91709/0.31718. Took 0.05 sec\n",
            "Epoch 484, Loss(train/val) 0.91602/0.31695. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.91272/0.31688. Took 0.05 sec\n",
            "Epoch 486, Loss(train/val) 0.91387/0.31680. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.91962/0.31675. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.90971/0.31665. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.91195/0.31662. Took 0.05 sec\n",
            "Epoch 490, Loss(train/val) 0.91480/0.31654. Took 0.05 sec\n",
            "Epoch 491, Loss(train/val) 0.90978/0.31647. Took 0.05 sec\n",
            "Epoch 492, Loss(train/val) 0.91400/0.31656. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.90363/0.31661. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.91050/0.31677. Took 0.04 sec\n",
            "Epoch 495, Loss(train/val) 0.90176/0.31697. Took 0.05 sec\n",
            "Epoch 496, Loss(train/val) 0.91250/0.31683. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 0.91115/0.31688. Took 0.05 sec\n",
            "Epoch 498, Loss(train/val) 0.90783/0.31674. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.90661/0.31671. Took 0.05 sec\n",
            "Epoch 500, Loss(train/val) 0.90809/0.31655. Took 0.05 sec\n",
            "Epoch 501, Loss(train/val) 0.90684/0.31649. Took 0.05 sec\n",
            "Epoch 502, Loss(train/val) 0.90352/0.31669. Took 0.04 sec\n",
            "Epoch 503, Loss(train/val) 0.90891/0.31730. Took 0.05 sec\n",
            "Epoch 504, Loss(train/val) 0.90373/0.31769. Took 0.05 sec\n",
            "Epoch 505, Loss(train/val) 0.90981/0.31743. Took 0.05 sec\n",
            "Epoch 506, Loss(train/val) 0.90453/0.31759. Took 0.05 sec\n",
            "Epoch 507, Loss(train/val) 0.91370/0.31817. Took 0.05 sec\n",
            "Epoch 508, Loss(train/val) 0.90362/0.31800. Took 0.05 sec\n",
            "Epoch 509, Loss(train/val) 0.90978/0.31716. Took 0.04 sec\n",
            "Epoch 510, Loss(train/val) 0.89559/0.31659. Took 0.05 sec\n",
            "Epoch 511, Loss(train/val) 0.90766/0.31636. Took 0.05 sec\n",
            "Epoch 512, Loss(train/val) 0.90741/0.31632. Took 0.05 sec\n",
            "Epoch 513, Loss(train/val) 0.90010/0.31652. Took 0.05 sec\n",
            "Epoch 514, Loss(train/val) 0.90454/0.31663. Took 0.05 sec\n",
            "Epoch 515, Loss(train/val) 0.90765/0.31637. Took 0.04 sec\n",
            "Epoch 516, Loss(train/val) 0.90117/0.31618. Took 0.06 sec\n",
            "Epoch 517, Loss(train/val) 0.89941/0.31519. Took 0.05 sec\n",
            "Epoch 518, Loss(train/val) 0.90454/0.31500. Took 0.05 sec\n",
            "Epoch 519, Loss(train/val) 0.89606/0.31485. Took 0.05 sec\n",
            "Epoch 520, Loss(train/val) 0.90218/0.31471. Took 0.05 sec\n",
            "Epoch 521, Loss(train/val) 0.90499/0.31469. Took 0.05 sec\n",
            "Epoch 522, Loss(train/val) 0.90259/0.31458. Took 0.05 sec\n",
            "Epoch 523, Loss(train/val) 0.90154/0.31457. Took 0.04 sec\n",
            "Epoch 524, Loss(train/val) 0.91107/0.31459. Took 0.04 sec\n",
            "Epoch 525, Loss(train/val) 0.89643/0.31450. Took 0.05 sec\n",
            "Epoch 526, Loss(train/val) 0.90148/0.31447. Took 0.05 sec\n",
            "Epoch 527, Loss(train/val) 0.90213/0.31454. Took 0.05 sec\n",
            "Epoch 528, Loss(train/val) 0.90012/0.31463. Took 0.05 sec\n",
            "Epoch 529, Loss(train/val) 0.90357/0.31456. Took 0.05 sec\n",
            "Epoch 530, Loss(train/val) 0.90471/0.31447. Took 0.04 sec\n",
            "Epoch 531, Loss(train/val) 0.90346/0.31440. Took 0.05 sec\n",
            "Epoch 532, Loss(train/val) 0.90533/0.31429. Took 0.05 sec\n",
            "Epoch 533, Loss(train/val) 0.89682/0.31423. Took 0.05 sec\n",
            "Epoch 534, Loss(train/val) 0.89696/0.31447. Took 0.05 sec\n",
            "Epoch 535, Loss(train/val) 0.89595/0.31420. Took 0.05 sec\n",
            "Epoch 536, Loss(train/val) 0.89344/0.31432. Took 0.05 sec\n",
            "Epoch 537, Loss(train/val) 0.88568/0.31423. Took 0.05 sec\n",
            "Epoch 538, Loss(train/val) 0.90052/0.31431. Took 0.04 sec\n",
            "Epoch 539, Loss(train/val) 0.90454/0.31477. Took 0.05 sec\n",
            "Epoch 540, Loss(train/val) 0.89731/0.31435. Took 0.05 sec\n",
            "Epoch 541, Loss(train/val) 0.89595/0.31387. Took 0.05 sec\n",
            "Epoch 542, Loss(train/val) 0.89350/0.31394. Took 0.05 sec\n",
            "Epoch 543, Loss(train/val) 0.89839/0.31366. Took 0.05 sec\n",
            "Epoch 544, Loss(train/val) 0.89878/0.31352. Took 0.05 sec\n",
            "Epoch 545, Loss(train/val) 0.89728/0.31342. Took 0.05 sec\n",
            "Epoch 546, Loss(train/val) 0.88814/0.31340. Took 0.05 sec\n",
            "Epoch 547, Loss(train/val) 0.89981/0.31329. Took 0.05 sec\n",
            "Epoch 548, Loss(train/val) 0.88980/0.31327. Took 0.05 sec\n",
            "Epoch 549, Loss(train/val) 0.89624/0.31321. Took 0.05 sec\n",
            "Epoch 550, Loss(train/val) 0.88994/0.31337. Took 0.05 sec\n",
            "Epoch 551, Loss(train/val) 0.89335/0.31318. Took 0.05 sec\n",
            "Epoch 552, Loss(train/val) 0.88890/0.31330. Took 0.05 sec\n",
            "Epoch 553, Loss(train/val) 0.89379/0.31334. Took 0.06 sec\n",
            "Epoch 554, Loss(train/val) 0.90054/0.31307. Took 0.05 sec\n",
            "Epoch 555, Loss(train/val) 0.89287/0.31337. Took 0.05 sec\n",
            "Epoch 556, Loss(train/val) 0.89474/0.31303. Took 0.05 sec\n",
            "Epoch 557, Loss(train/val) 0.89724/0.31301. Took 0.05 sec\n",
            "Epoch 558, Loss(train/val) 0.88529/0.31356. Took 0.05 sec\n",
            "Epoch 559, Loss(train/val) 0.89518/0.31394. Took 0.05 sec\n",
            "Epoch 560, Loss(train/val) 0.89764/0.31367. Took 0.05 sec\n",
            "Epoch 561, Loss(train/val) 0.88897/0.31367. Took 0.05 sec\n",
            "Epoch 562, Loss(train/val) 0.89505/0.31348. Took 0.05 sec\n",
            "Epoch 563, Loss(train/val) 0.88811/0.31295. Took 0.05 sec\n",
            "Epoch 564, Loss(train/val) 0.89715/0.31288. Took 0.05 sec\n",
            "Epoch 565, Loss(train/val) 0.89021/0.31274. Took 0.05 sec\n",
            "Epoch 566, Loss(train/val) 0.90024/0.31273. Took 0.05 sec\n",
            "Epoch 567, Loss(train/val) 0.89155/0.31267. Took 0.05 sec\n",
            "Epoch 568, Loss(train/val) 0.88728/0.31307. Took 0.04 sec\n",
            "Epoch 569, Loss(train/val) 0.88811/0.31330. Took 0.05 sec\n",
            "Epoch 570, Loss(train/val) 0.88729/0.31322. Took 0.05 sec\n",
            "Epoch 571, Loss(train/val) 0.89729/0.31358. Took 0.05 sec\n",
            "Epoch 572, Loss(train/val) 0.88376/0.31312. Took 0.04 sec\n",
            "Epoch 573, Loss(train/val) 0.89337/0.31314. Took 0.05 sec\n",
            "Epoch 574, Loss(train/val) 0.88791/0.31258. Took 0.05 sec\n",
            "Epoch 575, Loss(train/val) 0.88073/0.31205. Took 0.04 sec\n",
            "Epoch 576, Loss(train/val) 0.88674/0.31189. Took 0.05 sec\n",
            "Epoch 577, Loss(train/val) 0.88356/0.31164. Took 0.04 sec\n",
            "Epoch 578, Loss(train/val) 0.88819/0.31153. Took 0.05 sec\n",
            "Epoch 579, Loss(train/val) 0.88683/0.31148. Took 0.05 sec\n",
            "Epoch 580, Loss(train/val) 0.88759/0.31137. Took 0.04 sec\n",
            "Epoch 581, Loss(train/val) 0.89157/0.31127. Took 0.05 sec\n",
            "Epoch 582, Loss(train/val) 0.87640/0.31123. Took 0.05 sec\n",
            "Epoch 583, Loss(train/val) 0.88430/0.31117. Took 0.06 sec\n",
            "Epoch 584, Loss(train/val) 0.89069/0.31117. Took 0.05 sec\n",
            "Epoch 585, Loss(train/val) 0.88240/0.31118. Took 0.05 sec\n",
            "Epoch 586, Loss(train/val) 0.88267/0.31128. Took 0.05 sec\n",
            "Epoch 587, Loss(train/val) 0.88479/0.31124. Took 0.05 sec\n",
            "Epoch 588, Loss(train/val) 0.88808/0.31131. Took 0.04 sec\n",
            "Epoch 589, Loss(train/val) 0.88364/0.31125. Took 0.04 sec\n",
            "Epoch 590, Loss(train/val) 0.88863/0.31134. Took 0.05 sec\n",
            "Epoch 591, Loss(train/val) 0.88321/0.31113. Took 0.05 sec\n",
            "Epoch 592, Loss(train/val) 0.88796/0.31112. Took 0.05 sec\n",
            "Epoch 593, Loss(train/val) 0.88444/0.31092. Took 0.04 sec\n",
            "Epoch 594, Loss(train/val) 0.87899/0.31099. Took 0.05 sec\n",
            "Epoch 595, Loss(train/val) 0.88118/0.31097. Took 0.06 sec\n",
            "Epoch 596, Loss(train/val) 0.88560/0.31079. Took 0.04 sec\n",
            "Epoch 597, Loss(train/val) 0.88215/0.31084. Took 0.05 sec\n",
            "Epoch 598, Loss(train/val) 0.88668/0.31068. Took 0.04 sec\n",
            "Epoch 599, Loss(train/val) 0.88422/0.31053. Took 0.05 sec\n",
            "Epoch 600, Loss(train/val) 0.88389/0.31041. Took 0.05 sec\n",
            "Epoch 601, Loss(train/val) 0.88798/0.31039. Took 0.05 sec\n",
            "Epoch 602, Loss(train/val) 0.88515/0.31057. Took 0.05 sec\n",
            "Epoch 603, Loss(train/val) 0.87928/0.31052. Took 0.04 sec\n",
            "Epoch 604, Loss(train/val) 0.88226/0.31065. Took 0.05 sec\n",
            "Epoch 605, Loss(train/val) 0.88353/0.31068. Took 0.05 sec\n",
            "Epoch 606, Loss(train/val) 0.87837/0.31104. Took 0.05 sec\n",
            "Epoch 607, Loss(train/val) 0.88109/0.31087. Took 0.05 sec\n",
            "Epoch 608, Loss(train/val) 0.87734/0.31100. Took 0.05 sec\n",
            "Epoch 609, Loss(train/val) 0.88353/0.31123. Took 0.05 sec\n",
            "Epoch 610, Loss(train/val) 0.87601/0.31183. Took 0.05 sec\n",
            "Epoch 611, Loss(train/val) 0.88274/0.31216. Took 0.04 sec\n",
            "Epoch 612, Loss(train/val) 0.87967/0.31230. Took 0.05 sec\n",
            "Epoch 613, Loss(train/val) 0.87620/0.31206. Took 0.04 sec\n",
            "Epoch 614, Loss(train/val) 0.88212/0.31158. Took 0.05 sec\n",
            "Epoch 615, Loss(train/val) 0.88666/0.31127. Took 0.05 sec\n",
            "Epoch 616, Loss(train/val) 0.87902/0.31100. Took 0.04 sec\n",
            "Epoch 617, Loss(train/val) 0.88028/0.31077. Took 0.05 sec\n",
            "Epoch 618, Loss(train/val) 0.88285/0.31005. Took 0.05 sec\n",
            "Epoch 619, Loss(train/val) 0.88108/0.30985. Took 0.05 sec\n",
            "Epoch 620, Loss(train/val) 0.87564/0.30986. Took 0.05 sec\n",
            "Epoch 621, Loss(train/val) 0.87853/0.30948. Took 0.05 sec\n",
            "Epoch 622, Loss(train/val) 0.87645/0.30940. Took 0.05 sec\n",
            "Epoch 623, Loss(train/val) 0.88400/0.30967. Took 0.05 sec\n",
            "Epoch 624, Loss(train/val) 0.88764/0.30987. Took 0.05 sec\n",
            "Epoch 625, Loss(train/val) 0.88740/0.31053. Took 0.05 sec\n",
            "Epoch 626, Loss(train/val) 0.87848/0.31095. Took 0.05 sec\n",
            "Epoch 627, Loss(train/val) 0.87746/0.31042. Took 0.04 sec\n",
            "Epoch 628, Loss(train/val) 0.87243/0.31020. Took 0.04 sec\n",
            "Epoch 629, Loss(train/val) 0.87327/0.30985. Took 0.05 sec\n",
            "Epoch 630, Loss(train/val) 0.88180/0.30943. Took 0.05 sec\n",
            "Epoch 631, Loss(train/val) 0.87141/0.30939. Took 0.04 sec\n",
            "Epoch 632, Loss(train/val) 0.87974/0.30927. Took 0.05 sec\n",
            "Epoch 633, Loss(train/val) 0.86935/0.30909. Took 0.05 sec\n",
            "Epoch 634, Loss(train/val) 0.87701/0.30909. Took 0.04 sec\n",
            "Epoch 635, Loss(train/val) 0.87030/0.30899. Took 0.05 sec\n",
            "Epoch 636, Loss(train/val) 0.88000/0.30882. Took 0.05 sec\n",
            "Epoch 637, Loss(train/val) 0.88050/0.30907. Took 0.05 sec\n",
            "Epoch 638, Loss(train/val) 0.87144/0.30898. Took 0.06 sec\n",
            "Epoch 639, Loss(train/val) 0.86727/0.30890. Took 0.05 sec\n",
            "Epoch 640, Loss(train/val) 0.86687/0.30863. Took 0.05 sec\n",
            "Epoch 641, Loss(train/val) 0.87207/0.30853. Took 0.05 sec\n",
            "Epoch 642, Loss(train/val) 0.86917/0.30841. Took 0.05 sec\n",
            "Epoch 643, Loss(train/val) 0.86841/0.30834. Took 0.05 sec\n",
            "Epoch 644, Loss(train/val) 0.87393/0.30837. Took 0.05 sec\n",
            "Epoch 645, Loss(train/val) 0.87281/0.30843. Took 0.05 sec\n",
            "Epoch 646, Loss(train/val) 0.87315/0.30845. Took 0.05 sec\n",
            "Epoch 647, Loss(train/val) 0.86832/0.30847. Took 0.05 sec\n",
            "Epoch 648, Loss(train/val) 0.87136/0.30844. Took 0.05 sec\n",
            "Epoch 649, Loss(train/val) 0.86915/0.30844. Took 0.05 sec\n",
            "Epoch 650, Loss(train/val) 0.87467/0.30830. Took 0.06 sec\n",
            "Epoch 651, Loss(train/val) 0.86470/0.30822. Took 0.05 sec\n",
            "Epoch 652, Loss(train/val) 0.86544/0.30811. Took 0.04 sec\n",
            "Epoch 653, Loss(train/val) 0.86796/0.30814. Took 0.05 sec\n",
            "Epoch 654, Loss(train/val) 0.87836/0.30839. Took 0.05 sec\n",
            "Epoch 655, Loss(train/val) 0.86851/0.30810. Took 0.05 sec\n",
            "Epoch 656, Loss(train/val) 0.86760/0.30805. Took 0.04 sec\n",
            "Epoch 657, Loss(train/val) 0.86956/0.30808. Took 0.05 sec\n",
            "Epoch 658, Loss(train/val) 0.87170/0.30810. Took 0.05 sec\n",
            "Epoch 659, Loss(train/val) 0.86576/0.30805. Took 0.05 sec\n",
            "Epoch 660, Loss(train/val) 0.86052/0.30801. Took 0.06 sec\n",
            "Epoch 661, Loss(train/val) 0.86610/0.30808. Took 0.05 sec\n",
            "Epoch 662, Loss(train/val) 0.86915/0.30793. Took 0.05 sec\n",
            "Epoch 663, Loss(train/val) 0.86489/0.30816. Took 0.04 sec\n",
            "Epoch 664, Loss(train/val) 0.87335/0.30805. Took 0.05 sec\n",
            "Epoch 665, Loss(train/val) 0.87145/0.30785. Took 0.05 sec\n",
            "Epoch 666, Loss(train/val) 0.87248/0.30780. Took 0.05 sec\n",
            "Epoch 667, Loss(train/val) 0.87055/0.30772. Took 0.05 sec\n",
            "Epoch 668, Loss(train/val) 0.86615/0.30757. Took 0.05 sec\n",
            "Epoch 669, Loss(train/val) 0.87607/0.30773. Took 0.05 sec\n",
            "Epoch 670, Loss(train/val) 0.87289/0.30762. Took 0.05 sec\n",
            "Epoch 671, Loss(train/val) 0.86749/0.30729. Took 0.05 sec\n",
            "Epoch 672, Loss(train/val) 0.87375/0.30720. Took 0.05 sec\n",
            "Epoch 673, Loss(train/val) 0.86959/0.30732. Took 0.05 sec\n",
            "Epoch 674, Loss(train/val) 0.85919/0.30731. Took 0.05 sec\n",
            "Epoch 675, Loss(train/val) 0.86114/0.30717. Took 0.05 sec\n",
            "Epoch 676, Loss(train/val) 0.85834/0.30717. Took 0.05 sec\n",
            "Epoch 677, Loss(train/val) 0.87272/0.30726. Took 0.05 sec\n",
            "Epoch 678, Loss(train/val) 0.86328/0.30742. Took 0.05 sec\n",
            "Epoch 679, Loss(train/val) 0.86524/0.30709. Took 0.05 sec\n",
            "Epoch 680, Loss(train/val) 0.86190/0.30711. Took 0.05 sec\n",
            "Epoch 681, Loss(train/val) 0.86344/0.30735. Took 0.06 sec\n",
            "Epoch 682, Loss(train/val) 0.86560/0.30721. Took 0.05 sec\n",
            "Epoch 683, Loss(train/val) 0.85943/0.30717. Took 0.05 sec\n",
            "Epoch 684, Loss(train/val) 0.85933/0.30706. Took 0.05 sec\n",
            "Epoch 685, Loss(train/val) 0.87067/0.30727. Took 0.05 sec\n",
            "Epoch 686, Loss(train/val) 0.86160/0.30731. Took 0.05 sec\n",
            "Epoch 687, Loss(train/val) 0.85470/0.30735. Took 0.05 sec\n",
            "Epoch 688, Loss(train/val) 0.86771/0.30755. Took 0.05 sec\n",
            "Epoch 689, Loss(train/val) 0.86305/0.30720. Took 0.05 sec\n",
            "Epoch 690, Loss(train/val) 0.85561/0.30699. Took 0.05 sec\n",
            "Epoch 691, Loss(train/val) 0.85560/0.30735. Took 0.05 sec\n",
            "Epoch 692, Loss(train/val) 0.86624/0.30732. Took 0.05 sec\n",
            "Epoch 693, Loss(train/val) 0.85357/0.30705. Took 0.04 sec\n",
            "Epoch 694, Loss(train/val) 0.86568/0.30700. Took 0.05 sec\n",
            "Epoch 695, Loss(train/val) 0.86091/0.30713. Took 0.05 sec\n",
            "Epoch 696, Loss(train/val) 0.86202/0.30712. Took 0.05 sec\n",
            "Epoch 697, Loss(train/val) 0.86630/0.30729. Took 0.04 sec\n",
            "Epoch 698, Loss(train/val) 0.86182/0.30709. Took 0.04 sec\n",
            "Epoch 699, Loss(train/val) 0.86239/0.30688. Took 0.05 sec\n",
            "Epoch 700, Loss(train/val) 0.86876/0.30678. Took 0.05 sec\n",
            "Epoch 701, Loss(train/val) 0.86377/0.30656. Took 0.04 sec\n",
            "Epoch 702, Loss(train/val) 0.85385/0.30643. Took 0.05 sec\n",
            "Epoch 703, Loss(train/val) 0.85901/0.30629. Took 0.05 sec\n",
            "Epoch 704, Loss(train/val) 0.85988/0.30639. Took 0.04 sec\n",
            "Epoch 705, Loss(train/val) 0.85353/0.30642. Took 0.05 sec\n",
            "Epoch 706, Loss(train/val) 0.86294/0.30635. Took 0.05 sec\n",
            "Epoch 707, Loss(train/val) 0.86380/0.30605. Took 0.04 sec\n",
            "Epoch 708, Loss(train/val) 0.85947/0.30611. Took 0.05 sec\n",
            "Epoch 709, Loss(train/val) 0.86615/0.30607. Took 0.04 sec\n",
            "Epoch 710, Loss(train/val) 0.85743/0.30613. Took 0.05 sec\n",
            "Epoch 711, Loss(train/val) 0.85633/0.30636. Took 0.05 sec\n",
            "Epoch 712, Loss(train/val) 0.85670/0.30653. Took 0.05 sec\n",
            "Epoch 713, Loss(train/val) 0.85793/0.30622. Took 0.05 sec\n",
            "Epoch 714, Loss(train/val) 0.86140/0.30631. Took 0.04 sec\n",
            "Epoch 715, Loss(train/val) 0.86284/0.30603. Took 0.05 sec\n",
            "Epoch 716, Loss(train/val) 0.86281/0.30579. Took 0.05 sec\n",
            "Epoch 717, Loss(train/val) 0.85796/0.30590. Took 0.05 sec\n",
            "Epoch 718, Loss(train/val) 0.85083/0.30600. Took 0.05 sec\n",
            "Epoch 719, Loss(train/val) 0.85010/0.30583. Took 0.05 sec\n",
            "Epoch 720, Loss(train/val) 0.85464/0.30571. Took 0.05 sec\n",
            "Epoch 721, Loss(train/val) 0.85476/0.30561. Took 0.05 sec\n",
            "Epoch 722, Loss(train/val) 0.85745/0.30559. Took 0.05 sec\n",
            "Epoch 723, Loss(train/val) 0.85220/0.30570. Took 0.05 sec\n",
            "Epoch 724, Loss(train/val) 0.85693/0.30570. Took 0.06 sec\n",
            "Epoch 725, Loss(train/val) 0.85602/0.30574. Took 0.07 sec\n",
            "Epoch 726, Loss(train/val) 0.85341/0.30535. Took 0.05 sec\n",
            "Epoch 727, Loss(train/val) 0.85925/0.30532. Took 0.05 sec\n",
            "Epoch 728, Loss(train/val) 0.85353/0.30531. Took 0.05 sec\n",
            "Epoch 729, Loss(train/val) 0.85102/0.30539. Took 0.05 sec\n",
            "Epoch 730, Loss(train/val) 0.84920/0.30539. Took 0.05 sec\n",
            "Epoch 731, Loss(train/val) 0.85974/0.30546. Took 0.05 sec\n",
            "Epoch 732, Loss(train/val) 0.85381/0.30555. Took 0.05 sec\n",
            "Epoch 733, Loss(train/val) 0.85070/0.30562. Took 0.05 sec\n",
            "Epoch 734, Loss(train/val) 0.85920/0.30540. Took 0.05 sec\n",
            "Epoch 735, Loss(train/val) 0.85802/0.30549. Took 0.05 sec\n",
            "Epoch 736, Loss(train/val) 0.85571/0.30533. Took 0.04 sec\n",
            "Epoch 737, Loss(train/val) 0.86176/0.30548. Took 0.05 sec\n",
            "Epoch 738, Loss(train/val) 0.85646/0.30533. Took 0.04 sec\n",
            "Epoch 739, Loss(train/val) 0.85826/0.30547. Took 0.04 sec\n",
            "Epoch 740, Loss(train/val) 0.84364/0.30543. Took 0.05 sec\n",
            "Epoch 741, Loss(train/val) 0.85858/0.30535. Took 0.04 sec\n",
            "Epoch 742, Loss(train/val) 0.85379/0.30555. Took 0.04 sec\n",
            "Epoch 743, Loss(train/val) 0.85505/0.30581. Took 0.04 sec\n",
            "Epoch 744, Loss(train/val) 0.85461/0.30563. Took 0.05 sec\n",
            "Epoch 745, Loss(train/val) 0.85878/0.30563. Took 0.06 sec\n",
            "Epoch 746, Loss(train/val) 0.85978/0.30554. Took 0.05 sec\n",
            "Epoch 747, Loss(train/val) 0.85071/0.30527. Took 0.04 sec\n",
            "Epoch 748, Loss(train/val) 0.85663/0.30502. Took 0.05 sec\n",
            "Epoch 749, Loss(train/val) 0.84980/0.30486. Took 0.05 sec\n",
            "Epoch 750, Loss(train/val) 0.86041/0.30501. Took 0.05 sec\n",
            "Epoch 751, Loss(train/val) 0.85475/0.30505. Took 0.05 sec\n",
            "Epoch 752, Loss(train/val) 0.84690/0.30499. Took 0.05 sec\n",
            "Epoch 753, Loss(train/val) 0.85648/0.30507. Took 0.05 sec\n",
            "Epoch 754, Loss(train/val) 0.84990/0.30535. Took 0.04 sec\n",
            "Epoch 755, Loss(train/val) 0.85541/0.30510. Took 0.05 sec\n",
            "Epoch 756, Loss(train/val) 0.84924/0.30499. Took 0.05 sec\n",
            "Epoch 757, Loss(train/val) 0.85445/0.30484. Took 0.05 sec\n",
            "Epoch 758, Loss(train/val) 0.84715/0.30471. Took 0.04 sec\n",
            "Epoch 759, Loss(train/val) 0.85304/0.30473. Took 0.05 sec\n",
            "Epoch 760, Loss(train/val) 0.84577/0.30468. Took 0.05 sec\n",
            "Epoch 761, Loss(train/val) 0.85099/0.30475. Took 0.05 sec\n",
            "Epoch 762, Loss(train/val) 0.85024/0.30473. Took 0.05 sec\n",
            "Epoch 763, Loss(train/val) 0.84928/0.30462. Took 0.05 sec\n",
            "Epoch 764, Loss(train/val) 0.83704/0.30445. Took 0.05 sec\n",
            "Epoch 765, Loss(train/val) 0.85043/0.30425. Took 0.05 sec\n",
            "Epoch 766, Loss(train/val) 0.84816/0.30431. Took 0.05 sec\n",
            "Epoch 767, Loss(train/val) 0.85283/0.30445. Took 0.05 sec\n",
            "Epoch 768, Loss(train/val) 0.85028/0.30440. Took 0.05 sec\n",
            "Epoch 769, Loss(train/val) 0.84981/0.30420. Took 0.05 sec\n",
            "Epoch 770, Loss(train/val) 0.84724/0.30414. Took 0.05 sec\n",
            "Epoch 771, Loss(train/val) 0.85699/0.30435. Took 0.05 sec\n",
            "Epoch 772, Loss(train/val) 0.84848/0.30427. Took 0.05 sec\n",
            "Epoch 773, Loss(train/val) 0.84641/0.30411. Took 0.05 sec\n",
            "Epoch 774, Loss(train/val) 0.85169/0.30409. Took 0.05 sec\n",
            "Epoch 775, Loss(train/val) 0.84971/0.30388. Took 0.06 sec\n",
            "Epoch 776, Loss(train/val) 0.85175/0.30407. Took 0.05 sec\n",
            "Epoch 777, Loss(train/val) 0.84488/0.30430. Took 0.05 sec\n",
            "Epoch 778, Loss(train/val) 0.84278/0.30412. Took 0.04 sec\n",
            "Epoch 779, Loss(train/val) 0.85213/0.30413. Took 0.05 sec\n",
            "Epoch 780, Loss(train/val) 0.84355/0.30416. Took 0.05 sec\n",
            "Epoch 781, Loss(train/val) 0.85052/0.30425. Took 0.04 sec\n",
            "Epoch 782, Loss(train/val) 0.84714/0.30386. Took 0.05 sec\n",
            "Epoch 783, Loss(train/val) 0.84132/0.30370. Took 0.04 sec\n",
            "Epoch 784, Loss(train/val) 0.84878/0.30391. Took 0.05 sec\n",
            "Epoch 785, Loss(train/val) 0.84118/0.30396. Took 0.05 sec\n",
            "Epoch 786, Loss(train/val) 0.84166/0.30369. Took 0.05 sec\n",
            "Epoch 787, Loss(train/val) 0.84116/0.30356. Took 0.05 sec\n",
            "Epoch 788, Loss(train/val) 0.84287/0.30357. Took 0.05 sec\n",
            "Epoch 789, Loss(train/val) 0.83923/0.30398. Took 0.04 sec\n",
            "Epoch 790, Loss(train/val) 0.84499/0.30387. Took 0.05 sec\n",
            "Epoch 791, Loss(train/val) 0.84699/0.30396. Took 0.05 sec\n",
            "Epoch 792, Loss(train/val) 0.85014/0.30400. Took 0.05 sec\n",
            "Epoch 793, Loss(train/val) 0.84961/0.30412. Took 0.04 sec\n",
            "Epoch 794, Loss(train/val) 0.84548/0.30406. Took 0.05 sec\n",
            "Epoch 795, Loss(train/val) 0.83722/0.30393. Took 0.05 sec\n",
            "Epoch 796, Loss(train/val) 0.84456/0.30377. Took 0.05 sec\n",
            "Epoch 797, Loss(train/val) 0.84791/0.30349. Took 0.05 sec\n",
            "Epoch 798, Loss(train/val) 0.84205/0.30365. Took 0.05 sec\n",
            "Epoch 799, Loss(train/val) 0.83789/0.30344. Took 0.04 sec\n",
            "Epoch 800, Loss(train/val) 0.83634/0.30330. Took 0.06 sec\n",
            "Epoch 801, Loss(train/val) 0.84123/0.30315. Took 0.05 sec\n",
            "Epoch 802, Loss(train/val) 0.83710/0.30307. Took 0.04 sec\n",
            "Epoch 803, Loss(train/val) 0.84549/0.30301. Took 0.05 sec\n",
            "Epoch 804, Loss(train/val) 0.84566/0.30307. Took 0.05 sec\n",
            "Epoch 805, Loss(train/val) 0.84470/0.30297. Took 0.05 sec\n",
            "Epoch 806, Loss(train/val) 0.83884/0.30307. Took 0.05 sec\n",
            "Epoch 807, Loss(train/val) 0.84513/0.30309. Took 0.05 sec\n",
            "Epoch 808, Loss(train/val) 0.84080/0.30308. Took 0.05 sec\n",
            "Epoch 809, Loss(train/val) 0.84090/0.30305. Took 0.06 sec\n",
            "Epoch 810, Loss(train/val) 0.83465/0.30311. Took 0.05 sec\n",
            "Epoch 811, Loss(train/val) 0.84414/0.30283. Took 0.05 sec\n",
            "Epoch 812, Loss(train/val) 0.83728/0.30290. Took 0.05 sec\n",
            "Epoch 813, Loss(train/val) 0.83984/0.30276. Took 0.05 sec\n",
            "Epoch 814, Loss(train/val) 0.84797/0.30266. Took 0.05 sec\n",
            "Epoch 815, Loss(train/val) 0.83404/0.30257. Took 0.05 sec\n",
            "Epoch 816, Loss(train/val) 0.84143/0.30270. Took 0.05 sec\n",
            "Epoch 817, Loss(train/val) 0.83376/0.30279. Took 0.05 sec\n",
            "Epoch 818, Loss(train/val) 0.84041/0.30278. Took 0.05 sec\n",
            "Epoch 819, Loss(train/val) 0.83982/0.30258. Took 0.05 sec\n",
            "Epoch 820, Loss(train/val) 0.82884/0.30255. Took 0.05 sec\n",
            "Epoch 821, Loss(train/val) 0.84127/0.30258. Took 0.05 sec\n",
            "Epoch 822, Loss(train/val) 0.84206/0.30245. Took 0.04 sec\n",
            "Epoch 823, Loss(train/val) 0.84111/0.30207. Took 0.04 sec\n",
            "Epoch 824, Loss(train/val) 0.83942/0.30203. Took 0.05 sec\n",
            "Epoch 825, Loss(train/val) 0.83506/0.30194. Took 0.05 sec\n",
            "Epoch 826, Loss(train/val) 0.83199/0.30210. Took 0.05 sec\n",
            "Epoch 827, Loss(train/val) 0.83635/0.30228. Took 0.04 sec\n",
            "Epoch 828, Loss(train/val) 0.83650/0.30220. Took 0.04 sec\n",
            "Epoch 829, Loss(train/val) 0.84395/0.30234. Took 0.05 sec\n",
            "Epoch 830, Loss(train/val) 0.83209/0.30225. Took 0.05 sec\n",
            "Epoch 831, Loss(train/val) 0.83188/0.30209. Took 0.05 sec\n",
            "Epoch 832, Loss(train/val) 0.83754/0.30186. Took 0.05 sec\n",
            "Epoch 833, Loss(train/val) 0.82712/0.30200. Took 0.05 sec\n",
            "Epoch 834, Loss(train/val) 0.83133/0.30176. Took 0.05 sec\n",
            "Epoch 835, Loss(train/val) 0.83689/0.30167. Took 0.05 sec\n",
            "Epoch 836, Loss(train/val) 0.84086/0.30151. Took 0.05 sec\n",
            "Epoch 837, Loss(train/val) 0.83993/0.30163. Took 0.05 sec\n",
            "Epoch 838, Loss(train/val) 0.83863/0.30189. Took 0.05 sec\n",
            "Epoch 839, Loss(train/val) 0.83423/0.30155. Took 0.05 sec\n",
            "Epoch 840, Loss(train/val) 0.83005/0.30140. Took 0.05 sec\n",
            "Epoch 841, Loss(train/val) 0.83519/0.30123. Took 0.05 sec\n",
            "Epoch 842, Loss(train/val) 0.83345/0.30126. Took 0.05 sec\n",
            "Epoch 843, Loss(train/val) 0.83266/0.30124. Took 0.04 sec\n",
            "Epoch 844, Loss(train/val) 0.83980/0.30110. Took 0.05 sec\n",
            "Epoch 845, Loss(train/val) 0.83835/0.30134. Took 0.05 sec\n",
            "Epoch 846, Loss(train/val) 0.84070/0.30134. Took 0.04 sec\n",
            "Epoch 847, Loss(train/val) 0.83328/0.30112. Took 0.04 sec\n",
            "Epoch 848, Loss(train/val) 0.82633/0.30134. Took 0.05 sec\n",
            "Epoch 849, Loss(train/val) 0.83363/0.30152. Took 0.05 sec\n",
            "Epoch 850, Loss(train/val) 0.82768/0.30160. Took 0.04 sec\n",
            "Epoch 851, Loss(train/val) 0.82866/0.30163. Took 0.06 sec\n",
            "Epoch 852, Loss(train/val) 0.82873/0.30155. Took 0.05 sec\n",
            "Epoch 853, Loss(train/val) 0.83595/0.30153. Took 0.04 sec\n",
            "Epoch 854, Loss(train/val) 0.82863/0.30156. Took 0.05 sec\n",
            "Epoch 855, Loss(train/val) 0.83586/0.30174. Took 0.04 sec\n",
            "Epoch 856, Loss(train/val) 0.82045/0.30155. Took 0.04 sec\n",
            "Epoch 857, Loss(train/val) 0.82982/0.30112. Took 0.05 sec\n",
            "Epoch 858, Loss(train/val) 0.82415/0.30105. Took 0.05 sec\n",
            "Epoch 859, Loss(train/val) 0.83577/0.30095. Took 0.05 sec\n",
            "Epoch 860, Loss(train/val) 0.82704/0.30113. Took 0.05 sec\n",
            "Epoch 861, Loss(train/val) 0.82761/0.30134. Took 0.04 sec\n",
            "Epoch 862, Loss(train/val) 0.83311/0.30132. Took 0.05 sec\n",
            "Epoch 863, Loss(train/val) 0.83147/0.30115. Took 0.05 sec\n",
            "Epoch 864, Loss(train/val) 0.83379/0.30102. Took 0.05 sec\n",
            "Epoch 865, Loss(train/val) 0.83399/0.30123. Took 0.04 sec\n",
            "Epoch 866, Loss(train/val) 0.82949/0.30121. Took 0.04 sec\n",
            "Epoch 867, Loss(train/val) 0.83070/0.30135. Took 0.05 sec\n",
            "Epoch 868, Loss(train/val) 0.82835/0.30163. Took 0.04 sec\n",
            "Epoch 869, Loss(train/val) 0.82785/0.30126. Took 0.05 sec\n",
            "Epoch 870, Loss(train/val) 0.82598/0.30144. Took 0.04 sec\n",
            "Epoch 871, Loss(train/val) 0.83091/0.30127. Took 0.04 sec\n",
            "Epoch 872, Loss(train/val) 0.82291/0.30109. Took 0.04 sec\n",
            "Epoch 873, Loss(train/val) 0.83026/0.30087. Took 0.05 sec\n",
            "Epoch 874, Loss(train/val) 0.82913/0.30087. Took 0.05 sec\n",
            "Epoch 875, Loss(train/val) 0.83146/0.30076. Took 0.05 sec\n",
            "Epoch 876, Loss(train/val) 0.83556/0.30053. Took 0.05 sec\n",
            "Epoch 877, Loss(train/val) 0.82975/0.30044. Took 0.04 sec\n",
            "Epoch 878, Loss(train/val) 0.82627/0.30049. Took 0.04 sec\n",
            "Epoch 879, Loss(train/val) 0.83080/0.30054. Took 0.05 sec\n",
            "Epoch 880, Loss(train/val) 0.81937/0.30061. Took 0.05 sec\n",
            "Epoch 881, Loss(train/val) 0.83591/0.30066. Took 0.05 sec\n",
            "Epoch 882, Loss(train/val) 0.82233/0.30040. Took 0.04 sec\n",
            "Epoch 883, Loss(train/val) 0.82597/0.30031. Took 0.04 sec\n",
            "Epoch 884, Loss(train/val) 0.83061/0.30014. Took 0.05 sec\n",
            "Epoch 885, Loss(train/val) 0.82073/0.30024. Took 0.04 sec\n",
            "Epoch 886, Loss(train/val) 0.82888/0.30006. Took 0.05 sec\n",
            "Epoch 887, Loss(train/val) 0.82367/0.29999. Took 0.04 sec\n",
            "Epoch 888, Loss(train/val) 0.82869/0.30017. Took 0.04 sec\n",
            "Epoch 889, Loss(train/val) 0.82980/0.29961. Took 0.05 sec\n",
            "Epoch 890, Loss(train/val) 0.82110/0.29958. Took 0.05 sec\n",
            "Epoch 891, Loss(train/val) 0.82117/0.29932. Took 0.05 sec\n",
            "Epoch 892, Loss(train/val) 0.82499/0.29955. Took 0.04 sec\n",
            "Epoch 893, Loss(train/val) 0.82568/0.29941. Took 0.04 sec\n",
            "Epoch 894, Loss(train/val) 0.82927/0.29925. Took 0.05 sec\n",
            "Epoch 895, Loss(train/val) 0.83109/0.29905. Took 0.06 sec\n",
            "Epoch 896, Loss(train/val) 0.82556/0.29933. Took 0.05 sec\n",
            "Epoch 897, Loss(train/val) 0.82782/0.29959. Took 0.04 sec\n",
            "Epoch 898, Loss(train/val) 0.82033/0.29953. Took 0.04 sec\n",
            "Epoch 899, Loss(train/val) 0.82310/0.29939. Took 0.05 sec\n",
            "Epoch 900, Loss(train/val) 0.83343/0.29940. Took 0.04 sec\n",
            "Epoch 901, Loss(train/val) 0.81859/0.29913. Took 0.04 sec\n",
            "Epoch 902, Loss(train/val) 0.82335/0.29910. Took 0.04 sec\n",
            "Epoch 903, Loss(train/val) 0.81511/0.29887. Took 0.05 sec\n",
            "Epoch 904, Loss(train/val) 0.82585/0.29879. Took 0.05 sec\n",
            "Epoch 905, Loss(train/val) 0.82514/0.29902. Took 0.04 sec\n",
            "Epoch 906, Loss(train/val) 0.82331/0.29893. Took 0.05 sec\n",
            "Epoch 907, Loss(train/val) 0.82792/0.29868. Took 0.04 sec\n",
            "Epoch 908, Loss(train/val) 0.82060/0.29854. Took 0.04 sec\n",
            "Epoch 909, Loss(train/val) 0.82195/0.29837. Took 0.05 sec\n",
            "Epoch 910, Loss(train/val) 0.82460/0.29836. Took 0.04 sec\n",
            "Epoch 911, Loss(train/val) 0.82394/0.29852. Took 0.04 sec\n",
            "Epoch 912, Loss(train/val) 0.82760/0.29866. Took 0.04 sec\n",
            "Epoch 913, Loss(train/val) 0.82252/0.29872. Took 0.05 sec\n",
            "Epoch 914, Loss(train/val) 0.82530/0.29840. Took 0.05 sec\n",
            "Epoch 915, Loss(train/val) 0.82285/0.29832. Took 0.04 sec\n",
            "Epoch 916, Loss(train/val) 0.82459/0.29850. Took 0.05 sec\n",
            "Epoch 917, Loss(train/val) 0.81875/0.29839. Took 0.05 sec\n",
            "Epoch 918, Loss(train/val) 0.82355/0.29859. Took 0.05 sec\n",
            "Epoch 919, Loss(train/val) 0.82805/0.29893. Took 0.05 sec\n",
            "Epoch 920, Loss(train/val) 0.81855/0.29869. Took 0.05 sec\n",
            "Epoch 921, Loss(train/val) 0.82569/0.29884. Took 0.05 sec\n",
            "Epoch 922, Loss(train/val) 0.82349/0.29880. Took 0.05 sec\n",
            "Epoch 923, Loss(train/val) 0.82209/0.29848. Took 0.04 sec\n",
            "Epoch 924, Loss(train/val) 0.82121/0.29862. Took 0.05 sec\n",
            "Epoch 925, Loss(train/val) 0.81446/0.29901. Took 0.04 sec\n",
            "Epoch 926, Loss(train/val) 0.81464/0.29908. Took 0.05 sec\n",
            "Epoch 927, Loss(train/val) 0.82235/0.29882. Took 0.05 sec\n",
            "Epoch 928, Loss(train/val) 0.82364/0.29882. Took 0.05 sec\n",
            "Epoch 929, Loss(train/val) 0.80886/0.29862. Took 0.05 sec\n",
            "Epoch 930, Loss(train/val) 0.82258/0.29840. Took 0.05 sec\n",
            "Epoch 931, Loss(train/val) 0.81353/0.29831. Took 0.05 sec\n",
            "Epoch 932, Loss(train/val) 0.81736/0.29810. Took 0.05 sec\n",
            "Epoch 933, Loss(train/val) 0.81588/0.29812. Took 0.05 sec\n",
            "Epoch 934, Loss(train/val) 0.82017/0.29793. Took 0.05 sec\n",
            "Epoch 935, Loss(train/val) 0.81508/0.29763. Took 0.04 sec\n",
            "Epoch 936, Loss(train/val) 0.81832/0.29751. Took 0.05 sec\n",
            "Epoch 937, Loss(train/val) 0.80999/0.29750. Took 0.05 sec\n",
            "Epoch 938, Loss(train/val) 0.81766/0.29755. Took 0.05 sec\n",
            "Epoch 939, Loss(train/val) 0.82507/0.29734. Took 0.05 sec\n",
            "Epoch 940, Loss(train/val) 0.81619/0.29740. Took 0.05 sec\n",
            "Epoch 941, Loss(train/val) 0.81267/0.29721. Took 0.05 sec\n",
            "Epoch 942, Loss(train/val) 0.81891/0.29707. Took 0.05 sec\n",
            "Epoch 943, Loss(train/val) 0.81790/0.29735. Took 0.05 sec\n",
            "Epoch 944, Loss(train/val) 0.82054/0.29731. Took 0.05 sec\n",
            "Epoch 945, Loss(train/val) 0.81640/0.29703. Took 0.04 sec\n",
            "Epoch 946, Loss(train/val) 0.81586/0.29689. Took 0.04 sec\n",
            "Epoch 947, Loss(train/val) 0.81802/0.29701. Took 0.05 sec\n",
            "Epoch 948, Loss(train/val) 0.81543/0.29699. Took 0.05 sec\n",
            "Epoch 949, Loss(train/val) 0.82124/0.29714. Took 0.05 sec\n",
            "Epoch 950, Loss(train/val) 0.81612/0.29723. Took 0.04 sec\n",
            "Epoch 951, Loss(train/val) 0.81144/0.29724. Took 0.04 sec\n",
            "Epoch 952, Loss(train/val) 0.81681/0.29707. Took 0.05 sec\n",
            "Epoch 953, Loss(train/val) 0.81347/0.29701. Took 0.05 sec\n",
            "Epoch 954, Loss(train/val) 0.81046/0.29702. Took 0.05 sec\n",
            "Epoch 955, Loss(train/val) 0.81593/0.29728. Took 0.05 sec\n",
            "Epoch 956, Loss(train/val) 0.81050/0.29685. Took 0.05 sec\n",
            "Epoch 957, Loss(train/val) 0.81369/0.29656. Took 0.04 sec\n",
            "Epoch 958, Loss(train/val) 0.81173/0.29638. Took 0.05 sec\n",
            "Epoch 959, Loss(train/val) 0.81561/0.29628. Took 0.06 sec\n",
            "Epoch 960, Loss(train/val) 0.81628/0.29639. Took 0.05 sec\n",
            "Epoch 961, Loss(train/val) 0.81206/0.29628. Took 0.04 sec\n",
            "Epoch 962, Loss(train/val) 0.81551/0.29579. Took 0.05 sec\n",
            "Epoch 963, Loss(train/val) 0.82153/0.29593. Took 0.05 sec\n",
            "Epoch 964, Loss(train/val) 0.81231/0.29585. Took 0.05 sec\n",
            "Epoch 965, Loss(train/val) 0.81206/0.29581. Took 0.05 sec\n",
            "Epoch 966, Loss(train/val) 0.81554/0.29598. Took 0.05 sec\n",
            "Epoch 967, Loss(train/val) 0.81066/0.29588. Took 0.04 sec\n",
            "Epoch 968, Loss(train/val) 0.80832/0.29592. Took 0.04 sec\n",
            "Epoch 969, Loss(train/val) 0.81695/0.29613. Took 0.05 sec\n",
            "Epoch 970, Loss(train/val) 0.81326/0.29615. Took 0.05 sec\n",
            "Epoch 971, Loss(train/val) 0.81881/0.29561. Took 0.05 sec\n",
            "Epoch 972, Loss(train/val) 0.81341/0.29549. Took 0.05 sec\n",
            "Epoch 973, Loss(train/val) 0.81510/0.29571. Took 0.04 sec\n",
            "Epoch 974, Loss(train/val) 0.81085/0.29570. Took 0.05 sec\n",
            "Epoch 975, Loss(train/val) 0.81468/0.29584. Took 0.04 sec\n",
            "Epoch 976, Loss(train/val) 0.81073/0.29555. Took 0.04 sec\n",
            "Epoch 977, Loss(train/val) 0.81427/0.29550. Took 0.04 sec\n",
            "Epoch 978, Loss(train/val) 0.80813/0.29527. Took 0.04 sec\n",
            "Epoch 979, Loss(train/val) 0.81293/0.29529. Took 0.06 sec\n",
            "Epoch 980, Loss(train/val) 0.81666/0.29515. Took 0.04 sec\n",
            "Epoch 981, Loss(train/val) 0.81635/0.29539. Took 0.05 sec\n",
            "Epoch 982, Loss(train/val) 0.81252/0.29558. Took 0.04 sec\n",
            "Epoch 983, Loss(train/val) 0.81796/0.29539. Took 0.05 sec\n",
            "Epoch 984, Loss(train/val) 0.80554/0.29517. Took 0.05 sec\n",
            "Epoch 985, Loss(train/val) 0.81295/0.29538. Took 0.05 sec\n",
            "Epoch 986, Loss(train/val) 0.81298/0.29541. Took 0.05 sec\n",
            "Epoch 987, Loss(train/val) 0.81150/0.29559. Took 0.05 sec\n",
            "Epoch 988, Loss(train/val) 0.80882/0.29537. Took 0.04 sec\n",
            "Epoch 989, Loss(train/val) 0.80706/0.29522. Took 0.05 sec\n",
            "Epoch 990, Loss(train/val) 0.80844/0.29505. Took 0.05 sec\n",
            "Epoch 991, Loss(train/val) 0.80654/0.29509. Took 0.04 sec\n",
            "Epoch 992, Loss(train/val) 0.80110/0.29523. Took 0.05 sec\n",
            "Epoch 993, Loss(train/val) 0.80903/0.29510. Took 0.04 sec\n",
            "Epoch 994, Loss(train/val) 0.80937/0.29473. Took 0.05 sec\n",
            "Epoch 995, Loss(train/val) 0.80772/0.29462. Took 0.05 sec\n",
            "Epoch 996, Loss(train/val) 0.80482/0.29461. Took 0.04 sec\n",
            "Epoch 997, Loss(train/val) 0.80513/0.29459. Took 0.04 sec\n",
            "Epoch 998, Loss(train/val) 0.81590/0.29434. Took 0.04 sec\n",
            "Epoch 999, Loss(train/val) 0.80794/0.29403. Took 0.24 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=1000, exp_name='exp3_lr_deep', hid_dim=16, input_dim=1, l2=1e-05, lr=4e-05, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.07647/0.37372. Took 0.22 sec\n",
            "Epoch 1, Loss(train/val) 1.08054/0.37177. Took 0.17 sec\n",
            "Epoch 2, Loss(train/val) 1.07363/0.37015. Took 0.16 sec\n",
            "Epoch 3, Loss(train/val) 1.07588/0.36883. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.07297/0.36773. Took 0.04 sec\n",
            "Epoch 5, Loss(train/val) 1.07034/0.36683. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.07026/0.36608. Took 0.05 sec\n",
            "Epoch 7, Loss(train/val) 1.06726/0.36545. Took 0.05 sec\n",
            "Epoch 8, Loss(train/val) 1.07158/0.36492. Took 0.05 sec\n",
            "Epoch 9, Loss(train/val) 1.06786/0.36448. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.07003/0.36412. Took 0.06 sec\n",
            "Epoch 11, Loss(train/val) 1.07338/0.36382. Took 0.05 sec\n",
            "Epoch 12, Loss(train/val) 1.06956/0.36356. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 1.07220/0.36336. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.07433/0.36320. Took 0.04 sec\n",
            "Epoch 15, Loss(train/val) 1.07682/0.36307. Took 0.04 sec\n",
            "Epoch 16, Loss(train/val) 1.07328/0.36298. Took 0.04 sec\n",
            "Epoch 17, Loss(train/val) 1.06830/0.36292. Took 0.05 sec\n",
            "Epoch 18, Loss(train/val) 1.07347/0.36290. Took 0.05 sec\n",
            "Epoch 19, Loss(train/val) 1.07370/0.36292. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 1.07901/0.36302. Took 0.04 sec\n",
            "Epoch 21, Loss(train/val) 1.06514/0.36320. Took 0.04 sec\n",
            "Epoch 22, Loss(train/val) 1.07625/0.36344. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 1.06557/0.36373. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 1.05667/0.36410. Took 0.04 sec\n",
            "Epoch 25, Loss(train/val) 1.07370/0.36456. Took 0.05 sec\n",
            "Epoch 26, Loss(train/val) 1.07480/0.36512. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.07173/0.36581. Took 0.05 sec\n",
            "Epoch 28, Loss(train/val) 1.06718/0.36666. Took 0.05 sec\n",
            "Epoch 29, Loss(train/val) 1.06287/0.36767. Took 0.04 sec\n",
            "Epoch 30, Loss(train/val) 1.06441/0.36884. Took 0.04 sec\n",
            "Epoch 31, Loss(train/val) 1.07511/0.37022. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 1.07463/0.37082. Took 0.07 sec\n",
            "Epoch 33, Loss(train/val) 1.07624/0.37130. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 1.06872/0.37180. Took 0.04 sec\n",
            "Epoch 35, Loss(train/val) 1.07209/0.37231. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 1.06667/0.37283. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.07314/0.37333. Took 0.05 sec\n",
            "Epoch 38, Loss(train/val) 1.06741/0.37382. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 1.07251/0.37428. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 1.07618/0.37471. Took 0.05 sec\n",
            "Epoch 41, Loss(train/val) 1.06144/0.37508. Took 0.04 sec\n",
            "Epoch 42, Loss(train/val) 1.06414/0.37542. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 1.07217/0.37574. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 1.07414/0.37606. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 1.07181/0.37633. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 1.07389/0.37662. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 1.06767/0.37691. Took 0.05 sec\n",
            "Epoch 48, Loss(train/val) 1.07397/0.37720. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 1.06847/0.37752. Took 0.04 sec\n",
            "Epoch 50, Loss(train/val) 1.06689/0.37787. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 1.07381/0.37822. Took 0.04 sec\n",
            "Epoch 52, Loss(train/val) 1.07598/0.37859. Took 0.05 sec\n",
            "Epoch 53, Loss(train/val) 1.07515/0.37899. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 1.07392/0.37929. Took 0.06 sec\n",
            "Epoch 55, Loss(train/val) 1.06567/0.37960. Took 0.04 sec\n",
            "Epoch 56, Loss(train/val) 1.07130/0.37994. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 1.06401/0.38033. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 1.07504/0.38072. Took 0.05 sec\n",
            "Epoch 59, Loss(train/val) 1.06599/0.38115. Took 0.05 sec\n",
            "Epoch 60, Loss(train/val) 1.06364/0.38161. Took 0.05 sec\n",
            "Epoch 61, Loss(train/val) 1.07044/0.38205. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 1.07724/0.38257. Took 0.05 sec\n",
            "Epoch 63, Loss(train/val) 1.07432/0.38313. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 1.06945/0.38369. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 1.05822/0.38423. Took 0.04 sec\n",
            "Epoch 66, Loss(train/val) 1.06963/0.38481. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 1.06849/0.38548. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 1.06723/0.38614. Took 0.04 sec\n",
            "Epoch 69, Loss(train/val) 1.07191/0.38679. Took 0.05 sec\n",
            "Epoch 70, Loss(train/val) 1.06640/0.38745. Took 0.04 sec\n",
            "Epoch 71, Loss(train/val) 1.06624/0.38820. Took 0.05 sec\n",
            "Epoch 72, Loss(train/val) 1.07012/0.38898. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 1.06548/0.38984. Took 0.05 sec\n",
            "Epoch 74, Loss(train/val) 1.07438/0.39065. Took 0.04 sec\n",
            "Epoch 75, Loss(train/val) 1.07383/0.39153. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 1.06353/0.39246. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 1.06861/0.39336. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 1.06832/0.39433. Took 0.04 sec\n",
            "Epoch 79, Loss(train/val) 1.07163/0.39528. Took 0.04 sec\n",
            "Epoch 80, Loss(train/val) 1.06264/0.39626. Took 0.05 sec\n",
            "Epoch 81, Loss(train/val) 1.05674/0.39734. Took 0.04 sec\n",
            "Epoch 82, Loss(train/val) 1.06171/0.39833. Took 0.05 sec\n",
            "Epoch 83, Loss(train/val) 1.06892/0.39943. Took 0.05 sec\n",
            "Epoch 84, Loss(train/val) 1.05645/0.40044. Took 0.04 sec\n",
            "Epoch 85, Loss(train/val) 1.06944/0.40151. Took 0.04 sec\n",
            "Epoch 86, Loss(train/val) 1.06878/0.40265. Took 0.04 sec\n",
            "Epoch 87, Loss(train/val) 1.06830/0.40375. Took 0.06 sec\n",
            "Epoch 88, Loss(train/val) 1.06154/0.40481. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 1.07422/0.40602. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 1.06316/0.40728. Took 0.04 sec\n",
            "Epoch 91, Loss(train/val) 1.05615/0.40860. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 1.06613/0.40996. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 1.07128/0.41131. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 1.05753/0.41284. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 1.06824/0.41431. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 1.04862/0.41582. Took 0.05 sec\n",
            "Epoch 97, Loss(train/val) 1.06466/0.41743. Took 0.06 sec\n",
            "Epoch 98, Loss(train/val) 1.06452/0.41938. Took 0.05 sec\n",
            "Epoch 99, Loss(train/val) 1.06465/0.42114. Took 0.05 sec\n",
            "Epoch 100, Loss(train/val) 1.05675/0.42302. Took 0.05 sec\n",
            "Epoch 101, Loss(train/val) 1.06250/0.42487. Took 0.04 sec\n",
            "Epoch 102, Loss(train/val) 1.06038/0.42643. Took 0.05 sec\n",
            "Epoch 103, Loss(train/val) 1.06465/0.42793. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 1.06178/0.42966. Took 0.04 sec\n",
            "Epoch 105, Loss(train/val) 1.06142/0.43148. Took 0.04 sec\n",
            "Epoch 106, Loss(train/val) 1.06292/0.43358. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 1.06605/0.43595. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 1.05742/0.43840. Took 0.04 sec\n",
            "Epoch 109, Loss(train/val) 1.05443/0.43986. Took 0.04 sec\n",
            "Epoch 110, Loss(train/val) 1.05442/0.44068. Took 0.05 sec\n",
            "Epoch 111, Loss(train/val) 1.06566/0.44160. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 1.05031/0.44236. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 1.05806/0.44261. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 1.05895/0.44172. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 1.05845/0.44102. Took 0.05 sec\n",
            "Epoch 116, Loss(train/val) 1.06205/0.44056. Took 0.05 sec\n",
            "Epoch 117, Loss(train/val) 1.06058/0.44039. Took 0.05 sec\n",
            "Epoch 118, Loss(train/val) 1.06187/0.44034. Took 0.06 sec\n",
            "Epoch 119, Loss(train/val) 1.05477/0.44037. Took 0.04 sec\n",
            "Epoch 120, Loss(train/val) 1.06008/0.44051. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 1.05511/0.44078. Took 0.04 sec\n",
            "Epoch 122, Loss(train/val) 1.05424/0.44114. Took 0.05 sec\n",
            "Epoch 123, Loss(train/val) 1.05498/0.44146. Took 0.04 sec\n",
            "Epoch 124, Loss(train/val) 1.05274/0.44186. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 1.05452/0.44247. Took 0.04 sec\n",
            "Epoch 126, Loss(train/val) 1.05367/0.44342. Took 0.04 sec\n",
            "Epoch 127, Loss(train/val) 1.05013/0.44471. Took 0.05 sec\n",
            "Epoch 128, Loss(train/val) 1.05250/0.44614. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 1.05224/0.44744. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 1.04370/0.44863. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 1.04156/0.44956. Took 0.04 sec\n",
            "Epoch 132, Loss(train/val) 1.04662/0.45019. Took 0.06 sec\n",
            "Epoch 133, Loss(train/val) 1.03856/0.45058. Took 0.04 sec\n",
            "Epoch 134, Loss(train/val) 1.04844/0.44964. Took 0.04 sec\n",
            "Epoch 135, Loss(train/val) 1.04357/0.44827. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 1.04405/0.44677. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 1.04188/0.44521. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 1.02971/0.44351. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 1.03269/0.44173. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 1.03724/0.43985. Took 0.05 sec\n",
            "Epoch 141, Loss(train/val) 1.03695/0.43796. Took 0.04 sec\n",
            "Epoch 142, Loss(train/val) 1.02916/0.43597. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 1.02944/0.43377. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 1.03576/0.43157. Took 0.04 sec\n",
            "Epoch 145, Loss(train/val) 1.02803/0.42971. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 1.01996/0.42798. Took 0.04 sec\n",
            "Epoch 147, Loss(train/val) 1.01805/0.42637. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 1.02013/0.42473. Took 0.04 sec\n",
            "Epoch 149, Loss(train/val) 1.01575/0.42353. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 1.01503/0.42216. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 1.00865/0.42087. Took 0.04 sec\n",
            "Epoch 152, Loss(train/val) 1.00926/0.41962. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 0.99982/0.41801. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 1.00792/0.41629. Took 0.05 sec\n",
            "Epoch 155, Loss(train/val) 1.00678/0.41496. Took 0.04 sec\n",
            "Epoch 156, Loss(train/val) 1.00013/0.41334. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 0.99020/0.41212. Took 0.06 sec\n",
            "Epoch 158, Loss(train/val) 0.99369/0.41159. Took 0.04 sec\n",
            "Epoch 159, Loss(train/val) 0.99876/0.41113. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 0.98342/0.41075. Took 0.05 sec\n",
            "Epoch 161, Loss(train/val) 0.98862/0.41054. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 0.98195/0.40985. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 0.97974/0.40919. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 0.97547/0.40831. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 0.96573/0.40656. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.96930/0.40372. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 0.96886/0.40079. Took 0.06 sec\n",
            "Epoch 168, Loss(train/val) 0.96698/0.39636. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 0.94939/0.39216. Took 0.05 sec\n",
            "Epoch 170, Loss(train/val) 0.95873/0.38697. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 0.96271/0.38231. Took 0.05 sec\n",
            "Epoch 172, Loss(train/val) 0.95102/0.37821. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 0.94610/0.37405. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 0.94695/0.37088. Took 0.05 sec\n",
            "Epoch 175, Loss(train/val) 0.94102/0.36646. Took 0.05 sec\n",
            "Epoch 176, Loss(train/val) 0.94836/0.36294. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 0.93610/0.36009. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 0.93103/0.35779. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 0.93524/0.35541. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 0.93291/0.35183. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 0.93280/0.34889. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 0.91828/0.34580. Took 0.06 sec\n",
            "Epoch 183, Loss(train/val) 0.92335/0.34332. Took 0.05 sec\n",
            "Epoch 184, Loss(train/val) 0.92214/0.34184. Took 0.05 sec\n",
            "Epoch 185, Loss(train/val) 0.91606/0.34059. Took 0.05 sec\n",
            "Epoch 186, Loss(train/val) 0.91529/0.33751. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 0.91294/0.33605. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 0.91374/0.33441. Took 0.05 sec\n",
            "Epoch 189, Loss(train/val) 0.90471/0.33347. Took 0.04 sec\n",
            "Epoch 190, Loss(train/val) 0.90611/0.33243. Took 0.04 sec\n",
            "Epoch 191, Loss(train/val) 0.89665/0.33078. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 0.90259/0.32822. Took 0.05 sec\n",
            "Epoch 193, Loss(train/val) 0.89699/0.32644. Took 0.05 sec\n",
            "Epoch 194, Loss(train/val) 0.90206/0.32547. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 0.89246/0.32463. Took 0.05 sec\n",
            "Epoch 196, Loss(train/val) 0.87748/0.32331. Took 0.05 sec\n",
            "Epoch 197, Loss(train/val) 0.89824/0.32358. Took 0.05 sec\n",
            "Epoch 198, Loss(train/val) 0.89790/0.32284. Took 0.05 sec\n",
            "Epoch 199, Loss(train/val) 0.89279/0.32303. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 0.88534/0.32364. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 0.88637/0.32292. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 0.88503/0.32361. Took 0.05 sec\n",
            "Epoch 203, Loss(train/val) 0.88574/0.32336. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 0.88157/0.32392. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.88338/0.32279. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 0.87768/0.32166. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 0.85977/0.32136. Took 0.05 sec\n",
            "Epoch 208, Loss(train/val) 0.86992/0.32060. Took 0.05 sec\n",
            "Epoch 209, Loss(train/val) 0.86693/0.31944. Took 0.04 sec\n",
            "Epoch 210, Loss(train/val) 0.86988/0.31797. Took 0.05 sec\n",
            "Epoch 211, Loss(train/val) 0.86486/0.31784. Took 0.04 sec\n",
            "Epoch 212, Loss(train/val) 0.86445/0.31835. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 0.86908/0.31724. Took 0.04 sec\n",
            "Epoch 214, Loss(train/val) 0.85774/0.31721. Took 0.04 sec\n",
            "Epoch 215, Loss(train/val) 0.86033/0.31569. Took 0.04 sec\n",
            "Epoch 216, Loss(train/val) 0.86138/0.31519. Took 0.04 sec\n",
            "Epoch 217, Loss(train/val) 0.85800/0.31546. Took 0.05 sec\n",
            "Epoch 218, Loss(train/val) 0.84936/0.31479. Took 0.05 sec\n",
            "Epoch 219, Loss(train/val) 0.86224/0.31401. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 0.84979/0.31317. Took 0.04 sec\n",
            "Epoch 221, Loss(train/val) 0.84959/0.31219. Took 0.04 sec\n",
            "Epoch 222, Loss(train/val) 0.85203/0.31154. Took 0.06 sec\n",
            "Epoch 223, Loss(train/val) 0.84969/0.31131. Took 0.04 sec\n",
            "Epoch 224, Loss(train/val) 0.84760/0.31231. Took 0.04 sec\n",
            "Epoch 225, Loss(train/val) 0.84478/0.31185. Took 0.05 sec\n",
            "Epoch 226, Loss(train/val) 0.84524/0.31088. Took 0.04 sec\n",
            "Epoch 227, Loss(train/val) 0.83940/0.30998. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 0.84365/0.30821. Took 0.05 sec\n",
            "Epoch 229, Loss(train/val) 0.84266/0.30786. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 0.82983/0.30741. Took 0.06 sec\n",
            "Epoch 231, Loss(train/val) 0.84117/0.30646. Took 0.04 sec\n",
            "Epoch 232, Loss(train/val) 0.83571/0.30638. Took 0.05 sec\n",
            "Epoch 233, Loss(train/val) 0.83927/0.30588. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 0.83164/0.30555. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 0.82636/0.30523. Took 0.04 sec\n",
            "Epoch 236, Loss(train/val) 0.82912/0.30542. Took 0.05 sec\n",
            "Epoch 237, Loss(train/val) 0.83339/0.30533. Took 0.05 sec\n",
            "Epoch 238, Loss(train/val) 0.82868/0.30518. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 0.82825/0.30557. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 0.82717/0.30467. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 0.82707/0.30469. Took 0.04 sec\n",
            "Epoch 242, Loss(train/val) 0.82810/0.30396. Took 0.05 sec\n",
            "Epoch 243, Loss(train/val) 0.81383/0.30503. Took 0.05 sec\n",
            "Epoch 244, Loss(train/val) 0.82161/0.30477. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.82602/0.30411. Took 0.05 sec\n",
            "Epoch 246, Loss(train/val) 0.81842/0.30322. Took 0.05 sec\n",
            "Epoch 247, Loss(train/val) 0.81093/0.30307. Took 0.05 sec\n",
            "Epoch 248, Loss(train/val) 0.81004/0.30255. Took 0.04 sec\n",
            "Epoch 249, Loss(train/val) 0.81273/0.30181. Took 0.04 sec\n",
            "Epoch 250, Loss(train/val) 0.82244/0.30083. Took 0.05 sec\n",
            "Epoch 251, Loss(train/val) 0.80924/0.29958. Took 0.05 sec\n",
            "Epoch 252, Loss(train/val) 0.81321/0.29858. Took 0.05 sec\n",
            "Epoch 253, Loss(train/val) 0.81347/0.29792. Took 0.05 sec\n",
            "Epoch 254, Loss(train/val) 0.81035/0.29717. Took 0.04 sec\n",
            "Epoch 255, Loss(train/val) 0.81122/0.29700. Took 0.05 sec\n",
            "Epoch 256, Loss(train/val) 0.80513/0.29632. Took 0.04 sec\n",
            "Epoch 257, Loss(train/val) 0.80804/0.29711. Took 0.05 sec\n",
            "Epoch 258, Loss(train/val) 0.80749/0.29661. Took 0.04 sec\n",
            "Epoch 259, Loss(train/val) 0.80297/0.29662. Took 0.05 sec\n",
            "Epoch 260, Loss(train/val) 0.79801/0.29712. Took 0.04 sec\n",
            "Epoch 261, Loss(train/val) 0.80122/0.29645. Took 0.04 sec\n",
            "Epoch 262, Loss(train/val) 0.79552/0.29667. Took 0.05 sec\n",
            "Epoch 263, Loss(train/val) 0.80243/0.29621. Took 0.05 sec\n",
            "Epoch 264, Loss(train/val) 0.78631/0.29555. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.79342/0.29449. Took 0.04 sec\n",
            "Epoch 266, Loss(train/val) 0.79097/0.29372. Took 0.04 sec\n",
            "Epoch 267, Loss(train/val) 0.79909/0.29284. Took 0.05 sec\n",
            "Epoch 268, Loss(train/val) 0.78788/0.29242. Took 0.05 sec\n",
            "Epoch 269, Loss(train/val) 0.78783/0.29225. Took 0.04 sec\n",
            "Epoch 270, Loss(train/val) 0.79160/0.29232. Took 0.04 sec\n",
            "Epoch 271, Loss(train/val) 0.79326/0.29170. Took 0.05 sec\n",
            "Epoch 272, Loss(train/val) 0.78855/0.29125. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 0.79077/0.29087. Took 0.04 sec\n",
            "Epoch 274, Loss(train/val) 0.78595/0.29054. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.79031/0.29005. Took 0.04 sec\n",
            "Epoch 276, Loss(train/val) 0.78434/0.28925. Took 0.04 sec\n",
            "Epoch 277, Loss(train/val) 0.78941/0.28847. Took 0.05 sec\n",
            "Epoch 278, Loss(train/val) 0.77763/0.28825. Took 0.04 sec\n",
            "Epoch 279, Loss(train/val) 0.78702/0.28807. Took 0.05 sec\n",
            "Epoch 280, Loss(train/val) 0.78216/0.28771. Took 0.04 sec\n",
            "Epoch 281, Loss(train/val) 0.77917/0.28762. Took 0.04 sec\n",
            "Epoch 282, Loss(train/val) 0.78043/0.28716. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 0.77391/0.28701. Took 0.05 sec\n",
            "Epoch 284, Loss(train/val) 0.77830/0.28699. Took 0.05 sec\n",
            "Epoch 285, Loss(train/val) 0.76806/0.28677. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.77813/0.28629. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.77294/0.28596. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.77235/0.28563. Took 0.05 sec\n",
            "Epoch 289, Loss(train/val) 0.77270/0.28542. Took 0.04 sec\n",
            "Epoch 290, Loss(train/val) 0.77215/0.28487. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.77005/0.28432. Took 0.04 sec\n",
            "Epoch 292, Loss(train/val) 0.76546/0.28400. Took 0.05 sec\n",
            "Epoch 293, Loss(train/val) 0.76705/0.28353. Took 0.05 sec\n",
            "Epoch 294, Loss(train/val) 0.76424/0.28321. Took 0.05 sec\n",
            "Epoch 295, Loss(train/val) 0.75838/0.28312. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 0.76851/0.28306. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.76003/0.28278. Took 0.05 sec\n",
            "Epoch 298, Loss(train/val) 0.76169/0.28206. Took 0.05 sec\n",
            "Epoch 299, Loss(train/val) 0.76245/0.28166. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.76297/0.28111. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.76407/0.28073. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.75914/0.28050. Took 0.06 sec\n",
            "Epoch 303, Loss(train/val) 0.75804/0.28024. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 0.75137/0.27994. Took 0.05 sec\n",
            "Epoch 305, Loss(train/val) 0.75731/0.27970. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 0.75834/0.27939. Took 0.05 sec\n",
            "Epoch 307, Loss(train/val) 0.74643/0.27921. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.74137/0.27903. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.75589/0.27864. Took 0.04 sec\n",
            "Epoch 310, Loss(train/val) 0.74837/0.27839. Took 0.05 sec\n",
            "Epoch 311, Loss(train/val) 0.75114/0.27814. Took 0.05 sec\n",
            "Epoch 312, Loss(train/val) 0.74859/0.27778. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 0.75221/0.27741. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.74177/0.27714. Took 0.05 sec\n",
            "Epoch 315, Loss(train/val) 0.74645/0.27688. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.74728/0.27656. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.74164/0.27629. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.74204/0.27597. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.72831/0.27577. Took 0.05 sec\n",
            "Epoch 320, Loss(train/val) 0.74185/0.27550. Took 0.04 sec\n",
            "Epoch 321, Loss(train/val) 0.74083/0.27520. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.74066/0.27493. Took 0.05 sec\n",
            "Epoch 323, Loss(train/val) 0.73567/0.27461. Took 0.05 sec\n",
            "Epoch 324, Loss(train/val) 0.73798/0.27446. Took 0.04 sec\n",
            "Epoch 325, Loss(train/val) 0.74130/0.27421. Took 0.04 sec\n",
            "Epoch 326, Loss(train/val) 0.73272/0.27394. Took 0.04 sec\n",
            "Epoch 327, Loss(train/val) 0.73619/0.27361. Took 0.06 sec\n",
            "Epoch 328, Loss(train/val) 0.73191/0.27343. Took 0.05 sec\n",
            "Epoch 329, Loss(train/val) 0.72677/0.27308. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.73236/0.27271. Took 0.05 sec\n",
            "Epoch 331, Loss(train/val) 0.73005/0.27252. Took 0.04 sec\n",
            "Epoch 332, Loss(train/val) 0.72847/0.27230. Took 0.06 sec\n",
            "Epoch 333, Loss(train/val) 0.72906/0.27206. Took 0.05 sec\n",
            "Epoch 334, Loss(train/val) 0.72839/0.27175. Took 0.05 sec\n",
            "Epoch 335, Loss(train/val) 0.72580/0.27136. Took 0.05 sec\n",
            "Epoch 336, Loss(train/val) 0.72719/0.27111. Took 0.04 sec\n",
            "Epoch 337, Loss(train/val) 0.72889/0.27089. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.72221/0.27052. Took 0.05 sec\n",
            "Epoch 339, Loss(train/val) 0.72279/0.27037. Took 0.05 sec\n",
            "Epoch 340, Loss(train/val) 0.72279/0.27017. Took 0.05 sec\n",
            "Epoch 341, Loss(train/val) 0.72286/0.26987. Took 0.05 sec\n",
            "Epoch 342, Loss(train/val) 0.71807/0.26958. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.71404/0.26947. Took 0.04 sec\n",
            "Epoch 344, Loss(train/val) 0.71948/0.26904. Took 0.05 sec\n",
            "Epoch 345, Loss(train/val) 0.71111/0.26895. Took 0.04 sec\n",
            "Epoch 346, Loss(train/val) 0.71366/0.26869. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 0.71446/0.26834. Took 0.05 sec\n",
            "Epoch 348, Loss(train/val) 0.71044/0.26848. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 0.71155/0.26920. Took 0.04 sec\n",
            "Epoch 350, Loss(train/val) 0.71050/0.26908. Took 0.04 sec\n",
            "Epoch 351, Loss(train/val) 0.70439/0.26901. Took 0.05 sec\n",
            "Epoch 352, Loss(train/val) 0.71607/0.26875. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 0.71437/0.26862. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 0.71023/0.26906. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 0.71015/0.26887. Took 0.05 sec\n",
            "Epoch 356, Loss(train/val) 0.70724/0.26823. Took 0.05 sec\n",
            "Epoch 357, Loss(train/val) 0.70719/0.26764. Took 0.05 sec\n",
            "Epoch 358, Loss(train/val) 0.70783/0.26711. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 0.71057/0.26686. Took 0.05 sec\n",
            "Epoch 360, Loss(train/val) 0.70166/0.26697. Took 0.05 sec\n",
            "Epoch 361, Loss(train/val) 0.70250/0.26615. Took 0.04 sec\n",
            "Epoch 362, Loss(train/val) 0.70401/0.26531. Took 0.05 sec\n",
            "Epoch 363, Loss(train/val) 0.69368/0.26480. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 0.70119/0.26445. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.70253/0.26419. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.70380/0.26405. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 0.69668/0.26399. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.69621/0.26381. Took 0.05 sec\n",
            "Epoch 369, Loss(train/val) 0.69787/0.26390. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.69491/0.26414. Took 0.04 sec\n",
            "Epoch 371, Loss(train/val) 0.69808/0.26486. Took 0.04 sec\n",
            "Epoch 372, Loss(train/val) 0.68904/0.26472. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 0.69470/0.26460. Took 0.05 sec\n",
            "Epoch 374, Loss(train/val) 0.69283/0.26352. Took 0.04 sec\n",
            "Epoch 375, Loss(train/val) 0.68652/0.26268. Took 0.05 sec\n",
            "Epoch 376, Loss(train/val) 0.68934/0.26190. Took 0.05 sec\n",
            "Epoch 377, Loss(train/val) 0.69027/0.26149. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.69534/0.26127. Took 0.04 sec\n",
            "Epoch 379, Loss(train/val) 0.68743/0.26108. Took 0.05 sec\n",
            "Epoch 380, Loss(train/val) 0.68784/0.26095. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 0.68970/0.26089. Took 0.04 sec\n",
            "Epoch 382, Loss(train/val) 0.67825/0.26094. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.68616/0.26081. Took 0.05 sec\n",
            "Epoch 384, Loss(train/val) 0.68319/0.26111. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 0.68646/0.26107. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.68894/0.26098. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.67920/0.26089. Took 0.05 sec\n",
            "Epoch 388, Loss(train/val) 0.68120/0.26096. Took 0.04 sec\n",
            "Epoch 389, Loss(train/val) 0.67402/0.26157. Took 0.04 sec\n",
            "Epoch 390, Loss(train/val) 0.67729/0.26225. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.67384/0.26027. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.67638/0.25989. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.67959/0.25880. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 0.68226/0.25871. Took 0.04 sec\n",
            "Epoch 395, Loss(train/val) 0.67676/0.25831. Took 0.04 sec\n",
            "Epoch 396, Loss(train/val) 0.67914/0.25822. Took 0.04 sec\n",
            "Epoch 397, Loss(train/val) 0.66853/0.25858. Took 0.06 sec\n",
            "Epoch 398, Loss(train/val) 0.67002/0.25900. Took 0.04 sec\n",
            "Epoch 399, Loss(train/val) 0.66834/0.25868. Took 0.04 sec\n",
            "Epoch 400, Loss(train/val) 0.66771/0.25850. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 0.67249/0.25796. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.67089/0.25757. Took 0.06 sec\n",
            "Epoch 403, Loss(train/val) 0.67500/0.25754. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 0.67335/0.25794. Took 0.05 sec\n",
            "Epoch 405, Loss(train/val) 0.66599/0.25755. Took 0.05 sec\n",
            "Epoch 406, Loss(train/val) 0.66549/0.25716. Took 0.04 sec\n",
            "Epoch 407, Loss(train/val) 0.66464/0.25720. Took 0.05 sec\n",
            "Epoch 408, Loss(train/val) 0.66801/0.25748. Took 0.05 sec\n",
            "Epoch 409, Loss(train/val) 0.66502/0.25765. Took 0.04 sec\n",
            "Epoch 410, Loss(train/val) 0.66176/0.25692. Took 0.05 sec\n",
            "Epoch 411, Loss(train/val) 0.66714/0.25621. Took 0.04 sec\n",
            "Epoch 412, Loss(train/val) 0.65981/0.25641. Took 0.05 sec\n",
            "Epoch 413, Loss(train/val) 0.65873/0.25693. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.65774/0.25790. Took 0.04 sec\n",
            "Epoch 415, Loss(train/val) 0.66311/0.25983. Took 0.04 sec\n",
            "Epoch 416, Loss(train/val) 0.65690/0.26125. Took 0.04 sec\n",
            "Epoch 417, Loss(train/val) 0.66047/0.26045. Took 0.05 sec\n",
            "Epoch 418, Loss(train/val) 0.66152/0.25886. Took 0.05 sec\n",
            "Epoch 419, Loss(train/val) 0.65548/0.25750. Took 0.04 sec\n",
            "Epoch 420, Loss(train/val) 0.65467/0.25710. Took 0.04 sec\n",
            "Epoch 421, Loss(train/val) 0.65563/0.25633. Took 0.05 sec\n",
            "Epoch 422, Loss(train/val) 0.66017/0.25708. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.65780/0.25561. Took 0.05 sec\n",
            "Epoch 424, Loss(train/val) 0.65672/0.25550. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.64821/0.25450. Took 0.04 sec\n",
            "Epoch 426, Loss(train/val) 0.65260/0.25389. Took 0.05 sec\n",
            "Epoch 427, Loss(train/val) 0.65426/0.25333. Took 0.05 sec\n",
            "Epoch 428, Loss(train/val) 0.65448/0.25305. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.65005/0.25285. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.65120/0.25264. Took 0.04 sec\n",
            "Epoch 431, Loss(train/val) 0.64758/0.25260. Took 0.05 sec\n",
            "Epoch 432, Loss(train/val) 0.64708/0.25282. Took 0.05 sec\n",
            "Epoch 433, Loss(train/val) 0.64651/0.25310. Took 0.04 sec\n",
            "Epoch 434, Loss(train/val) 0.64378/0.25290. Took 0.04 sec\n",
            "Epoch 435, Loss(train/val) 0.64706/0.25360. Took 0.04 sec\n",
            "Epoch 436, Loss(train/val) 0.64349/0.25489. Took 0.04 sec\n",
            "Epoch 437, Loss(train/val) 0.64709/0.25571. Took 0.05 sec\n",
            "Epoch 438, Loss(train/val) 0.64101/0.25426. Took 0.04 sec\n",
            "Epoch 439, Loss(train/val) 0.64527/0.25349. Took 0.04 sec\n",
            "Epoch 440, Loss(train/val) 0.64742/0.25279. Took 0.06 sec\n",
            "Epoch 441, Loss(train/val) 0.64187/0.25306. Took 0.05 sec\n",
            "Epoch 442, Loss(train/val) 0.64217/0.25267. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 0.63667/0.25255. Took 0.05 sec\n",
            "Epoch 444, Loss(train/val) 0.64128/0.25272. Took 0.05 sec\n",
            "Epoch 445, Loss(train/val) 0.64468/0.25220. Took 0.05 sec\n",
            "Epoch 446, Loss(train/val) 0.63754/0.25231. Took 0.05 sec\n",
            "Epoch 447, Loss(train/val) 0.63947/0.25360. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 0.63852/0.25436. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 0.63250/0.25355. Took 0.04 sec\n",
            "Epoch 450, Loss(train/val) 0.63662/0.25395. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 0.63003/0.25561. Took 0.05 sec\n",
            "Epoch 452, Loss(train/val) 0.63394/0.25604. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.63503/0.25670. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.62199/0.25672. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 0.63340/0.25838. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.63302/0.25636. Took 0.06 sec\n",
            "Epoch 457, Loss(train/val) 0.62882/0.25391. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.62820/0.25314. Took 0.04 sec\n",
            "Epoch 459, Loss(train/val) 0.63137/0.25001. Took 0.04 sec\n",
            "Epoch 460, Loss(train/val) 0.63526/0.24966. Took 0.04 sec\n",
            "Epoch 461, Loss(train/val) 0.62615/0.24985. Took 0.05 sec\n",
            "Epoch 462, Loss(train/val) 0.62832/0.25020. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.62967/0.25033. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.62771/0.25032. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.62648/0.25060. Took 0.05 sec\n",
            "Epoch 466, Loss(train/val) 0.62813/0.24914. Took 0.04 sec\n",
            "Epoch 467, Loss(train/val) 0.62816/0.24873. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.62890/0.24827. Took 0.04 sec\n",
            "Epoch 469, Loss(train/val) 0.62278/0.24805. Took 0.04 sec\n",
            "Epoch 470, Loss(train/val) 0.62071/0.24735. Took 0.04 sec\n",
            "Epoch 471, Loss(train/val) 0.61735/0.24726. Took 0.05 sec\n",
            "Epoch 472, Loss(train/val) 0.61883/0.24741. Took 0.05 sec\n",
            "Epoch 473, Loss(train/val) 0.62304/0.24710. Took 0.05 sec\n",
            "Epoch 474, Loss(train/val) 0.61703/0.24769. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 0.61675/0.24756. Took 0.04 sec\n",
            "Epoch 476, Loss(train/val) 0.62240/0.24831. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.61296/0.24845. Took 0.05 sec\n",
            "Epoch 478, Loss(train/val) 0.61892/0.24970. Took 0.05 sec\n",
            "Epoch 479, Loss(train/val) 0.61517/0.25129. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.61503/0.25240. Took 0.04 sec\n",
            "Epoch 481, Loss(train/val) 0.61568/0.25231. Took 0.05 sec\n",
            "Epoch 482, Loss(train/val) 0.61513/0.25143. Took 0.05 sec\n",
            "Epoch 483, Loss(train/val) 0.62192/0.25264. Took 0.05 sec\n",
            "Epoch 484, Loss(train/val) 0.60962/0.25115. Took 0.04 sec\n",
            "Epoch 485, Loss(train/val) 0.61617/0.25061. Took 0.05 sec\n",
            "Epoch 486, Loss(train/val) 0.61352/0.25219. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.60729/0.25379. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.60371/0.25349. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.60871/0.25194. Took 0.04 sec\n",
            "Epoch 490, Loss(train/val) 0.60794/0.24980. Took 0.04 sec\n",
            "Epoch 491, Loss(train/val) 0.60504/0.24941. Took 0.04 sec\n",
            "Epoch 492, Loss(train/val) 0.59975/0.24754. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.60826/0.24687. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.60975/0.24833. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.60376/0.24976. Took 0.04 sec\n",
            "Epoch 496, Loss(train/val) 0.61022/0.24835. Took 0.04 sec\n",
            "Epoch 497, Loss(train/val) 0.60086/0.24773. Took 0.05 sec\n",
            "Epoch 498, Loss(train/val) 0.60195/0.24656. Took 0.04 sec\n",
            "Epoch 499, Loss(train/val) 0.60137/0.24651. Took 0.04 sec\n",
            "Epoch 500, Loss(train/val) 0.59396/0.24826. Took 0.05 sec\n",
            "Epoch 501, Loss(train/val) 0.60075/0.24918. Took 0.04 sec\n",
            "Epoch 502, Loss(train/val) 0.60169/0.24873. Took 0.05 sec\n",
            "Epoch 503, Loss(train/val) 0.59649/0.24750. Took 0.05 sec\n",
            "Epoch 504, Loss(train/val) 0.60184/0.24802. Took 0.04 sec\n",
            "Epoch 505, Loss(train/val) 0.59541/0.24756. Took 0.06 sec\n",
            "Epoch 506, Loss(train/val) 0.59812/0.24633. Took 0.04 sec\n",
            "Epoch 507, Loss(train/val) 0.59880/0.24671. Took 0.05 sec\n",
            "Epoch 508, Loss(train/val) 0.59928/0.24610. Took 0.05 sec\n",
            "Epoch 509, Loss(train/val) 0.59219/0.24697. Took 0.04 sec\n",
            "Epoch 510, Loss(train/val) 0.59182/0.24572. Took 0.04 sec\n",
            "Epoch 511, Loss(train/val) 0.59460/0.24542. Took 0.05 sec\n",
            "Epoch 512, Loss(train/val) 0.59863/0.24668. Took 0.05 sec\n",
            "Epoch 513, Loss(train/val) 0.58930/0.24713. Took 0.04 sec\n",
            "Epoch 514, Loss(train/val) 0.59289/0.24848. Took 0.04 sec\n",
            "Epoch 515, Loss(train/val) 0.58888/0.24715. Took 0.04 sec\n",
            "Epoch 516, Loss(train/val) 0.58966/0.24606. Took 0.05 sec\n",
            "Epoch 517, Loss(train/val) 0.59003/0.24497. Took 0.05 sec\n",
            "Epoch 518, Loss(train/val) 0.59842/0.24503. Took 0.05 sec\n",
            "Epoch 519, Loss(train/val) 0.58742/0.24577. Took 0.05 sec\n",
            "Epoch 520, Loss(train/val) 0.58766/0.24626. Took 0.05 sec\n",
            "Epoch 521, Loss(train/val) 0.58793/0.24691. Took 0.05 sec\n",
            "Epoch 522, Loss(train/val) 0.58947/0.24708. Took 0.05 sec\n",
            "Epoch 523, Loss(train/val) 0.58454/0.24757. Took 0.05 sec\n",
            "Epoch 524, Loss(train/val) 0.57638/0.24670. Took 0.05 sec\n",
            "Epoch 525, Loss(train/val) 0.58138/0.24670. Took 0.05 sec\n",
            "Epoch 526, Loss(train/val) 0.58116/0.24720. Took 0.05 sec\n",
            "Epoch 527, Loss(train/val) 0.58648/0.24861. Took 0.05 sec\n",
            "Epoch 528, Loss(train/val) 0.58322/0.24908. Took 0.05 sec\n",
            "Epoch 529, Loss(train/val) 0.58931/0.24900. Took 0.05 sec\n",
            "Epoch 530, Loss(train/val) 0.58323/0.24822. Took 0.04 sec\n",
            "Epoch 531, Loss(train/val) 0.58102/0.24739. Took 0.04 sec\n",
            "Epoch 532, Loss(train/val) 0.58838/0.24606. Took 0.05 sec\n",
            "Epoch 533, Loss(train/val) 0.58435/0.24600. Took 0.05 sec\n",
            "Epoch 534, Loss(train/val) 0.57871/0.24474. Took 0.05 sec\n",
            "Epoch 535, Loss(train/val) 0.58359/0.24396. Took 0.04 sec\n",
            "Epoch 536, Loss(train/val) 0.57619/0.24313. Took 0.04 sec\n",
            "Epoch 537, Loss(train/val) 0.58146/0.24206. Took 0.05 sec\n",
            "Epoch 538, Loss(train/val) 0.58016/0.24217. Took 0.05 sec\n",
            "Epoch 539, Loss(train/val) 0.58118/0.24294. Took 0.05 sec\n",
            "Epoch 540, Loss(train/val) 0.57554/0.24314. Took 0.04 sec\n",
            "Epoch 541, Loss(train/val) 0.57782/0.24323. Took 0.05 sec\n",
            "Epoch 542, Loss(train/val) 0.57886/0.24461. Took 0.05 sec\n",
            "Epoch 543, Loss(train/val) 0.57729/0.24470. Took 0.04 sec\n",
            "Epoch 544, Loss(train/val) 0.57414/0.24490. Took 0.04 sec\n",
            "Epoch 545, Loss(train/val) 0.57844/0.24622. Took 0.05 sec\n",
            "Epoch 546, Loss(train/val) 0.57570/0.24778. Took 0.04 sec\n",
            "Epoch 547, Loss(train/val) 0.57341/0.24856. Took 0.05 sec\n",
            "Epoch 548, Loss(train/val) 0.57552/0.25025. Took 0.05 sec\n",
            "Epoch 549, Loss(train/val) 0.56743/0.25161. Took 0.05 sec\n",
            "Epoch 550, Loss(train/val) 0.56901/0.24588. Took 0.04 sec\n",
            "Epoch 551, Loss(train/val) 0.57019/0.24543. Took 0.05 sec\n",
            "Epoch 552, Loss(train/val) 0.57225/0.24435. Took 0.05 sec\n",
            "Epoch 553, Loss(train/val) 0.57252/0.24530. Took 0.05 sec\n",
            "Epoch 554, Loss(train/val) 0.57873/0.24620. Took 0.04 sec\n",
            "Epoch 555, Loss(train/val) 0.56506/0.24521. Took 0.04 sec\n",
            "Epoch 556, Loss(train/val) 0.56354/0.24776. Took 0.05 sec\n",
            "Epoch 557, Loss(train/val) 0.56472/0.24733. Took 0.06 sec\n",
            "Epoch 558, Loss(train/val) 0.56212/0.25121. Took 0.05 sec\n",
            "Epoch 559, Loss(train/val) 0.56723/0.25045. Took 0.05 sec\n",
            "Epoch 560, Loss(train/val) 0.57286/0.25347. Took 0.04 sec\n",
            "Epoch 561, Loss(train/val) 0.56241/0.25358. Took 0.04 sec\n",
            "Epoch 562, Loss(train/val) 0.56329/0.24761. Took 0.05 sec\n",
            "Epoch 563, Loss(train/val) 0.56234/0.24493. Took 0.05 sec\n",
            "Epoch 564, Loss(train/val) 0.56124/0.24480. Took 0.05 sec\n",
            "Epoch 565, Loss(train/val) 0.56092/0.24384. Took 0.05 sec\n",
            "Epoch 566, Loss(train/val) 0.55963/0.24258. Took 0.04 sec\n",
            "Epoch 567, Loss(train/val) 0.56344/0.24106. Took 0.05 sec\n",
            "Epoch 568, Loss(train/val) 0.56253/0.24065. Took 0.05 sec\n",
            "Epoch 569, Loss(train/val) 0.56324/0.24193. Took 0.05 sec\n",
            "Epoch 570, Loss(train/val) 0.55969/0.24204. Took 0.04 sec\n",
            "Epoch 571, Loss(train/val) 0.56781/0.24197. Took 0.04 sec\n",
            "Epoch 572, Loss(train/val) 0.56033/0.24262. Took 0.05 sec\n",
            "Epoch 573, Loss(train/val) 0.55398/0.24407. Took 0.04 sec\n",
            "Epoch 574, Loss(train/val) 0.56167/0.24302. Took 0.04 sec\n",
            "Epoch 575, Loss(train/val) 0.55637/0.24230. Took 0.05 sec\n",
            "Epoch 576, Loss(train/val) 0.56054/0.23982. Took 0.05 sec\n",
            "Epoch 577, Loss(train/val) 0.55711/0.23951. Took 0.05 sec\n",
            "Epoch 578, Loss(train/val) 0.55181/0.24005. Took 0.04 sec\n",
            "Epoch 579, Loss(train/val) 0.55306/0.24074. Took 0.04 sec\n",
            "Epoch 580, Loss(train/val) 0.55736/0.24199. Took 0.04 sec\n",
            "Epoch 581, Loss(train/val) 0.54745/0.24223. Took 0.04 sec\n",
            "Epoch 582, Loss(train/val) 0.55573/0.24300. Took 0.05 sec\n",
            "Epoch 583, Loss(train/val) 0.55416/0.24428. Took 0.05 sec\n",
            "Epoch 584, Loss(train/val) 0.55281/0.24454. Took 0.05 sec\n",
            "Epoch 585, Loss(train/val) 0.55033/0.24361. Took 0.04 sec\n",
            "Epoch 586, Loss(train/val) 0.54726/0.24218. Took 0.04 sec\n",
            "Epoch 587, Loss(train/val) 0.54786/0.24227. Took 0.05 sec\n",
            "Epoch 588, Loss(train/val) 0.54825/0.24356. Took 0.05 sec\n",
            "Epoch 589, Loss(train/val) 0.54441/0.24490. Took 0.04 sec\n",
            "Epoch 590, Loss(train/val) 0.54920/0.24542. Took 0.04 sec\n",
            "Epoch 591, Loss(train/val) 0.54516/0.24427. Took 0.05 sec\n",
            "Epoch 592, Loss(train/val) 0.54530/0.24372. Took 0.05 sec\n",
            "Epoch 593, Loss(train/val) 0.55252/0.24209. Took 0.05 sec\n",
            "Epoch 594, Loss(train/val) 0.54577/0.24370. Took 0.05 sec\n",
            "Epoch 595, Loss(train/val) 0.54344/0.24202. Took 0.04 sec\n",
            "Epoch 596, Loss(train/val) 0.55028/0.24231. Took 0.05 sec\n",
            "Epoch 597, Loss(train/val) 0.54450/0.24317. Took 0.05 sec\n",
            "Epoch 598, Loss(train/val) 0.55273/0.24482. Took 0.05 sec\n",
            "Epoch 599, Loss(train/val) 0.54600/0.24405. Took 0.04 sec\n",
            "Epoch 600, Loss(train/val) 0.54541/0.24231. Took 0.04 sec\n",
            "Epoch 601, Loss(train/val) 0.54786/0.24296. Took 0.05 sec\n",
            "Epoch 602, Loss(train/val) 0.54509/0.24178. Took 0.05 sec\n",
            "Epoch 603, Loss(train/val) 0.53915/0.24065. Took 0.05 sec\n",
            "Epoch 604, Loss(train/val) 0.54038/0.23944. Took 0.05 sec\n",
            "Epoch 605, Loss(train/val) 0.54240/0.23884. Took 0.04 sec\n",
            "Epoch 606, Loss(train/val) 0.54588/0.23908. Took 0.04 sec\n",
            "Epoch 607, Loss(train/val) 0.53973/0.24193. Took 0.05 sec\n",
            "Epoch 608, Loss(train/val) 0.53695/0.24182. Took 0.05 sec\n",
            "Epoch 609, Loss(train/val) 0.53823/0.24209. Took 0.05 sec\n",
            "Epoch 610, Loss(train/val) 0.53648/0.24063. Took 0.06 sec\n",
            "Epoch 611, Loss(train/val) 0.54173/0.24019. Took 0.05 sec\n",
            "Epoch 612, Loss(train/val) 0.54268/0.24187. Took 0.06 sec\n",
            "Epoch 613, Loss(train/val) 0.53696/0.23937. Took 0.05 sec\n",
            "Epoch 614, Loss(train/val) 0.54116/0.23903. Took 0.04 sec\n",
            "Epoch 615, Loss(train/val) 0.52926/0.23954. Took 0.05 sec\n",
            "Epoch 616, Loss(train/val) 0.53598/0.23805. Took 0.05 sec\n",
            "Epoch 617, Loss(train/val) 0.53671/0.23852. Took 0.05 sec\n",
            "Epoch 618, Loss(train/val) 0.54414/0.23946. Took 0.05 sec\n",
            "Epoch 619, Loss(train/val) 0.53585/0.23969. Took 0.05 sec\n",
            "Epoch 620, Loss(train/val) 0.53397/0.23767. Took 0.04 sec\n",
            "Epoch 621, Loss(train/val) 0.53508/0.23822. Took 0.05 sec\n",
            "Epoch 622, Loss(train/val) 0.53441/0.23867. Took 0.05 sec\n",
            "Epoch 623, Loss(train/val) 0.53287/0.24038. Took 0.05 sec\n",
            "Epoch 624, Loss(train/val) 0.53287/0.24111. Took 0.05 sec\n",
            "Epoch 625, Loss(train/val) 0.53144/0.24129. Took 0.05 sec\n",
            "Epoch 626, Loss(train/val) 0.53041/0.24327. Took 0.05 sec\n",
            "Epoch 627, Loss(train/val) 0.53628/0.24704. Took 0.05 sec\n",
            "Epoch 628, Loss(train/val) 0.52850/0.24473. Took 0.04 sec\n",
            "Epoch 629, Loss(train/val) 0.53185/0.24591. Took 0.05 sec\n",
            "Epoch 630, Loss(train/val) 0.53087/0.24564. Took 0.05 sec\n",
            "Epoch 631, Loss(train/val) 0.52975/0.24353. Took 0.05 sec\n",
            "Epoch 632, Loss(train/val) 0.53950/0.24396. Took 0.04 sec\n",
            "Epoch 633, Loss(train/val) 0.52736/0.24120. Took 0.04 sec\n",
            "Epoch 634, Loss(train/val) 0.52624/0.24104. Took 0.06 sec\n",
            "Epoch 635, Loss(train/val) 0.53140/0.24057. Took 0.05 sec\n",
            "Epoch 636, Loss(train/val) 0.52434/0.23881. Took 0.05 sec\n",
            "Epoch 637, Loss(train/val) 0.53110/0.23869. Took 0.04 sec\n",
            "Epoch 638, Loss(train/val) 0.52934/0.23978. Took 0.04 sec\n",
            "Epoch 639, Loss(train/val) 0.51972/0.23991. Took 0.04 sec\n",
            "Epoch 640, Loss(train/val) 0.53533/0.24015. Took 0.04 sec\n",
            "Epoch 641, Loss(train/val) 0.53857/0.24265. Took 0.05 sec\n",
            "Epoch 642, Loss(train/val) 0.52809/0.24250. Took 0.05 sec\n",
            "Epoch 643, Loss(train/val) 0.52462/0.24250. Took 0.04 sec\n",
            "Epoch 644, Loss(train/val) 0.52227/0.24084. Took 0.05 sec\n",
            "Epoch 645, Loss(train/val) 0.52384/0.23958. Took 0.05 sec\n",
            "Epoch 646, Loss(train/val) 0.51938/0.23912. Took 0.05 sec\n",
            "Epoch 647, Loss(train/val) 0.52467/0.24070. Took 0.05 sec\n",
            "Epoch 648, Loss(train/val) 0.52183/0.24085. Took 0.04 sec\n",
            "Epoch 649, Loss(train/val) 0.51844/0.24035. Took 0.04 sec\n",
            "Epoch 650, Loss(train/val) 0.52629/0.24118. Took 0.05 sec\n",
            "Epoch 651, Loss(train/val) 0.52113/0.24018. Took 0.05 sec\n",
            "Epoch 652, Loss(train/val) 0.51618/0.23910. Took 0.04 sec\n",
            "Epoch 653, Loss(train/val) 0.51096/0.23735. Took 0.05 sec\n",
            "Epoch 654, Loss(train/val) 0.51473/0.23681. Took 0.05 sec\n",
            "Epoch 655, Loss(train/val) 0.51686/0.23789. Took 0.06 sec\n",
            "Epoch 656, Loss(train/val) 0.51339/0.23723. Took 0.05 sec\n",
            "Epoch 657, Loss(train/val) 0.51930/0.23696. Took 0.05 sec\n",
            "Epoch 658, Loss(train/val) 0.51026/0.23735. Took 0.04 sec\n",
            "Epoch 659, Loss(train/val) 0.51337/0.23945. Took 0.05 sec\n",
            "Epoch 660, Loss(train/val) 0.51878/0.24216. Took 0.05 sec\n",
            "Epoch 661, Loss(train/val) 0.51818/0.24328. Took 0.05 sec\n",
            "Epoch 662, Loss(train/val) 0.51558/0.24279. Took 0.05 sec\n",
            "Epoch 663, Loss(train/val) 0.52281/0.24082. Took 0.05 sec\n",
            "Epoch 664, Loss(train/val) 0.51202/0.24146. Took 0.05 sec\n",
            "Epoch 665, Loss(train/val) 0.52129/0.24303. Took 0.05 sec\n",
            "Epoch 666, Loss(train/val) 0.51702/0.24439. Took 0.06 sec\n",
            "Epoch 667, Loss(train/val) 0.50860/0.24310. Took 0.05 sec\n",
            "Epoch 668, Loss(train/val) 0.51763/0.24072. Took 0.05 sec\n",
            "Epoch 669, Loss(train/val) 0.51403/0.23873. Took 0.05 sec\n",
            "Epoch 670, Loss(train/val) 0.51283/0.23862. Took 0.05 sec\n",
            "Epoch 671, Loss(train/val) 0.50839/0.23756. Took 0.05 sec\n",
            "Epoch 672, Loss(train/val) 0.50830/0.23804. Took 0.05 sec\n",
            "Epoch 673, Loss(train/val) 0.50978/0.23608. Took 0.05 sec\n",
            "Epoch 674, Loss(train/val) 0.50541/0.23568. Took 0.04 sec\n",
            "Epoch 675, Loss(train/val) 0.51447/0.23549. Took 0.05 sec\n",
            "Epoch 676, Loss(train/val) 0.51085/0.23588. Took 0.07 sec\n",
            "Epoch 677, Loss(train/val) 0.50390/0.23668. Took 0.05 sec\n",
            "Epoch 678, Loss(train/val) 0.50789/0.23666. Took 0.04 sec\n",
            "Epoch 679, Loss(train/val) 0.50191/0.23749. Took 0.04 sec\n",
            "Epoch 680, Loss(train/val) 0.50730/0.23596. Took 0.05 sec\n",
            "Epoch 681, Loss(train/val) 0.50396/0.23670. Took 0.05 sec\n",
            "Epoch 682, Loss(train/val) 0.49961/0.23685. Took 0.04 sec\n",
            "Epoch 683, Loss(train/val) 0.50599/0.23814. Took 0.04 sec\n",
            "Epoch 684, Loss(train/val) 0.50146/0.23801. Took 0.04 sec\n",
            "Epoch 685, Loss(train/val) 0.50124/0.23745. Took 0.05 sec\n",
            "Epoch 686, Loss(train/val) 0.50235/0.23585. Took 0.05 sec\n",
            "Epoch 687, Loss(train/val) 0.50844/0.23541. Took 0.05 sec\n",
            "Epoch 688, Loss(train/val) 0.50627/0.23537. Took 0.04 sec\n",
            "Epoch 689, Loss(train/val) 0.50389/0.23637. Took 0.05 sec\n",
            "Epoch 690, Loss(train/val) 0.49885/0.23606. Took 0.05 sec\n",
            "Epoch 691, Loss(train/val) 0.50175/0.23513. Took 0.05 sec\n",
            "Epoch 692, Loss(train/val) 0.50449/0.23630. Took 0.05 sec\n",
            "Epoch 693, Loss(train/val) 0.48968/0.23768. Took 0.05 sec\n",
            "Epoch 694, Loss(train/val) 0.50619/0.24055. Took 0.04 sec\n",
            "Epoch 695, Loss(train/val) 0.49454/0.24167. Took 0.05 sec\n",
            "Epoch 696, Loss(train/val) 0.49581/0.24252. Took 0.17 sec\n",
            "Epoch 697, Loss(train/val) 0.49957/0.24220. Took 0.18 sec\n",
            "Epoch 698, Loss(train/val) 0.49761/0.23927. Took 0.18 sec\n",
            "Epoch 699, Loss(train/val) 0.49358/0.23688. Took 0.16 sec\n",
            "Epoch 700, Loss(train/val) 0.49005/0.23599. Took 0.15 sec\n",
            "Epoch 701, Loss(train/val) 0.49385/0.23609. Took 0.05 sec\n",
            "Epoch 702, Loss(train/val) 0.49284/0.23469. Took 0.06 sec\n",
            "Epoch 703, Loss(train/val) 0.48632/0.23507. Took 0.05 sec\n",
            "Epoch 704, Loss(train/val) 0.50111/0.23434. Took 0.05 sec\n",
            "Epoch 705, Loss(train/val) 0.49605/0.23595. Took 0.06 sec\n",
            "Epoch 706, Loss(train/val) 0.48932/0.23699. Took 0.05 sec\n",
            "Epoch 707, Loss(train/val) 0.49695/0.23778. Took 0.05 sec\n",
            "Epoch 708, Loss(train/val) 0.48977/0.23925. Took 0.05 sec\n",
            "Epoch 709, Loss(train/val) 0.49458/0.24001. Took 0.05 sec\n",
            "Epoch 710, Loss(train/val) 0.48739/0.24195. Took 0.05 sec\n",
            "Epoch 711, Loss(train/val) 0.48702/0.23950. Took 0.05 sec\n",
            "Epoch 712, Loss(train/val) 0.48679/0.23876. Took 0.05 sec\n",
            "Epoch 713, Loss(train/val) 0.49374/0.23748. Took 0.05 sec\n",
            "Epoch 714, Loss(train/val) 0.49154/0.23908. Took 0.06 sec\n",
            "Epoch 715, Loss(train/val) 0.49739/0.23922. Took 0.05 sec\n",
            "Epoch 716, Loss(train/val) 0.48952/0.23786. Took 0.05 sec\n",
            "Epoch 717, Loss(train/val) 0.48884/0.23549. Took 0.06 sec\n",
            "Epoch 718, Loss(train/val) 0.49269/0.23576. Took 0.05 sec\n",
            "Epoch 719, Loss(train/val) 0.49127/0.23598. Took 0.05 sec\n",
            "Epoch 720, Loss(train/val) 0.49295/0.23614. Took 0.05 sec\n",
            "Epoch 721, Loss(train/val) 0.48750/0.23685. Took 0.05 sec\n",
            "Epoch 722, Loss(train/val) 0.48414/0.23817. Took 0.05 sec\n",
            "Epoch 723, Loss(train/val) 0.48720/0.23876. Took 0.05 sec\n",
            "Epoch 724, Loss(train/val) 0.48411/0.23978. Took 0.05 sec\n",
            "Epoch 725, Loss(train/val) 0.48197/0.23884. Took 0.05 sec\n",
            "Epoch 726, Loss(train/val) 0.47729/0.23659. Took 0.06 sec\n",
            "Epoch 727, Loss(train/val) 0.48615/0.23443. Took 0.05 sec\n",
            "Epoch 728, Loss(train/val) 0.48184/0.23446. Took 0.05 sec\n",
            "Epoch 729, Loss(train/val) 0.48328/0.23395. Took 0.04 sec\n",
            "Epoch 730, Loss(train/val) 0.47704/0.23365. Took 0.05 sec\n",
            "Epoch 731, Loss(train/val) 0.47852/0.23363. Took 0.05 sec\n",
            "Epoch 732, Loss(train/val) 0.47982/0.23361. Took 0.04 sec\n",
            "Epoch 733, Loss(train/val) 0.47606/0.23555. Took 0.04 sec\n",
            "Epoch 734, Loss(train/val) 0.48110/0.23580. Took 0.05 sec\n",
            "Epoch 735, Loss(train/val) 0.48247/0.23458. Took 0.05 sec\n",
            "Epoch 736, Loss(train/val) 0.48072/0.23740. Took 0.05 sec\n",
            "Epoch 737, Loss(train/val) 0.47608/0.23639. Took 0.05 sec\n",
            "Epoch 738, Loss(train/val) 0.47490/0.23782. Took 0.04 sec\n",
            "Epoch 739, Loss(train/val) 0.47934/0.24258. Took 0.04 sec\n",
            "Epoch 740, Loss(train/val) 0.47141/0.23956. Took 0.05 sec\n",
            "Epoch 741, Loss(train/val) 0.48115/0.23805. Took 0.06 sec\n",
            "Epoch 742, Loss(train/val) 0.47192/0.23491. Took 0.04 sec\n",
            "Epoch 743, Loss(train/val) 0.47634/0.23431. Took 0.04 sec\n",
            "Epoch 744, Loss(train/val) 0.47730/0.23311. Took 0.04 sec\n",
            "Epoch 745, Loss(train/val) 0.47806/0.23597. Took 0.05 sec\n",
            "Epoch 746, Loss(train/val) 0.47707/0.23495. Took 0.05 sec\n",
            "Epoch 747, Loss(train/val) 0.47957/0.23780. Took 0.04 sec\n",
            "Epoch 748, Loss(train/val) 0.47829/0.23839. Took 0.05 sec\n",
            "Epoch 749, Loss(train/val) 0.48630/0.23646. Took 0.04 sec\n",
            "Epoch 750, Loss(train/val) 0.47431/0.23468. Took 0.04 sec\n",
            "Epoch 751, Loss(train/val) 0.47487/0.23431. Took 0.05 sec\n",
            "Epoch 752, Loss(train/val) 0.46956/0.23413. Took 0.05 sec\n",
            "Epoch 753, Loss(train/val) 0.46914/0.23674. Took 0.06 sec\n",
            "Epoch 754, Loss(train/val) 0.46602/0.23562. Took 0.06 sec\n",
            "Epoch 755, Loss(train/val) 0.47453/0.23343. Took 0.07 sec\n",
            "Epoch 756, Loss(train/val) 0.47388/0.23193. Took 0.05 sec\n",
            "Epoch 757, Loss(train/val) 0.47784/0.23163. Took 0.05 sec\n",
            "Epoch 758, Loss(train/val) 0.46418/0.23143. Took 0.05 sec\n",
            "Epoch 759, Loss(train/val) 0.47620/0.23184. Took 0.05 sec\n",
            "Epoch 760, Loss(train/val) 0.47002/0.23288. Took 0.05 sec\n",
            "Epoch 761, Loss(train/val) 0.47589/0.23429. Took 0.05 sec\n",
            "Epoch 762, Loss(train/val) 0.47220/0.23385. Took 0.05 sec\n",
            "Epoch 763, Loss(train/val) 0.46623/0.23290. Took 0.05 sec\n",
            "Epoch 764, Loss(train/val) 0.47457/0.23323. Took 0.04 sec\n",
            "Epoch 765, Loss(train/val) 0.46766/0.23641. Took 0.05 sec\n",
            "Epoch 766, Loss(train/val) 0.46461/0.23683. Took 0.05 sec\n",
            "Epoch 767, Loss(train/val) 0.46795/0.23745. Took 0.04 sec\n",
            "Epoch 768, Loss(train/val) 0.46385/0.23791. Took 0.05 sec\n",
            "Epoch 769, Loss(train/val) 0.46405/0.23659. Took 0.04 sec\n",
            "Epoch 770, Loss(train/val) 0.46998/0.23580. Took 0.06 sec\n",
            "Epoch 771, Loss(train/val) 0.46654/0.23454. Took 0.05 sec\n",
            "Epoch 772, Loss(train/val) 0.46525/0.23462. Took 0.04 sec\n",
            "Epoch 773, Loss(train/val) 0.47382/0.23462. Took 0.05 sec\n",
            "Epoch 774, Loss(train/val) 0.46003/0.23546. Took 0.05 sec\n",
            "Epoch 775, Loss(train/val) 0.46400/0.23385. Took 0.05 sec\n",
            "Epoch 776, Loss(train/val) 0.45704/0.23437. Took 0.04 sec\n",
            "Epoch 777, Loss(train/val) 0.46300/0.23722. Took 0.05 sec\n",
            "Epoch 778, Loss(train/val) 0.45728/0.23769. Took 0.05 sec\n",
            "Epoch 779, Loss(train/val) 0.46550/0.23707. Took 0.05 sec\n",
            "Epoch 780, Loss(train/val) 0.46782/0.23547. Took 0.05 sec\n",
            "Epoch 781, Loss(train/val) 0.45985/0.23400. Took 0.05 sec\n",
            "Epoch 782, Loss(train/val) 0.46307/0.23226. Took 0.05 sec\n",
            "Epoch 783, Loss(train/val) 0.46629/0.23197. Took 0.05 sec\n",
            "Epoch 784, Loss(train/val) 0.45623/0.23141. Took 0.04 sec\n",
            "Epoch 785, Loss(train/val) 0.44812/0.23166. Took 0.05 sec\n",
            "Epoch 786, Loss(train/val) 0.45292/0.23092. Took 0.05 sec\n",
            "Epoch 787, Loss(train/val) 0.45820/0.23068. Took 0.05 sec\n",
            "Epoch 788, Loss(train/val) 0.46035/0.23162. Took 0.05 sec\n",
            "Epoch 789, Loss(train/val) 0.46750/0.23337. Took 0.06 sec\n",
            "Epoch 790, Loss(train/val) 0.45621/0.23519. Took 0.06 sec\n",
            "Epoch 791, Loss(train/val) 0.46389/0.23441. Took 0.05 sec\n",
            "Epoch 792, Loss(train/val) 0.45006/0.23443. Took 0.05 sec\n",
            "Epoch 793, Loss(train/val) 0.44879/0.23512. Took 0.05 sec\n",
            "Epoch 794, Loss(train/val) 0.45375/0.23635. Took 0.05 sec\n",
            "Epoch 795, Loss(train/val) 0.45989/0.23479. Took 0.05 sec\n",
            "Epoch 796, Loss(train/val) 0.46002/0.23549. Took 0.06 sec\n",
            "Epoch 797, Loss(train/val) 0.45730/0.23494. Took 0.05 sec\n",
            "Epoch 798, Loss(train/val) 0.45071/0.23400. Took 0.05 sec\n",
            "Epoch 799, Loss(train/val) 0.45611/0.23076. Took 0.05 sec\n",
            "Epoch 800, Loss(train/val) 0.44906/0.23064. Took 0.05 sec\n",
            "Epoch 801, Loss(train/val) 0.45191/0.23170. Took 0.05 sec\n",
            "Epoch 802, Loss(train/val) 0.45208/0.23409. Took 0.05 sec\n",
            "Epoch 803, Loss(train/val) 0.44183/0.23421. Took 0.04 sec\n",
            "Epoch 804, Loss(train/val) 0.44824/0.23261. Took 0.05 sec\n",
            "Epoch 805, Loss(train/val) 0.45590/0.23394. Took 0.05 sec\n",
            "Epoch 806, Loss(train/val) 0.44989/0.23308. Took 0.04 sec\n",
            "Epoch 807, Loss(train/val) 0.44659/0.23233. Took 0.04 sec\n",
            "Epoch 808, Loss(train/val) 0.44634/0.23162. Took 0.04 sec\n",
            "Epoch 809, Loss(train/val) 0.44192/0.23089. Took 0.04 sec\n",
            "Epoch 810, Loss(train/val) 0.44714/0.23101. Took 0.05 sec\n",
            "Epoch 811, Loss(train/val) 0.45956/0.23072. Took 0.05 sec\n",
            "Epoch 812, Loss(train/val) 0.44927/0.22993. Took 0.04 sec\n",
            "Epoch 813, Loss(train/val) 0.44812/0.22920. Took 0.04 sec\n",
            "Epoch 814, Loss(train/val) 0.44917/0.22910. Took 0.04 sec\n",
            "Epoch 815, Loss(train/val) 0.44571/0.22944. Took 0.05 sec\n",
            "Epoch 816, Loss(train/val) 0.44811/0.22987. Took 0.04 sec\n",
            "Epoch 817, Loss(train/val) 0.44410/0.23030. Took 0.05 sec\n",
            "Epoch 818, Loss(train/val) 0.44101/0.23180. Took 0.05 sec\n",
            "Epoch 819, Loss(train/val) 0.43990/0.23329. Took 0.05 sec\n",
            "Epoch 820, Loss(train/val) 0.44634/0.23347. Took 0.05 sec\n",
            "Epoch 821, Loss(train/val) 0.46679/0.23488. Took 0.04 sec\n",
            "Epoch 822, Loss(train/val) 0.44478/0.23475. Took 0.04 sec\n",
            "Epoch 823, Loss(train/val) 0.44815/0.23521. Took 0.05 sec\n",
            "Epoch 824, Loss(train/val) 0.44685/0.23542. Took 0.05 sec\n",
            "Epoch 825, Loss(train/val) 0.44469/0.23663. Took 0.05 sec\n",
            "Epoch 826, Loss(train/val) 0.43940/0.23325. Took 0.05 sec\n",
            "Epoch 827, Loss(train/val) 0.43953/0.23092. Took 0.04 sec\n",
            "Epoch 828, Loss(train/val) 0.43906/0.22973. Took 0.04 sec\n",
            "Epoch 829, Loss(train/val) 0.44372/0.22953. Took 0.04 sec\n",
            "Epoch 830, Loss(train/val) 0.43339/0.22890. Took 0.05 sec\n",
            "Epoch 831, Loss(train/val) 0.43930/0.22994. Took 0.05 sec\n",
            "Epoch 832, Loss(train/val) 0.43673/0.22999. Took 0.05 sec\n",
            "Epoch 833, Loss(train/val) 0.43470/0.23071. Took 0.04 sec\n",
            "Epoch 834, Loss(train/val) 0.43990/0.23095. Took 0.05 sec\n",
            "Epoch 835, Loss(train/val) 0.45082/0.23052. Took 0.05 sec\n",
            "Epoch 836, Loss(train/val) 0.43460/0.23100. Took 0.04 sec\n",
            "Epoch 837, Loss(train/val) 0.43791/0.22911. Took 0.04 sec\n",
            "Epoch 838, Loss(train/val) 0.44611/0.22870. Took 0.04 sec\n",
            "Epoch 839, Loss(train/val) 0.44148/0.22830. Took 0.05 sec\n",
            "Epoch 840, Loss(train/val) 0.43272/0.22833. Took 0.05 sec\n",
            "Epoch 841, Loss(train/val) 0.43667/0.22831. Took 0.05 sec\n",
            "Epoch 842, Loss(train/val) 0.43173/0.22822. Took 0.04 sec\n",
            "Epoch 843, Loss(train/val) 0.43034/0.22870. Took 0.04 sec\n",
            "Epoch 844, Loss(train/val) 0.43437/0.23257. Took 0.04 sec\n",
            "Epoch 845, Loss(train/val) 0.43062/0.23593. Took 0.05 sec\n",
            "Epoch 846, Loss(train/val) 0.43798/0.23863. Took 0.04 sec\n",
            "Epoch 847, Loss(train/val) 0.44666/0.24021. Took 0.05 sec\n",
            "Epoch 848, Loss(train/val) 0.43483/0.23513. Took 0.04 sec\n",
            "Epoch 849, Loss(train/val) 0.43099/0.23323. Took 0.04 sec\n",
            "Epoch 850, Loss(train/val) 0.43061/0.23130. Took 0.05 sec\n",
            "Epoch 851, Loss(train/val) 0.43144/0.22945. Took 0.04 sec\n",
            "Epoch 852, Loss(train/val) 0.43576/0.22879. Took 0.04 sec\n",
            "Epoch 853, Loss(train/val) 0.43091/0.22835. Took 0.04 sec\n",
            "Epoch 854, Loss(train/val) 0.43728/0.22818. Took 0.05 sec\n",
            "Epoch 855, Loss(train/val) 0.42950/0.22895. Took 0.05 sec\n",
            "Epoch 856, Loss(train/val) 0.43005/0.23007. Took 0.05 sec\n",
            "Epoch 857, Loss(train/val) 0.42747/0.23038. Took 0.04 sec\n",
            "Epoch 858, Loss(train/val) 0.42911/0.22993. Took 0.04 sec\n",
            "Epoch 859, Loss(train/val) 0.43844/0.23025. Took 0.04 sec\n",
            "Epoch 860, Loss(train/val) 0.42740/0.23144. Took 0.05 sec\n",
            "Epoch 861, Loss(train/val) 0.42526/0.23188. Took 0.05 sec\n",
            "Epoch 862, Loss(train/val) 0.43707/0.23161. Took 0.05 sec\n",
            "Epoch 863, Loss(train/val) 0.42204/0.22846. Took 0.05 sec\n",
            "Epoch 864, Loss(train/val) 0.42783/0.22810. Took 0.05 sec\n",
            "Epoch 865, Loss(train/val) 0.42120/0.22791. Took 0.05 sec\n",
            "Epoch 866, Loss(train/val) 0.42695/0.22765. Took 0.05 sec\n",
            "Epoch 867, Loss(train/val) 0.43424/0.22788. Took 0.05 sec\n",
            "Epoch 868, Loss(train/val) 0.43047/0.22801. Took 0.05 sec\n",
            "Epoch 869, Loss(train/val) 0.42792/0.22798. Took 0.04 sec\n",
            "Epoch 870, Loss(train/val) 0.42260/0.22832. Took 0.05 sec\n",
            "Epoch 871, Loss(train/val) 0.41570/0.22984. Took 0.05 sec\n",
            "Epoch 872, Loss(train/val) 0.42040/0.23318. Took 0.04 sec\n",
            "Epoch 873, Loss(train/val) 0.42427/0.23856. Took 0.04 sec\n",
            "Epoch 874, Loss(train/val) 0.42315/0.23936. Took 0.04 sec\n",
            "Epoch 875, Loss(train/val) 0.42122/0.23528. Took 0.05 sec\n",
            "Epoch 876, Loss(train/val) 0.42489/0.23297. Took 0.05 sec\n",
            "Epoch 877, Loss(train/val) 0.41744/0.22955. Took 0.05 sec\n",
            "Epoch 878, Loss(train/val) 0.43240/0.22777. Took 0.05 sec\n",
            "Epoch 879, Loss(train/val) 0.41696/0.22752. Took 0.05 sec\n",
            "Epoch 880, Loss(train/val) 0.42040/0.22747. Took 0.05 sec\n",
            "Epoch 881, Loss(train/val) 0.42085/0.22738. Took 0.06 sec\n",
            "Epoch 882, Loss(train/val) 0.41910/0.22785. Took 0.05 sec\n",
            "Epoch 883, Loss(train/val) 0.41362/0.22917. Took 0.05 sec\n",
            "Epoch 884, Loss(train/val) 0.41506/0.22927. Took 0.04 sec\n",
            "Epoch 885, Loss(train/val) 0.41576/0.22970. Took 0.05 sec\n",
            "Epoch 886, Loss(train/val) 0.42024/0.22817. Took 0.05 sec\n",
            "Epoch 887, Loss(train/val) 0.41767/0.22744. Took 0.05 sec\n",
            "Epoch 888, Loss(train/val) 0.42664/0.22740. Took 0.04 sec\n",
            "Epoch 889, Loss(train/val) 0.41969/0.22788. Took 0.05 sec\n",
            "Epoch 890, Loss(train/val) 0.41141/0.22884. Took 0.05 sec\n",
            "Epoch 891, Loss(train/val) 0.42278/0.22874. Took 0.05 sec\n",
            "Epoch 892, Loss(train/val) 0.41767/0.23041. Took 0.05 sec\n",
            "Epoch 893, Loss(train/val) 0.41393/0.23075. Took 0.04 sec\n",
            "Epoch 894, Loss(train/val) 0.41289/0.23202. Took 0.04 sec\n",
            "Epoch 895, Loss(train/val) 0.40724/0.23230. Took 0.05 sec\n",
            "Epoch 896, Loss(train/val) 0.41780/0.22932. Took 0.05 sec\n",
            "Epoch 897, Loss(train/val) 0.41220/0.22742. Took 0.05 sec\n",
            "Epoch 898, Loss(train/val) 0.41734/0.22699. Took 0.05 sec\n",
            "Epoch 899, Loss(train/val) 0.41209/0.22713. Took 0.05 sec\n",
            "Epoch 900, Loss(train/val) 0.40324/0.22800. Took 0.05 sec\n",
            "Epoch 901, Loss(train/val) 0.42463/0.22867. Took 0.05 sec\n",
            "Epoch 902, Loss(train/val) 0.41010/0.22891. Took 0.05 sec\n",
            "Epoch 903, Loss(train/val) 0.41435/0.23114. Took 0.05 sec\n",
            "Epoch 904, Loss(train/val) 0.40621/0.23149. Took 0.05 sec\n",
            "Epoch 905, Loss(train/val) 0.41024/0.23032. Took 0.05 sec\n",
            "Epoch 906, Loss(train/val) 0.41337/0.22785. Took 0.05 sec\n",
            "Epoch 907, Loss(train/val) 0.40678/0.22691. Took 0.05 sec\n",
            "Epoch 908, Loss(train/val) 0.41251/0.22693. Took 0.05 sec\n",
            "Epoch 909, Loss(train/val) 0.41032/0.22656. Took 0.05 sec\n",
            "Epoch 910, Loss(train/val) 0.40568/0.22664. Took 0.05 sec\n",
            "Epoch 911, Loss(train/val) 0.41083/0.22727. Took 0.05 sec\n",
            "Epoch 912, Loss(train/val) 0.40700/0.22749. Took 0.05 sec\n",
            "Epoch 913, Loss(train/val) 0.40507/0.23001. Took 0.05 sec\n",
            "Epoch 914, Loss(train/val) 0.41581/0.23074. Took 0.05 sec\n",
            "Epoch 915, Loss(train/val) 0.40819/0.23105. Took 0.05 sec\n",
            "Epoch 916, Loss(train/val) 0.40355/0.23011. Took 0.05 sec\n",
            "Epoch 917, Loss(train/val) 0.41053/0.22816. Took 0.05 sec\n",
            "Epoch 918, Loss(train/val) 0.40164/0.22697. Took 0.06 sec\n",
            "Epoch 919, Loss(train/val) 0.40472/0.22750. Took 0.05 sec\n",
            "Epoch 920, Loss(train/val) 0.41335/0.22745. Took 0.05 sec\n",
            "Epoch 921, Loss(train/val) 0.41292/0.22724. Took 0.05 sec\n",
            "Epoch 922, Loss(train/val) 0.39486/0.22685. Took 0.05 sec\n",
            "Epoch 923, Loss(train/val) 0.40034/0.22738. Took 0.05 sec\n",
            "Epoch 924, Loss(train/val) 0.40158/0.22727. Took 0.05 sec\n",
            "Epoch 925, Loss(train/val) 0.40030/0.22642. Took 0.05 sec\n",
            "Epoch 926, Loss(train/val) 0.40757/0.22660. Took 0.05 sec\n",
            "Epoch 927, Loss(train/val) 0.40598/0.22623. Took 0.05 sec\n",
            "Epoch 928, Loss(train/val) 0.39925/0.22678. Took 0.04 sec\n",
            "Epoch 929, Loss(train/val) 0.41041/0.22598. Took 0.05 sec\n",
            "Epoch 930, Loss(train/val) 0.39493/0.22605. Took 0.05 sec\n",
            "Epoch 931, Loss(train/val) 0.39531/0.22603. Took 0.05 sec\n",
            "Epoch 932, Loss(train/val) 0.40321/0.22578. Took 0.05 sec\n",
            "Epoch 933, Loss(train/val) 0.39459/0.22612. Took 0.05 sec\n",
            "Epoch 934, Loss(train/val) 0.39654/0.22744. Took 0.05 sec\n",
            "Epoch 935, Loss(train/val) 0.39848/0.22892. Took 0.04 sec\n",
            "Epoch 936, Loss(train/val) 0.40328/0.23105. Took 0.05 sec\n",
            "Epoch 937, Loss(train/val) 0.39580/0.22866. Took 0.04 sec\n",
            "Epoch 938, Loss(train/val) 0.40064/0.22788. Took 0.04 sec\n",
            "Epoch 939, Loss(train/val) 0.40221/0.22928. Took 0.06 sec\n",
            "Epoch 940, Loss(train/val) 0.39806/0.22852. Took 0.04 sec\n",
            "Epoch 941, Loss(train/val) 0.39635/0.22820. Took 0.04 sec\n",
            "Epoch 942, Loss(train/val) 0.39520/0.22638. Took 0.05 sec\n",
            "Epoch 943, Loss(train/val) 0.39704/0.22587. Took 0.05 sec\n",
            "Epoch 944, Loss(train/val) 0.39585/0.22603. Took 0.05 sec\n",
            "Epoch 945, Loss(train/val) 0.38914/0.22568. Took 0.05 sec\n",
            "Epoch 946, Loss(train/val) 0.40247/0.22574. Took 0.05 sec\n",
            "Epoch 947, Loss(train/val) 0.39938/0.22569. Took 0.05 sec\n",
            "Epoch 948, Loss(train/val) 0.38341/0.22575. Took 0.04 sec\n",
            "Epoch 949, Loss(train/val) 0.39324/0.22573. Took 0.05 sec\n",
            "Epoch 950, Loss(train/val) 0.40242/0.22535. Took 0.05 sec\n",
            "Epoch 951, Loss(train/val) 0.39514/0.22527. Took 0.05 sec\n",
            "Epoch 952, Loss(train/val) 0.39533/0.22759. Took 0.04 sec\n",
            "Epoch 953, Loss(train/val) 0.40042/0.22961. Took 0.04 sec\n",
            "Epoch 954, Loss(train/val) 0.39445/0.22897. Took 0.05 sec\n",
            "Epoch 955, Loss(train/val) 0.39469/0.23090. Took 0.05 sec\n",
            "Epoch 956, Loss(train/val) 0.39059/0.22860. Took 0.05 sec\n",
            "Epoch 957, Loss(train/val) 0.40447/0.22784. Took 0.04 sec\n",
            "Epoch 958, Loss(train/val) 0.39673/0.22690. Took 0.04 sec\n",
            "Epoch 959, Loss(train/val) 0.39206/0.22616. Took 0.06 sec\n",
            "Epoch 960, Loss(train/val) 0.39389/0.22576. Took 0.05 sec\n",
            "Epoch 961, Loss(train/val) 0.38857/0.22585. Took 0.05 sec\n",
            "Epoch 962, Loss(train/val) 0.38754/0.22564. Took 0.05 sec\n",
            "Epoch 963, Loss(train/val) 0.39317/0.22601. Took 0.04 sec\n",
            "Epoch 964, Loss(train/val) 0.38972/0.22690. Took 0.05 sec\n",
            "Epoch 965, Loss(train/val) 0.37950/0.22723. Took 0.04 sec\n",
            "Epoch 966, Loss(train/val) 0.40043/0.22781. Took 0.04 sec\n",
            "Epoch 967, Loss(train/val) 0.38908/0.22613. Took 0.05 sec\n",
            "Epoch 968, Loss(train/val) 0.39475/0.22650. Took 0.05 sec\n",
            "Epoch 969, Loss(train/val) 0.39281/0.22546. Took 0.05 sec\n",
            "Epoch 970, Loss(train/val) 0.38709/0.22584. Took 0.04 sec\n",
            "Epoch 971, Loss(train/val) 0.38354/0.22666. Took 0.05 sec\n",
            "Epoch 972, Loss(train/val) 0.38800/0.22663. Took 0.05 sec\n",
            "Epoch 973, Loss(train/val) 0.38868/0.22636. Took 0.05 sec\n",
            "Epoch 974, Loss(train/val) 0.38164/0.22661. Took 0.05 sec\n",
            "Epoch 975, Loss(train/val) 0.39608/0.22603. Took 0.05 sec\n",
            "Epoch 976, Loss(train/val) 0.38943/0.22513. Took 0.04 sec\n",
            "Epoch 977, Loss(train/val) 0.37665/0.22531. Took 0.05 sec\n",
            "Epoch 978, Loss(train/val) 0.38764/0.22531. Took 0.05 sec\n",
            "Epoch 979, Loss(train/val) 0.38041/0.22522. Took 0.05 sec\n",
            "Epoch 980, Loss(train/val) 0.38610/0.22512. Took 0.05 sec\n",
            "Epoch 981, Loss(train/val) 0.38373/0.22520. Took 0.05 sec\n",
            "Epoch 982, Loss(train/val) 0.38687/0.22629. Took 0.05 sec\n",
            "Epoch 983, Loss(train/val) 0.38629/0.22729. Took 0.05 sec\n",
            "Epoch 984, Loss(train/val) 0.38217/0.22803. Took 0.05 sec\n",
            "Epoch 985, Loss(train/val) 0.38239/0.22868. Took 0.05 sec\n",
            "Epoch 986, Loss(train/val) 0.38372/0.22975. Took 0.05 sec\n",
            "Epoch 987, Loss(train/val) 0.38702/0.22672. Took 0.05 sec\n",
            "Epoch 988, Loss(train/val) 0.38694/0.22615. Took 0.05 sec\n",
            "Epoch 989, Loss(train/val) 0.38434/0.22513. Took 0.05 sec\n",
            "Epoch 990, Loss(train/val) 0.38538/0.22490. Took 0.05 sec\n",
            "Epoch 991, Loss(train/val) 0.37894/0.22548. Took 0.05 sec\n",
            "Epoch 992, Loss(train/val) 0.37954/0.22455. Took 0.05 sec\n",
            "Epoch 993, Loss(train/val) 0.38243/0.22450. Took 0.05 sec\n",
            "Epoch 994, Loss(train/val) 0.37866/0.22426. Took 0.05 sec\n",
            "Epoch 995, Loss(train/val) 0.37421/0.22506. Took 0.04 sec\n",
            "Epoch 996, Loss(train/val) 0.37430/0.22731. Took 0.04 sec\n",
            "Epoch 997, Loss(train/val) 0.37493/0.22749. Took 0.04 sec\n",
            "Epoch 998, Loss(train/val) 0.38468/0.22565. Took 0.05 sec\n",
            "Epoch 999, Loss(train/val) 0.38243/0.22422. Took 0.05 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=1000, exp_name='exp3_lr_deep', hid_dim=16, input_dim=1, l2=1e-05, lr=5e-05, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.10277/0.38676. Took 0.04 sec\n",
            "Epoch 1, Loss(train/val) 1.11126/0.38680. Took 0.04 sec\n",
            "Epoch 2, Loss(train/val) 1.10019/0.38683. Took 0.05 sec\n",
            "Epoch 3, Loss(train/val) 1.10521/0.38684. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.10833/0.38687. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.10373/0.38688. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.09980/0.38688. Took 0.05 sec\n",
            "Epoch 7, Loss(train/val) 1.10410/0.38687. Took 0.05 sec\n",
            "Epoch 8, Loss(train/val) 1.10653/0.38684. Took 0.05 sec\n",
            "Epoch 9, Loss(train/val) 1.10748/0.38680. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.09608/0.38670. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.10377/0.38660. Took 0.04 sec\n",
            "Epoch 12, Loss(train/val) 1.09726/0.38650. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 1.10783/0.38639. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.09776/0.38629. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.10733/0.38619. Took 0.05 sec\n",
            "Epoch 16, Loss(train/val) 1.09463/0.38608. Took 0.05 sec\n",
            "Epoch 17, Loss(train/val) 1.10730/0.38598. Took 0.04 sec\n",
            "Epoch 18, Loss(train/val) 1.09801/0.38586. Took 0.04 sec\n",
            "Epoch 19, Loss(train/val) 1.09365/0.38575. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 1.10306/0.38563. Took 0.05 sec\n",
            "Epoch 21, Loss(train/val) 1.10552/0.38550. Took 0.04 sec\n",
            "Epoch 22, Loss(train/val) 1.09754/0.38537. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 1.08985/0.38523. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 1.10249/0.38508. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 1.10002/0.38492. Took 0.06 sec\n",
            "Epoch 26, Loss(train/val) 1.10227/0.38476. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.10590/0.38459. Took 0.05 sec\n",
            "Epoch 28, Loss(train/val) 1.09866/0.38441. Took 0.04 sec\n",
            "Epoch 29, Loss(train/val) 1.10624/0.38423. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 1.09624/0.38404. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 1.09656/0.38385. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 1.10690/0.38366. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 1.09826/0.38348. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 1.10580/0.38332. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 1.10050/0.38318. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 1.10758/0.38309. Took 0.04 sec\n",
            "Epoch 37, Loss(train/val) 1.09358/0.38302. Took 0.04 sec\n",
            "Epoch 38, Loss(train/val) 1.10603/0.38300. Took 0.04 sec\n",
            "Epoch 39, Loss(train/val) 1.09741/0.38303. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 1.10513/0.38312. Took 0.04 sec\n",
            "Epoch 41, Loss(train/val) 1.09891/0.38324. Took 0.04 sec\n",
            "Epoch 42, Loss(train/val) 1.09688/0.38340. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 1.10878/0.38362. Took 0.05 sec\n",
            "Epoch 44, Loss(train/val) 1.10496/0.38386. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 1.10277/0.38414. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 1.09733/0.38445. Took 0.06 sec\n",
            "Epoch 47, Loss(train/val) 1.10220/0.38479. Took 0.04 sec\n",
            "Epoch 48, Loss(train/val) 1.10315/0.38517. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 1.08912/0.38559. Took 0.05 sec\n",
            "Epoch 50, Loss(train/val) 1.10055/0.38556. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 1.10187/0.38546. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 1.10148/0.38536. Took 0.05 sec\n",
            "Epoch 53, Loss(train/val) 1.10311/0.38526. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 1.10161/0.38516. Took 0.06 sec\n",
            "Epoch 55, Loss(train/val) 1.09864/0.38507. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 1.10177/0.38499. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 1.09595/0.38490. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 1.10033/0.38484. Took 0.05 sec\n",
            "Epoch 59, Loss(train/val) 1.09531/0.38477. Took 0.05 sec\n",
            "Epoch 60, Loss(train/val) 1.09783/0.38471. Took 0.05 sec\n",
            "Epoch 61, Loss(train/val) 1.08759/0.38466. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 1.09643/0.38462. Took 0.04 sec\n",
            "Epoch 63, Loss(train/val) 1.09027/0.38459. Took 0.04 sec\n",
            "Epoch 64, Loss(train/val) 1.09567/0.38458. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 1.08870/0.38458. Took 0.04 sec\n",
            "Epoch 66, Loss(train/val) 1.10152/0.38460. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 1.09257/0.38464. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 1.10186/0.38471. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 1.09379/0.38479. Took 0.06 sec\n",
            "Epoch 70, Loss(train/val) 1.09684/0.38492. Took 0.05 sec\n",
            "Epoch 71, Loss(train/val) 1.09211/0.38507. Took 0.05 sec\n",
            "Epoch 72, Loss(train/val) 1.09030/0.38526. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 1.09499/0.38547. Took 0.05 sec\n",
            "Epoch 74, Loss(train/val) 1.09532/0.38572. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 1.09901/0.38598. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 1.09828/0.38623. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 1.09637/0.38648. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 1.09519/0.38651. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 1.09787/0.38646. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 1.09680/0.38641. Took 0.04 sec\n",
            "Epoch 81, Loss(train/val) 1.08680/0.38635. Took 0.04 sec\n",
            "Epoch 82, Loss(train/val) 1.09636/0.38630. Took 0.04 sec\n",
            "Epoch 83, Loss(train/val) 1.08510/0.38625. Took 0.04 sec\n",
            "Epoch 84, Loss(train/val) 1.09680/0.38619. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 1.09232/0.38614. Took 0.05 sec\n",
            "Epoch 86, Loss(train/val) 1.09211/0.38609. Took 0.05 sec\n",
            "Epoch 87, Loss(train/val) 1.09576/0.38603. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 1.09952/0.38598. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 1.08969/0.38593. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 1.09424/0.38588. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 1.09394/0.38582. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 1.08905/0.38577. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 1.08997/0.38572. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 1.09421/0.38567. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 1.09146/0.38561. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 1.08572/0.38556. Took 0.05 sec\n",
            "Epoch 97, Loss(train/val) 1.08929/0.38551. Took 0.04 sec\n",
            "Epoch 98, Loss(train/val) 1.09238/0.38546. Took 0.05 sec\n",
            "Epoch 99, Loss(train/val) 1.08294/0.38541. Took 0.05 sec\n",
            "Epoch 100, Loss(train/val) 1.08347/0.38535. Took 0.05 sec\n",
            "Epoch 101, Loss(train/val) 1.08174/0.38530. Took 0.05 sec\n",
            "Epoch 102, Loss(train/val) 1.09219/0.38525. Took 0.05 sec\n",
            "Epoch 103, Loss(train/val) 1.09195/0.38520. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 1.08692/0.38515. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 1.08931/0.38510. Took 0.05 sec\n",
            "Epoch 106, Loss(train/val) 1.08658/0.38504. Took 0.04 sec\n",
            "Epoch 107, Loss(train/val) 1.08883/0.38499. Took 0.04 sec\n",
            "Epoch 108, Loss(train/val) 1.07988/0.38494. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 1.08116/0.38489. Took 0.06 sec\n",
            "Epoch 110, Loss(train/val) 1.08212/0.38477. Took 0.05 sec\n",
            "Epoch 111, Loss(train/val) 1.07381/0.38303. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 1.07837/0.38047. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 1.06998/0.37808. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 1.07508/0.37644. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 1.07526/0.37538. Took 0.04 sec\n",
            "Epoch 116, Loss(train/val) 1.07301/0.37467. Took 0.05 sec\n",
            "Epoch 117, Loss(train/val) 1.07135/0.37451. Took 0.04 sec\n",
            "Epoch 118, Loss(train/val) 1.06805/0.37467. Took 0.05 sec\n",
            "Epoch 119, Loss(train/val) 1.06071/0.37458. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 1.05990/0.37395. Took 0.04 sec\n",
            "Epoch 121, Loss(train/val) 1.06512/0.37326. Took 0.04 sec\n",
            "Epoch 122, Loss(train/val) 1.05777/0.37232. Took 0.05 sec\n",
            "Epoch 123, Loss(train/val) 1.04921/0.37074. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 1.05052/0.36824. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 1.04837/0.36598. Took 0.04 sec\n",
            "Epoch 126, Loss(train/val) 1.04187/0.36386. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 1.04380/0.36259. Took 0.05 sec\n",
            "Epoch 128, Loss(train/val) 1.02529/0.36272. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 1.02963/0.36363. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 1.02623/0.36497. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 1.02351/0.36687. Took 0.06 sec\n",
            "Epoch 132, Loss(train/val) 1.00865/0.36837. Took 0.05 sec\n",
            "Epoch 133, Loss(train/val) 1.01726/0.36926. Took 0.05 sec\n",
            "Epoch 134, Loss(train/val) 0.99956/0.36923. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 0.99844/0.36825. Took 0.05 sec\n",
            "Epoch 136, Loss(train/val) 0.99554/0.36686. Took 0.04 sec\n",
            "Epoch 137, Loss(train/val) 0.98248/0.36505. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 0.97421/0.36278. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 0.97381/0.36052. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 0.96746/0.35861. Took 0.04 sec\n",
            "Epoch 141, Loss(train/val) 0.94953/0.35685. Took 0.04 sec\n",
            "Epoch 142, Loss(train/val) 0.94909/0.35474. Took 0.04 sec\n",
            "Epoch 143, Loss(train/val) 0.94497/0.35213. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 0.94494/0.35015. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 0.93427/0.34777. Took 0.04 sec\n",
            "Epoch 146, Loss(train/val) 0.93335/0.34529. Took 0.04 sec\n",
            "Epoch 147, Loss(train/val) 0.92769/0.34238. Took 0.04 sec\n",
            "Epoch 148, Loss(train/val) 0.92714/0.33970. Took 0.04 sec\n",
            "Epoch 149, Loss(train/val) 0.92396/0.33681. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 0.90678/0.33343. Took 0.04 sec\n",
            "Epoch 151, Loss(train/val) 0.90997/0.33041. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 0.90524/0.32797. Took 0.04 sec\n",
            "Epoch 153, Loss(train/val) 0.89391/0.32533. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 0.89601/0.32225. Took 0.05 sec\n",
            "Epoch 155, Loss(train/val) 0.89064/0.31997. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 0.88751/0.31810. Took 0.04 sec\n",
            "Epoch 157, Loss(train/val) 0.88340/0.31637. Took 0.04 sec\n",
            "Epoch 158, Loss(train/val) 0.88107/0.31477. Took 0.05 sec\n",
            "Epoch 159, Loss(train/val) 0.87288/0.31286. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 0.86716/0.31089. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 0.86839/0.31007. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 0.85638/0.30882. Took 0.04 sec\n",
            "Epoch 163, Loss(train/val) 0.85500/0.30766. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 0.85856/0.30647. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 0.85516/0.30563. Took 0.04 sec\n",
            "Epoch 166, Loss(train/val) 0.85423/0.30519. Took 0.04 sec\n",
            "Epoch 167, Loss(train/val) 0.84924/0.30467. Took 0.04 sec\n",
            "Epoch 168, Loss(train/val) 0.84615/0.30438. Took 0.04 sec\n",
            "Epoch 169, Loss(train/val) 0.84482/0.30442. Took 0.05 sec\n",
            "Epoch 170, Loss(train/val) 0.83899/0.30437. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 0.83836/0.30472. Took 0.04 sec\n",
            "Epoch 172, Loss(train/val) 0.83535/0.30409. Took 0.04 sec\n",
            "Epoch 173, Loss(train/val) 0.82891/0.30363. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 0.82934/0.30312. Took 0.06 sec\n",
            "Epoch 175, Loss(train/val) 0.82397/0.30297. Took 0.05 sec\n",
            "Epoch 176, Loss(train/val) 0.83048/0.30247. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 0.82010/0.30159. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 0.81346/0.30152. Took 0.04 sec\n",
            "Epoch 179, Loss(train/val) 0.81285/0.30079. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 0.81114/0.30160. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 0.81101/0.30105. Took 0.04 sec\n",
            "Epoch 182, Loss(train/val) 0.80681/0.30123. Took 0.04 sec\n",
            "Epoch 183, Loss(train/val) 0.81296/0.30098. Took 0.05 sec\n",
            "Epoch 184, Loss(train/val) 0.80358/0.30007. Took 0.05 sec\n",
            "Epoch 185, Loss(train/val) 0.79886/0.29960. Took 0.05 sec\n",
            "Epoch 186, Loss(train/val) 0.80070/0.29965. Took 0.04 sec\n",
            "Epoch 187, Loss(train/val) 0.79449/0.29958. Took 0.04 sec\n",
            "Epoch 188, Loss(train/val) 0.79886/0.29902. Took 0.04 sec\n",
            "Epoch 189, Loss(train/val) 0.79667/0.29889. Took 0.05 sec\n",
            "Epoch 190, Loss(train/val) 0.78529/0.30031. Took 0.05 sec\n",
            "Epoch 191, Loss(train/val) 0.80000/0.30109. Took 0.04 sec\n",
            "Epoch 192, Loss(train/val) 0.78505/0.30182. Took 0.04 sec\n",
            "Epoch 193, Loss(train/val) 0.79259/0.30262. Took 0.05 sec\n",
            "Epoch 194, Loss(train/val) 0.78726/0.30196. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 0.79159/0.30080. Took 0.04 sec\n",
            "Epoch 196, Loss(train/val) 0.78667/0.30057. Took 0.05 sec\n",
            "Epoch 197, Loss(train/val) 0.78476/0.29950. Took 0.05 sec\n",
            "Epoch 198, Loss(train/val) 0.78472/0.29910. Took 0.05 sec\n",
            "Epoch 199, Loss(train/val) 0.78386/0.29948. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 0.77868/0.29962. Took 0.04 sec\n",
            "Epoch 201, Loss(train/val) 0.76969/0.29955. Took 0.04 sec\n",
            "Epoch 202, Loss(train/val) 0.77880/0.30063. Took 0.05 sec\n",
            "Epoch 203, Loss(train/val) 0.77202/0.30105. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 0.76952/0.30090. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.76837/0.30247. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 0.76261/0.30299. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 0.76758/0.30312. Took 0.05 sec\n",
            "Epoch 208, Loss(train/val) 0.77046/0.30398. Took 0.05 sec\n",
            "Epoch 209, Loss(train/val) 0.76307/0.30189. Took 0.05 sec\n",
            "Epoch 210, Loss(train/val) 0.75593/0.30218. Took 0.04 sec\n",
            "Epoch 211, Loss(train/val) 0.75632/0.30055. Took 0.05 sec\n",
            "Epoch 212, Loss(train/val) 0.75074/0.29768. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 0.75918/0.29640. Took 0.04 sec\n",
            "Epoch 214, Loss(train/val) 0.75566/0.29491. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.75837/0.29566. Took 0.05 sec\n",
            "Epoch 216, Loss(train/val) 0.75410/0.29550. Took 0.04 sec\n",
            "Epoch 217, Loss(train/val) 0.74757/0.29685. Took 0.05 sec\n",
            "Epoch 218, Loss(train/val) 0.74253/0.29752. Took 0.05 sec\n",
            "Epoch 219, Loss(train/val) 0.74638/0.29751. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 0.74327/0.29734. Took 0.04 sec\n",
            "Epoch 221, Loss(train/val) 0.74439/0.29826. Took 0.04 sec\n",
            "Epoch 222, Loss(train/val) 0.74422/0.30042. Took 0.04 sec\n",
            "Epoch 223, Loss(train/val) 0.74157/0.30160. Took 0.05 sec\n",
            "Epoch 224, Loss(train/val) 0.74309/0.30050. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 0.74144/0.30084. Took 0.04 sec\n",
            "Epoch 226, Loss(train/val) 0.73622/0.30040. Took 0.04 sec\n",
            "Epoch 227, Loss(train/val) 0.74096/0.30168. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 0.73140/0.29996. Took 0.05 sec\n",
            "Epoch 229, Loss(train/val) 0.73376/0.29807. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 0.73444/0.29711. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 0.73417/0.29809. Took 0.04 sec\n",
            "Epoch 232, Loss(train/val) 0.72870/0.29674. Took 0.05 sec\n",
            "Epoch 233, Loss(train/val) 0.72681/0.29416. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 0.72499/0.29394. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 0.72079/0.29395. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 0.72165/0.29361. Took 0.04 sec\n",
            "Epoch 237, Loss(train/val) 0.72083/0.29167. Took 0.04 sec\n",
            "Epoch 238, Loss(train/val) 0.72183/0.29041. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 0.71737/0.29052. Took 0.06 sec\n",
            "Epoch 240, Loss(train/val) 0.71946/0.29181. Took 0.04 sec\n",
            "Epoch 241, Loss(train/val) 0.71508/0.29356. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 0.71848/0.29552. Took 0.04 sec\n",
            "Epoch 243, Loss(train/val) 0.71983/0.29369. Took 0.04 sec\n",
            "Epoch 244, Loss(train/val) 0.71109/0.29362. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.71271/0.29463. Took 0.05 sec\n",
            "Epoch 246, Loss(train/val) 0.71627/0.29319. Took 0.04 sec\n",
            "Epoch 247, Loss(train/val) 0.70772/0.29491. Took 0.04 sec\n",
            "Epoch 248, Loss(train/val) 0.70632/0.29461. Took 0.05 sec\n",
            "Epoch 249, Loss(train/val) 0.70473/0.29569. Took 0.05 sec\n",
            "Epoch 250, Loss(train/val) 0.70292/0.29585. Took 0.04 sec\n",
            "Epoch 251, Loss(train/val) 0.70689/0.29457. Took 0.04 sec\n",
            "Epoch 252, Loss(train/val) 0.70424/0.29530. Took 0.04 sec\n",
            "Epoch 253, Loss(train/val) 0.70340/0.29612. Took 0.05 sec\n",
            "Epoch 254, Loss(train/val) 0.70208/0.29542. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.69310/0.29648. Took 0.05 sec\n",
            "Epoch 256, Loss(train/val) 0.69497/0.29723. Took 0.05 sec\n",
            "Epoch 257, Loss(train/val) 0.69868/0.29588. Took 0.05 sec\n",
            "Epoch 258, Loss(train/val) 0.70037/0.29693. Took 0.04 sec\n",
            "Epoch 259, Loss(train/val) 0.69767/0.29716. Took 0.05 sec\n",
            "Epoch 260, Loss(train/val) 0.68562/0.29598. Took 0.05 sec\n",
            "Epoch 261, Loss(train/val) 0.68946/0.29530. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.69568/0.29560. Took 0.04 sec\n",
            "Epoch 263, Loss(train/val) 0.69300/0.29605. Took 0.05 sec\n",
            "Epoch 264, Loss(train/val) 0.68151/0.29448. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.68839/0.29129. Took 0.05 sec\n",
            "Epoch 266, Loss(train/val) 0.68751/0.28967. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 0.68934/0.29164. Took 0.05 sec\n",
            "Epoch 268, Loss(train/val) 0.68573/0.29045. Took 0.04 sec\n",
            "Epoch 269, Loss(train/val) 0.68055/0.29167. Took 0.05 sec\n",
            "Epoch 270, Loss(train/val) 0.68028/0.29042. Took 0.05 sec\n",
            "Epoch 271, Loss(train/val) 0.68059/0.29112. Took 0.04 sec\n",
            "Epoch 272, Loss(train/val) 0.67944/0.29085. Took 0.04 sec\n",
            "Epoch 273, Loss(train/val) 0.67359/0.28818. Took 0.04 sec\n",
            "Epoch 274, Loss(train/val) 0.67731/0.28553. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.67282/0.28699. Took 0.05 sec\n",
            "Epoch 276, Loss(train/val) 0.67596/0.28745. Took 0.05 sec\n",
            "Epoch 277, Loss(train/val) 0.67737/0.28708. Took 0.04 sec\n",
            "Epoch 278, Loss(train/val) 0.66745/0.28699. Took 0.04 sec\n",
            "Epoch 279, Loss(train/val) 0.66816/0.28450. Took 0.05 sec\n",
            "Epoch 280, Loss(train/val) 0.67163/0.28559. Took 0.05 sec\n",
            "Epoch 281, Loss(train/val) 0.66700/0.28307. Took 0.05 sec\n",
            "Epoch 282, Loss(train/val) 0.66616/0.28093. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 0.66695/0.28185. Took 0.05 sec\n",
            "Epoch 284, Loss(train/val) 0.65691/0.28250. Took 0.05 sec\n",
            "Epoch 285, Loss(train/val) 0.66471/0.28199. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.66003/0.28307. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.66582/0.28473. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.65595/0.28797. Took 0.05 sec\n",
            "Epoch 289, Loss(train/val) 0.66396/0.28814. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.66107/0.28982. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.65857/0.28788. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.65878/0.28828. Took 0.04 sec\n",
            "Epoch 293, Loss(train/val) 0.65266/0.28810. Took 0.04 sec\n",
            "Epoch 294, Loss(train/val) 0.65218/0.28925. Took 0.05 sec\n",
            "Epoch 295, Loss(train/val) 0.65725/0.28866. Took 0.04 sec\n",
            "Epoch 296, Loss(train/val) 0.65056/0.28839. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.65162/0.28482. Took 0.05 sec\n",
            "Epoch 298, Loss(train/val) 0.65354/0.28657. Took 0.05 sec\n",
            "Epoch 299, Loss(train/val) 0.65021/0.28594. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.64747/0.28548. Took 0.04 sec\n",
            "Epoch 301, Loss(train/val) 0.64850/0.28478. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.64689/0.28336. Took 0.04 sec\n",
            "Epoch 303, Loss(train/val) 0.64495/0.28418. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 0.64890/0.28225. Took 0.06 sec\n",
            "Epoch 305, Loss(train/val) 0.64771/0.28408. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 0.63804/0.28334. Took 0.04 sec\n",
            "Epoch 307, Loss(train/val) 0.64411/0.28277. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.63764/0.28433. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.63917/0.28433. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.62806/0.28488. Took 0.04 sec\n",
            "Epoch 311, Loss(train/val) 0.62800/0.28614. Took 0.05 sec\n",
            "Epoch 312, Loss(train/val) 0.62912/0.28713. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 0.62645/0.28578. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.63283/0.28360. Took 0.05 sec\n",
            "Epoch 315, Loss(train/val) 0.62760/0.28403. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.63877/0.28239. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.62909/0.28055. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.62833/0.27867. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.61631/0.27827. Took 0.05 sec\n",
            "Epoch 320, Loss(train/val) 0.62729/0.27845. Took 0.04 sec\n",
            "Epoch 321, Loss(train/val) 0.62132/0.27959. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.63116/0.27930. Took 0.05 sec\n",
            "Epoch 323, Loss(train/val) 0.62200/0.27738. Took 0.05 sec\n",
            "Epoch 324, Loss(train/val) 0.62417/0.27642. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.62231/0.27690. Took 0.05 sec\n",
            "Epoch 326, Loss(train/val) 0.61841/0.27815. Took 0.04 sec\n",
            "Epoch 327, Loss(train/val) 0.61709/0.27627. Took 0.04 sec\n",
            "Epoch 328, Loss(train/val) 0.61909/0.27570. Took 0.05 sec\n",
            "Epoch 329, Loss(train/val) 0.61393/0.27773. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.61279/0.27782. Took 0.05 sec\n",
            "Epoch 331, Loss(train/val) 0.61396/0.27905. Took 0.04 sec\n",
            "Epoch 332, Loss(train/val) 0.61260/0.27807. Took 0.04 sec\n",
            "Epoch 333, Loss(train/val) 0.61838/0.28039. Took 0.06 sec\n",
            "Epoch 334, Loss(train/val) 0.61904/0.28091. Took 0.04 sec\n",
            "Epoch 335, Loss(train/val) 0.61451/0.27762. Took 0.04 sec\n",
            "Epoch 336, Loss(train/val) 0.61363/0.27435. Took 0.05 sec\n",
            "Epoch 337, Loss(train/val) 0.60995/0.27093. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.60816/0.26725. Took 0.05 sec\n",
            "Epoch 339, Loss(train/val) 0.61044/0.26652. Took 0.04 sec\n",
            "Epoch 340, Loss(train/val) 0.60894/0.26650. Took 0.04 sec\n",
            "Epoch 341, Loss(train/val) 0.61324/0.26693. Took 0.05 sec\n",
            "Epoch 342, Loss(train/val) 0.60900/0.26721. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.60359/0.26691. Took 0.05 sec\n",
            "Epoch 344, Loss(train/val) 0.60033/0.26794. Took 0.05 sec\n",
            "Epoch 345, Loss(train/val) 0.59902/0.26831. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.60579/0.27123. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 0.60104/0.27046. Took 0.04 sec\n",
            "Epoch 348, Loss(train/val) 0.59639/0.26886. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 0.59267/0.26608. Took 0.05 sec\n",
            "Epoch 350, Loss(train/val) 0.59977/0.26354. Took 0.05 sec\n",
            "Epoch 351, Loss(train/val) 0.59654/0.26148. Took 0.04 sec\n",
            "Epoch 352, Loss(train/val) 0.59603/0.25937. Took 0.04 sec\n",
            "Epoch 353, Loss(train/val) 0.59988/0.25972. Took 0.06 sec\n",
            "Epoch 354, Loss(train/val) 0.59490/0.25967. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 0.59347/0.25979. Took 0.04 sec\n",
            "Epoch 356, Loss(train/val) 0.59624/0.26042. Took 0.04 sec\n",
            "Epoch 357, Loss(train/val) 0.58468/0.26202. Took 0.04 sec\n",
            "Epoch 358, Loss(train/val) 0.59223/0.26458. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 0.59277/0.26483. Took 0.04 sec\n",
            "Epoch 360, Loss(train/val) 0.58695/0.26461. Took 0.04 sec\n",
            "Epoch 361, Loss(train/val) 0.58950/0.26396. Took 0.05 sec\n",
            "Epoch 362, Loss(train/val) 0.59061/0.26352. Took 0.04 sec\n",
            "Epoch 363, Loss(train/val) 0.58921/0.26261. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 0.58350/0.26289. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.59153/0.26305. Took 0.04 sec\n",
            "Epoch 366, Loss(train/val) 0.58343/0.26117. Took 0.04 sec\n",
            "Epoch 367, Loss(train/val) 0.57578/0.26136. Took 0.04 sec\n",
            "Epoch 368, Loss(train/val) 0.58162/0.26092. Took 0.06 sec\n",
            "Epoch 369, Loss(train/val) 0.58023/0.26130. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.57735/0.26425. Took 0.05 sec\n",
            "Epoch 371, Loss(train/val) 0.57718/0.26822. Took 0.05 sec\n",
            "Epoch 372, Loss(train/val) 0.57985/0.26813. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 0.57706/0.26573. Took 0.05 sec\n",
            "Epoch 374, Loss(train/val) 0.57167/0.26415. Took 0.05 sec\n",
            "Epoch 375, Loss(train/val) 0.58058/0.26386. Took 0.05 sec\n",
            "Epoch 376, Loss(train/val) 0.57905/0.26299. Took 0.05 sec\n",
            "Epoch 377, Loss(train/val) 0.57728/0.26416. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.57868/0.26451. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 0.56971/0.26265. Took 0.05 sec\n",
            "Epoch 380, Loss(train/val) 0.58115/0.26425. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 0.57309/0.26377. Took 0.04 sec\n",
            "Epoch 382, Loss(train/val) 0.57694/0.26466. Took 0.04 sec\n",
            "Epoch 383, Loss(train/val) 0.57602/0.26680. Took 0.05 sec\n",
            "Epoch 384, Loss(train/val) 0.57493/0.26267. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 0.57194/0.26200. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.56326/0.26130. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.56818/0.26136. Took 0.05 sec\n",
            "Epoch 388, Loss(train/val) 0.56664/0.26165. Took 0.06 sec\n",
            "Epoch 389, Loss(train/val) 0.56603/0.26072. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.56143/0.25914. Took 0.04 sec\n",
            "Epoch 391, Loss(train/val) 0.56840/0.25832. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.57640/0.25711. Took 0.04 sec\n",
            "Epoch 393, Loss(train/val) 0.55972/0.25822. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 0.56948/0.25893. Took 0.05 sec\n",
            "Epoch 395, Loss(train/val) 0.56057/0.25875. Took 0.05 sec\n",
            "Epoch 396, Loss(train/val) 0.56483/0.26058. Took 0.05 sec\n",
            "Epoch 397, Loss(train/val) 0.56248/0.26128. Took 0.05 sec\n",
            "Epoch 398, Loss(train/val) 0.56120/0.26427. Took 0.05 sec\n",
            "Epoch 399, Loss(train/val) 0.56370/0.26534. Took 0.05 sec\n",
            "Epoch 400, Loss(train/val) 0.56006/0.26662. Took 0.04 sec\n",
            "Epoch 401, Loss(train/val) 0.55331/0.26641. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.55747/0.26737. Took 0.04 sec\n",
            "Epoch 403, Loss(train/val) 0.55694/0.26727. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 0.55771/0.26881. Took 0.04 sec\n",
            "Epoch 405, Loss(train/val) 0.55868/0.27051. Took 0.04 sec\n",
            "Epoch 406, Loss(train/val) 0.55714/0.27051. Took 0.05 sec\n",
            "Epoch 407, Loss(train/val) 0.55300/0.26780. Took 0.04 sec\n",
            "Epoch 408, Loss(train/val) 0.55690/0.26341. Took 0.05 sec\n",
            "Epoch 409, Loss(train/val) 0.54546/0.26084. Took 0.04 sec\n",
            "Epoch 410, Loss(train/val) 0.56109/0.25939. Took 0.04 sec\n",
            "Epoch 411, Loss(train/val) 0.54057/0.25631. Took 0.05 sec\n",
            "Epoch 412, Loss(train/val) 0.55788/0.25550. Took 0.05 sec\n",
            "Epoch 413, Loss(train/val) 0.54542/0.25477. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.55522/0.25477. Took 0.05 sec\n",
            "Epoch 415, Loss(train/val) 0.54873/0.25599. Took 0.04 sec\n",
            "Epoch 416, Loss(train/val) 0.55046/0.25499. Took 0.05 sec\n",
            "Epoch 417, Loss(train/val) 0.55249/0.25645. Took 0.04 sec\n",
            "Epoch 418, Loss(train/val) 0.54938/0.25848. Took 0.05 sec\n",
            "Epoch 419, Loss(train/val) 0.54687/0.25988. Took 0.05 sec\n",
            "Epoch 420, Loss(train/val) 0.54819/0.26000. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.54639/0.25976. Took 0.04 sec\n",
            "Epoch 422, Loss(train/val) 0.55351/0.26127. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.55167/0.26050. Took 0.05 sec\n",
            "Epoch 424, Loss(train/val) 0.54457/0.25790. Took 0.04 sec\n",
            "Epoch 425, Loss(train/val) 0.55290/0.25592. Took 0.05 sec\n",
            "Epoch 426, Loss(train/val) 0.53958/0.25572. Took 0.04 sec\n",
            "Epoch 427, Loss(train/val) 0.54258/0.25597. Took 0.04 sec\n",
            "Epoch 428, Loss(train/val) 0.53721/0.25705. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.54172/0.25863. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.54509/0.25974. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.54276/0.25955. Took 0.05 sec\n",
            "Epoch 432, Loss(train/val) 0.53820/0.25893. Took 0.06 sec\n",
            "Epoch 433, Loss(train/val) 0.53951/0.25725. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.52828/0.25599. Took 0.05 sec\n",
            "Epoch 435, Loss(train/val) 0.53851/0.25501. Took 0.05 sec\n",
            "Epoch 436, Loss(train/val) 0.53396/0.25456. Took 0.04 sec\n",
            "Epoch 437, Loss(train/val) 0.53192/0.25441. Took 0.05 sec\n",
            "Epoch 438, Loss(train/val) 0.53537/0.25479. Took 0.05 sec\n",
            "Epoch 439, Loss(train/val) 0.53300/0.25779. Took 0.04 sec\n",
            "Epoch 440, Loss(train/val) 0.54186/0.25819. Took 0.04 sec\n",
            "Epoch 441, Loss(train/val) 0.53170/0.26049. Took 0.05 sec\n",
            "Epoch 442, Loss(train/val) 0.53210/0.26074. Took 0.06 sec\n",
            "Epoch 443, Loss(train/val) 0.53204/0.26041. Took 0.05 sec\n",
            "Epoch 444, Loss(train/val) 0.52722/0.26050. Took 0.05 sec\n",
            "Epoch 445, Loss(train/val) 0.52535/0.26083. Took 0.05 sec\n",
            "Epoch 446, Loss(train/val) 0.53241/0.25857. Took 0.05 sec\n",
            "Epoch 447, Loss(train/val) 0.53300/0.25889. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 0.53050/0.25724. Took 0.05 sec\n",
            "Epoch 449, Loss(train/val) 0.52273/0.25667. Took 0.05 sec\n",
            "Epoch 450, Loss(train/val) 0.52388/0.25634. Took 0.04 sec\n",
            "Epoch 451, Loss(train/val) 0.52734/0.25612. Took 0.05 sec\n",
            "Epoch 452, Loss(train/val) 0.52598/0.25542. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.52709/0.25493. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.52291/0.25638. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 0.52509/0.25644. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.52781/0.25742. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.52472/0.25724. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.52607/0.25647. Took 0.05 sec\n",
            "Epoch 459, Loss(train/val) 0.52287/0.25486. Took 0.04 sec\n",
            "Epoch 460, Loss(train/val) 0.52224/0.25486. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.52820/0.25692. Took 0.04 sec\n",
            "Epoch 462, Loss(train/val) 0.52262/0.25683. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.52292/0.25657. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.52063/0.25505. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.52539/0.25497. Took 0.04 sec\n",
            "Epoch 466, Loss(train/val) 0.50764/0.25561. Took 0.04 sec\n",
            "Epoch 467, Loss(train/val) 0.51537/0.25587. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.51933/0.25561. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 0.51788/0.25376. Took 0.05 sec\n",
            "Epoch 470, Loss(train/val) 0.51955/0.25386. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 0.51518/0.25418. Took 0.04 sec\n",
            "Epoch 472, Loss(train/val) 0.51873/0.25436. Took 0.05 sec\n",
            "Epoch 473, Loss(train/val) 0.51306/0.25455. Took 0.04 sec\n",
            "Epoch 474, Loss(train/val) 0.51372/0.25456. Took 0.04 sec\n",
            "Epoch 475, Loss(train/val) 0.51444/0.25450. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.51575/0.25379. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.50407/0.25298. Took 0.06 sec\n",
            "Epoch 478, Loss(train/val) 0.50891/0.25361. Took 0.05 sec\n",
            "Epoch 479, Loss(train/val) 0.50089/0.25359. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.51455/0.25455. Took 0.04 sec\n",
            "Epoch 481, Loss(train/val) 0.50700/0.25521. Took 0.04 sec\n",
            "Epoch 482, Loss(train/val) 0.51179/0.25553. Took 0.05 sec\n",
            "Epoch 483, Loss(train/val) 0.50933/0.25652. Took 0.04 sec\n",
            "Epoch 484, Loss(train/val) 0.51099/0.25494. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.50470/0.25809. Took 0.04 sec\n",
            "Epoch 486, Loss(train/val) 0.50946/0.25896. Took 0.04 sec\n",
            "Epoch 487, Loss(train/val) 0.50082/0.25740. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.51073/0.25644. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.50286/0.25769. Took 0.05 sec\n",
            "Epoch 490, Loss(train/val) 0.50559/0.25811. Took 0.05 sec\n",
            "Epoch 491, Loss(train/val) 0.49660/0.25716. Took 0.05 sec\n",
            "Epoch 492, Loss(train/val) 0.50871/0.25832. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.50531/0.25992. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.51056/0.25725. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.49537/0.25706. Took 0.05 sec\n",
            "Epoch 496, Loss(train/val) 0.50181/0.25364. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 0.50352/0.25058. Took 0.05 sec\n",
            "Epoch 498, Loss(train/val) 0.50467/0.25083. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.49963/0.25069. Took 0.05 sec\n",
            "Epoch 500, Loss(train/val) 0.50132/0.25103. Took 0.05 sec\n",
            "Epoch 501, Loss(train/val) 0.50175/0.25083. Took 0.05 sec\n",
            "Epoch 502, Loss(train/val) 0.50511/0.25097. Took 0.05 sec\n",
            "Epoch 503, Loss(train/val) 0.50135/0.25330. Took 0.04 sec\n",
            "Epoch 504, Loss(train/val) 0.49704/0.25329. Took 0.05 sec\n",
            "Epoch 505, Loss(train/val) 0.49683/0.25485. Took 0.05 sec\n",
            "Epoch 506, Loss(train/val) 0.49851/0.25595. Took 0.05 sec\n",
            "Epoch 507, Loss(train/val) 0.49768/0.25459. Took 0.06 sec\n",
            "Epoch 508, Loss(train/val) 0.49324/0.25475. Took 0.05 sec\n",
            "Epoch 509, Loss(train/val) 0.49620/0.25489. Took 0.04 sec\n",
            "Epoch 510, Loss(train/val) 0.49196/0.25381. Took 0.05 sec\n",
            "Epoch 511, Loss(train/val) 0.49120/0.25374. Took 0.05 sec\n",
            "Epoch 512, Loss(train/val) 0.48857/0.25463. Took 0.05 sec\n",
            "Epoch 513, Loss(train/val) 0.48912/0.25456. Took 0.05 sec\n",
            "Epoch 514, Loss(train/val) 0.49104/0.25362. Took 0.05 sec\n",
            "Epoch 515, Loss(train/val) 0.49430/0.25125. Took 0.05 sec\n",
            "Epoch 516, Loss(train/val) 0.49410/0.25231. Took 0.05 sec\n",
            "Epoch 517, Loss(train/val) 0.49757/0.25280. Took 0.06 sec\n",
            "Epoch 518, Loss(train/val) 0.48335/0.25297. Took 0.05 sec\n",
            "Epoch 519, Loss(train/val) 0.48578/0.25308. Took 0.05 sec\n",
            "Epoch 520, Loss(train/val) 0.49170/0.25187. Took 0.05 sec\n",
            "Epoch 521, Loss(train/val) 0.49206/0.25173. Took 0.05 sec\n",
            "Epoch 522, Loss(train/val) 0.48986/0.25213. Took 0.05 sec\n",
            "Epoch 523, Loss(train/val) 0.48950/0.25286. Took 0.04 sec\n",
            "Epoch 524, Loss(train/val) 0.48873/0.25305. Took 0.05 sec\n",
            "Epoch 525, Loss(train/val) 0.48689/0.25130. Took 0.04 sec\n",
            "Epoch 526, Loss(train/val) 0.48197/0.25024. Took 0.05 sec\n",
            "Epoch 527, Loss(train/val) 0.48678/0.25110. Took 0.05 sec\n",
            "Epoch 528, Loss(train/val) 0.49260/0.25310. Took 0.04 sec\n",
            "Epoch 529, Loss(train/val) 0.48309/0.25668. Took 0.04 sec\n",
            "Epoch 530, Loss(train/val) 0.48861/0.25786. Took 0.04 sec\n",
            "Epoch 531, Loss(train/val) 0.48465/0.25607. Took 0.05 sec\n",
            "Epoch 532, Loss(train/val) 0.48768/0.25509. Took 0.05 sec\n",
            "Epoch 533, Loss(train/val) 0.49444/0.25258. Took 0.05 sec\n",
            "Epoch 534, Loss(train/val) 0.48715/0.25180. Took 0.04 sec\n",
            "Epoch 535, Loss(train/val) 0.48587/0.25198. Took 0.04 sec\n",
            "Epoch 536, Loss(train/val) 0.48147/0.25077. Took 0.04 sec\n",
            "Epoch 537, Loss(train/val) 0.48508/0.24944. Took 0.05 sec\n",
            "Epoch 538, Loss(train/val) 0.50066/0.24908. Took 0.05 sec\n",
            "Epoch 539, Loss(train/val) 0.47594/0.24918. Took 0.05 sec\n",
            "Epoch 540, Loss(train/val) 0.47958/0.25013. Took 0.04 sec\n",
            "Epoch 541, Loss(train/val) 0.47485/0.25193. Took 0.05 sec\n",
            "Epoch 542, Loss(train/val) 0.48890/0.25522. Took 0.05 sec\n",
            "Epoch 543, Loss(train/val) 0.47833/0.25378. Took 0.05 sec\n",
            "Epoch 544, Loss(train/val) 0.47549/0.25362. Took 0.04 sec\n",
            "Epoch 545, Loss(train/val) 0.47856/0.25144. Took 0.04 sec\n",
            "Epoch 546, Loss(train/val) 0.47708/0.25091. Took 0.06 sec\n",
            "Epoch 547, Loss(train/val) 0.47650/0.24919. Took 0.05 sec\n",
            "Epoch 548, Loss(train/val) 0.47904/0.24858. Took 0.04 sec\n",
            "Epoch 549, Loss(train/val) 0.46610/0.24930. Took 0.05 sec\n",
            "Epoch 550, Loss(train/val) 0.47753/0.25198. Took 0.05 sec\n",
            "Epoch 551, Loss(train/val) 0.46840/0.25209. Took 0.04 sec\n",
            "Epoch 552, Loss(train/val) 0.47666/0.25289. Took 0.06 sec\n",
            "Epoch 553, Loss(train/val) 0.47656/0.25322. Took 0.04 sec\n",
            "Epoch 554, Loss(train/val) 0.47249/0.25255. Took 0.04 sec\n",
            "Epoch 555, Loss(train/val) 0.47150/0.25392. Took 0.04 sec\n",
            "Epoch 556, Loss(train/val) 0.47096/0.25112. Took 0.04 sec\n",
            "Epoch 557, Loss(train/val) 0.47222/0.25179. Took 0.05 sec\n",
            "Epoch 558, Loss(train/val) 0.47095/0.25323. Took 0.05 sec\n",
            "Epoch 559, Loss(train/val) 0.47397/0.25369. Took 0.04 sec\n",
            "Epoch 560, Loss(train/val) 0.46897/0.25266. Took 0.05 sec\n",
            "Epoch 561, Loss(train/val) 0.47650/0.25161. Took 0.05 sec\n",
            "Epoch 562, Loss(train/val) 0.47815/0.25171. Took 0.05 sec\n",
            "Epoch 563, Loss(train/val) 0.47553/0.24869. Took 0.05 sec\n",
            "Epoch 564, Loss(train/val) 0.47149/0.24894. Took 0.05 sec\n",
            "Epoch 565, Loss(train/val) 0.47036/0.24870. Took 0.04 sec\n",
            "Epoch 566, Loss(train/val) 0.46252/0.24919. Took 0.05 sec\n",
            "Epoch 567, Loss(train/val) 0.47129/0.24967. Took 0.05 sec\n",
            "Epoch 568, Loss(train/val) 0.45751/0.24831. Took 0.05 sec\n",
            "Epoch 569, Loss(train/val) 0.47170/0.24745. Took 0.05 sec\n",
            "Epoch 570, Loss(train/val) 0.46790/0.24802. Took 0.05 sec\n",
            "Epoch 571, Loss(train/val) 0.46512/0.24867. Took 0.05 sec\n",
            "Epoch 572, Loss(train/val) 0.47148/0.24945. Took 0.05 sec\n",
            "Epoch 573, Loss(train/val) 0.46664/0.25078. Took 0.05 sec\n",
            "Epoch 574, Loss(train/val) 0.46814/0.24951. Took 0.05 sec\n",
            "Epoch 575, Loss(train/val) 0.45868/0.24944. Took 0.05 sec\n",
            "Epoch 576, Loss(train/val) 0.46630/0.25021. Took 0.05 sec\n",
            "Epoch 577, Loss(train/val) 0.46103/0.25142. Took 0.05 sec\n",
            "Epoch 578, Loss(train/val) 0.46637/0.25255. Took 0.04 sec\n",
            "Epoch 579, Loss(train/val) 0.47081/0.25554. Took 0.05 sec\n",
            "Epoch 580, Loss(train/val) 0.46419/0.25571. Took 0.04 sec\n",
            "Epoch 581, Loss(train/val) 0.46817/0.25686. Took 0.05 sec\n",
            "Epoch 582, Loss(train/val) 0.45483/0.25443. Took 0.05 sec\n",
            "Epoch 583, Loss(train/val) 0.46359/0.25011. Took 0.05 sec\n",
            "Epoch 584, Loss(train/val) 0.45772/0.24802. Took 0.05 sec\n",
            "Epoch 585, Loss(train/val) 0.46411/0.24688. Took 0.05 sec\n",
            "Epoch 586, Loss(train/val) 0.46359/0.24733. Took 0.04 sec\n",
            "Epoch 587, Loss(train/val) 0.45428/0.24897. Took 0.05 sec\n",
            "Epoch 588, Loss(train/val) 0.46076/0.25110. Took 0.05 sec\n",
            "Epoch 589, Loss(train/val) 0.44918/0.25249. Took 0.05 sec\n",
            "Epoch 590, Loss(train/val) 0.46233/0.25210. Took 0.05 sec\n",
            "Epoch 591, Loss(train/val) 0.45430/0.25007. Took 0.04 sec\n",
            "Epoch 592, Loss(train/val) 0.45670/0.24994. Took 0.05 sec\n",
            "Epoch 593, Loss(train/val) 0.46008/0.24875. Took 0.05 sec\n",
            "Epoch 594, Loss(train/val) 0.45009/0.24832. Took 0.05 sec\n",
            "Epoch 595, Loss(train/val) 0.45923/0.24726. Took 0.05 sec\n",
            "Epoch 596, Loss(train/val) 0.45919/0.24818. Took 0.05 sec\n",
            "Epoch 597, Loss(train/val) 0.45840/0.25045. Took 0.05 sec\n",
            "Epoch 598, Loss(train/val) 0.45514/0.25085. Took 0.05 sec\n",
            "Epoch 599, Loss(train/val) 0.45278/0.25104. Took 0.04 sec\n",
            "Epoch 600, Loss(train/val) 0.44943/0.24880. Took 0.05 sec\n",
            "Epoch 601, Loss(train/val) 0.45827/0.24827. Took 0.04 sec\n",
            "Epoch 602, Loss(train/val) 0.45296/0.24735. Took 0.05 sec\n",
            "Epoch 603, Loss(train/val) 0.45761/0.24685. Took 0.05 sec\n",
            "Epoch 604, Loss(train/val) 0.45245/0.24790. Took 0.05 sec\n",
            "Epoch 605, Loss(train/val) 0.45148/0.24984. Took 0.05 sec\n",
            "Epoch 606, Loss(train/val) 0.44760/0.25188. Took 0.04 sec\n",
            "Epoch 607, Loss(train/val) 0.45345/0.25266. Took 0.05 sec\n",
            "Epoch 608, Loss(train/val) 0.45913/0.25426. Took 0.05 sec\n",
            "Epoch 609, Loss(train/val) 0.45470/0.24999. Took 0.04 sec\n",
            "Epoch 610, Loss(train/val) 0.44789/0.25048. Took 0.05 sec\n",
            "Epoch 611, Loss(train/val) 0.44633/0.24803. Took 0.05 sec\n",
            "Epoch 612, Loss(train/val) 0.45850/0.24729. Took 0.05 sec\n",
            "Epoch 613, Loss(train/val) 0.45123/0.24593. Took 0.04 sec\n",
            "Epoch 614, Loss(train/val) 0.45402/0.24570. Took 0.04 sec\n",
            "Epoch 615, Loss(train/val) 0.45538/0.24678. Took 0.05 sec\n",
            "Epoch 616, Loss(train/val) 0.44814/0.24780. Took 0.04 sec\n",
            "Epoch 617, Loss(train/val) 0.44944/0.24764. Took 0.05 sec\n",
            "Epoch 618, Loss(train/val) 0.43923/0.24835. Took 0.04 sec\n",
            "Epoch 619, Loss(train/val) 0.43897/0.24878. Took 0.05 sec\n",
            "Epoch 620, Loss(train/val) 0.44994/0.24743. Took 0.05 sec\n",
            "Epoch 621, Loss(train/val) 0.45252/0.24861. Took 0.04 sec\n",
            "Epoch 622, Loss(train/val) 0.44957/0.25118. Took 0.05 sec\n",
            "Epoch 623, Loss(train/val) 0.44412/0.25024. Took 0.04 sec\n",
            "Epoch 624, Loss(train/val) 0.43370/0.24898. Took 0.05 sec\n",
            "Epoch 625, Loss(train/val) 0.44767/0.24917. Took 0.05 sec\n",
            "Epoch 626, Loss(train/val) 0.44381/0.24770. Took 0.05 sec\n",
            "Epoch 627, Loss(train/val) 0.43115/0.24752. Took 0.05 sec\n",
            "Epoch 628, Loss(train/val) 0.43891/0.24704. Took 0.05 sec\n",
            "Epoch 629, Loss(train/val) 0.44243/0.24631. Took 0.04 sec\n",
            "Epoch 630, Loss(train/val) 0.45142/0.24683. Took 0.04 sec\n",
            "Epoch 631, Loss(train/val) 0.43448/0.24818. Took 0.04 sec\n",
            "Epoch 632, Loss(train/val) 0.43447/0.24909. Took 0.05 sec\n",
            "Epoch 633, Loss(train/val) 0.44471/0.24801. Took 0.04 sec\n",
            "Epoch 634, Loss(train/val) 0.43789/0.24854. Took 0.04 sec\n",
            "Epoch 635, Loss(train/val) 0.43740/0.24823. Took 0.05 sec\n",
            "Epoch 636, Loss(train/val) 0.43672/0.24680. Took 0.06 sec\n",
            "Epoch 637, Loss(train/val) 0.43079/0.24568. Took 0.05 sec\n",
            "Epoch 638, Loss(train/val) 0.44341/0.24572. Took 0.05 sec\n",
            "Epoch 639, Loss(train/val) 0.43793/0.24493. Took 0.05 sec\n",
            "Epoch 640, Loss(train/val) 0.42913/0.24547. Took 0.04 sec\n",
            "Epoch 641, Loss(train/val) 0.43967/0.24578. Took 0.04 sec\n",
            "Epoch 642, Loss(train/val) 0.43965/0.24745. Took 0.05 sec\n",
            "Epoch 643, Loss(train/val) 0.43009/0.25068. Took 0.05 sec\n",
            "Epoch 644, Loss(train/val) 0.44109/0.25186. Took 0.04 sec\n",
            "Epoch 645, Loss(train/val) 0.43445/0.25165. Took 0.05 sec\n",
            "Epoch 646, Loss(train/val) 0.43606/0.25123. Took 0.06 sec\n",
            "Epoch 647, Loss(train/val) 0.43332/0.24897. Took 0.05 sec\n",
            "Epoch 648, Loss(train/val) 0.43251/0.24857. Took 0.04 sec\n",
            "Epoch 649, Loss(train/val) 0.43762/0.24786. Took 0.05 sec\n",
            "Epoch 650, Loss(train/val) 0.43037/0.24742. Took 0.04 sec\n",
            "Epoch 651, Loss(train/val) 0.43132/0.24643. Took 0.05 sec\n",
            "Epoch 652, Loss(train/val) 0.44089/0.24600. Took 0.05 sec\n",
            "Epoch 653, Loss(train/val) 0.42638/0.24631. Took 0.05 sec\n",
            "Epoch 654, Loss(train/val) 0.43174/0.24804. Took 0.05 sec\n",
            "Epoch 655, Loss(train/val) 0.43184/0.24726. Took 0.04 sec\n",
            "Epoch 656, Loss(train/val) 0.43182/0.24674. Took 0.05 sec\n",
            "Epoch 657, Loss(train/val) 0.42633/0.24626. Took 0.05 sec\n",
            "Epoch 658, Loss(train/val) 0.42920/0.24635. Took 0.05 sec\n",
            "Epoch 659, Loss(train/val) 0.42724/0.24751. Took 0.04 sec\n",
            "Epoch 660, Loss(train/val) 0.42945/0.24794. Took 0.04 sec\n",
            "Epoch 661, Loss(train/val) 0.43099/0.24747. Took 0.05 sec\n",
            "Epoch 662, Loss(train/val) 0.42840/0.24896. Took 0.05 sec\n",
            "Epoch 663, Loss(train/val) 0.43073/0.24819. Took 0.04 sec\n",
            "Epoch 664, Loss(train/val) 0.42946/0.24820. Took 0.05 sec\n",
            "Epoch 665, Loss(train/val) 0.42503/0.24811. Took 0.04 sec\n",
            "Epoch 666, Loss(train/val) 0.42809/0.24838. Took 0.04 sec\n",
            "Epoch 667, Loss(train/val) 0.42668/0.24979. Took 0.06 sec\n",
            "Epoch 668, Loss(train/val) 0.42847/0.25058. Took 0.05 sec\n",
            "Epoch 669, Loss(train/val) 0.42931/0.24980. Took 0.04 sec\n",
            "Epoch 670, Loss(train/val) 0.41946/0.24828. Took 0.04 sec\n",
            "Epoch 671, Loss(train/val) 0.42459/0.24882. Took 0.04 sec\n",
            "Epoch 672, Loss(train/val) 0.44051/0.24774. Took 0.05 sec\n",
            "Epoch 673, Loss(train/val) 0.43237/0.24614. Took 0.05 sec\n",
            "Epoch 674, Loss(train/val) 0.42150/0.24555. Took 0.05 sec\n",
            "Epoch 675, Loss(train/val) 0.41904/0.24529. Took 0.05 sec\n",
            "Epoch 676, Loss(train/val) 0.42185/0.24507. Took 0.04 sec\n",
            "Epoch 677, Loss(train/val) 0.42750/0.24508. Took 0.05 sec\n",
            "Epoch 678, Loss(train/val) 0.42039/0.24569. Took 0.05 sec\n",
            "Epoch 679, Loss(train/val) 0.42189/0.24714. Took 0.05 sec\n",
            "Epoch 680, Loss(train/val) 0.42906/0.24891. Took 0.05 sec\n",
            "Epoch 681, Loss(train/val) 0.42672/0.25278. Took 0.05 sec\n",
            "Epoch 682, Loss(train/val) 0.41902/0.25454. Took 0.05 sec\n",
            "Epoch 683, Loss(train/val) 0.41991/0.25255. Took 0.05 sec\n",
            "Epoch 684, Loss(train/val) 0.41711/0.24986. Took 0.05 sec\n",
            "Epoch 685, Loss(train/val) 0.41616/0.24643. Took 0.05 sec\n",
            "Epoch 686, Loss(train/val) 0.41261/0.24539. Took 0.05 sec\n",
            "Epoch 687, Loss(train/val) 0.41653/0.24535. Took 0.05 sec\n",
            "Epoch 688, Loss(train/val) 0.41414/0.24477. Took 0.05 sec\n",
            "Epoch 689, Loss(train/val) 0.42079/0.24530. Took 0.05 sec\n",
            "Epoch 690, Loss(train/val) 0.42080/0.24507. Took 0.05 sec\n",
            "Epoch 691, Loss(train/val) 0.41040/0.24537. Took 0.05 sec\n",
            "Epoch 692, Loss(train/val) 0.43961/0.24548. Took 0.05 sec\n",
            "Epoch 693, Loss(train/val) 0.42343/0.24502. Took 0.04 sec\n",
            "Epoch 694, Loss(train/val) 0.41871/0.24471. Took 0.04 sec\n",
            "Epoch 695, Loss(train/val) 0.41473/0.24447. Took 0.05 sec\n",
            "Epoch 696, Loss(train/val) 0.41837/0.24481. Took 0.05 sec\n",
            "Epoch 697, Loss(train/val) 0.40833/0.24508. Took 0.05 sec\n",
            "Epoch 698, Loss(train/val) 0.42016/0.24596. Took 0.05 sec\n",
            "Epoch 699, Loss(train/val) 0.41319/0.24835. Took 0.05 sec\n",
            "Epoch 700, Loss(train/val) 0.41332/0.25042. Took 0.05 sec\n",
            "Epoch 701, Loss(train/val) 0.40859/0.24838. Took 0.05 sec\n",
            "Epoch 702, Loss(train/val) 0.41824/0.24702. Took 0.05 sec\n",
            "Epoch 703, Loss(train/val) 0.41696/0.24648. Took 0.04 sec\n",
            "Epoch 704, Loss(train/val) 0.41530/0.24836. Took 0.04 sec\n",
            "Epoch 705, Loss(train/val) 0.41075/0.25004. Took 0.05 sec\n",
            "Epoch 706, Loss(train/val) 0.42444/0.25043. Took 0.05 sec\n",
            "Epoch 707, Loss(train/val) 0.41196/0.24988. Took 0.05 sec\n",
            "Epoch 708, Loss(train/val) 0.41626/0.24736. Took 0.04 sec\n",
            "Epoch 709, Loss(train/val) 0.40346/0.24686. Took 0.05 sec\n",
            "Epoch 710, Loss(train/val) 0.41350/0.24560. Took 0.05 sec\n",
            "Epoch 711, Loss(train/val) 0.41050/0.24600. Took 0.05 sec\n",
            "Epoch 712, Loss(train/val) 0.40802/0.24698. Took 0.05 sec\n",
            "Epoch 713, Loss(train/val) 0.41607/0.24521. Took 0.05 sec\n",
            "Epoch 714, Loss(train/val) 0.41588/0.24655. Took 0.04 sec\n",
            "Epoch 715, Loss(train/val) 0.41378/0.24732. Took 0.04 sec\n",
            "Epoch 716, Loss(train/val) 0.40906/0.25006. Took 0.05 sec\n",
            "Epoch 717, Loss(train/val) 0.40614/0.24934. Took 0.05 sec\n",
            "Epoch 718, Loss(train/val) 0.41289/0.24750. Took 0.05 sec\n",
            "Epoch 719, Loss(train/val) 0.41150/0.24601. Took 0.04 sec\n",
            "Epoch 720, Loss(train/val) 0.40947/0.24567. Took 0.05 sec\n",
            "Epoch 721, Loss(train/val) 0.40589/0.24470. Took 0.05 sec\n",
            "Epoch 722, Loss(train/val) 0.40425/0.24461. Took 0.05 sec\n",
            "Epoch 723, Loss(train/val) 0.40593/0.24439. Took 0.05 sec\n",
            "Epoch 724, Loss(train/val) 0.40621/0.24452. Took 0.04 sec\n",
            "Epoch 725, Loss(train/val) 0.40514/0.24451. Took 0.04 sec\n",
            "Epoch 726, Loss(train/val) 0.40397/0.24483. Took 0.05 sec\n",
            "Epoch 727, Loss(train/val) 0.40322/0.24530. Took 0.05 sec\n",
            "Epoch 728, Loss(train/val) 0.40730/0.24662. Took 0.04 sec\n",
            "Epoch 729, Loss(train/val) 0.40379/0.24754. Took 0.05 sec\n",
            "Epoch 730, Loss(train/val) 0.40657/0.24742. Took 0.05 sec\n",
            "Epoch 731, Loss(train/val) 0.39328/0.24816. Took 0.05 sec\n",
            "Epoch 732, Loss(train/val) 0.40194/0.24744. Took 0.06 sec\n",
            "Epoch 733, Loss(train/val) 0.40864/0.24634. Took 0.04 sec\n",
            "Epoch 734, Loss(train/val) 0.40328/0.24609. Took 0.04 sec\n",
            "Epoch 735, Loss(train/val) 0.40400/0.24597. Took 0.04 sec\n",
            "Epoch 736, Loss(train/val) 0.39006/0.24478. Took 0.05 sec\n",
            "Epoch 737, Loss(train/val) 0.40180/0.24468. Took 0.05 sec\n",
            "Epoch 738, Loss(train/val) 0.40664/0.24476. Took 0.04 sec\n",
            "Epoch 739, Loss(train/val) 0.39980/0.24501. Took 0.04 sec\n",
            "Epoch 740, Loss(train/val) 0.40164/0.24565. Took 0.04 sec\n",
            "Epoch 741, Loss(train/val) 0.39957/0.24461. Took 0.05 sec\n",
            "Epoch 742, Loss(train/val) 0.39998/0.24548. Took 0.06 sec\n",
            "Epoch 743, Loss(train/val) 0.40195/0.24635. Took 0.05 sec\n",
            "Epoch 744, Loss(train/val) 0.39726/0.24685. Took 0.04 sec\n",
            "Epoch 745, Loss(train/val) 0.40435/0.24610. Took 0.05 sec\n",
            "Epoch 746, Loss(train/val) 0.39973/0.24460. Took 0.04 sec\n",
            "Epoch 747, Loss(train/val) 0.39741/0.24478. Took 0.05 sec\n",
            "Epoch 748, Loss(train/val) 0.39803/0.24422. Took 0.05 sec\n",
            "Epoch 749, Loss(train/val) 0.39110/0.24472. Took 0.05 sec\n",
            "Epoch 750, Loss(train/val) 0.39508/0.24492. Took 0.04 sec\n",
            "Epoch 751, Loss(train/val) 0.39153/0.24457. Took 0.04 sec\n",
            "Epoch 752, Loss(train/val) 0.39804/0.24480. Took 0.05 sec\n",
            "Epoch 753, Loss(train/val) 0.40362/0.24646. Took 0.06 sec\n",
            "Epoch 754, Loss(train/val) 0.38729/0.24933. Took 0.05 sec\n",
            "Epoch 755, Loss(train/val) 0.40141/0.24756. Took 0.04 sec\n",
            "Epoch 756, Loss(train/val) 0.39778/0.24755. Took 0.04 sec\n",
            "Epoch 757, Loss(train/val) 0.40005/0.24828. Took 0.05 sec\n",
            "Epoch 758, Loss(train/val) 0.38899/0.24885. Took 0.05 sec\n",
            "Epoch 759, Loss(train/val) 0.39827/0.24880. Took 0.04 sec\n",
            "Epoch 760, Loss(train/val) 0.39921/0.24938. Took 0.04 sec\n",
            "Epoch 761, Loss(train/val) 0.39586/0.24702. Took 0.05 sec\n",
            "Epoch 762, Loss(train/val) 0.39330/0.24521. Took 0.06 sec\n",
            "Epoch 763, Loss(train/val) 0.39612/0.24465. Took 0.05 sec\n",
            "Epoch 764, Loss(train/val) 0.39661/0.24412. Took 0.05 sec\n",
            "Epoch 765, Loss(train/val) 0.39015/0.24456. Took 0.05 sec\n",
            "Epoch 766, Loss(train/val) 0.40044/0.24439. Took 0.05 sec\n",
            "Epoch 767, Loss(train/val) 0.38919/0.24414. Took 0.05 sec\n",
            "Epoch 768, Loss(train/val) 0.39561/0.24414. Took 0.04 sec\n",
            "Epoch 769, Loss(train/val) 0.39092/0.24432. Took 0.05 sec\n",
            "Epoch 770, Loss(train/val) 0.39491/0.24446. Took 0.04 sec\n",
            "Epoch 771, Loss(train/val) 0.38962/0.24550. Took 0.05 sec\n",
            "Epoch 772, Loss(train/val) 0.38922/0.24566. Took 0.05 sec\n",
            "Epoch 773, Loss(train/val) 0.39034/0.24739. Took 0.04 sec\n",
            "Epoch 774, Loss(train/val) 0.38213/0.24832. Took 0.04 sec\n",
            "Epoch 775, Loss(train/val) 0.38655/0.24687. Took 0.05 sec\n",
            "Epoch 776, Loss(train/val) 0.38999/0.24729. Took 0.04 sec\n",
            "Epoch 777, Loss(train/val) 0.39121/0.24767. Took 0.05 sec\n",
            "Epoch 778, Loss(train/val) 0.39723/0.24824. Took 0.05 sec\n",
            "Epoch 779, Loss(train/val) 0.38627/0.24575. Took 0.05 sec\n",
            "Epoch 780, Loss(train/val) 0.38773/0.24375. Took 0.04 sec\n",
            "Epoch 781, Loss(train/val) 0.38828/0.24387. Took 0.05 sec\n",
            "Epoch 782, Loss(train/val) 0.38284/0.24398. Took 0.05 sec\n",
            "Epoch 783, Loss(train/val) 0.38627/0.24428. Took 0.05 sec\n",
            "Epoch 784, Loss(train/val) 0.38587/0.24396. Took 0.05 sec\n",
            "Epoch 785, Loss(train/val) 0.39864/0.24449. Took 0.05 sec\n",
            "Epoch 786, Loss(train/val) 0.39044/0.24545. Took 0.04 sec\n",
            "Epoch 787, Loss(train/val) 0.38578/0.24690. Took 0.05 sec\n",
            "Epoch 788, Loss(train/val) 0.38677/0.24638. Took 0.05 sec\n",
            "Epoch 789, Loss(train/val) 0.39375/0.24575. Took 0.04 sec\n",
            "Epoch 790, Loss(train/val) 0.38700/0.24625. Took 0.05 sec\n",
            "Epoch 791, Loss(train/val) 0.39172/0.24677. Took 0.05 sec\n",
            "Epoch 792, Loss(train/val) 0.37887/0.24683. Took 0.05 sec\n",
            "Epoch 793, Loss(train/val) 0.38701/0.24672. Took 0.04 sec\n",
            "Epoch 794, Loss(train/val) 0.39437/0.24608. Took 0.05 sec\n",
            "Epoch 795, Loss(train/val) 0.38845/0.24414. Took 0.04 sec\n",
            "Epoch 796, Loss(train/val) 0.37499/0.24420. Took 0.05 sec\n",
            "Epoch 797, Loss(train/val) 0.37897/0.24401. Took 0.05 sec\n",
            "Epoch 798, Loss(train/val) 0.37934/0.24389. Took 0.05 sec\n",
            "Epoch 799, Loss(train/val) 0.38777/0.24401. Took 0.05 sec\n",
            "Epoch 800, Loss(train/val) 0.38465/0.24472. Took 0.05 sec\n",
            "Epoch 801, Loss(train/val) 0.38297/0.24534. Took 0.05 sec\n",
            "Epoch 802, Loss(train/val) 0.38376/0.24611. Took 0.05 sec\n",
            "Epoch 803, Loss(train/val) 0.39122/0.24713. Took 0.05 sec\n",
            "Epoch 804, Loss(train/val) 0.37079/0.24506. Took 0.04 sec\n",
            "Epoch 805, Loss(train/val) 0.37306/0.24454. Took 0.05 sec\n",
            "Epoch 806, Loss(train/val) 0.37540/0.24476. Took 0.05 sec\n",
            "Epoch 807, Loss(train/val) 0.38180/0.24450. Took 0.06 sec\n",
            "Epoch 808, Loss(train/val) 0.37709/0.24473. Took 0.05 sec\n",
            "Epoch 809, Loss(train/val) 0.39019/0.24441. Took 0.05 sec\n",
            "Epoch 810, Loss(train/val) 0.37819/0.24439. Took 0.05 sec\n",
            "Epoch 811, Loss(train/val) 0.37861/0.24557. Took 0.05 sec\n",
            "Epoch 812, Loss(train/val) 0.37282/0.24485. Took 0.05 sec\n",
            "Epoch 813, Loss(train/val) 0.38895/0.24362. Took 0.04 sec\n",
            "Epoch 814, Loss(train/val) 0.37071/0.24360. Took 0.04 sec\n",
            "Epoch 815, Loss(train/val) 0.38281/0.24357. Took 0.05 sec\n",
            "Epoch 816, Loss(train/val) 0.37065/0.24336. Took 0.05 sec\n",
            "Epoch 817, Loss(train/val) 0.37929/0.24340. Took 0.06 sec\n",
            "Epoch 818, Loss(train/val) 0.37846/0.24426. Took 0.05 sec\n",
            "Epoch 819, Loss(train/val) 0.38163/0.24371. Took 0.05 sec\n",
            "Epoch 820, Loss(train/val) 0.36905/0.24442. Took 0.05 sec\n",
            "Epoch 821, Loss(train/val) 0.37538/0.24371. Took 0.05 sec\n",
            "Epoch 822, Loss(train/val) 0.37580/0.24401. Took 0.05 sec\n",
            "Epoch 823, Loss(train/val) 0.38023/0.24369. Took 0.05 sec\n",
            "Epoch 824, Loss(train/val) 0.37431/0.24460. Took 0.04 sec\n",
            "Epoch 825, Loss(train/val) 0.37892/0.24356. Took 0.05 sec\n",
            "Epoch 826, Loss(train/val) 0.37145/0.24376. Took 0.05 sec\n",
            "Epoch 827, Loss(train/val) 0.37234/0.24409. Took 0.05 sec\n",
            "Epoch 828, Loss(train/val) 0.36682/0.24353. Took 0.05 sec\n",
            "Epoch 829, Loss(train/val) 0.37437/0.24431. Took 0.04 sec\n",
            "Epoch 830, Loss(train/val) 0.37061/0.24480. Took 0.05 sec\n",
            "Epoch 831, Loss(train/val) 0.36109/0.24443. Took 0.04 sec\n",
            "Epoch 832, Loss(train/val) 0.37190/0.24365. Took 0.05 sec\n",
            "Epoch 833, Loss(train/val) 0.36812/0.24305. Took 0.05 sec\n",
            "Epoch 834, Loss(train/val) 0.37403/0.24314. Took 0.04 sec\n",
            "Epoch 835, Loss(train/val) 0.37072/0.24332. Took 0.04 sec\n",
            "Epoch 836, Loss(train/val) 0.37781/0.24320. Took 0.04 sec\n",
            "Epoch 837, Loss(train/val) 0.36588/0.24379. Took 0.05 sec\n",
            "Epoch 838, Loss(train/val) 0.36815/0.24348. Took 0.05 sec\n",
            "Epoch 839, Loss(train/val) 0.37109/0.24330. Took 0.05 sec\n",
            "Epoch 840, Loss(train/val) 0.37827/0.24334. Took 0.04 sec\n",
            "Epoch 841, Loss(train/val) 0.37464/0.24335. Took 0.05 sec\n",
            "Epoch 842, Loss(train/val) 0.36848/0.24323. Took 0.05 sec\n",
            "Epoch 843, Loss(train/val) 0.36716/0.24273. Took 0.05 sec\n",
            "Epoch 844, Loss(train/val) 0.36356/0.24323. Took 0.05 sec\n",
            "Epoch 845, Loss(train/val) 0.37055/0.24278. Took 0.04 sec\n",
            "Epoch 846, Loss(train/val) 0.36960/0.24305. Took 0.05 sec\n",
            "Epoch 847, Loss(train/val) 0.36500/0.24295. Took 0.05 sec\n",
            "Epoch 848, Loss(train/val) 0.36992/0.24295. Took 0.05 sec\n",
            "Epoch 849, Loss(train/val) 0.36282/0.24288. Took 0.05 sec\n",
            "Epoch 850, Loss(train/val) 0.36332/0.24290. Took 0.05 sec\n",
            "Epoch 851, Loss(train/val) 0.36219/0.24290. Took 0.05 sec\n",
            "Epoch 852, Loss(train/val) 0.37450/0.24302. Took 0.05 sec\n",
            "Epoch 853, Loss(train/val) 0.37137/0.24330. Took 0.05 sec\n",
            "Epoch 854, Loss(train/val) 0.37065/0.24275. Took 0.05 sec\n",
            "Epoch 855, Loss(train/val) 0.36796/0.24305. Took 0.04 sec\n",
            "Epoch 856, Loss(train/val) 0.36203/0.24403. Took 0.05 sec\n",
            "Epoch 857, Loss(train/val) 0.36918/0.24398. Took 0.06 sec\n",
            "Epoch 858, Loss(train/val) 0.37138/0.24415. Took 0.05 sec\n",
            "Epoch 859, Loss(train/val) 0.36377/0.24487. Took 0.05 sec\n",
            "Epoch 860, Loss(train/val) 0.36320/0.24609. Took 0.05 sec\n",
            "Epoch 861, Loss(train/val) 0.36168/0.24525. Took 0.04 sec\n",
            "Epoch 862, Loss(train/val) 0.36668/0.24447. Took 0.05 sec\n",
            "Epoch 863, Loss(train/val) 0.35976/0.24375. Took 0.05 sec\n",
            "Epoch 864, Loss(train/val) 0.37406/0.24290. Took 0.04 sec\n",
            "Epoch 865, Loss(train/val) 0.36342/0.24247. Took 0.04 sec\n",
            "Epoch 866, Loss(train/val) 0.36518/0.24224. Took 0.04 sec\n",
            "Epoch 867, Loss(train/val) 0.36328/0.24250. Took 0.05 sec\n",
            "Epoch 868, Loss(train/val) 0.37074/0.24269. Took 0.04 sec\n",
            "Epoch 869, Loss(train/val) 0.35975/0.24285. Took 0.05 sec\n",
            "Epoch 870, Loss(train/val) 0.37013/0.24268. Took 0.05 sec\n",
            "Epoch 871, Loss(train/val) 0.36048/0.24292. Took 0.05 sec\n",
            "Epoch 872, Loss(train/val) 0.36877/0.24246. Took 0.05 sec\n",
            "Epoch 873, Loss(train/val) 0.36157/0.24205. Took 0.04 sec\n",
            "Epoch 874, Loss(train/val) 0.36042/0.24216. Took 0.05 sec\n",
            "Epoch 875, Loss(train/val) 0.36469/0.24200. Took 0.04 sec\n",
            "Epoch 876, Loss(train/val) 0.37069/0.24213. Took 0.04 sec\n",
            "Epoch 877, Loss(train/val) 0.36181/0.24297. Took 0.05 sec\n",
            "Epoch 878, Loss(train/val) 0.38594/0.24404. Took 0.04 sec\n",
            "Epoch 879, Loss(train/val) 0.36448/0.24320. Took 0.05 sec\n",
            "Epoch 880, Loss(train/val) 0.35883/0.24274. Took 0.05 sec\n",
            "Epoch 881, Loss(train/val) 0.35959/0.24230. Took 0.04 sec\n",
            "Epoch 882, Loss(train/val) 0.36410/0.24246. Took 0.06 sec\n",
            "Epoch 883, Loss(train/val) 0.35270/0.24248. Took 0.05 sec\n",
            "Epoch 884, Loss(train/val) 0.36564/0.24251. Took 0.04 sec\n",
            "Epoch 885, Loss(train/val) 0.35324/0.24232. Took 0.05 sec\n",
            "Epoch 886, Loss(train/val) 0.36076/0.24201. Took 0.04 sec\n",
            "Epoch 887, Loss(train/val) 0.36012/0.24150. Took 0.05 sec\n",
            "Epoch 888, Loss(train/val) 0.34973/0.24146. Took 0.05 sec\n",
            "Epoch 889, Loss(train/val) 0.36159/0.24138. Took 0.05 sec\n",
            "Epoch 890, Loss(train/val) 0.36125/0.24113. Took 0.05 sec\n",
            "Epoch 891, Loss(train/val) 0.35863/0.24163. Took 0.05 sec\n",
            "Epoch 892, Loss(train/val) 0.36655/0.24289. Took 0.05 sec\n",
            "Epoch 893, Loss(train/val) 0.35860/0.24182. Took 0.04 sec\n",
            "Epoch 894, Loss(train/val) 0.35815/0.24223. Took 0.04 sec\n",
            "Epoch 895, Loss(train/val) 0.35129/0.24165. Took 0.05 sec\n",
            "Epoch 896, Loss(train/val) 0.36436/0.24138. Took 0.04 sec\n",
            "Epoch 897, Loss(train/val) 0.35365/0.24127. Took 0.05 sec\n",
            "Epoch 898, Loss(train/val) 0.35237/0.24165. Took 0.04 sec\n",
            "Epoch 899, Loss(train/val) 0.35454/0.24149. Took 0.04 sec\n",
            "Epoch 900, Loss(train/val) 0.35281/0.24208. Took 0.05 sec\n",
            "Epoch 901, Loss(train/val) 0.36046/0.24257. Took 0.04 sec\n",
            "Epoch 902, Loss(train/val) 0.34842/0.24278. Took 0.05 sec\n",
            "Epoch 903, Loss(train/val) 0.35724/0.24281. Took 0.04 sec\n",
            "Epoch 904, Loss(train/val) 0.35540/0.24352. Took 0.05 sec\n",
            "Epoch 905, Loss(train/val) 0.36107/0.24271. Took 0.04 sec\n",
            "Epoch 906, Loss(train/val) 0.34448/0.24304. Took 0.05 sec\n",
            "Epoch 907, Loss(train/val) 0.35232/0.24209. Took 0.05 sec\n",
            "Epoch 908, Loss(train/val) 0.35521/0.24143. Took 0.05 sec\n",
            "Epoch 909, Loss(train/val) 0.35184/0.24105. Took 0.04 sec\n",
            "Epoch 910, Loss(train/val) 0.35834/0.24091. Took 0.04 sec\n",
            "Epoch 911, Loss(train/val) 0.35175/0.24086. Took 0.04 sec\n",
            "Epoch 912, Loss(train/val) 0.35151/0.24079. Took 0.05 sec\n",
            "Epoch 913, Loss(train/val) 0.35107/0.24071. Took 0.04 sec\n",
            "Epoch 914, Loss(train/val) 0.37334/0.24077. Took 0.05 sec\n",
            "Epoch 915, Loss(train/val) 0.35756/0.24054. Took 0.04 sec\n",
            "Epoch 916, Loss(train/val) 0.34877/0.24045. Took 0.04 sec\n",
            "Epoch 917, Loss(train/val) 0.35146/0.24034. Took 0.05 sec\n",
            "Epoch 918, Loss(train/val) 0.34939/0.24087. Took 0.04 sec\n",
            "Epoch 919, Loss(train/val) 0.34781/0.24050. Took 0.04 sec\n",
            "Epoch 920, Loss(train/val) 0.34690/0.24072. Took 0.04 sec\n",
            "Epoch 921, Loss(train/val) 0.34865/0.24059. Took 0.04 sec\n",
            "Epoch 922, Loss(train/val) 0.34767/0.24045. Took 0.05 sec\n",
            "Epoch 923, Loss(train/val) 0.35001/0.24085. Took 0.05 sec\n",
            "Epoch 924, Loss(train/val) 0.35040/0.24100. Took 0.05 sec\n",
            "Epoch 925, Loss(train/val) 0.34139/0.24260. Took 0.04 sec\n",
            "Epoch 926, Loss(train/val) 0.36110/0.24245. Took 0.05 sec\n",
            "Epoch 927, Loss(train/val) 0.35450/0.24203. Took 0.05 sec\n",
            "Epoch 928, Loss(train/val) 0.35647/0.24058. Took 0.05 sec\n",
            "Epoch 929, Loss(train/val) 0.36098/0.24013. Took 0.04 sec\n",
            "Epoch 930, Loss(train/val) 0.34365/0.24077. Took 0.04 sec\n",
            "Epoch 931, Loss(train/val) 0.34028/0.24130. Took 0.05 sec\n",
            "Epoch 932, Loss(train/val) 0.35241/0.24011. Took 0.05 sec\n",
            "Epoch 933, Loss(train/val) 0.34922/0.23994. Took 0.05 sec\n",
            "Epoch 934, Loss(train/val) 0.33812/0.23990. Took 0.05 sec\n",
            "Epoch 935, Loss(train/val) 0.34250/0.24037. Took 0.05 sec\n",
            "Epoch 936, Loss(train/val) 0.34179/0.24127. Took 0.05 sec\n",
            "Epoch 937, Loss(train/val) 0.35575/0.24104. Took 0.05 sec\n",
            "Epoch 938, Loss(train/val) 0.34009/0.24062. Took 0.05 sec\n",
            "Epoch 939, Loss(train/val) 0.35383/0.24002. Took 0.05 sec\n",
            "Epoch 940, Loss(train/val) 0.34215/0.23979. Took 0.04 sec\n",
            "Epoch 941, Loss(train/val) 0.34175/0.23980. Took 0.04 sec\n",
            "Epoch 942, Loss(train/val) 0.34450/0.23976. Took 0.05 sec\n",
            "Epoch 943, Loss(train/val) 0.35727/0.23937. Took 0.05 sec\n",
            "Epoch 944, Loss(train/val) 0.35268/0.23955. Took 0.05 sec\n",
            "Epoch 945, Loss(train/val) 0.34218/0.23900. Took 0.05 sec\n",
            "Epoch 946, Loss(train/val) 0.33807/0.23912. Took 0.05 sec\n",
            "Epoch 947, Loss(train/val) 0.34482/0.23889. Took 0.07 sec\n",
            "Epoch 948, Loss(train/val) 0.34271/0.23890. Took 0.05 sec\n",
            "Epoch 949, Loss(train/val) 0.34262/0.23890. Took 0.04 sec\n",
            "Epoch 950, Loss(train/val) 0.34327/0.23916. Took 0.04 sec\n",
            "Epoch 951, Loss(train/val) 0.32795/0.23904. Took 0.05 sec\n",
            "Epoch 952, Loss(train/val) 0.35096/0.23765. Took 0.05 sec\n",
            "Epoch 953, Loss(train/val) 0.34323/0.23765. Took 0.05 sec\n",
            "Epoch 954, Loss(train/val) 0.33927/0.23729. Took 0.04 sec\n",
            "Epoch 955, Loss(train/val) 0.34481/0.23741. Took 0.05 sec\n",
            "Epoch 956, Loss(train/val) 0.33987/0.23790. Took 0.05 sec\n",
            "Epoch 957, Loss(train/val) 0.34054/0.23767. Took 0.05 sec\n",
            "Epoch 958, Loss(train/val) 0.33073/0.23689. Took 0.04 sec\n",
            "Epoch 959, Loss(train/val) 0.34682/0.23654. Took 0.05 sec\n",
            "Epoch 960, Loss(train/val) 0.34537/0.23574. Took 0.05 sec\n",
            "Epoch 961, Loss(train/val) 0.33310/0.23489. Took 0.05 sec\n",
            "Epoch 962, Loss(train/val) 0.34694/0.23454. Took 0.06 sec\n",
            "Epoch 963, Loss(train/val) 0.34861/0.23404. Took 0.05 sec\n",
            "Epoch 964, Loss(train/val) 0.36062/0.23332. Took 0.05 sec\n",
            "Epoch 965, Loss(train/val) 0.33940/0.23392. Took 0.05 sec\n",
            "Epoch 966, Loss(train/val) 0.34423/0.23367. Took 0.05 sec\n",
            "Epoch 967, Loss(train/val) 0.34162/0.23360. Took 0.05 sec\n",
            "Epoch 968, Loss(train/val) 0.33975/0.23214. Took 0.06 sec\n",
            "Epoch 969, Loss(train/val) 0.34438/0.23023. Took 0.05 sec\n",
            "Epoch 970, Loss(train/val) 0.33932/0.23010. Took 0.05 sec\n",
            "Epoch 971, Loss(train/val) 0.34582/0.22975. Took 0.06 sec\n",
            "Epoch 972, Loss(train/val) 0.34027/0.22927. Took 0.05 sec\n",
            "Epoch 973, Loss(train/val) 0.33839/0.22820. Took 0.05 sec\n",
            "Epoch 974, Loss(train/val) 0.33332/0.22775. Took 0.05 sec\n",
            "Epoch 975, Loss(train/val) 0.33442/0.22685. Took 0.05 sec\n",
            "Epoch 976, Loss(train/val) 0.33378/0.22636. Took 0.05 sec\n",
            "Epoch 977, Loss(train/val) 0.33842/0.22706. Took 0.05 sec\n",
            "Epoch 978, Loss(train/val) 0.33225/0.22827. Took 0.06 sec\n",
            "Epoch 979, Loss(train/val) 0.33713/0.22875. Took 0.06 sec\n",
            "Epoch 980, Loss(train/val) 0.33798/0.22653. Took 0.05 sec\n",
            "Epoch 981, Loss(train/val) 0.33113/0.22601. Took 0.07 sec\n",
            "Epoch 982, Loss(train/val) 0.33303/0.22769. Took 0.05 sec\n",
            "Epoch 983, Loss(train/val) 0.33663/0.22884. Took 0.05 sec\n",
            "Epoch 984, Loss(train/val) 0.34033/0.22954. Took 0.05 sec\n",
            "Epoch 985, Loss(train/val) 0.33746/0.23068. Took 0.04 sec\n",
            "Epoch 986, Loss(train/val) 0.33283/0.23106. Took 0.04 sec\n",
            "Epoch 987, Loss(train/val) 0.33066/0.22850. Took 0.06 sec\n",
            "Epoch 988, Loss(train/val) 0.32973/0.22597. Took 0.05 sec\n",
            "Epoch 989, Loss(train/val) 0.33276/0.22534. Took 0.05 sec\n",
            "Epoch 990, Loss(train/val) 0.32987/0.22497. Took 0.04 sec\n",
            "Epoch 991, Loss(train/val) 0.32309/0.22487. Took 0.05 sec\n",
            "Epoch 992, Loss(train/val) 0.32876/0.22510. Took 0.05 sec\n",
            "Epoch 993, Loss(train/val) 0.32527/0.22563. Took 0.05 sec\n",
            "Epoch 994, Loss(train/val) 0.32535/0.22720. Took 0.05 sec\n",
            "Epoch 995, Loss(train/val) 0.33390/0.22667. Took 0.05 sec\n",
            "Epoch 996, Loss(train/val) 0.33618/0.22605. Took 0.05 sec\n",
            "Epoch 997, Loss(train/val) 0.33239/0.22486. Took 0.05 sec\n",
            "Epoch 998, Loss(train/val) 0.34072/0.22444. Took 0.05 sec\n",
            "Epoch 999, Loss(train/val) 0.32600/0.22463. Took 0.04 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var1 = 'lr'\n",
        "df = load_exp_result('exp3_lr_deep')\n",
        "\n",
        "plot_loss_variation(var1, df, sharey=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "onqYp3VzvLJW",
        "outputId": "01bec728-4dea-4a2f-f788-1b193f5850ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 923.375x216 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 666\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp3_lr_deep2\"\n",
        "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "args.batch_size = 193\n",
        "args.x_frames = 4\n",
        "args.y_frames = 4\n",
        "trainset = MyDataset(args.x_frames, args.y_frames, mydata, 0, int(0.6*len(mydata)))\n",
        "valset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.6*len(mydata)), int(0.8*len(mydata)))\n",
        "testset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.8*len(mydata)), int(len(mydata)))\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
        "\n",
        "args.input_dim = 1\n",
        "args.hid_dim = 16\n",
        "args.n_layers = 8\n",
        "\n",
        "args.l2 = 0.00001\n",
        "args.dropout = 0.0\n",
        "args.use_bn = True\n",
        "\n",
        "args.optim = 'Adam'\n",
        "args.lr = 0.0001\n",
        "args.epoch = 5000\n",
        "\n",
        "name_var1 = 'lr'\n",
        "list_var1 = [0.00003, 0.00004, 0.00005]\n",
        "\n",
        "for var1 in list_var1:\n",
        "    setattr(args, name_var1, var1)\n",
        "    print(args)\n",
        "                \n",
        "    setting, result = experiment(partition, deepcopy(args))\n",
        "    save_exp_result(setting, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9aW_l8PwLaa",
        "outputId": "ae25afbf-0454-455f-b6c3-4b2e7a1c4478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Epoch 0, Loss(train/val) 1.06803/0.35077. Took 0.05 sec\n",
            "Epoch 1, Loss(train/val) 1.07323/0.35086. Took 0.05 sec\n",
            "Epoch 2, Loss(train/val) 1.07331/0.35092. Took 0.04 sec\n",
            "Epoch 3, Loss(train/val) 1.06841/0.35097. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.06699/0.35100. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.06735/0.35102. Took 0.04 sec\n",
            "Epoch 6, Loss(train/val) 1.07085/0.35102. Took 0.05 sec\n",
            "Epoch 7, Loss(train/val) 1.07397/0.35102. Took 0.05 sec\n",
            "Epoch 8, Loss(train/val) 1.06935/0.35100. Took 0.05 sec\n",
            "Epoch 9, Loss(train/val) 1.07315/0.35098. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.07262/0.35095. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.07131/0.35092. Took 0.05 sec\n",
            "Epoch 12, Loss(train/val) 1.07178/0.35088. Took 0.06 sec\n",
            "Epoch 13, Loss(train/val) 1.07353/0.35084. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.07575/0.35080. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.07032/0.35076. Took 0.05 sec\n",
            "Epoch 16, Loss(train/val) 1.06560/0.35071. Took 0.05 sec\n",
            "Epoch 17, Loss(train/val) 1.07142/0.35066. Took 0.05 sec\n",
            "Epoch 18, Loss(train/val) 1.06735/0.35061. Took 0.05 sec\n",
            "Epoch 19, Loss(train/val) 1.06796/0.35057. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 1.07157/0.35052. Took 0.05 sec\n",
            "Epoch 21, Loss(train/val) 1.07392/0.35048. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 1.07418/0.35043. Took 0.05 sec\n",
            "Epoch 23, Loss(train/val) 1.07385/0.35039. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 1.06844/0.35035. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 1.07316/0.35032. Took 0.04 sec\n",
            "Epoch 26, Loss(train/val) 1.07108/0.35028. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.06759/0.35026. Took 0.05 sec\n",
            "Epoch 28, Loss(train/val) 1.06821/0.35023. Took 0.04 sec\n",
            "Epoch 29, Loss(train/val) 1.06603/0.35020. Took 0.04 sec\n",
            "Epoch 30, Loss(train/val) 1.07029/0.35018. Took 0.04 sec\n",
            "Epoch 31, Loss(train/val) 1.06069/0.35015. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 1.07435/0.35012. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 1.07376/0.35009. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 1.07406/0.35004. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 1.07356/0.34997. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 1.06199/0.34990. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.06749/0.34981. Took 0.06 sec\n",
            "Epoch 38, Loss(train/val) 1.06360/0.34972. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 1.07138/0.34964. Took 0.04 sec\n",
            "Epoch 40, Loss(train/val) 1.06777/0.34954. Took 0.05 sec\n",
            "Epoch 41, Loss(train/val) 1.07268/0.34943. Took 0.04 sec\n",
            "Epoch 42, Loss(train/val) 1.06700/0.34933. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 1.06882/0.34922. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 1.06409/0.34913. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 1.06727/0.34906. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 1.06378/0.34899. Took 0.04 sec\n",
            "Epoch 47, Loss(train/val) 1.06992/0.34894. Took 0.05 sec\n",
            "Epoch 48, Loss(train/val) 1.05238/0.34890. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 1.07220/0.34888. Took 0.05 sec\n",
            "Epoch 50, Loss(train/val) 1.06650/0.34886. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 1.06506/0.34884. Took 0.04 sec\n",
            "Epoch 52, Loss(train/val) 1.06015/0.34884. Took 0.05 sec\n",
            "Epoch 53, Loss(train/val) 1.06704/0.34887. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 1.06658/0.34890. Took 0.05 sec\n",
            "Epoch 55, Loss(train/val) 1.06937/0.34896. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 1.05552/0.34903. Took 0.04 sec\n",
            "Epoch 57, Loss(train/val) 1.06062/0.34912. Took 0.06 sec\n",
            "Epoch 58, Loss(train/val) 1.07133/0.34921. Took 0.04 sec\n",
            "Epoch 59, Loss(train/val) 1.06712/0.34928. Took 0.04 sec\n",
            "Epoch 60, Loss(train/val) 1.07108/0.34942. Took 0.05 sec\n",
            "Epoch 61, Loss(train/val) 1.06276/0.34960. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 1.06743/0.34979. Took 0.05 sec\n",
            "Epoch 63, Loss(train/val) 1.05619/0.35000. Took 0.04 sec\n",
            "Epoch 64, Loss(train/val) 1.06800/0.35024. Took 0.04 sec\n",
            "Epoch 65, Loss(train/val) 1.06941/0.35049. Took 0.04 sec\n",
            "Epoch 66, Loss(train/val) 1.06632/0.35076. Took 0.05 sec\n",
            "Epoch 67, Loss(train/val) 1.06139/0.35089. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 1.06885/0.35102. Took 0.04 sec\n",
            "Epoch 69, Loss(train/val) 1.06665/0.35114. Took 0.04 sec\n",
            "Epoch 70, Loss(train/val) 1.06557/0.35132. Took 0.05 sec\n",
            "Epoch 71, Loss(train/val) 1.06811/0.35144. Took 0.04 sec\n",
            "Epoch 72, Loss(train/val) 1.06233/0.35155. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 1.05625/0.35172. Took 0.04 sec\n",
            "Epoch 74, Loss(train/val) 1.05925/0.35190. Took 0.04 sec\n",
            "Epoch 75, Loss(train/val) 1.06317/0.35209. Took 0.04 sec\n",
            "Epoch 76, Loss(train/val) 1.06057/0.35222. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 1.06812/0.35241. Took 0.06 sec\n",
            "Epoch 78, Loss(train/val) 1.06544/0.35259. Took 0.04 sec\n",
            "Epoch 79, Loss(train/val) 1.06911/0.35278. Took 0.04 sec\n",
            "Epoch 80, Loss(train/val) 1.05888/0.35294. Took 0.04 sec\n",
            "Epoch 81, Loss(train/val) 1.06527/0.35310. Took 0.05 sec\n",
            "Epoch 82, Loss(train/val) 1.06284/0.35332. Took 0.05 sec\n",
            "Epoch 83, Loss(train/val) 1.06791/0.35334. Took 0.05 sec\n",
            "Epoch 84, Loss(train/val) 1.06531/0.35343. Took 0.04 sec\n",
            "Epoch 85, Loss(train/val) 1.06711/0.35355. Took 0.05 sec\n",
            "Epoch 86, Loss(train/val) 1.06527/0.35381. Took 0.04 sec\n",
            "Epoch 87, Loss(train/val) 1.06247/0.35410. Took 0.06 sec\n",
            "Epoch 88, Loss(train/val) 1.05529/0.35454. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 1.06174/0.35501. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 1.06705/0.35570. Took 0.04 sec\n",
            "Epoch 91, Loss(train/val) 1.05914/0.35654. Took 0.04 sec\n",
            "Epoch 92, Loss(train/val) 1.06418/0.35753. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 1.06636/0.35863. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 1.05616/0.35979. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 1.06127/0.36111. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 1.06030/0.36265. Took 0.04 sec\n",
            "Epoch 97, Loss(train/val) 1.06208/0.36430. Took 0.05 sec\n",
            "Epoch 98, Loss(train/val) 1.05699/0.36611. Took 0.06 sec\n",
            "Epoch 99, Loss(train/val) 1.05543/0.36808. Took 0.04 sec\n",
            "Epoch 100, Loss(train/val) 1.05932/0.37022. Took 0.04 sec\n",
            "Epoch 101, Loss(train/val) 1.05360/0.37250. Took 0.05 sec\n",
            "Epoch 102, Loss(train/val) 1.05911/0.37487. Took 0.06 sec\n",
            "Epoch 103, Loss(train/val) 1.04958/0.37735. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 1.06155/0.37984. Took 0.04 sec\n",
            "Epoch 105, Loss(train/val) 1.06278/0.38240. Took 0.05 sec\n",
            "Epoch 106, Loss(train/val) 1.06356/0.38497. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 1.05078/0.38778. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 1.05875/0.39158. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 1.05828/0.39805. Took 0.04 sec\n",
            "Epoch 110, Loss(train/val) 1.05730/0.40806. Took 0.05 sec\n",
            "Epoch 111, Loss(train/val) 1.05383/0.42054. Took 0.04 sec\n",
            "Epoch 112, Loss(train/val) 1.04524/0.43312. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 1.04593/0.44467. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 1.05275/0.45569. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 1.05562/0.46495. Took 0.05 sec\n",
            "Epoch 116, Loss(train/val) 1.04726/0.47083. Took 0.05 sec\n",
            "Epoch 117, Loss(train/val) 1.05413/0.47481. Took 0.05 sec\n",
            "Epoch 118, Loss(train/val) 1.05321/0.47620. Took 0.05 sec\n",
            "Epoch 119, Loss(train/val) 1.05370/0.47663. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 1.04844/0.47705. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 1.04344/0.47803. Took 0.05 sec\n",
            "Epoch 122, Loss(train/val) 1.04751/0.48123. Took 0.05 sec\n",
            "Epoch 123, Loss(train/val) 1.04737/0.48844. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 1.04337/0.49983. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 1.04875/0.51644. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 1.04201/0.53561. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 1.04312/0.55357. Took 0.04 sec\n",
            "Epoch 128, Loss(train/val) 1.03985/0.57014. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 1.04357/0.58446. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 1.03994/0.59672. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 1.03690/0.60688. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 1.02589/0.61570. Took 0.04 sec\n",
            "Epoch 133, Loss(train/val) 1.03577/0.62374. Took 0.05 sec\n",
            "Epoch 134, Loss(train/val) 1.03294/0.63043. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 1.03092/0.63498. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 1.02999/0.63784. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 1.03054/0.63908. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 1.01320/0.63915. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 1.01577/0.63883. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 1.01562/0.63824. Took 0.05 sec\n",
            "Epoch 141, Loss(train/val) 1.01028/0.63773. Took 0.05 sec\n",
            "Epoch 142, Loss(train/val) 1.01118/0.63508. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 1.00715/0.63047. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 1.00031/0.62510. Took 0.04 sec\n",
            "Epoch 145, Loss(train/val) 1.00032/0.61789. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 0.98477/0.61052. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 0.99600/0.60029. Took 0.04 sec\n",
            "Epoch 148, Loss(train/val) 0.98565/0.58924. Took 0.04 sec\n",
            "Epoch 149, Loss(train/val) 0.98600/0.58004. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 0.97904/0.56897. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 0.98062/0.55913. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 0.96836/0.54949. Took 0.04 sec\n",
            "Epoch 153, Loss(train/val) 0.97337/0.53877. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 0.97214/0.53238. Took 0.04 sec\n",
            "Epoch 155, Loss(train/val) 0.96487/0.52622. Took 0.04 sec\n",
            "Epoch 156, Loss(train/val) 0.95626/0.51944. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 0.95678/0.51438. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 0.95139/0.50983. Took 0.05 sec\n",
            "Epoch 159, Loss(train/val) 0.94681/0.50724. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 0.94423/0.50116. Took 0.05 sec\n",
            "Epoch 161, Loss(train/val) 0.94235/0.49717. Took 0.06 sec\n",
            "Epoch 162, Loss(train/val) 0.93862/0.49499. Took 0.04 sec\n",
            "Epoch 163, Loss(train/val) 0.93275/0.49375. Took 0.04 sec\n",
            "Epoch 164, Loss(train/val) 0.93079/0.49419. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 0.92655/0.49396. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.92411/0.49902. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 0.92327/0.51316. Took 0.05 sec\n",
            "Epoch 168, Loss(train/val) 0.91440/0.52948. Took 0.04 sec\n",
            "Epoch 169, Loss(train/val) 0.91610/0.54904. Took 0.04 sec\n",
            "Epoch 170, Loss(train/val) 0.90720/0.56744. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 0.90434/0.58096. Took 0.05 sec\n",
            "Epoch 172, Loss(train/val) 0.90146/0.59292. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 0.89981/0.60041. Took 0.04 sec\n",
            "Epoch 174, Loss(train/val) 0.89984/0.60062. Took 0.04 sec\n",
            "Epoch 175, Loss(train/val) 0.89288/0.59855. Took 0.06 sec\n",
            "Epoch 176, Loss(train/val) 0.89355/0.59937. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 0.87147/0.60128. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 0.88373/0.59849. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 0.88083/0.59511. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 0.88564/0.59118. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 0.87734/0.58769. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 0.87253/0.58356. Took 0.05 sec\n",
            "Epoch 183, Loss(train/val) 0.87597/0.57653. Took 0.05 sec\n",
            "Epoch 184, Loss(train/val) 0.87390/0.57093. Took 0.04 sec\n",
            "Epoch 185, Loss(train/val) 0.85370/0.56686. Took 0.04 sec\n",
            "Epoch 186, Loss(train/val) 0.85456/0.56095. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 0.85478/0.55309. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 0.85603/0.54274. Took 0.04 sec\n",
            "Epoch 189, Loss(train/val) 0.86403/0.53438. Took 0.04 sec\n",
            "Epoch 190, Loss(train/val) 0.85964/0.52819. Took 0.05 sec\n",
            "Epoch 191, Loss(train/val) 0.84738/0.52330. Took 0.06 sec\n",
            "Epoch 192, Loss(train/val) 0.85340/0.51512. Took 0.04 sec\n",
            "Epoch 193, Loss(train/val) 0.84530/0.50876. Took 0.05 sec\n",
            "Epoch 194, Loss(train/val) 0.85139/0.50408. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 0.84690/0.49689. Took 0.04 sec\n",
            "Epoch 196, Loss(train/val) 0.84730/0.49141. Took 0.05 sec\n",
            "Epoch 197, Loss(train/val) 0.84422/0.48639. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 0.83975/0.48047. Took 0.04 sec\n",
            "Epoch 199, Loss(train/val) 0.83506/0.47402. Took 0.04 sec\n",
            "Epoch 200, Loss(train/val) 0.83732/0.46872. Took 0.04 sec\n",
            "Epoch 201, Loss(train/val) 0.82993/0.46466. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 0.83048/0.46003. Took 0.05 sec\n",
            "Epoch 203, Loss(train/val) 0.82046/0.45643. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 0.82928/0.44964. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.81255/0.43943. Took 0.04 sec\n",
            "Epoch 206, Loss(train/val) 0.82502/0.42813. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 0.80791/0.41589. Took 0.05 sec\n",
            "Epoch 208, Loss(train/val) 0.81948/0.40474. Took 0.05 sec\n",
            "Epoch 209, Loss(train/val) 0.81429/0.39658. Took 0.05 sec\n",
            "Epoch 210, Loss(train/val) 0.81583/0.39049. Took 0.04 sec\n",
            "Epoch 211, Loss(train/val) 0.80742/0.38773. Took 0.05 sec\n",
            "Epoch 212, Loss(train/val) 0.81163/0.38612. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 0.80570/0.38279. Took 0.04 sec\n",
            "Epoch 214, Loss(train/val) 0.80027/0.38091. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.79413/0.37752. Took 0.04 sec\n",
            "Epoch 216, Loss(train/val) 0.79544/0.37088. Took 0.05 sec\n",
            "Epoch 217, Loss(train/val) 0.79847/0.36592. Took 0.05 sec\n",
            "Epoch 218, Loss(train/val) 0.79428/0.36236. Took 0.04 sec\n",
            "Epoch 219, Loss(train/val) 0.79666/0.35800. Took 0.04 sec\n",
            "Epoch 220, Loss(train/val) 0.78420/0.35709. Took 0.04 sec\n",
            "Epoch 221, Loss(train/val) 0.79004/0.35428. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 0.78540/0.35449. Took 0.05 sec\n",
            "Epoch 223, Loss(train/val) 0.79123/0.35300. Took 0.04 sec\n",
            "Epoch 224, Loss(train/val) 0.78463/0.35195. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 0.78210/0.34724. Took 0.05 sec\n",
            "Epoch 226, Loss(train/val) 0.77148/0.34526. Took 0.06 sec\n",
            "Epoch 227, Loss(train/val) 0.77206/0.34212. Took 0.04 sec\n",
            "Epoch 228, Loss(train/val) 0.77570/0.33912. Took 0.05 sec\n",
            "Epoch 229, Loss(train/val) 0.77775/0.33497. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 0.77895/0.33186. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 0.76681/0.33234. Took 0.05 sec\n",
            "Epoch 232, Loss(train/val) 0.75957/0.33019. Took 0.04 sec\n",
            "Epoch 233, Loss(train/val) 0.76157/0.33231. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 0.76945/0.33260. Took 0.04 sec\n",
            "Epoch 235, Loss(train/val) 0.76178/0.33260. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 0.75864/0.33061. Took 0.05 sec\n",
            "Epoch 237, Loss(train/val) 0.75855/0.32689. Took 0.05 sec\n",
            "Epoch 238, Loss(train/val) 0.74714/0.32567. Took 0.06 sec\n",
            "Epoch 239, Loss(train/val) 0.75004/0.32433. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 0.75084/0.32367. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 0.74735/0.32558. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 0.73381/0.32577. Took 0.05 sec\n",
            "Epoch 243, Loss(train/val) 0.73622/0.32505. Took 0.05 sec\n",
            "Epoch 244, Loss(train/val) 0.73524/0.31996. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.71959/0.31542. Took 0.06 sec\n",
            "Epoch 246, Loss(train/val) 0.73736/0.31522. Took 0.05 sec\n",
            "Epoch 247, Loss(train/val) 0.73785/0.31570. Took 0.04 sec\n",
            "Epoch 248, Loss(train/val) 0.72755/0.31573. Took 0.04 sec\n",
            "Epoch 249, Loss(train/val) 0.72965/0.31447. Took 0.04 sec\n",
            "Epoch 250, Loss(train/val) 0.72488/0.31446. Took 0.05 sec\n",
            "Epoch 251, Loss(train/val) 0.72497/0.31258. Took 0.04 sec\n",
            "Epoch 252, Loss(train/val) 0.72735/0.31424. Took 0.04 sec\n",
            "Epoch 253, Loss(train/val) 0.72114/0.30959. Took 0.05 sec\n",
            "Epoch 254, Loss(train/val) 0.71777/0.30472. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.71730/0.30353. Took 0.05 sec\n",
            "Epoch 256, Loss(train/val) 0.71719/0.30303. Took 0.04 sec\n",
            "Epoch 257, Loss(train/val) 0.71044/0.30438. Took 0.05 sec\n",
            "Epoch 258, Loss(train/val) 0.70979/0.30362. Took 0.05 sec\n",
            "Epoch 259, Loss(train/val) 0.71300/0.30194. Took 0.04 sec\n",
            "Epoch 260, Loss(train/val) 0.71003/0.29917. Took 0.05 sec\n",
            "Epoch 261, Loss(train/val) 0.70793/0.29749. Took 0.04 sec\n",
            "Epoch 262, Loss(train/val) 0.70020/0.29713. Took 0.04 sec\n",
            "Epoch 263, Loss(train/val) 0.70164/0.29846. Took 0.04 sec\n",
            "Epoch 264, Loss(train/val) 0.70283/0.30083. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.70455/0.29941. Took 0.05 sec\n",
            "Epoch 266, Loss(train/val) 0.68984/0.29680. Took 0.04 sec\n",
            "Epoch 267, Loss(train/val) 0.69401/0.29573. Took 0.06 sec\n",
            "Epoch 268, Loss(train/val) 0.69235/0.29420. Took 0.05 sec\n",
            "Epoch 269, Loss(train/val) 0.69252/0.29391. Took 0.04 sec\n",
            "Epoch 270, Loss(train/val) 0.68738/0.28953. Took 0.05 sec\n",
            "Epoch 271, Loss(train/val) 0.69206/0.28981. Took 0.05 sec\n",
            "Epoch 272, Loss(train/val) 0.68334/0.29089. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 0.67705/0.28747. Took 0.04 sec\n",
            "Epoch 274, Loss(train/val) 0.69007/0.28754. Took 0.05 sec\n",
            "Epoch 275, Loss(train/val) 0.68328/0.28628. Took 0.05 sec\n",
            "Epoch 276, Loss(train/val) 0.68374/0.28498. Took 0.04 sec\n",
            "Epoch 277, Loss(train/val) 0.67878/0.28440. Took 0.04 sec\n",
            "Epoch 278, Loss(train/val) 0.67495/0.28499. Took 0.05 sec\n",
            "Epoch 279, Loss(train/val) 0.67638/0.28573. Took 0.05 sec\n",
            "Epoch 280, Loss(train/val) 0.65537/0.28385. Took 0.05 sec\n",
            "Epoch 281, Loss(train/val) 0.67195/0.28459. Took 0.04 sec\n",
            "Epoch 282, Loss(train/val) 0.65699/0.28422. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 0.67041/0.28408. Took 0.04 sec\n",
            "Epoch 284, Loss(train/val) 0.66440/0.28402. Took 0.05 sec\n",
            "Epoch 285, Loss(train/val) 0.66393/0.28375. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.66268/0.28074. Took 0.05 sec\n",
            "Epoch 287, Loss(train/val) 0.66283/0.28063. Took 0.04 sec\n",
            "Epoch 288, Loss(train/val) 0.66294/0.27948. Took 0.04 sec\n",
            "Epoch 289, Loss(train/val) 0.65044/0.27638. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.65652/0.27492. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.65522/0.27617. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.65578/0.27645. Took 0.05 sec\n",
            "Epoch 293, Loss(train/val) 0.65249/0.27627. Took 0.05 sec\n",
            "Epoch 294, Loss(train/val) 0.63559/0.27254. Took 0.04 sec\n",
            "Epoch 295, Loss(train/val) 0.64745/0.27038. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 0.64054/0.27099. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.64798/0.27403. Took 0.04 sec\n",
            "Epoch 298, Loss(train/val) 0.63575/0.27383. Took 0.05 sec\n",
            "Epoch 299, Loss(train/val) 0.64255/0.27182. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.63206/0.27093. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.63094/0.27111. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.64573/0.27189. Took 0.05 sec\n",
            "Epoch 303, Loss(train/val) 0.63327/0.27164. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 0.62148/0.27221. Took 0.05 sec\n",
            "Epoch 305, Loss(train/val) 0.63091/0.27018. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 0.63115/0.27054. Took 0.05 sec\n",
            "Epoch 307, Loss(train/val) 0.62470/0.26840. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.63457/0.26809. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.62207/0.26743. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.62667/0.26485. Took 0.07 sec\n",
            "Epoch 311, Loss(train/val) 0.62323/0.26414. Took 0.05 sec\n",
            "Epoch 312, Loss(train/val) 0.61240/0.26528. Took 0.04 sec\n",
            "Epoch 313, Loss(train/val) 0.61972/0.26520. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.62234/0.26357. Took 0.04 sec\n",
            "Epoch 315, Loss(train/val) 0.61654/0.26267. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.60033/0.26322. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.61360/0.26555. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.61147/0.26589. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.60990/0.26710. Took 0.05 sec\n",
            "Epoch 320, Loss(train/val) 0.61504/0.26734. Took 0.06 sec\n",
            "Epoch 321, Loss(train/val) 0.60208/0.26886. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.59963/0.26755. Took 0.05 sec\n",
            "Epoch 323, Loss(train/val) 0.60222/0.26608. Took 0.05 sec\n",
            "Epoch 324, Loss(train/val) 0.60794/0.26218. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.60274/0.26037. Took 0.05 sec\n",
            "Epoch 326, Loss(train/val) 0.59988/0.25937. Took 0.05 sec\n",
            "Epoch 327, Loss(train/val) 0.59423/0.25792. Took 0.05 sec\n",
            "Epoch 328, Loss(train/val) 0.59094/0.25774. Took 0.06 sec\n",
            "Epoch 329, Loss(train/val) 0.59354/0.25759. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.59222/0.25735. Took 0.06 sec\n",
            "Epoch 331, Loss(train/val) 0.58460/0.25463. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 0.57764/0.25581. Took 0.06 sec\n",
            "Epoch 333, Loss(train/val) 0.58607/0.25623. Took 0.05 sec\n",
            "Epoch 334, Loss(train/val) 0.58768/0.25736. Took 0.05 sec\n",
            "Epoch 335, Loss(train/val) 0.58241/0.25840. Took 0.05 sec\n",
            "Epoch 336, Loss(train/val) 0.56799/0.25929. Took 0.06 sec\n",
            "Epoch 337, Loss(train/val) 0.57894/0.25957. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.58104/0.26085. Took 0.05 sec\n",
            "Epoch 339, Loss(train/val) 0.57923/0.26075. Took 0.05 sec\n",
            "Epoch 340, Loss(train/val) 0.57878/0.26072. Took 0.05 sec\n",
            "Epoch 341, Loss(train/val) 0.57236/0.25876. Took 0.05 sec\n",
            "Epoch 342, Loss(train/val) 0.57471/0.25751. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.56708/0.25709. Took 0.05 sec\n",
            "Epoch 344, Loss(train/val) 0.55878/0.25659. Took 0.05 sec\n",
            "Epoch 345, Loss(train/val) 0.55930/0.25814. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.56772/0.25899. Took 0.04 sec\n",
            "Epoch 347, Loss(train/val) 0.56834/0.26008. Took 0.04 sec\n",
            "Epoch 348, Loss(train/val) 0.56411/0.25915. Took 0.04 sec\n",
            "Epoch 349, Loss(train/val) 0.55925/0.25615. Took 0.05 sec\n",
            "Epoch 350, Loss(train/val) 0.56073/0.25360. Took 0.06 sec\n",
            "Epoch 351, Loss(train/val) 0.56128/0.25355. Took 0.05 sec\n",
            "Epoch 352, Loss(train/val) 0.56093/0.25333. Took 0.04 sec\n",
            "Epoch 353, Loss(train/val) 0.55314/0.25101. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 0.56119/0.25268. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 0.55646/0.25257. Took 0.05 sec\n",
            "Epoch 356, Loss(train/val) 0.54742/0.25092. Took 0.05 sec\n",
            "Epoch 357, Loss(train/val) 0.55528/0.25081. Took 0.04 sec\n",
            "Epoch 358, Loss(train/val) 0.54963/0.25086. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 0.53900/0.25002. Took 0.05 sec\n",
            "Epoch 360, Loss(train/val) 0.53985/0.25200. Took 0.06 sec\n",
            "Epoch 361, Loss(train/val) 0.55437/0.25051. Took 0.05 sec\n",
            "Epoch 362, Loss(train/val) 0.54783/0.24989. Took 0.05 sec\n",
            "Epoch 363, Loss(train/val) 0.53605/0.25062. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 0.54218/0.25166. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.53843/0.25192. Took 0.06 sec\n",
            "Epoch 366, Loss(train/val) 0.53752/0.25092. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 0.53601/0.25013. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.53353/0.24774. Took 0.05 sec\n",
            "Epoch 369, Loss(train/val) 0.53524/0.24767. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.52862/0.24672. Took 0.06 sec\n",
            "Epoch 371, Loss(train/val) 0.53489/0.24503. Took 0.06 sec\n",
            "Epoch 372, Loss(train/val) 0.52989/0.24568. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 0.53433/0.24790. Took 0.05 sec\n",
            "Epoch 374, Loss(train/val) 0.52896/0.25085. Took 0.04 sec\n",
            "Epoch 375, Loss(train/val) 0.52798/0.24961. Took 0.05 sec\n",
            "Epoch 376, Loss(train/val) 0.52637/0.24810. Took 0.04 sec\n",
            "Epoch 377, Loss(train/val) 0.51786/0.24652. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.51397/0.24574. Took 0.04 sec\n",
            "Epoch 379, Loss(train/val) 0.51243/0.24858. Took 0.04 sec\n",
            "Epoch 380, Loss(train/val) 0.52394/0.25101. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 0.51832/0.25086. Took 0.05 sec\n",
            "Epoch 382, Loss(train/val) 0.51551/0.24962. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.51476/0.24960. Took 0.04 sec\n",
            "Epoch 384, Loss(train/val) 0.51336/0.24585. Took 0.04 sec\n",
            "Epoch 385, Loss(train/val) 0.50382/0.24496. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.50474/0.24466. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.51319/0.24426. Took 0.04 sec\n",
            "Epoch 388, Loss(train/val) 0.50717/0.24308. Took 0.05 sec\n",
            "Epoch 389, Loss(train/val) 0.50629/0.24218. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.51029/0.24342. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.50553/0.24588. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.49500/0.24514. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.49732/0.24310. Took 0.04 sec\n",
            "Epoch 394, Loss(train/val) 0.50370/0.24300. Took 0.04 sec\n",
            "Epoch 395, Loss(train/val) 0.50005/0.24191. Took 0.05 sec\n",
            "Epoch 396, Loss(train/val) 0.50363/0.23956. Took 0.05 sec\n",
            "Epoch 397, Loss(train/val) 0.49976/0.23837. Took 0.05 sec\n",
            "Epoch 398, Loss(train/val) 0.49449/0.23909. Took 0.05 sec\n",
            "Epoch 399, Loss(train/val) 0.49410/0.24236. Took 0.05 sec\n",
            "Epoch 400, Loss(train/val) 0.49158/0.24251. Took 0.06 sec\n",
            "Epoch 401, Loss(train/val) 0.50247/0.24131. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.49099/0.24243. Took 0.05 sec\n",
            "Epoch 403, Loss(train/val) 0.48956/0.24115. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 0.49168/0.23923. Took 0.04 sec\n",
            "Epoch 405, Loss(train/val) 0.49594/0.23775. Took 0.05 sec\n",
            "Epoch 406, Loss(train/val) 0.48977/0.23637. Took 0.04 sec\n",
            "Epoch 407, Loss(train/val) 0.48594/0.23660. Took 0.05 sec\n",
            "Epoch 408, Loss(train/val) 0.47979/0.23781. Took 0.05 sec\n",
            "Epoch 409, Loss(train/val) 0.48004/0.23980. Took 0.04 sec\n",
            "Epoch 410, Loss(train/val) 0.48652/0.24240. Took 0.05 sec\n",
            "Epoch 411, Loss(train/val) 0.48136/0.24248. Took 0.05 sec\n",
            "Epoch 412, Loss(train/val) 0.48070/0.24108. Took 0.04 sec\n",
            "Epoch 413, Loss(train/val) 0.48066/0.24276. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.47892/0.24395. Took 0.05 sec\n",
            "Epoch 415, Loss(train/val) 0.47919/0.24092. Took 0.06 sec\n",
            "Epoch 416, Loss(train/val) 0.47995/0.24047. Took 0.05 sec\n",
            "Epoch 417, Loss(train/val) 0.47119/0.24192. Took 0.05 sec\n",
            "Epoch 418, Loss(train/val) 0.47407/0.24196. Took 0.04 sec\n",
            "Epoch 419, Loss(train/val) 0.46908/0.24016. Took 0.05 sec\n",
            "Epoch 420, Loss(train/val) 0.47364/0.23819. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.46710/0.23613. Took 0.05 sec\n",
            "Epoch 422, Loss(train/val) 0.47420/0.23700. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.47225/0.23665. Took 0.04 sec\n",
            "Epoch 424, Loss(train/val) 0.47439/0.23709. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.45870/0.23935. Took 0.05 sec\n",
            "Epoch 426, Loss(train/val) 0.46689/0.23923. Took 0.05 sec\n",
            "Epoch 427, Loss(train/val) 0.46908/0.23689. Took 0.05 sec\n",
            "Epoch 428, Loss(train/val) 0.46356/0.23524. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.45103/0.23561. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.46889/0.23607. Took 0.05 sec\n",
            "Epoch 431, Loss(train/val) 0.46321/0.23773. Took 0.05 sec\n",
            "Epoch 432, Loss(train/val) 0.45484/0.23757. Took 0.05 sec\n",
            "Epoch 433, Loss(train/val) 0.45962/0.23680. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.45610/0.23402. Took 0.06 sec\n",
            "Epoch 435, Loss(train/val) 0.45325/0.23380. Took 0.05 sec\n",
            "Epoch 436, Loss(train/val) 0.46037/0.23371. Took 0.04 sec\n",
            "Epoch 437, Loss(train/val) 0.45079/0.23357. Took 0.04 sec\n",
            "Epoch 438, Loss(train/val) 0.45657/0.23389. Took 0.05 sec\n",
            "Epoch 439, Loss(train/val) 0.45134/0.23514. Took 0.05 sec\n",
            "Epoch 440, Loss(train/val) 0.45517/0.23703. Took 0.05 sec\n",
            "Epoch 441, Loss(train/val) 0.45229/0.23828. Took 0.04 sec\n",
            "Epoch 442, Loss(train/val) 0.44628/0.23683. Took 0.04 sec\n",
            "Epoch 443, Loss(train/val) 0.44723/0.23686. Took 0.04 sec\n",
            "Epoch 444, Loss(train/val) 0.44591/0.23783. Took 0.06 sec\n",
            "Epoch 445, Loss(train/val) 0.45044/0.23927. Took 0.04 sec\n",
            "Epoch 446, Loss(train/val) 0.43924/0.24029. Took 0.04 sec\n",
            "Epoch 447, Loss(train/val) 0.43380/0.23796. Took 0.04 sec\n",
            "Epoch 448, Loss(train/val) 0.45277/0.23989. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 0.44991/0.23952. Took 0.05 sec\n",
            "Epoch 450, Loss(train/val) 0.44231/0.23852. Took 0.04 sec\n",
            "Epoch 451, Loss(train/val) 0.44246/0.23433. Took 0.05 sec\n",
            "Epoch 452, Loss(train/val) 0.45429/0.23388. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.44226/0.23381. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.43415/0.23371. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 0.44775/0.23382. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.43877/0.23284. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.43113/0.23299. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.43946/0.23283. Took 0.04 sec\n",
            "Epoch 459, Loss(train/val) 0.44027/0.23268. Took 0.05 sec\n",
            "Epoch 460, Loss(train/val) 0.44443/0.23255. Took 0.04 sec\n",
            "Epoch 461, Loss(train/val) 0.43133/0.23312. Took 0.05 sec\n",
            "Epoch 462, Loss(train/val) 0.43704/0.23416. Took 0.04 sec\n",
            "Epoch 463, Loss(train/val) 0.42202/0.23291. Took 0.04 sec\n",
            "Epoch 464, Loss(train/val) 0.43213/0.23309. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.42950/0.23299. Took 0.05 sec\n",
            "Epoch 466, Loss(train/val) 0.43095/0.23335. Took 0.04 sec\n",
            "Epoch 467, Loss(train/val) 0.42203/0.23582. Took 0.04 sec\n",
            "Epoch 468, Loss(train/val) 0.42964/0.24113. Took 0.04 sec\n",
            "Epoch 469, Loss(train/val) 0.42765/0.24084. Took 0.05 sec\n",
            "Epoch 470, Loss(train/val) 0.42398/0.24162. Took 0.05 sec\n",
            "Epoch 471, Loss(train/val) 0.42310/0.24132. Took 0.05 sec\n",
            "Epoch 472, Loss(train/val) 0.42014/0.24453. Took 0.04 sec\n",
            "Epoch 473, Loss(train/val) 0.43527/0.24032. Took 0.04 sec\n",
            "Epoch 474, Loss(train/val) 0.42630/0.23741. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 0.42990/0.23357. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.42680/0.23224. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.42281/0.23315. Took 0.05 sec\n",
            "Epoch 478, Loss(train/val) 0.41798/0.23241. Took 0.06 sec\n",
            "Epoch 479, Loss(train/val) 0.42893/0.23217. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.41880/0.23211. Took 0.05 sec\n",
            "Epoch 481, Loss(train/val) 0.42308/0.23219. Took 0.05 sec\n",
            "Epoch 482, Loss(train/val) 0.41965/0.23250. Took 0.05 sec\n",
            "Epoch 483, Loss(train/val) 0.42125/0.23430. Took 0.05 sec\n",
            "Epoch 484, Loss(train/val) 0.42030/0.23691. Took 0.06 sec\n",
            "Epoch 485, Loss(train/val) 0.41746/0.23695. Took 0.05 sec\n",
            "Epoch 486, Loss(train/val) 0.42430/0.23525. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.42609/0.23355. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.42227/0.23298. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.41590/0.23239. Took 0.06 sec\n",
            "Epoch 490, Loss(train/val) 0.41513/0.23271. Took 0.05 sec\n",
            "Epoch 491, Loss(train/val) 0.42424/0.23224. Took 0.05 sec\n",
            "Epoch 492, Loss(train/val) 0.41361/0.23304. Took 0.04 sec\n",
            "Epoch 493, Loss(train/val) 0.41100/0.23198. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.41145/0.23202. Took 0.05 sec\n",
            "Epoch 495, Loss(train/val) 0.41413/0.23199. Took 0.05 sec\n",
            "Epoch 496, Loss(train/val) 0.41120/0.23231. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 0.40890/0.23250. Took 0.05 sec\n",
            "Epoch 498, Loss(train/val) 0.41331/0.23396. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.40926/0.23234. Took 0.06 sec\n",
            "Epoch 500, Loss(train/val) 0.41270/0.23213. Took 0.05 sec\n",
            "Epoch 501, Loss(train/val) 0.40178/0.23326. Took 0.05 sec\n",
            "Epoch 502, Loss(train/val) 0.40465/0.23310. Took 0.05 sec\n",
            "Epoch 503, Loss(train/val) 0.41010/0.23463. Took 0.05 sec\n",
            "Epoch 504, Loss(train/val) 0.41251/0.23464. Took 0.05 sec\n",
            "Epoch 505, Loss(train/val) 0.40808/0.23311. Took 0.05 sec\n",
            "Epoch 506, Loss(train/val) 0.41934/0.23304. Took 0.05 sec\n",
            "Epoch 507, Loss(train/val) 0.40379/0.23247. Took 0.05 sec\n",
            "Epoch 508, Loss(train/val) 0.41065/0.23179. Took 0.05 sec\n",
            "Epoch 509, Loss(train/val) 0.40344/0.23178. Took 0.05 sec\n",
            "Epoch 510, Loss(train/val) 0.41696/0.23179. Took 0.05 sec\n",
            "Epoch 511, Loss(train/val) 0.39855/0.23261. Took 0.05 sec\n",
            "Epoch 512, Loss(train/val) 0.40816/0.23321. Took 0.05 sec\n",
            "Epoch 513, Loss(train/val) 0.40456/0.23318. Took 0.05 sec\n",
            "Epoch 514, Loss(train/val) 0.40608/0.23400. Took 0.06 sec\n",
            "Epoch 515, Loss(train/val) 0.40901/0.23301. Took 0.05 sec\n",
            "Epoch 516, Loss(train/val) 0.39900/0.23261. Took 0.05 sec\n",
            "Epoch 517, Loss(train/val) 0.40051/0.23354. Took 0.05 sec\n",
            "Epoch 518, Loss(train/val) 0.40721/0.23461. Took 0.04 sec\n",
            "Epoch 519, Loss(train/val) 0.39862/0.23390. Took 0.05 sec\n",
            "Epoch 520, Loss(train/val) 0.39843/0.23332. Took 0.05 sec\n",
            "Epoch 521, Loss(train/val) 0.40117/0.23303. Took 0.05 sec\n",
            "Epoch 522, Loss(train/val) 0.39729/0.23155. Took 0.04 sec\n",
            "Epoch 523, Loss(train/val) 0.39825/0.23141. Took 0.04 sec\n",
            "Epoch 524, Loss(train/val) 0.39223/0.23215. Took 0.05 sec\n",
            "Epoch 525, Loss(train/val) 0.40522/0.23322. Took 0.05 sec\n",
            "Epoch 526, Loss(train/val) 0.39478/0.23252. Took 0.05 sec\n",
            "Epoch 527, Loss(train/val) 0.39218/0.23183. Took 0.05 sec\n",
            "Epoch 528, Loss(train/val) 0.40438/0.23158. Took 0.05 sec\n",
            "Epoch 529, Loss(train/val) 0.39791/0.23133. Took 0.05 sec\n",
            "Epoch 530, Loss(train/val) 0.40437/0.23143. Took 0.05 sec\n",
            "Epoch 531, Loss(train/val) 0.40293/0.23195. Took 0.05 sec\n",
            "Epoch 532, Loss(train/val) 0.39562/0.23225. Took 0.05 sec\n",
            "Epoch 533, Loss(train/val) 0.39388/0.23376. Took 0.05 sec\n",
            "Epoch 534, Loss(train/val) 0.39504/0.23349. Took 0.06 sec\n",
            "Epoch 535, Loss(train/val) 0.38813/0.23443. Took 0.05 sec\n",
            "Epoch 536, Loss(train/val) 0.39381/0.23347. Took 0.04 sec\n",
            "Epoch 537, Loss(train/val) 0.40128/0.23341. Took 0.05 sec\n",
            "Epoch 538, Loss(train/val) 0.38752/0.23279. Took 0.05 sec\n",
            "Epoch 539, Loss(train/val) 0.39919/0.23193. Took 0.05 sec\n",
            "Epoch 540, Loss(train/val) 0.39964/0.23175. Took 0.05 sec\n",
            "Epoch 541, Loss(train/val) 0.39331/0.23163. Took 0.06 sec\n",
            "Epoch 542, Loss(train/val) 0.39752/0.23139. Took 0.05 sec\n",
            "Epoch 543, Loss(train/val) 0.39055/0.23131. Took 0.04 sec\n",
            "Epoch 544, Loss(train/val) 0.39007/0.23123. Took 0.05 sec\n",
            "Epoch 545, Loss(train/val) 0.38245/0.23224. Took 0.05 sec\n",
            "Epoch 546, Loss(train/val) 0.38541/0.23258. Took 0.05 sec\n",
            "Epoch 547, Loss(train/val) 0.38916/0.23267. Took 0.05 sec\n",
            "Epoch 548, Loss(train/val) 0.39291/0.23185. Took 0.05 sec\n",
            "Epoch 549, Loss(train/val) 0.38791/0.23143. Took 0.05 sec\n",
            "Epoch 550, Loss(train/val) 0.38544/0.23146. Took 0.04 sec\n",
            "Epoch 551, Loss(train/val) 0.37254/0.23131. Took 0.04 sec\n",
            "Epoch 552, Loss(train/val) 0.38240/0.23152. Took 0.05 sec\n",
            "Epoch 553, Loss(train/val) 0.39552/0.23164. Took 0.05 sec\n",
            "Epoch 554, Loss(train/val) 0.39368/0.23187. Took 0.05 sec\n",
            "Epoch 555, Loss(train/val) 0.38523/0.23139. Took 0.04 sec\n",
            "Epoch 556, Loss(train/val) 0.38729/0.23146. Took 0.04 sec\n",
            "Epoch 557, Loss(train/val) 0.38156/0.23265. Took 0.05 sec\n",
            "Epoch 558, Loss(train/val) 0.38154/0.23128. Took 0.04 sec\n",
            "Epoch 559, Loss(train/val) 0.37828/0.23184. Took 0.05 sec\n",
            "Epoch 560, Loss(train/val) 0.39252/0.23318. Took 0.04 sec\n",
            "Epoch 561, Loss(train/val) 0.38059/0.23419. Took 0.05 sec\n",
            "Epoch 562, Loss(train/val) 0.38312/0.23106. Took 0.05 sec\n",
            "Epoch 563, Loss(train/val) 0.38304/0.23097. Took 0.05 sec\n",
            "Epoch 564, Loss(train/val) 0.38159/0.23127. Took 0.05 sec\n",
            "Epoch 565, Loss(train/val) 0.38219/0.23191. Took 0.04 sec\n",
            "Epoch 566, Loss(train/val) 0.38713/0.23135. Took 0.05 sec\n",
            "Epoch 567, Loss(train/val) 0.37523/0.23147. Took 0.05 sec\n",
            "Epoch 568, Loss(train/val) 0.38167/0.23089. Took 0.05 sec\n",
            "Epoch 569, Loss(train/val) 0.37027/0.23094. Took 0.05 sec\n",
            "Epoch 570, Loss(train/val) 0.37838/0.23114. Took 0.04 sec\n",
            "Epoch 571, Loss(train/val) 0.37800/0.23135. Took 0.04 sec\n",
            "Epoch 572, Loss(train/val) 0.37695/0.23164. Took 0.05 sec\n",
            "Epoch 573, Loss(train/val) 0.36844/0.23210. Took 0.05 sec\n",
            "Epoch 574, Loss(train/val) 0.37285/0.23327. Took 0.05 sec\n",
            "Epoch 575, Loss(train/val) 0.37640/0.23200. Took 0.05 sec\n",
            "Epoch 576, Loss(train/val) 0.37273/0.23152. Took 0.05 sec\n",
            "Epoch 577, Loss(train/val) 0.37274/0.23092. Took 0.04 sec\n",
            "Epoch 578, Loss(train/val) 0.37737/0.23141. Took 0.05 sec\n",
            "Epoch 579, Loss(train/val) 0.38263/0.23146. Took 0.05 sec\n",
            "Epoch 580, Loss(train/val) 0.36471/0.23246. Took 0.05 sec\n",
            "Epoch 581, Loss(train/val) 0.37794/0.23270. Took 0.05 sec\n",
            "Epoch 582, Loss(train/val) 0.37684/0.23259. Took 0.05 sec\n",
            "Epoch 583, Loss(train/val) 0.37435/0.23508. Took 0.06 sec\n",
            "Epoch 584, Loss(train/val) 0.37790/0.23716. Took 0.06 sec\n",
            "Epoch 585, Loss(train/val) 0.37183/0.23612. Took 0.05 sec\n",
            "Epoch 586, Loss(train/val) 0.38403/0.23142. Took 0.05 sec\n",
            "Epoch 587, Loss(train/val) 0.37833/0.23037. Took 0.04 sec\n",
            "Epoch 588, Loss(train/val) 0.37706/0.23079. Took 0.05 sec\n",
            "Epoch 589, Loss(train/val) 0.37543/0.23045. Took 0.04 sec\n",
            "Epoch 590, Loss(train/val) 0.37413/0.23067. Took 0.04 sec\n",
            "Epoch 591, Loss(train/val) 0.38917/0.23095. Took 0.04 sec\n",
            "Epoch 592, Loss(train/val) 0.38497/0.23020. Took 0.04 sec\n",
            "Epoch 593, Loss(train/val) 0.37362/0.23010. Took 0.06 sec\n",
            "Epoch 594, Loss(train/val) 0.37820/0.23018. Took 0.05 sec\n",
            "Epoch 595, Loss(train/val) 0.36782/0.23031. Took 0.05 sec\n",
            "Epoch 596, Loss(train/val) 0.36368/0.23037. Took 0.04 sec\n",
            "Epoch 597, Loss(train/val) 0.36988/0.23014. Took 0.05 sec\n",
            "Epoch 598, Loss(train/val) 0.36331/0.23114. Took 0.05 sec\n",
            "Epoch 599, Loss(train/val) 0.37145/0.23093. Took 0.05 sec\n",
            "Epoch 600, Loss(train/val) 0.37308/0.23148. Took 0.04 sec\n",
            "Epoch 601, Loss(train/val) 0.36029/0.23008. Took 0.05 sec\n",
            "Epoch 602, Loss(train/val) 0.37611/0.23016. Took 0.05 sec\n",
            "Epoch 603, Loss(train/val) 0.36942/0.22980. Took 0.05 sec\n",
            "Epoch 604, Loss(train/val) 0.37038/0.23009. Took 0.04 sec\n",
            "Epoch 605, Loss(train/val) 0.36433/0.23034. Took 0.05 sec\n",
            "Epoch 606, Loss(train/val) 0.36912/0.23178. Took 0.05 sec\n",
            "Epoch 607, Loss(train/val) 0.36202/0.23505. Took 0.05 sec\n",
            "Epoch 608, Loss(train/val) 0.36684/0.23427. Took 0.05 sec\n",
            "Epoch 609, Loss(train/val) 0.37108/0.23320. Took 0.05 sec\n",
            "Epoch 610, Loss(train/val) 0.37239/0.23159. Took 0.05 sec\n",
            "Epoch 611, Loss(train/val) 0.36481/0.23276. Took 0.04 sec\n",
            "Epoch 612, Loss(train/val) 0.37144/0.23065. Took 0.04 sec\n",
            "Epoch 613, Loss(train/val) 0.35853/0.23001. Took 0.05 sec\n",
            "Epoch 614, Loss(train/val) 0.36763/0.22939. Took 0.05 sec\n",
            "Epoch 615, Loss(train/val) 0.36379/0.22930. Took 0.05 sec\n",
            "Epoch 616, Loss(train/val) 0.37207/0.22938. Took 0.05 sec\n",
            "Epoch 617, Loss(train/val) 0.37552/0.22937. Took 0.05 sec\n",
            "Epoch 618, Loss(train/val) 0.36718/0.22966. Took 0.05 sec\n",
            "Epoch 619, Loss(train/val) 0.37246/0.22927. Took 0.05 sec\n",
            "Epoch 620, Loss(train/val) 0.37444/0.22934. Took 0.04 sec\n",
            "Epoch 621, Loss(train/val) 0.36713/0.22996. Took 0.04 sec\n",
            "Epoch 622, Loss(train/val) 0.35936/0.22957. Took 0.06 sec\n",
            "Epoch 623, Loss(train/val) 0.36739/0.22998. Took 0.06 sec\n",
            "Epoch 624, Loss(train/val) 0.37218/0.23000. Took 0.05 sec\n",
            "Epoch 625, Loss(train/val) 0.36024/0.22952. Took 0.05 sec\n",
            "Epoch 626, Loss(train/val) 0.37235/0.22995. Took 0.05 sec\n",
            "Epoch 627, Loss(train/val) 0.36727/0.22928. Took 0.04 sec\n",
            "Epoch 628, Loss(train/val) 0.36351/0.22906. Took 0.05 sec\n",
            "Epoch 629, Loss(train/val) 0.36461/0.22867. Took 0.05 sec\n",
            "Epoch 630, Loss(train/val) 0.36892/0.22882. Took 0.05 sec\n",
            "Epoch 631, Loss(train/val) 0.36908/0.22992. Took 0.05 sec\n",
            "Epoch 632, Loss(train/val) 0.35740/0.23226. Took 0.05 sec\n",
            "Epoch 633, Loss(train/val) 0.36832/0.23381. Took 0.06 sec\n",
            "Epoch 634, Loss(train/val) 0.35622/0.23240. Took 0.05 sec\n",
            "Epoch 635, Loss(train/val) 0.36057/0.22939. Took 0.05 sec\n",
            "Epoch 636, Loss(train/val) 0.35757/0.22846. Took 0.05 sec\n",
            "Epoch 637, Loss(train/val) 0.36397/0.22863. Took 0.05 sec\n",
            "Epoch 638, Loss(train/val) 0.36032/0.22875. Took 0.06 sec\n",
            "Epoch 639, Loss(train/val) 0.36069/0.22917. Took 0.05 sec\n",
            "Epoch 640, Loss(train/val) 0.36480/0.22896. Took 0.05 sec\n",
            "Epoch 641, Loss(train/val) 0.35678/0.22874. Took 0.05 sec\n",
            "Epoch 642, Loss(train/val) 0.36473/0.22859. Took 0.05 sec\n",
            "Epoch 643, Loss(train/val) 0.35890/0.22905. Took 0.06 sec\n",
            "Epoch 644, Loss(train/val) 0.36826/0.22843. Took 0.05 sec\n",
            "Epoch 645, Loss(train/val) 0.35963/0.22856. Took 0.05 sec\n",
            "Epoch 646, Loss(train/val) 0.35257/0.23206. Took 0.05 sec\n",
            "Epoch 647, Loss(train/val) 0.36034/0.23581. Took 0.05 sec\n",
            "Epoch 648, Loss(train/val) 0.36038/0.23348. Took 0.05 sec\n",
            "Epoch 649, Loss(train/val) 0.36109/0.23060. Took 0.05 sec\n",
            "Epoch 650, Loss(train/val) 0.37373/0.23002. Took 0.05 sec\n",
            "Epoch 651, Loss(train/val) 0.35786/0.22868. Took 0.05 sec\n",
            "Epoch 652, Loss(train/val) 0.35766/0.22762. Took 0.04 sec\n",
            "Epoch 653, Loss(train/val) 0.35997/0.22766. Took 0.05 sec\n",
            "Epoch 654, Loss(train/val) 0.35994/0.22775. Took 0.05 sec\n",
            "Epoch 655, Loss(train/val) 0.35846/0.22884. Took 0.04 sec\n",
            "Epoch 656, Loss(train/val) 0.36283/0.22795. Took 0.05 sec\n",
            "Epoch 657, Loss(train/val) 0.36329/0.22764. Took 0.05 sec\n",
            "Epoch 658, Loss(train/val) 0.35577/0.22771. Took 0.05 sec\n",
            "Epoch 659, Loss(train/val) 0.36395/0.22813. Took 0.05 sec\n",
            "Epoch 660, Loss(train/val) 0.35542/0.22787. Took 0.05 sec\n",
            "Epoch 661, Loss(train/val) 0.35486/0.22772. Took 0.04 sec\n",
            "Epoch 662, Loss(train/val) 0.35890/0.22776. Took 0.05 sec\n",
            "Epoch 663, Loss(train/val) 0.35372/0.22857. Took 0.05 sec\n",
            "Epoch 664, Loss(train/val) 0.35398/0.22814. Took 0.04 sec\n",
            "Epoch 665, Loss(train/val) 0.35864/0.22784. Took 0.05 sec\n",
            "Epoch 666, Loss(train/val) 0.36191/0.22797. Took 0.04 sec\n",
            "Epoch 667, Loss(train/val) 0.35678/0.22739. Took 0.05 sec\n",
            "Epoch 668, Loss(train/val) 0.37114/0.22741. Took 0.05 sec\n",
            "Epoch 669, Loss(train/val) 0.35474/0.22736. Took 0.05 sec\n",
            "Epoch 670, Loss(train/val) 0.36049/0.22699. Took 0.05 sec\n",
            "Epoch 671, Loss(train/val) 0.36080/0.22707. Took 0.05 sec\n",
            "Epoch 672, Loss(train/val) 0.35190/0.22737. Took 0.05 sec\n",
            "Epoch 673, Loss(train/val) 0.35302/0.22719. Took 0.05 sec\n",
            "Epoch 674, Loss(train/val) 0.35910/0.22701. Took 0.04 sec\n",
            "Epoch 675, Loss(train/val) 0.35500/0.22731. Took 0.04 sec\n",
            "Epoch 676, Loss(train/val) 0.36113/0.22840. Took 0.05 sec\n",
            "Epoch 677, Loss(train/val) 0.35060/0.22848. Took 0.04 sec\n",
            "Epoch 678, Loss(train/val) 0.35736/0.22761. Took 0.06 sec\n",
            "Epoch 679, Loss(train/val) 0.36478/0.22705. Took 0.05 sec\n",
            "Epoch 680, Loss(train/val) 0.35192/0.22755. Took 0.05 sec\n",
            "Epoch 681, Loss(train/val) 0.35827/0.22706. Took 0.04 sec\n",
            "Epoch 682, Loss(train/val) 0.35094/0.22738. Took 0.05 sec\n",
            "Epoch 683, Loss(train/val) 0.35220/0.22686. Took 0.05 sec\n",
            "Epoch 684, Loss(train/val) 0.35219/0.22643. Took 0.05 sec\n",
            "Epoch 685, Loss(train/val) 0.35476/0.22634. Took 0.05 sec\n",
            "Epoch 686, Loss(train/val) 0.35696/0.22719. Took 0.05 sec\n",
            "Epoch 687, Loss(train/val) 0.34688/0.22701. Took 0.05 sec\n",
            "Epoch 688, Loss(train/val) 0.35176/0.22699. Took 0.05 sec\n",
            "Epoch 689, Loss(train/val) 0.35612/0.22765. Took 0.04 sec\n",
            "Epoch 690, Loss(train/val) 0.36482/0.22729. Took 0.05 sec\n",
            "Epoch 691, Loss(train/val) 0.35687/0.22776. Took 0.04 sec\n",
            "Epoch 692, Loss(train/val) 0.34521/0.22912. Took 0.05 sec\n",
            "Epoch 693, Loss(train/val) 0.34708/0.22840. Took 0.05 sec\n",
            "Epoch 694, Loss(train/val) 0.35543/0.22776. Took 0.05 sec\n",
            "Epoch 695, Loss(train/val) 0.34811/0.22672. Took 0.04 sec\n",
            "Epoch 696, Loss(train/val) 0.35961/0.22621. Took 0.04 sec\n",
            "Epoch 697, Loss(train/val) 0.35163/0.22643. Took 0.05 sec\n",
            "Epoch 698, Loss(train/val) 0.34799/0.22626. Took 0.05 sec\n",
            "Epoch 699, Loss(train/val) 0.34822/0.22599. Took 0.05 sec\n",
            "Epoch 700, Loss(train/val) 0.36975/0.22616. Took 0.04 sec\n",
            "Epoch 701, Loss(train/val) 0.35267/0.22629. Took 0.04 sec\n",
            "Epoch 702, Loss(train/val) 0.37173/0.22619. Took 0.04 sec\n",
            "Epoch 703, Loss(train/val) 0.35305/0.22575. Took 0.06 sec\n",
            "Epoch 704, Loss(train/val) 0.35527/0.22576. Took 0.05 sec\n",
            "Epoch 705, Loss(train/val) 0.34954/0.22566. Took 0.05 sec\n",
            "Epoch 706, Loss(train/val) 0.36186/0.22546. Took 0.04 sec\n",
            "Epoch 707, Loss(train/val) 0.34799/0.22538. Took 0.05 sec\n",
            "Epoch 708, Loss(train/val) 0.34051/0.22552. Took 0.05 sec\n",
            "Epoch 709, Loss(train/val) 0.35031/0.22587. Took 0.05 sec\n",
            "Epoch 710, Loss(train/val) 0.36074/0.22700. Took 0.05 sec\n",
            "Epoch 711, Loss(train/val) 0.34018/0.22758. Took 0.05 sec\n",
            "Epoch 712, Loss(train/val) 0.34873/0.22719. Took 0.05 sec\n",
            "Epoch 713, Loss(train/val) 0.34658/0.22507. Took 0.05 sec\n",
            "Epoch 714, Loss(train/val) 0.35138/0.22647. Took 0.05 sec\n",
            "Epoch 715, Loss(train/val) 0.35542/0.22659. Took 0.04 sec\n",
            "Epoch 716, Loss(train/val) 0.35264/0.22818. Took 0.05 sec\n",
            "Epoch 717, Loss(train/val) 0.34525/0.22623. Took 0.04 sec\n",
            "Epoch 718, Loss(train/val) 0.35169/0.22489. Took 0.05 sec\n",
            "Epoch 719, Loss(train/val) 0.35016/0.22472. Took 0.05 sec\n",
            "Epoch 720, Loss(train/val) 0.34941/0.22538. Took 0.05 sec\n",
            "Epoch 721, Loss(train/val) 0.34658/0.22538. Took 0.05 sec\n",
            "Epoch 722, Loss(train/val) 0.35089/0.22539. Took 0.05 sec\n",
            "Epoch 723, Loss(train/val) 0.34927/0.22526. Took 0.05 sec\n",
            "Epoch 724, Loss(train/val) 0.34509/0.22479. Took 0.04 sec\n",
            "Epoch 725, Loss(train/val) 0.34623/0.22438. Took 0.05 sec\n",
            "Epoch 726, Loss(train/val) 0.34402/0.22471. Took 0.04 sec\n",
            "Epoch 727, Loss(train/val) 0.34194/0.22509. Took 0.05 sec\n",
            "Epoch 728, Loss(train/val) 0.34709/0.22555. Took 0.05 sec\n",
            "Epoch 729, Loss(train/val) 0.35258/0.22540. Took 0.05 sec\n",
            "Epoch 730, Loss(train/val) 0.34755/0.22525. Took 0.05 sec\n",
            "Epoch 731, Loss(train/val) 0.34734/0.22538. Took 0.04 sec\n",
            "Epoch 732, Loss(train/val) 0.36008/0.22428. Took 0.05 sec\n",
            "Epoch 733, Loss(train/val) 0.34578/0.22422. Took 0.06 sec\n",
            "Epoch 734, Loss(train/val) 0.34519/0.22421. Took 0.04 sec\n",
            "Epoch 735, Loss(train/val) 0.34171/0.22391. Took 0.05 sec\n",
            "Epoch 736, Loss(train/val) 0.34421/0.22403. Took 0.05 sec\n",
            "Epoch 737, Loss(train/val) 0.34861/0.22402. Took 0.04 sec\n",
            "Epoch 738, Loss(train/val) 0.34860/0.22495. Took 0.05 sec\n",
            "Epoch 739, Loss(train/val) 0.35523/0.22547. Took 0.05 sec\n",
            "Epoch 740, Loss(train/val) 0.34332/0.22489. Took 0.05 sec\n",
            "Epoch 741, Loss(train/val) 0.34090/0.22486. Took 0.05 sec\n",
            "Epoch 742, Loss(train/val) 0.35457/0.22401. Took 0.04 sec\n",
            "Epoch 743, Loss(train/val) 0.34523/0.22376. Took 0.05 sec\n",
            "Epoch 744, Loss(train/val) 0.34557/0.22400. Took 0.05 sec\n",
            "Epoch 745, Loss(train/val) 0.35359/0.22431. Took 0.05 sec\n",
            "Epoch 746, Loss(train/val) 0.33773/0.22417. Took 0.05 sec\n",
            "Epoch 747, Loss(train/val) 0.34367/0.22397. Took 0.05 sec\n",
            "Epoch 748, Loss(train/val) 0.35068/0.22410. Took 0.05 sec\n",
            "Epoch 749, Loss(train/val) 0.34288/0.22351. Took 0.05 sec\n",
            "Epoch 750, Loss(train/val) 0.36025/0.22364. Took 0.04 sec\n",
            "Epoch 751, Loss(train/val) 0.35456/0.22324. Took 0.05 sec\n",
            "Epoch 752, Loss(train/val) 0.34600/0.22341. Took 0.05 sec\n",
            "Epoch 753, Loss(train/val) 0.34139/0.22326. Took 0.05 sec\n",
            "Epoch 754, Loss(train/val) 0.33984/0.22309. Took 0.05 sec\n",
            "Epoch 755, Loss(train/val) 0.34359/0.22340. Took 0.05 sec\n",
            "Epoch 756, Loss(train/val) 0.34119/0.22314. Took 0.05 sec\n",
            "Epoch 757, Loss(train/val) 0.34786/0.22302. Took 0.04 sec\n",
            "Epoch 758, Loss(train/val) 0.34498/0.22295. Took 0.06 sec\n",
            "Epoch 759, Loss(train/val) 0.33547/0.22301. Took 0.04 sec\n",
            "Epoch 760, Loss(train/val) 0.33681/0.22312. Took 0.05 sec\n",
            "Epoch 761, Loss(train/val) 0.34540/0.22290. Took 0.05 sec\n",
            "Epoch 762, Loss(train/val) 0.34721/0.22265. Took 0.04 sec\n",
            "Epoch 763, Loss(train/val) 0.34269/0.22269. Took 0.05 sec\n",
            "Epoch 764, Loss(train/val) 0.34171/0.22255. Took 0.05 sec\n",
            "Epoch 765, Loss(train/val) 0.34818/0.22295. Took 0.04 sec\n",
            "Epoch 766, Loss(train/val) 0.34933/0.22308. Took 0.05 sec\n",
            "Epoch 767, Loss(train/val) 0.34320/0.22290. Took 0.05 sec\n",
            "Epoch 768, Loss(train/val) 0.34034/0.22297. Took 0.05 sec\n",
            "Epoch 769, Loss(train/val) 0.34278/0.22269. Took 0.05 sec\n",
            "Epoch 770, Loss(train/val) 0.34703/0.22264. Took 0.05 sec\n",
            "Epoch 771, Loss(train/val) 0.34725/0.22292. Took 0.05 sec\n",
            "Epoch 772, Loss(train/val) 0.34007/0.22267. Took 0.05 sec\n",
            "Epoch 773, Loss(train/val) 0.33821/0.22267. Took 0.05 sec\n",
            "Epoch 774, Loss(train/val) 0.33873/0.22364. Took 0.05 sec\n",
            "Epoch 775, Loss(train/val) 0.34098/0.22607. Took 0.05 sec\n",
            "Epoch 776, Loss(train/val) 0.33549/0.22729. Took 0.05 sec\n",
            "Epoch 777, Loss(train/val) 0.33954/0.22523. Took 0.05 sec\n",
            "Epoch 778, Loss(train/val) 0.34257/0.22269. Took 0.05 sec\n",
            "Epoch 779, Loss(train/val) 0.34881/0.22274. Took 0.05 sec\n",
            "Epoch 780, Loss(train/val) 0.34351/0.22251. Took 0.04 sec\n",
            "Epoch 781, Loss(train/val) 0.34320/0.22272. Took 0.04 sec\n",
            "Epoch 782, Loss(train/val) 0.33942/0.22219. Took 0.05 sec\n",
            "Epoch 783, Loss(train/val) 0.33581/0.22186. Took 0.05 sec\n",
            "Epoch 784, Loss(train/val) 0.34126/0.22160. Took 0.04 sec\n",
            "Epoch 785, Loss(train/val) 0.33867/0.22166. Took 0.04 sec\n",
            "Epoch 786, Loss(train/val) 0.33695/0.22153. Took 0.05 sec\n",
            "Epoch 787, Loss(train/val) 0.35634/0.22152. Took 0.05 sec\n",
            "Epoch 788, Loss(train/val) 0.34426/0.22183. Took 0.05 sec\n",
            "Epoch 789, Loss(train/val) 0.34090/0.22196. Took 0.05 sec\n",
            "Epoch 790, Loss(train/val) 0.33759/0.22167. Took 0.05 sec\n",
            "Epoch 791, Loss(train/val) 0.33381/0.22199. Took 0.05 sec\n",
            "Epoch 792, Loss(train/val) 0.33956/0.22231. Took 0.05 sec\n",
            "Epoch 793, Loss(train/val) 0.34004/0.22153. Took 0.06 sec\n",
            "Epoch 794, Loss(train/val) 0.34231/0.22233. Took 0.05 sec\n",
            "Epoch 795, Loss(train/val) 0.33615/0.22122. Took 0.05 sec\n",
            "Epoch 796, Loss(train/val) 0.33224/0.22147. Took 0.06 sec\n",
            "Epoch 797, Loss(train/val) 0.34052/0.22078. Took 0.05 sec\n",
            "Epoch 798, Loss(train/val) 0.33725/0.22085. Took 0.05 sec\n",
            "Epoch 799, Loss(train/val) 0.33694/0.22104. Took 0.05 sec\n",
            "Epoch 800, Loss(train/val) 0.33717/0.22118. Took 0.05 sec\n",
            "Epoch 801, Loss(train/val) 0.33712/0.22148. Took 0.05 sec\n",
            "Epoch 802, Loss(train/val) 0.33891/0.22190. Took 0.05 sec\n",
            "Epoch 803, Loss(train/val) 0.33825/0.22089. Took 0.05 sec\n",
            "Epoch 804, Loss(train/val) 0.33696/0.22073. Took 0.04 sec\n",
            "Epoch 805, Loss(train/val) 0.33363/0.22050. Took 0.04 sec\n",
            "Epoch 806, Loss(train/val) 0.33346/0.22058. Took 0.04 sec\n",
            "Epoch 807, Loss(train/val) 0.34249/0.22066. Took 0.05 sec\n",
            "Epoch 808, Loss(train/val) 0.33485/0.22089. Took 0.05 sec\n",
            "Epoch 809, Loss(train/val) 0.33243/0.22050. Took 0.05 sec\n",
            "Epoch 810, Loss(train/val) 0.33946/0.22034. Took 0.04 sec\n",
            "Epoch 811, Loss(train/val) 0.34355/0.22111. Took 0.05 sec\n",
            "Epoch 812, Loss(train/val) 0.34682/0.22059. Took 0.05 sec\n",
            "Epoch 813, Loss(train/val) 0.33288/0.22038. Took 0.05 sec\n",
            "Epoch 814, Loss(train/val) 0.34597/0.22031. Took 0.05 sec\n",
            "Epoch 815, Loss(train/val) 0.33453/0.22027. Took 0.04 sec\n",
            "Epoch 816, Loss(train/val) 0.34647/0.22013. Took 0.05 sec\n",
            "Epoch 817, Loss(train/val) 0.32785/0.21996. Took 0.05 sec\n",
            "Epoch 818, Loss(train/val) 0.34873/0.22032. Took 0.05 sec\n",
            "Epoch 819, Loss(train/val) 0.34688/0.22096. Took 0.05 sec\n",
            "Epoch 820, Loss(train/val) 0.33284/0.22190. Took 0.05 sec\n",
            "Epoch 821, Loss(train/val) 0.32873/0.22224. Took 0.04 sec\n",
            "Epoch 822, Loss(train/val) 0.33799/0.22098. Took 0.05 sec\n",
            "Epoch 823, Loss(train/val) 0.34508/0.22016. Took 0.05 sec\n",
            "Epoch 824, Loss(train/val) 0.33469/0.22001. Took 0.05 sec\n",
            "Epoch 825, Loss(train/val) 0.33188/0.21976. Took 0.05 sec\n",
            "Epoch 826, Loss(train/val) 0.33867/0.21955. Took 0.04 sec\n",
            "Epoch 827, Loss(train/val) 0.34390/0.21933. Took 0.05 sec\n",
            "Epoch 828, Loss(train/val) 0.33836/0.21909. Took 0.05 sec\n",
            "Epoch 829, Loss(train/val) 0.34378/0.21893. Took 0.05 sec\n",
            "Epoch 830, Loss(train/val) 0.33977/0.21970. Took 0.04 sec\n",
            "Epoch 831, Loss(train/val) 0.33822/0.22000. Took 0.04 sec\n",
            "Epoch 832, Loss(train/val) 0.33083/0.22095. Took 0.05 sec\n",
            "Epoch 833, Loss(train/val) 0.33758/0.21883. Took 0.04 sec\n",
            "Epoch 834, Loss(train/val) 0.33392/0.21873. Took 0.04 sec\n",
            "Epoch 835, Loss(train/val) 0.33790/0.21892. Took 0.04 sec\n",
            "Epoch 836, Loss(train/val) 0.33099/0.21959. Took 0.04 sec\n",
            "Epoch 837, Loss(train/val) 0.32708/0.21972. Took 0.05 sec\n",
            "Epoch 838, Loss(train/val) 0.32170/0.21997. Took 0.05 sec\n",
            "Epoch 839, Loss(train/val) 0.33156/0.21972. Took 0.06 sec\n",
            "Epoch 840, Loss(train/val) 0.34659/0.21954. Took 0.04 sec\n",
            "Epoch 841, Loss(train/val) 0.32726/0.21898. Took 0.04 sec\n",
            "Epoch 842, Loss(train/val) 0.33104/0.21879. Took 0.05 sec\n",
            "Epoch 843, Loss(train/val) 0.33975/0.21885. Took 0.05 sec\n",
            "Epoch 844, Loss(train/val) 0.34892/0.21844. Took 0.04 sec\n",
            "Epoch 845, Loss(train/val) 0.33399/0.21839. Took 0.05 sec\n",
            "Epoch 846, Loss(train/val) 0.33119/0.21822. Took 0.05 sec\n",
            "Epoch 847, Loss(train/val) 0.33517/0.21835. Took 0.05 sec\n",
            "Epoch 848, Loss(train/val) 0.32980/0.21827. Took 0.05 sec\n",
            "Epoch 849, Loss(train/val) 0.33018/0.21831. Took 0.04 sec\n",
            "Epoch 850, Loss(train/val) 0.34220/0.21831. Took 0.04 sec\n",
            "Epoch 851, Loss(train/val) 0.33887/0.21836. Took 0.05 sec\n",
            "Epoch 852, Loss(train/val) 0.33516/0.21823. Took 0.05 sec\n",
            "Epoch 853, Loss(train/val) 0.33192/0.21794. Took 0.05 sec\n",
            "Epoch 854, Loss(train/val) 0.33034/0.21774. Took 0.05 sec\n",
            "Epoch 855, Loss(train/val) 0.32864/0.21769. Took 0.04 sec\n",
            "Epoch 856, Loss(train/val) 0.33086/0.21883. Took 0.05 sec\n",
            "Epoch 857, Loss(train/val) 0.34934/0.21775. Took 0.05 sec\n",
            "Epoch 858, Loss(train/val) 0.33364/0.21852. Took 0.04 sec\n",
            "Epoch 859, Loss(train/val) 0.31685/0.21986. Took 0.04 sec\n",
            "Epoch 860, Loss(train/val) 0.33521/0.21922. Took 0.04 sec\n",
            "Epoch 861, Loss(train/val) 0.33710/0.22028. Took 0.05 sec\n",
            "Epoch 862, Loss(train/val) 0.33048/0.21879. Took 0.05 sec\n",
            "Epoch 863, Loss(train/val) 0.33149/0.21854. Took 0.04 sec\n",
            "Epoch 864, Loss(train/val) 0.32856/0.21738. Took 0.05 sec\n",
            "Epoch 865, Loss(train/val) 0.33153/0.21791. Took 0.05 sec\n",
            "Epoch 866, Loss(train/val) 0.33360/0.21874. Took 0.05 sec\n",
            "Epoch 867, Loss(train/val) 0.33089/0.22086. Took 0.06 sec\n",
            "Epoch 868, Loss(train/val) 0.33092/0.21900. Took 0.05 sec\n",
            "Epoch 869, Loss(train/val) 0.33619/0.21746. Took 0.04 sec\n",
            "Epoch 870, Loss(train/val) 0.33394/0.21704. Took 0.04 sec\n",
            "Epoch 871, Loss(train/val) 0.32938/0.21715. Took 0.04 sec\n",
            "Epoch 872, Loss(train/val) 0.32499/0.21744. Took 0.05 sec\n",
            "Epoch 873, Loss(train/val) 0.32546/0.21882. Took 0.05 sec\n",
            "Epoch 874, Loss(train/val) 0.32702/0.21835. Took 0.04 sec\n",
            "Epoch 875, Loss(train/val) 0.32139/0.21776. Took 0.04 sec\n",
            "Epoch 876, Loss(train/val) 0.33272/0.21678. Took 0.04 sec\n",
            "Epoch 877, Loss(train/val) 0.32345/0.21639. Took 0.06 sec\n",
            "Epoch 878, Loss(train/val) 0.33279/0.21646. Took 0.04 sec\n",
            "Epoch 879, Loss(train/val) 0.32624/0.21672. Took 0.04 sec\n",
            "Epoch 880, Loss(train/val) 0.32782/0.21661. Took 0.05 sec\n",
            "Epoch 881, Loss(train/val) 0.33202/0.21799. Took 0.04 sec\n",
            "Epoch 882, Loss(train/val) 0.33198/0.21736. Took 0.06 sec\n",
            "Epoch 883, Loss(train/val) 0.34464/0.21821. Took 0.05 sec\n",
            "Epoch 884, Loss(train/val) 0.32861/0.21733. Took 0.05 sec\n",
            "Epoch 885, Loss(train/val) 0.32473/0.21670. Took 0.05 sec\n",
            "Epoch 886, Loss(train/val) 0.33682/0.21633. Took 0.05 sec\n",
            "Epoch 887, Loss(train/val) 0.33204/0.21626. Took 0.05 sec\n",
            "Epoch 888, Loss(train/val) 0.33131/0.21577. Took 0.05 sec\n",
            "Epoch 889, Loss(train/val) 0.32274/0.21589. Took 0.04 sec\n",
            "Epoch 890, Loss(train/val) 0.32836/0.21588. Took 0.04 sec\n",
            "Epoch 891, Loss(train/val) 0.32836/0.21652. Took 0.05 sec\n",
            "Epoch 892, Loss(train/val) 0.32565/0.21566. Took 0.05 sec\n",
            "Epoch 893, Loss(train/val) 0.33162/0.21530. Took 0.05 sec\n",
            "Epoch 894, Loss(train/val) 0.32228/0.21516. Took 0.05 sec\n",
            "Epoch 895, Loss(train/val) 0.32817/0.21524. Took 0.04 sec\n",
            "Epoch 896, Loss(train/val) 0.32539/0.21545. Took 0.04 sec\n",
            "Epoch 897, Loss(train/val) 0.32817/0.21543. Took 0.05 sec\n",
            "Epoch 898, Loss(train/val) 0.32945/0.21575. Took 0.05 sec\n",
            "Epoch 899, Loss(train/val) 0.32571/0.21613. Took 0.05 sec\n",
            "Epoch 900, Loss(train/val) 0.33370/0.21614. Took 0.05 sec\n",
            "Epoch 901, Loss(train/val) 0.32094/0.21585. Took 0.04 sec\n",
            "Epoch 902, Loss(train/val) 0.33121/0.21572. Took 0.05 sec\n",
            "Epoch 903, Loss(train/val) 0.33974/0.21501. Took 0.05 sec\n",
            "Epoch 904, Loss(train/val) 0.31928/0.21499. Took 0.05 sec\n",
            "Epoch 905, Loss(train/val) 0.32320/0.21518. Took 0.05 sec\n",
            "Epoch 906, Loss(train/val) 0.32693/0.21499. Took 0.05 sec\n",
            "Epoch 907, Loss(train/val) 0.32476/0.21459. Took 0.05 sec\n",
            "Epoch 908, Loss(train/val) 0.32488/0.21450. Took 0.05 sec\n",
            "Epoch 909, Loss(train/val) 0.32880/0.21453. Took 0.04 sec\n",
            "Epoch 910, Loss(train/val) 0.32845/0.21452. Took 0.04 sec\n",
            "Epoch 911, Loss(train/val) 0.34242/0.21446. Took 0.04 sec\n",
            "Epoch 912, Loss(train/val) 0.33314/0.21423. Took 0.06 sec\n",
            "Epoch 913, Loss(train/val) 0.32504/0.21405. Took 0.04 sec\n",
            "Epoch 914, Loss(train/val) 0.32656/0.21398. Took 0.05 sec\n",
            "Epoch 915, Loss(train/val) 0.32887/0.21407. Took 0.05 sec\n",
            "Epoch 916, Loss(train/val) 0.32417/0.21393. Took 0.05 sec\n",
            "Epoch 917, Loss(train/val) 0.32199/0.21389. Took 0.05 sec\n",
            "Epoch 918, Loss(train/val) 0.32018/0.21419. Took 0.05 sec\n",
            "Epoch 919, Loss(train/val) 0.32380/0.21409. Took 0.05 sec\n",
            "Epoch 920, Loss(train/val) 0.32395/0.21369. Took 0.04 sec\n",
            "Epoch 921, Loss(train/val) 0.31846/0.21362. Took 0.04 sec\n",
            "Epoch 922, Loss(train/val) 0.32213/0.21349. Took 0.05 sec\n",
            "Epoch 923, Loss(train/val) 0.33381/0.21341. Took 0.05 sec\n",
            "Epoch 924, Loss(train/val) 0.32385/0.21349. Took 0.05 sec\n",
            "Epoch 925, Loss(train/val) 0.33159/0.21355. Took 0.05 sec\n",
            "Epoch 926, Loss(train/val) 0.32409/0.21396. Took 0.04 sec\n",
            "Epoch 927, Loss(train/val) 0.32983/0.21380. Took 0.05 sec\n",
            "Epoch 928, Loss(train/val) 0.32357/0.21399. Took 0.04 sec\n",
            "Epoch 929, Loss(train/val) 0.33038/0.21337. Took 0.05 sec\n",
            "Epoch 930, Loss(train/val) 0.32329/0.21318. Took 0.04 sec\n",
            "Epoch 931, Loss(train/val) 0.32539/0.21283. Took 0.05 sec\n",
            "Epoch 932, Loss(train/val) 0.32307/0.21282. Took 0.05 sec\n",
            "Epoch 933, Loss(train/val) 0.32737/0.21276. Took 0.04 sec\n",
            "Epoch 934, Loss(train/val) 0.32219/0.21361. Took 0.04 sec\n",
            "Epoch 935, Loss(train/val) 0.32767/0.21423. Took 0.04 sec\n",
            "Epoch 936, Loss(train/val) 0.31415/0.21327. Took 0.04 sec\n",
            "Epoch 937, Loss(train/val) 0.32174/0.21292. Took 0.05 sec\n",
            "Epoch 938, Loss(train/val) 0.32614/0.21234. Took 0.05 sec\n",
            "Epoch 939, Loss(train/val) 0.32204/0.21231. Took 0.05 sec\n",
            "Epoch 940, Loss(train/val) 0.32602/0.21227. Took 0.05 sec\n",
            "Epoch 941, Loss(train/val) 0.32747/0.21203. Took 0.04 sec\n",
            "Epoch 942, Loss(train/val) 0.32396/0.21205. Took 0.05 sec\n",
            "Epoch 943, Loss(train/val) 0.33129/0.21201. Took 0.04 sec\n",
            "Epoch 944, Loss(train/val) 0.32311/0.21168. Took 0.04 sec\n",
            "Epoch 945, Loss(train/val) 0.32574/0.21169. Took 0.05 sec\n",
            "Epoch 946, Loss(train/val) 0.32640/0.21162. Took 0.04 sec\n",
            "Epoch 947, Loss(train/val) 0.32052/0.21168. Took 0.06 sec\n",
            "Epoch 948, Loss(train/val) 0.32685/0.21162. Took 0.04 sec\n",
            "Epoch 949, Loss(train/val) 0.32694/0.21160. Took 0.04 sec\n",
            "Epoch 950, Loss(train/val) 0.31186/0.21167. Took 0.05 sec\n",
            "Epoch 951, Loss(train/val) 0.33051/0.21131. Took 0.05 sec\n",
            "Epoch 952, Loss(train/val) 0.32747/0.21122. Took 0.05 sec\n",
            "Epoch 953, Loss(train/val) 0.32207/0.21149. Took 0.04 sec\n",
            "Epoch 954, Loss(train/val) 0.32847/0.21141. Took 0.05 sec\n",
            "Epoch 955, Loss(train/val) 0.32611/0.21096. Took 0.05 sec\n",
            "Epoch 956, Loss(train/val) 0.33517/0.21084. Took 0.04 sec\n",
            "Epoch 957, Loss(train/val) 0.31552/0.21082. Took 0.05 sec\n",
            "Epoch 958, Loss(train/val) 0.32058/0.21076. Took 0.05 sec\n",
            "Epoch 959, Loss(train/val) 0.31277/0.21100. Took 0.05 sec\n",
            "Epoch 960, Loss(train/val) 0.32245/0.21127. Took 0.05 sec\n",
            "Epoch 961, Loss(train/val) 0.32174/0.21177. Took 0.05 sec\n",
            "Epoch 962, Loss(train/val) 0.31568/0.21079. Took 0.05 sec\n",
            "Epoch 963, Loss(train/val) 0.31223/0.21077. Took 0.04 sec\n",
            "Epoch 964, Loss(train/val) 0.31582/0.21026. Took 0.04 sec\n",
            "Epoch 965, Loss(train/val) 0.31902/0.21024. Took 0.05 sec\n",
            "Epoch 966, Loss(train/val) 0.32763/0.21005. Took 0.05 sec\n",
            "Epoch 967, Loss(train/val) 0.31252/0.21009. Took 0.05 sec\n",
            "Epoch 968, Loss(train/val) 0.33850/0.20982. Took 0.05 sec\n",
            "Epoch 969, Loss(train/val) 0.33022/0.21029. Took 0.05 sec\n",
            "Epoch 970, Loss(train/val) 0.31514/0.21059. Took 0.04 sec\n",
            "Epoch 971, Loss(train/val) 0.31880/0.21040. Took 0.05 sec\n",
            "Epoch 972, Loss(train/val) 0.31253/0.21073. Took 0.05 sec\n",
            "Epoch 973, Loss(train/val) 0.32234/0.21019. Took 0.04 sec\n",
            "Epoch 974, Loss(train/val) 0.31450/0.20945. Took 0.04 sec\n",
            "Epoch 975, Loss(train/val) 0.33149/0.20975. Took 0.04 sec\n",
            "Epoch 976, Loss(train/val) 0.32042/0.20985. Took 0.05 sec\n",
            "Epoch 977, Loss(train/val) 0.31300/0.20918. Took 0.05 sec\n",
            "Epoch 978, Loss(train/val) 0.30493/0.20893. Took 0.04 sec\n",
            "Epoch 979, Loss(train/val) 0.31644/0.20904. Took 0.05 sec\n",
            "Epoch 980, Loss(train/val) 0.31892/0.21138. Took 0.04 sec\n",
            "Epoch 981, Loss(train/val) 0.32490/0.21231. Took 0.04 sec\n",
            "Epoch 982, Loss(train/val) 0.31957/0.21157. Took 0.05 sec\n",
            "Epoch 983, Loss(train/val) 0.31990/0.21049. Took 0.04 sec\n",
            "Epoch 984, Loss(train/val) 0.30944/0.20931. Took 0.05 sec\n",
            "Epoch 985, Loss(train/val) 0.31531/0.20850. Took 0.04 sec\n",
            "Epoch 986, Loss(train/val) 0.31113/0.20860. Took 0.04 sec\n",
            "Epoch 987, Loss(train/val) 0.31841/0.20870. Took 0.05 sec\n",
            "Epoch 988, Loss(train/val) 0.31148/0.20917. Took 0.05 sec\n",
            "Epoch 989, Loss(train/val) 0.32450/0.20888. Took 0.05 sec\n",
            "Epoch 990, Loss(train/val) 0.31905/0.20776. Took 0.05 sec\n",
            "Epoch 991, Loss(train/val) 0.31703/0.20762. Took 0.04 sec\n",
            "Epoch 992, Loss(train/val) 0.32562/0.20763. Took 0.05 sec\n",
            "Epoch 993, Loss(train/val) 0.31592/0.20751. Took 0.05 sec\n",
            "Epoch 994, Loss(train/val) 0.31555/0.20740. Took 0.04 sec\n",
            "Epoch 995, Loss(train/val) 0.31342/0.20745. Took 0.05 sec\n",
            "Epoch 996, Loss(train/val) 0.32043/0.20739. Took 0.04 sec\n",
            "Epoch 997, Loss(train/val) 0.30756/0.20716. Took 0.05 sec\n",
            "Epoch 998, Loss(train/val) 0.31058/0.20710. Took 0.04 sec\n",
            "Epoch 999, Loss(train/val) 0.31233/0.20728. Took 0.05 sec\n",
            "Epoch 1000, Loss(train/val) 0.32965/0.20730. Took 0.05 sec\n",
            "Epoch 1001, Loss(train/val) 0.31592/0.20754. Took 0.04 sec\n",
            "Epoch 1002, Loss(train/val) 0.31685/0.20696. Took 0.05 sec\n",
            "Epoch 1003, Loss(train/val) 0.31592/0.20719. Took 0.05 sec\n",
            "Epoch 1004, Loss(train/val) 0.31136/0.20657. Took 0.04 sec\n",
            "Epoch 1005, Loss(train/val) 0.31455/0.20648. Took 0.05 sec\n",
            "Epoch 1006, Loss(train/val) 0.31519/0.20625. Took 0.05 sec\n",
            "Epoch 1007, Loss(train/val) 0.31223/0.20595. Took 0.06 sec\n",
            "Epoch 1008, Loss(train/val) 0.30796/0.20586. Took 0.05 sec\n",
            "Epoch 1009, Loss(train/val) 0.31591/0.20621. Took 0.04 sec\n",
            "Epoch 1010, Loss(train/val) 0.31100/0.20781. Took 0.04 sec\n",
            "Epoch 1011, Loss(train/val) 0.31266/0.20780. Took 0.05 sec\n",
            "Epoch 1012, Loss(train/val) 0.31178/0.20777. Took 0.05 sec\n",
            "Epoch 1013, Loss(train/val) 0.30916/0.20648. Took 0.05 sec\n",
            "Epoch 1014, Loss(train/val) 0.30973/0.20648. Took 0.05 sec\n",
            "Epoch 1015, Loss(train/val) 0.31773/0.20537. Took 0.05 sec\n",
            "Epoch 1016, Loss(train/val) 0.31832/0.20532. Took 0.04 sec\n",
            "Epoch 1017, Loss(train/val) 0.33214/0.20504. Took 0.06 sec\n",
            "Epoch 1018, Loss(train/val) 0.31716/0.20515. Took 0.04 sec\n",
            "Epoch 1019, Loss(train/val) 0.31084/0.20570. Took 0.04 sec\n",
            "Epoch 1020, Loss(train/val) 0.32288/0.20594. Took 0.05 sec\n",
            "Epoch 1021, Loss(train/val) 0.31560/0.20590. Took 0.05 sec\n",
            "Epoch 1022, Loss(train/val) 0.32063/0.20525. Took 0.05 sec\n",
            "Epoch 1023, Loss(train/val) 0.32084/0.20604. Took 0.04 sec\n",
            "Epoch 1024, Loss(train/val) 0.31395/0.20547. Took 0.05 sec\n",
            "Epoch 1025, Loss(train/val) 0.32727/0.20468. Took 0.04 sec\n",
            "Epoch 1026, Loss(train/val) 0.30515/0.20399. Took 0.04 sec\n",
            "Epoch 1027, Loss(train/val) 0.31299/0.20392. Took 0.06 sec\n",
            "Epoch 1028, Loss(train/val) 0.31822/0.20422. Took 0.05 sec\n",
            "Epoch 1029, Loss(train/val) 0.31373/0.20491. Took 0.05 sec\n",
            "Epoch 1030, Loss(train/val) 0.31124/0.20381. Took 0.05 sec\n",
            "Epoch 1031, Loss(train/val) 0.30889/0.20345. Took 0.05 sec\n",
            "Epoch 1032, Loss(train/val) 0.31368/0.20330. Took 0.05 sec\n",
            "Epoch 1033, Loss(train/val) 0.30610/0.20312. Took 0.05 sec\n",
            "Epoch 1034, Loss(train/val) 0.31758/0.20301. Took 0.05 sec\n",
            "Epoch 1035, Loss(train/val) 0.31328/0.20307. Took 0.04 sec\n",
            "Epoch 1036, Loss(train/val) 0.30373/0.20345. Took 0.04 sec\n",
            "Epoch 1037, Loss(train/val) 0.30192/0.20292. Took 0.05 sec\n",
            "Epoch 1038, Loss(train/val) 0.31884/0.20268. Took 0.04 sec\n",
            "Epoch 1039, Loss(train/val) 0.30895/0.20255. Took 0.04 sec\n",
            "Epoch 1040, Loss(train/val) 0.30924/0.20253. Took 0.05 sec\n",
            "Epoch 1041, Loss(train/val) 0.29958/0.20233. Took 0.05 sec\n",
            "Epoch 1042, Loss(train/val) 0.32074/0.20217. Took 0.05 sec\n",
            "Epoch 1043, Loss(train/val) 0.31266/0.20207. Took 0.05 sec\n",
            "Epoch 1044, Loss(train/val) 0.30579/0.20197. Took 0.05 sec\n",
            "Epoch 1045, Loss(train/val) 0.30620/0.20186. Took 0.05 sec\n",
            "Epoch 1046, Loss(train/val) 0.29769/0.20186. Took 0.04 sec\n",
            "Epoch 1047, Loss(train/val) 0.30473/0.20143. Took 0.05 sec\n",
            "Epoch 1048, Loss(train/val) 0.30667/0.20130. Took 0.05 sec\n",
            "Epoch 1049, Loss(train/val) 0.30336/0.20118. Took 0.05 sec\n",
            "Epoch 1050, Loss(train/val) 0.30420/0.20107. Took 0.04 sec\n",
            "Epoch 1051, Loss(train/val) 0.31035/0.20102. Took 0.04 sec\n",
            "Epoch 1052, Loss(train/val) 0.30767/0.20118. Took 0.05 sec\n",
            "Epoch 1053, Loss(train/val) 0.31699/0.20109. Took 0.04 sec\n",
            "Epoch 1054, Loss(train/val) 0.30605/0.20145. Took 0.05 sec\n",
            "Epoch 1055, Loss(train/val) 0.30369/0.20059. Took 0.04 sec\n",
            "Epoch 1056, Loss(train/val) 0.32466/0.20063. Took 0.05 sec\n",
            "Epoch 1057, Loss(train/val) 0.30487/0.20036. Took 0.05 sec\n",
            "Epoch 1058, Loss(train/val) 0.30258/0.20019. Took 0.05 sec\n",
            "Epoch 1059, Loss(train/val) 0.33279/0.19985. Took 0.05 sec\n",
            "Epoch 1060, Loss(train/val) 0.30982/0.19971. Took 0.05 sec\n",
            "Epoch 1061, Loss(train/val) 0.30039/0.19996. Took 0.04 sec\n",
            "Epoch 1062, Loss(train/val) 0.30244/0.19945. Took 0.05 sec\n",
            "Epoch 1063, Loss(train/val) 0.30020/0.19897. Took 0.05 sec\n",
            "Epoch 1064, Loss(train/val) 0.30062/0.19905. Took 0.05 sec\n",
            "Epoch 1065, Loss(train/val) 0.29670/0.19934. Took 0.04 sec\n",
            "Epoch 1066, Loss(train/val) 0.30550/0.19931. Took 0.05 sec\n",
            "Epoch 1067, Loss(train/val) 0.29940/0.19875. Took 0.05 sec\n",
            "Epoch 1068, Loss(train/val) 0.29784/0.19875. Took 0.05 sec\n",
            "Epoch 1069, Loss(train/val) 0.30737/0.19866. Took 0.05 sec\n",
            "Epoch 1070, Loss(train/val) 0.30334/0.19859. Took 0.04 sec\n",
            "Epoch 1071, Loss(train/val) 0.29551/0.19859. Took 0.04 sec\n",
            "Epoch 1072, Loss(train/val) 0.29874/0.19837. Took 0.05 sec\n",
            "Epoch 1073, Loss(train/val) 0.29946/0.19721. Took 0.05 sec\n",
            "Epoch 1074, Loss(train/val) 0.29579/0.19675. Took 0.04 sec\n",
            "Epoch 1075, Loss(train/val) 0.30711/0.19661. Took 0.04 sec\n",
            "Epoch 1076, Loss(train/val) 0.30683/0.19642. Took 0.05 sec\n",
            "Epoch 1077, Loss(train/val) 0.29408/0.19640. Took 0.05 sec\n",
            "Epoch 1078, Loss(train/val) 0.29744/0.19597. Took 0.05 sec\n",
            "Epoch 1079, Loss(train/val) 0.30993/0.19582. Took 0.05 sec\n",
            "Epoch 1080, Loss(train/val) 0.30465/0.19594. Took 0.05 sec\n",
            "Epoch 1081, Loss(train/val) 0.31249/0.19577. Took 0.04 sec\n",
            "Epoch 1082, Loss(train/val) 0.30480/0.19602. Took 0.05 sec\n",
            "Epoch 1083, Loss(train/val) 0.29935/0.19570. Took 0.05 sec\n",
            "Epoch 1084, Loss(train/val) 0.30901/0.19685. Took 0.04 sec\n",
            "Epoch 1085, Loss(train/val) 0.29056/0.19551. Took 0.05 sec\n",
            "Epoch 1086, Loss(train/val) 0.29180/0.19490. Took 0.05 sec\n",
            "Epoch 1087, Loss(train/val) 0.29719/0.19482. Took 0.05 sec\n",
            "Epoch 1088, Loss(train/val) 0.31338/0.19454. Took 0.04 sec\n",
            "Epoch 1089, Loss(train/val) 0.28848/0.19476. Took 0.05 sec\n",
            "Epoch 1090, Loss(train/val) 0.30041/0.19407. Took 0.04 sec\n",
            "Epoch 1091, Loss(train/val) 0.29718/0.19428. Took 0.05 sec\n",
            "Epoch 1092, Loss(train/val) 0.29871/0.19467. Took 0.05 sec\n",
            "Epoch 1093, Loss(train/val) 0.30029/0.19466. Took 0.05 sec\n",
            "Epoch 1094, Loss(train/val) 0.30225/0.19395. Took 0.05 sec\n",
            "Epoch 1095, Loss(train/val) 0.29993/0.19374. Took 0.05 sec\n",
            "Epoch 1096, Loss(train/val) 0.30903/0.19359. Took 0.05 sec\n",
            "Epoch 1097, Loss(train/val) 0.29393/0.19378. Took 0.06 sec\n",
            "Epoch 1098, Loss(train/val) 0.28932/0.19389. Took 0.05 sec\n",
            "Epoch 1099, Loss(train/val) 0.29429/0.19508. Took 0.05 sec\n",
            "Epoch 1100, Loss(train/val) 0.28741/0.19567. Took 0.04 sec\n",
            "Epoch 1101, Loss(train/val) 0.29495/0.19273. Took 0.05 sec\n",
            "Epoch 1102, Loss(train/val) 0.29115/0.19225. Took 0.05 sec\n",
            "Epoch 1103, Loss(train/val) 0.29311/0.19077. Took 0.05 sec\n",
            "Epoch 1104, Loss(train/val) 0.29820/0.19059. Took 0.05 sec\n",
            "Epoch 1105, Loss(train/val) 0.29004/0.19080. Took 0.05 sec\n",
            "Epoch 1106, Loss(train/val) 0.28094/0.19043. Took 0.05 sec\n",
            "Epoch 1107, Loss(train/val) 0.29227/0.19001. Took 0.05 sec\n",
            "Epoch 1108, Loss(train/val) 0.28447/0.19129. Took 0.05 sec\n",
            "Epoch 1109, Loss(train/val) 0.29266/0.19222. Took 0.04 sec\n",
            "Epoch 1110, Loss(train/val) 0.29136/0.19047. Took 0.04 sec\n",
            "Epoch 1111, Loss(train/val) 0.29678/0.19034. Took 0.04 sec\n",
            "Epoch 1112, Loss(train/val) 0.29283/0.18912. Took 0.05 sec\n",
            "Epoch 1113, Loss(train/val) 0.30771/0.18900. Took 0.05 sec\n",
            "Epoch 1114, Loss(train/val) 0.29005/0.18990. Took 0.04 sec\n",
            "Epoch 1115, Loss(train/val) 0.28183/0.19294. Took 0.04 sec\n",
            "Epoch 1116, Loss(train/val) 0.29242/0.18863. Took 0.05 sec\n",
            "Epoch 1117, Loss(train/val) 0.29310/0.18718. Took 0.06 sec\n",
            "Epoch 1118, Loss(train/val) 0.28907/0.18844. Took 0.05 sec\n",
            "Epoch 1119, Loss(train/val) 0.29368/0.18892. Took 0.05 sec\n",
            "Epoch 1120, Loss(train/val) 0.28519/0.18971. Took 0.05 sec\n",
            "Epoch 1121, Loss(train/val) 0.28075/0.19249. Took 0.04 sec\n",
            "Epoch 1122, Loss(train/val) 0.29817/0.19570. Took 0.05 sec\n",
            "Epoch 1123, Loss(train/val) 0.29486/0.19688. Took 0.05 sec\n",
            "Epoch 1124, Loss(train/val) 0.28473/0.19304. Took 0.05 sec\n",
            "Epoch 1125, Loss(train/val) 0.27965/0.18907. Took 0.04 sec\n",
            "Epoch 1126, Loss(train/val) 0.28481/0.18698. Took 0.04 sec\n",
            "Epoch 1127, Loss(train/val) 0.28150/0.18538. Took 0.05 sec\n",
            "Epoch 1128, Loss(train/val) 0.28438/0.18456. Took 0.04 sec\n",
            "Epoch 1129, Loss(train/val) 0.28186/0.18461. Took 0.05 sec\n",
            "Epoch 1130, Loss(train/val) 0.28253/0.18411. Took 0.04 sec\n",
            "Epoch 1131, Loss(train/val) 0.29157/0.18412. Took 0.04 sec\n",
            "Epoch 1132, Loss(train/val) 0.28341/0.18487. Took 0.05 sec\n",
            "Epoch 1133, Loss(train/val) 0.27587/0.18773. Took 0.04 sec\n",
            "Epoch 1134, Loss(train/val) 0.28449/0.18914. Took 0.05 sec\n",
            "Epoch 1135, Loss(train/val) 0.28377/0.19035. Took 0.04 sec\n",
            "Epoch 1136, Loss(train/val) 0.27695/0.18369. Took 0.05 sec\n",
            "Epoch 1137, Loss(train/val) 0.28151/0.18226. Took 0.06 sec\n",
            "Epoch 1138, Loss(train/val) 0.27646/0.18269. Took 0.05 sec\n",
            "Epoch 1139, Loss(train/val) 0.28121/0.18162. Took 0.05 sec\n",
            "Epoch 1140, Loss(train/val) 0.28136/0.18300. Took 0.05 sec\n",
            "Epoch 1141, Loss(train/val) 0.27676/0.18563. Took 0.05 sec\n",
            "Epoch 1142, Loss(train/val) 0.28182/0.18766. Took 0.06 sec\n",
            "Epoch 1143, Loss(train/val) 0.26795/0.18780. Took 0.04 sec\n",
            "Epoch 1144, Loss(train/val) 0.27952/0.18990. Took 0.05 sec\n",
            "Epoch 1145, Loss(train/val) 0.27550/0.18718. Took 0.05 sec\n",
            "Epoch 1146, Loss(train/val) 0.28763/0.18366. Took 0.04 sec\n",
            "Epoch 1147, Loss(train/val) 0.28588/0.18168. Took 0.05 sec\n",
            "Epoch 1148, Loss(train/val) 0.28805/0.18036. Took 0.05 sec\n",
            "Epoch 1149, Loss(train/val) 0.28732/0.18031. Took 0.05 sec\n",
            "Epoch 1150, Loss(train/val) 0.28642/0.18023. Took 0.04 sec\n",
            "Epoch 1151, Loss(train/val) 0.27405/0.18129. Took 0.05 sec\n",
            "Epoch 1152, Loss(train/val) 0.27407/0.18388. Took 0.05 sec\n",
            "Epoch 1153, Loss(train/val) 0.28283/0.17974. Took 0.05 sec\n",
            "Epoch 1154, Loss(train/val) 0.28805/0.17962. Took 0.05 sec\n",
            "Epoch 1155, Loss(train/val) 0.27604/0.17995. Took 0.05 sec\n",
            "Epoch 1156, Loss(train/val) 0.27366/0.17994. Took 0.05 sec\n",
            "Epoch 1157, Loss(train/val) 0.27972/0.17785. Took 0.06 sec\n",
            "Epoch 1158, Loss(train/val) 0.28909/0.17841. Took 0.05 sec\n",
            "Epoch 1159, Loss(train/val) 0.27703/0.17820. Took 0.05 sec\n",
            "Epoch 1160, Loss(train/val) 0.27085/0.17912. Took 0.05 sec\n",
            "Epoch 1161, Loss(train/val) 0.28297/0.17900. Took 0.06 sec\n",
            "Epoch 1162, Loss(train/val) 0.26984/0.17743. Took 0.05 sec\n",
            "Epoch 1163, Loss(train/val) 0.28681/0.17713. Took 0.05 sec\n",
            "Epoch 1164, Loss(train/val) 0.27876/0.17736. Took 0.04 sec\n",
            "Epoch 1165, Loss(train/val) 0.27156/0.17689. Took 0.04 sec\n",
            "Epoch 1166, Loss(train/val) 0.26517/0.17624. Took 0.04 sec\n",
            "Epoch 1167, Loss(train/val) 0.26981/0.17628. Took 0.06 sec\n",
            "Epoch 1168, Loss(train/val) 0.27740/0.17679. Took 0.05 sec\n",
            "Epoch 1169, Loss(train/val) 0.26605/0.18051. Took 0.05 sec\n",
            "Epoch 1170, Loss(train/val) 0.26540/0.18183. Took 0.04 sec\n",
            "Epoch 1171, Loss(train/val) 0.28096/0.17714. Took 0.05 sec\n",
            "Epoch 1172, Loss(train/val) 0.27387/0.17728. Took 0.05 sec\n",
            "Epoch 1173, Loss(train/val) 0.27170/0.17841. Took 0.04 sec\n",
            "Epoch 1174, Loss(train/val) 0.27300/0.18025. Took 0.04 sec\n",
            "Epoch 1175, Loss(train/val) 0.26149/0.18157. Took 0.05 sec\n",
            "Epoch 1176, Loss(train/val) 0.26301/0.17726. Took 0.05 sec\n",
            "Epoch 1177, Loss(train/val) 0.26535/0.17436. Took 0.05 sec\n",
            "Epoch 1178, Loss(train/val) 0.26281/0.17391. Took 0.05 sec\n",
            "Epoch 1179, Loss(train/val) 0.26984/0.17415. Took 0.04 sec\n",
            "Epoch 1180, Loss(train/val) 0.26779/0.17411. Took 0.05 sec\n",
            "Epoch 1181, Loss(train/val) 0.26984/0.17462. Took 0.04 sec\n",
            "Epoch 1182, Loss(train/val) 0.27640/0.17534. Took 0.06 sec\n",
            "Epoch 1183, Loss(train/val) 0.27338/0.17447. Took 0.05 sec\n",
            "Epoch 1184, Loss(train/val) 0.26127/0.17423. Took 0.04 sec\n",
            "Epoch 1185, Loss(train/val) 0.26815/0.17749. Took 0.04 sec\n",
            "Epoch 1186, Loss(train/val) 0.27839/0.17671. Took 0.05 sec\n",
            "Epoch 1187, Loss(train/val) 0.27526/0.17716. Took 0.05 sec\n",
            "Epoch 1188, Loss(train/val) 0.26332/0.17553. Took 0.04 sec\n",
            "Epoch 1189, Loss(train/val) 0.26348/0.17429. Took 0.04 sec\n",
            "Epoch 1190, Loss(train/val) 0.26027/0.17385. Took 0.04 sec\n",
            "Epoch 1191, Loss(train/val) 0.26761/0.17312. Took 0.04 sec\n",
            "Epoch 1192, Loss(train/val) 0.26924/0.17263. Took 0.05 sec\n",
            "Epoch 1193, Loss(train/val) 0.27476/0.17307. Took 0.04 sec\n",
            "Epoch 1194, Loss(train/val) 0.28161/0.17232. Took 0.04 sec\n",
            "Epoch 1195, Loss(train/val) 0.26006/0.17423. Took 0.05 sec\n",
            "Epoch 1196, Loss(train/val) 0.26800/0.17428. Took 0.04 sec\n",
            "Epoch 1197, Loss(train/val) 0.26726/0.17458. Took 0.05 sec\n",
            "Epoch 1198, Loss(train/val) 0.26349/0.17500. Took 0.05 sec\n",
            "Epoch 1199, Loss(train/val) 0.26672/0.17409. Took 0.05 sec\n",
            "Epoch 1200, Loss(train/val) 0.25921/0.17459. Took 0.05 sec\n",
            "Epoch 1201, Loss(train/val) 0.26095/0.17408. Took 0.04 sec\n",
            "Epoch 1202, Loss(train/val) 0.25885/0.17720. Took 0.05 sec\n",
            "Epoch 1203, Loss(train/val) 0.26214/0.17584. Took 0.05 sec\n",
            "Epoch 1204, Loss(train/val) 0.26582/0.17340. Took 0.06 sec\n",
            "Epoch 1205, Loss(train/val) 0.26439/0.17344. Took 0.05 sec\n",
            "Epoch 1206, Loss(train/val) 0.25648/0.17248. Took 0.05 sec\n",
            "Epoch 1207, Loss(train/val) 0.26326/0.17222. Took 0.05 sec\n",
            "Epoch 1208, Loss(train/val) 0.26427/0.17210. Took 0.05 sec\n",
            "Epoch 1209, Loss(train/val) 0.26655/0.17249. Took 0.05 sec\n",
            "Epoch 1210, Loss(train/val) 0.26305/0.17304. Took 0.04 sec\n",
            "Epoch 1211, Loss(train/val) 0.26387/0.17258. Took 0.05 sec\n",
            "Epoch 1212, Loss(train/val) 0.25971/0.17194. Took 0.05 sec\n",
            "Epoch 1213, Loss(train/val) 0.25648/0.17252. Took 0.05 sec\n",
            "Epoch 1214, Loss(train/val) 0.27445/0.17798. Took 0.04 sec\n",
            "Epoch 1215, Loss(train/val) 0.26307/0.18175. Took 0.04 sec\n",
            "Epoch 1216, Loss(train/val) 0.26776/0.18022. Took 0.05 sec\n",
            "Epoch 1217, Loss(train/val) 0.25763/0.17526. Took 0.05 sec\n",
            "Epoch 1218, Loss(train/val) 0.26314/0.17209. Took 0.05 sec\n",
            "Epoch 1219, Loss(train/val) 0.26657/0.17148. Took 0.05 sec\n",
            "Epoch 1220, Loss(train/val) 0.24919/0.17189. Took 0.04 sec\n",
            "Epoch 1221, Loss(train/val) 0.26604/0.17161. Took 0.05 sec\n",
            "Epoch 1222, Loss(train/val) 0.25259/0.17148. Took 0.05 sec\n",
            "Epoch 1223, Loss(train/val) 0.25855/0.17191. Took 0.04 sec\n",
            "Epoch 1224, Loss(train/val) 0.26039/0.17464. Took 0.05 sec\n",
            "Epoch 1225, Loss(train/val) 0.26150/0.17568. Took 0.05 sec\n",
            "Epoch 1226, Loss(train/val) 0.25650/0.17741. Took 0.05 sec\n",
            "Epoch 1227, Loss(train/val) 0.25405/0.17614. Took 0.04 sec\n",
            "Epoch 1228, Loss(train/val) 0.25507/0.17332. Took 0.05 sec\n",
            "Epoch 1229, Loss(train/val) 0.27623/0.17179. Took 0.04 sec\n",
            "Epoch 1230, Loss(train/val) 0.26976/0.17136. Took 0.05 sec\n",
            "Epoch 1231, Loss(train/val) 0.25591/0.17144. Took 0.06 sec\n",
            "Epoch 1232, Loss(train/val) 0.26095/0.17238. Took 0.04 sec\n",
            "Epoch 1233, Loss(train/val) 0.25492/0.17297. Took 0.04 sec\n",
            "Epoch 1234, Loss(train/val) 0.26107/0.17225. Took 0.05 sec\n",
            "Epoch 1235, Loss(train/val) 0.25901/0.17255. Took 0.04 sec\n",
            "Epoch 1236, Loss(train/val) 0.25170/0.17356. Took 0.05 sec\n",
            "Epoch 1237, Loss(train/val) 0.24952/0.17470. Took 0.05 sec\n",
            "Epoch 1238, Loss(train/val) 0.25717/0.17514. Took 0.05 sec\n",
            "Epoch 1239, Loss(train/val) 0.25493/0.17298. Took 0.05 sec\n",
            "Epoch 1240, Loss(train/val) 0.25458/0.17146. Took 0.05 sec\n",
            "Epoch 1241, Loss(train/val) 0.25995/0.17163. Took 0.05 sec\n",
            "Epoch 1242, Loss(train/val) 0.25487/0.17271. Took 0.04 sec\n",
            "Epoch 1243, Loss(train/val) 0.27241/0.17206. Took 0.05 sec\n",
            "Epoch 1244, Loss(train/val) 0.25715/0.17254. Took 0.05 sec\n",
            "Epoch 1245, Loss(train/val) 0.25440/0.17176. Took 0.04 sec\n",
            "Epoch 1246, Loss(train/val) 0.25250/0.17401. Took 0.05 sec\n",
            "Epoch 1247, Loss(train/val) 0.25904/0.17529. Took 0.05 sec\n",
            "Epoch 1248, Loss(train/val) 0.25057/0.17395. Took 0.04 sec\n",
            "Epoch 1249, Loss(train/val) 0.25497/0.17393. Took 0.04 sec\n",
            "Epoch 1250, Loss(train/val) 0.25546/0.17277. Took 0.05 sec\n",
            "Epoch 1251, Loss(train/val) 0.27086/0.17228. Took 0.05 sec\n",
            "Epoch 1252, Loss(train/val) 0.26389/0.17452. Took 0.05 sec\n",
            "Epoch 1253, Loss(train/val) 0.26157/0.17235. Took 0.04 sec\n",
            "Epoch 1254, Loss(train/val) 0.26151/0.17183. Took 0.05 sec\n",
            "Epoch 1255, Loss(train/val) 0.25427/0.17142. Took 0.05 sec\n",
            "Epoch 1256, Loss(train/val) 0.25729/0.17157. Took 0.05 sec\n",
            "Epoch 1257, Loss(train/val) 0.24908/0.17187. Took 0.04 sec\n",
            "Epoch 1258, Loss(train/val) 0.26204/0.17133. Took 0.05 sec\n",
            "Epoch 1259, Loss(train/val) 0.26363/0.17120. Took 0.04 sec\n",
            "Epoch 1260, Loss(train/val) 0.26443/0.17134. Took 0.05 sec\n",
            "Epoch 1261, Loss(train/val) 0.27071/0.17156. Took 0.05 sec\n",
            "Epoch 1262, Loss(train/val) 0.25521/0.17365. Took 0.05 sec\n",
            "Epoch 1263, Loss(train/val) 0.26315/0.17639. Took 0.05 sec\n",
            "Epoch 1264, Loss(train/val) 0.24685/0.17446. Took 0.05 sec\n",
            "Epoch 1265, Loss(train/val) 0.25823/0.17149. Took 0.05 sec\n",
            "Epoch 1266, Loss(train/val) 0.25472/0.17288. Took 0.05 sec\n",
            "Epoch 1267, Loss(train/val) 0.26538/0.17430. Took 0.05 sec\n",
            "Epoch 1268, Loss(train/val) 0.25712/0.17355. Took 0.06 sec\n",
            "Epoch 1269, Loss(train/val) 0.25764/0.17260. Took 0.05 sec\n",
            "Epoch 1270, Loss(train/val) 0.24988/0.17207. Took 0.05 sec\n",
            "Epoch 1271, Loss(train/val) 0.25201/0.17167. Took 0.05 sec\n",
            "Epoch 1272, Loss(train/val) 0.25023/0.17520. Took 0.05 sec\n",
            "Epoch 1273, Loss(train/val) 0.26249/0.17512. Took 0.05 sec\n",
            "Epoch 1274, Loss(train/val) 0.25367/0.17352. Took 0.05 sec\n",
            "Epoch 1275, Loss(train/val) 0.25020/0.17187. Took 0.05 sec\n",
            "Epoch 1276, Loss(train/val) 0.27765/0.17253. Took 0.06 sec\n",
            "Epoch 1277, Loss(train/val) 0.25255/0.17191. Took 0.05 sec\n",
            "Epoch 1278, Loss(train/val) 0.25103/0.17179. Took 0.05 sec\n",
            "Epoch 1279, Loss(train/val) 0.25897/0.17150. Took 0.05 sec\n",
            "Epoch 1280, Loss(train/val) 0.26974/0.17309. Took 0.05 sec\n",
            "Epoch 1281, Loss(train/val) 0.25487/0.17208. Took 0.05 sec\n",
            "Epoch 1282, Loss(train/val) 0.25019/0.17491. Took 0.05 sec\n",
            "Epoch 1283, Loss(train/val) 0.25249/0.17690. Took 0.05 sec\n",
            "Epoch 1284, Loss(train/val) 0.26097/0.17299. Took 0.05 sec\n",
            "Epoch 1285, Loss(train/val) 0.26524/0.17138. Took 0.05 sec\n",
            "Epoch 1286, Loss(train/val) 0.25783/0.17176. Took 0.06 sec\n",
            "Epoch 1287, Loss(train/val) 0.25344/0.17196. Took 0.05 sec\n",
            "Epoch 1288, Loss(train/val) 0.26646/0.17217. Took 0.05 sec\n",
            "Epoch 1289, Loss(train/val) 0.26365/0.17145. Took 0.04 sec\n",
            "Epoch 1290, Loss(train/val) 0.25077/0.17128. Took 0.04 sec\n",
            "Epoch 1291, Loss(train/val) 0.24824/0.17143. Took 0.05 sec\n",
            "Epoch 1292, Loss(train/val) 0.25106/0.17110. Took 0.04 sec\n",
            "Epoch 1293, Loss(train/val) 0.25848/0.17104. Took 0.04 sec\n",
            "Epoch 1294, Loss(train/val) 0.25172/0.17116. Took 0.05 sec\n",
            "Epoch 1295, Loss(train/val) 0.26407/0.17146. Took 0.05 sec\n",
            "Epoch 1296, Loss(train/val) 0.25241/0.17171. Took 0.05 sec\n",
            "Epoch 1297, Loss(train/val) 0.25609/0.17253. Took 0.05 sec\n",
            "Epoch 1298, Loss(train/val) 0.26059/0.17198. Took 0.04 sec\n",
            "Epoch 1299, Loss(train/val) 0.25735/0.17198. Took 0.04 sec\n",
            "Epoch 1300, Loss(train/val) 0.25590/0.17196. Took 0.04 sec\n",
            "Epoch 1301, Loss(train/val) 0.25310/0.17367. Took 0.05 sec\n",
            "Epoch 1302, Loss(train/val) 0.24870/0.17339. Took 0.04 sec\n",
            "Epoch 1303, Loss(train/val) 0.25032/0.17228. Took 0.04 sec\n",
            "Epoch 1304, Loss(train/val) 0.26608/0.17162. Took 0.04 sec\n",
            "Epoch 1305, Loss(train/val) 0.28284/0.17216. Took 0.05 sec\n",
            "Epoch 1306, Loss(train/val) 0.25454/0.17193. Took 0.06 sec\n",
            "Epoch 1307, Loss(train/val) 0.26470/0.17193. Took 0.05 sec\n",
            "Epoch 1308, Loss(train/val) 0.25442/0.17310. Took 0.05 sec\n",
            "Epoch 1309, Loss(train/val) 0.28956/0.17619. Took 0.05 sec\n",
            "Epoch 1310, Loss(train/val) 0.26217/0.17718. Took 0.05 sec\n",
            "Epoch 1311, Loss(train/val) 0.24811/0.17140. Took 0.05 sec\n",
            "Epoch 1312, Loss(train/val) 0.25868/0.17387. Took 0.05 sec\n",
            "Epoch 1313, Loss(train/val) 0.24719/0.17771. Took 0.04 sec\n",
            "Epoch 1314, Loss(train/val) 0.25808/0.17976. Took 0.04 sec\n",
            "Epoch 1315, Loss(train/val) 0.24785/0.17814. Took 0.05 sec\n",
            "Epoch 1316, Loss(train/val) 0.25065/0.17399. Took 0.05 sec\n",
            "Epoch 1317, Loss(train/val) 0.25782/0.17147. Took 0.05 sec\n",
            "Epoch 1318, Loss(train/val) 0.25791/0.17343. Took 0.04 sec\n",
            "Epoch 1319, Loss(train/val) 0.24600/0.17531. Took 0.05 sec\n",
            "Epoch 1320, Loss(train/val) 0.27261/0.17593. Took 0.04 sec\n",
            "Epoch 1321, Loss(train/val) 0.25109/0.17657. Took 0.05 sec\n",
            "Epoch 1322, Loss(train/val) 0.26436/0.17221. Took 0.05 sec\n",
            "Epoch 1323, Loss(train/val) 0.24596/0.17151. Took 0.04 sec\n",
            "Epoch 1324, Loss(train/val) 0.25983/0.17763. Took 0.04 sec\n",
            "Epoch 1325, Loss(train/val) 0.25415/0.17900. Took 0.05 sec\n",
            "Epoch 1326, Loss(train/val) 0.26037/0.17438. Took 0.05 sec\n",
            "Epoch 1327, Loss(train/val) 0.25966/0.17123. Took 0.05 sec\n",
            "Epoch 1328, Loss(train/val) 0.26454/0.17432. Took 0.05 sec\n",
            "Epoch 1329, Loss(train/val) 0.25168/0.17626. Took 0.05 sec\n",
            "Epoch 1330, Loss(train/val) 0.25628/0.17431. Took 0.04 sec\n",
            "Epoch 1331, Loss(train/val) 0.25139/0.17087. Took 0.06 sec\n",
            "Epoch 1332, Loss(train/val) 0.25986/0.17116. Took 0.05 sec\n",
            "Epoch 1333, Loss(train/val) 0.24386/0.17363. Took 0.04 sec\n",
            "Epoch 1334, Loss(train/val) 0.25675/0.17358. Took 0.05 sec\n",
            "Epoch 1335, Loss(train/val) 0.25539/0.17315. Took 0.04 sec\n",
            "Epoch 1336, Loss(train/val) 0.25194/0.17256. Took 0.05 sec\n",
            "Epoch 1337, Loss(train/val) 0.24675/0.17238. Took 0.04 sec\n",
            "Epoch 1338, Loss(train/val) 0.26145/0.17096. Took 0.05 sec\n",
            "Epoch 1339, Loss(train/val) 0.25275/0.17102. Took 0.05 sec\n",
            "Epoch 1340, Loss(train/val) 0.25628/0.17085. Took 0.04 sec\n",
            "Epoch 1341, Loss(train/val) 0.25370/0.17074. Took 0.05 sec\n",
            "Epoch 1342, Loss(train/val) 0.25021/0.17144. Took 0.05 sec\n",
            "Epoch 1343, Loss(train/val) 0.25331/0.17191. Took 0.05 sec\n",
            "Epoch 1344, Loss(train/val) 0.25109/0.17184. Took 0.05 sec\n",
            "Epoch 1345, Loss(train/val) 0.24975/0.17140. Took 0.04 sec\n",
            "Epoch 1346, Loss(train/val) 0.25069/0.17135. Took 0.05 sec\n",
            "Epoch 1347, Loss(train/val) 0.25403/0.17162. Took 0.04 sec\n",
            "Epoch 1348, Loss(train/val) 0.26540/0.17185. Took 0.04 sec\n",
            "Epoch 1349, Loss(train/val) 0.25159/0.17383. Took 0.05 sec\n",
            "Epoch 1350, Loss(train/val) 0.26777/0.17228. Took 0.05 sec\n",
            "Epoch 1351, Loss(train/val) 0.26022/0.17084. Took 0.06 sec\n",
            "Epoch 1352, Loss(train/val) 0.25205/0.17141. Took 0.04 sec\n",
            "Epoch 1353, Loss(train/val) 0.25864/0.17160. Took 0.05 sec\n",
            "Epoch 1354, Loss(train/val) 0.26760/0.17158. Took 0.05 sec\n",
            "Epoch 1355, Loss(train/val) 0.25792/0.17110. Took 0.04 sec\n",
            "Epoch 1356, Loss(train/val) 0.25610/0.17080. Took 0.05 sec\n",
            "Epoch 1357, Loss(train/val) 0.25209/0.17075. Took 0.05 sec\n",
            "Epoch 1358, Loss(train/val) 0.25695/0.17084. Took 0.04 sec\n",
            "Epoch 1359, Loss(train/val) 0.25138/0.17190. Took 0.05 sec\n",
            "Epoch 1360, Loss(train/val) 0.24745/0.17069. Took 0.04 sec\n",
            "Epoch 1361, Loss(train/val) 0.25165/0.17065. Took 0.06 sec\n",
            "Epoch 1362, Loss(train/val) 0.25293/0.17089. Took 0.05 sec\n",
            "Epoch 1363, Loss(train/val) 0.24548/0.17112. Took 0.04 sec\n",
            "Epoch 1364, Loss(train/val) 0.25951/0.17056. Took 0.05 sec\n",
            "Epoch 1365, Loss(train/val) 0.24829/0.17146. Took 0.05 sec\n",
            "Epoch 1366, Loss(train/val) 0.25353/0.17320. Took 0.06 sec\n",
            "Epoch 1367, Loss(train/val) 0.25286/0.17140. Took 0.04 sec\n",
            "Epoch 1368, Loss(train/val) 0.27093/0.17066. Took 0.04 sec\n",
            "Epoch 1369, Loss(train/val) 0.25597/0.17167. Took 0.04 sec\n",
            "Epoch 1370, Loss(train/val) 0.25407/0.17289. Took 0.05 sec\n",
            "Epoch 1371, Loss(train/val) 0.25816/0.17522. Took 0.05 sec\n",
            "Epoch 1372, Loss(train/val) 0.24988/0.17146. Took 0.05 sec\n",
            "Epoch 1373, Loss(train/val) 0.25298/0.17053. Took 0.05 sec\n",
            "Epoch 1374, Loss(train/val) 0.25007/0.17094. Took 0.05 sec\n",
            "Epoch 1375, Loss(train/val) 0.27821/0.17150. Took 0.05 sec\n",
            "Epoch 1376, Loss(train/val) 0.24939/0.17105. Took 0.06 sec\n",
            "Epoch 1377, Loss(train/val) 0.25288/0.17086. Took 0.05 sec\n",
            "Epoch 1378, Loss(train/val) 0.25922/0.17079. Took 0.05 sec\n",
            "Epoch 1379, Loss(train/val) 0.25137/0.17045. Took 0.04 sec\n",
            "Epoch 1380, Loss(train/val) 0.25684/0.17029. Took 0.05 sec\n",
            "Epoch 1381, Loss(train/val) 0.25864/0.17017. Took 0.05 sec\n",
            "Epoch 1382, Loss(train/val) 0.25313/0.17022. Took 0.05 sec\n",
            "Epoch 1383, Loss(train/val) 0.25667/0.17025. Took 0.04 sec\n",
            "Epoch 1384, Loss(train/val) 0.25545/0.17048. Took 0.05 sec\n",
            "Epoch 1385, Loss(train/val) 0.25511/0.17055. Took 0.05 sec\n",
            "Epoch 1386, Loss(train/val) 0.25309/0.17094. Took 0.05 sec\n",
            "Epoch 1387, Loss(train/val) 0.24638/0.17134. Took 0.04 sec\n",
            "Epoch 1388, Loss(train/val) 0.25687/0.17110. Took 0.05 sec\n",
            "Epoch 1389, Loss(train/val) 0.27173/0.17106. Took 0.05 sec\n",
            "Epoch 1390, Loss(train/val) 0.25721/0.17035. Took 0.05 sec\n",
            "Epoch 1391, Loss(train/val) 0.24789/0.17003. Took 0.05 sec\n",
            "Epoch 1392, Loss(train/val) 0.24734/0.17037. Took 0.04 sec\n",
            "Epoch 1393, Loss(train/val) 0.24680/0.17106. Took 0.04 sec\n",
            "Epoch 1394, Loss(train/val) 0.26108/0.17085. Took 0.04 sec\n",
            "Epoch 1395, Loss(train/val) 0.25271/0.17177. Took 0.05 sec\n",
            "Epoch 1396, Loss(train/val) 0.26335/0.17083. Took 0.06 sec\n",
            "Epoch 1397, Loss(train/val) 0.25716/0.17032. Took 0.05 sec\n",
            "Epoch 1398, Loss(train/val) 0.26081/0.17036. Took 0.04 sec\n",
            "Epoch 1399, Loss(train/val) 0.27642/0.17021. Took 0.05 sec\n",
            "Epoch 1400, Loss(train/val) 0.23848/0.17025. Took 0.05 sec\n",
            "Epoch 1401, Loss(train/val) 0.24967/0.17058. Took 0.05 sec\n",
            "Epoch 1402, Loss(train/val) 0.25769/0.17186. Took 0.05 sec\n",
            "Epoch 1403, Loss(train/val) 0.26452/0.17118. Took 0.04 sec\n",
            "Epoch 1404, Loss(train/val) 0.25084/0.17015. Took 0.04 sec\n",
            "Epoch 1405, Loss(train/val) 0.25127/0.17113. Took 0.05 sec\n",
            "Epoch 1406, Loss(train/val) 0.26520/0.17357. Took 0.05 sec\n",
            "Epoch 1407, Loss(train/val) 0.25436/0.17328. Took 0.04 sec\n",
            "Epoch 1408, Loss(train/val) 0.25055/0.17072. Took 0.05 sec\n",
            "Epoch 1409, Loss(train/val) 0.25206/0.17063. Took 0.05 sec\n",
            "Epoch 1410, Loss(train/val) 0.24963/0.17045. Took 0.04 sec\n",
            "Epoch 1411, Loss(train/val) 0.24601/0.17004. Took 0.05 sec\n",
            "Epoch 1412, Loss(train/val) 0.25389/0.16993. Took 0.05 sec\n",
            "Epoch 1413, Loss(train/val) 0.25029/0.17051. Took 0.05 sec\n",
            "Epoch 1414, Loss(train/val) 0.26197/0.17079. Took 0.05 sec\n",
            "Epoch 1415, Loss(train/val) 0.25492/0.17057. Took 0.05 sec\n",
            "Epoch 1416, Loss(train/val) 0.24963/0.17191. Took 0.05 sec\n",
            "Epoch 1417, Loss(train/val) 0.24770/0.17254. Took 0.05 sec\n",
            "Epoch 1418, Loss(train/val) 0.24589/0.17019. Took 0.05 sec\n",
            "Epoch 1419, Loss(train/val) 0.25233/0.17019. Took 0.05 sec\n",
            "Epoch 1420, Loss(train/val) 0.24701/0.17177. Took 0.05 sec\n",
            "Epoch 1421, Loss(train/val) 0.25449/0.17221. Took 0.06 sec\n",
            "Epoch 1422, Loss(train/val) 0.25243/0.17116. Took 0.05 sec\n",
            "Epoch 1423, Loss(train/val) 0.25253/0.17018. Took 0.04 sec\n",
            "Epoch 1424, Loss(train/val) 0.26449/0.16977. Took 0.04 sec\n",
            "Epoch 1425, Loss(train/val) 0.24511/0.17004. Took 0.04 sec\n",
            "Epoch 1426, Loss(train/val) 0.25024/0.17328. Took 0.05 sec\n",
            "Epoch 1427, Loss(train/val) 0.24355/0.17337. Took 0.05 sec\n",
            "Epoch 1428, Loss(train/val) 0.24661/0.17089. Took 0.05 sec\n",
            "Epoch 1429, Loss(train/val) 0.24555/0.17018. Took 0.05 sec\n",
            "Epoch 1430, Loss(train/val) 0.24228/0.16943. Took 0.05 sec\n",
            "Epoch 1431, Loss(train/val) 0.24782/0.17105. Took 0.05 sec\n",
            "Epoch 1432, Loss(train/val) 0.24269/0.17508. Took 0.05 sec\n",
            "Epoch 1433, Loss(train/val) 0.25286/0.17964. Took 0.05 sec\n",
            "Epoch 1434, Loss(train/val) 0.24292/0.18126. Took 0.05 sec\n",
            "Epoch 1435, Loss(train/val) 0.25483/0.17249. Took 0.04 sec\n",
            "Epoch 1436, Loss(train/val) 0.25230/0.17027. Took 0.06 sec\n",
            "Epoch 1437, Loss(train/val) 0.26310/0.17012. Took 0.05 sec\n",
            "Epoch 1438, Loss(train/val) 0.25232/0.17198. Took 0.05 sec\n",
            "Epoch 1439, Loss(train/val) 0.25681/0.17500. Took 0.05 sec\n",
            "Epoch 1440, Loss(train/val) 0.24554/0.17101. Took 0.04 sec\n",
            "Epoch 1441, Loss(train/val) 0.25579/0.17055. Took 0.05 sec\n",
            "Epoch 1442, Loss(train/val) 0.27172/0.17005. Took 0.04 sec\n",
            "Epoch 1443, Loss(train/val) 0.24740/0.16948. Took 0.05 sec\n",
            "Epoch 1444, Loss(train/val) 0.24484/0.17081. Took 0.04 sec\n",
            "Epoch 1445, Loss(train/val) 0.24575/0.17163. Took 0.05 sec\n",
            "Epoch 1446, Loss(train/val) 0.23902/0.16974. Took 0.05 sec\n",
            "Epoch 1447, Loss(train/val) 0.25172/0.16913. Took 0.05 sec\n",
            "Epoch 1448, Loss(train/val) 0.24844/0.16904. Took 0.05 sec\n",
            "Epoch 1449, Loss(train/val) 0.26211/0.16897. Took 0.05 sec\n",
            "Epoch 1450, Loss(train/val) 0.26134/0.16990. Took 0.05 sec\n",
            "Epoch 1451, Loss(train/val) 0.25256/0.17161. Took 0.05 sec\n",
            "Epoch 1452, Loss(train/val) 0.24907/0.17003. Took 0.04 sec\n",
            "Epoch 1453, Loss(train/val) 0.24508/0.16946. Took 0.05 sec\n",
            "Epoch 1454, Loss(train/val) 0.24217/0.16899. Took 0.05 sec\n",
            "Epoch 1455, Loss(train/val) 0.24686/0.16978. Took 0.05 sec\n",
            "Epoch 1456, Loss(train/val) 0.24629/0.17036. Took 0.05 sec\n",
            "Epoch 1457, Loss(train/val) 0.26515/0.16916. Took 0.04 sec\n",
            "Epoch 1458, Loss(train/val) 0.24317/0.16907. Took 0.05 sec\n",
            "Epoch 1459, Loss(train/val) 0.25718/0.16937. Took 0.04 sec\n",
            "Epoch 1460, Loss(train/val) 0.24392/0.16925. Took 0.05 sec\n",
            "Epoch 1461, Loss(train/val) 0.24555/0.16911. Took 0.05 sec\n",
            "Epoch 1462, Loss(train/val) 0.24043/0.16924. Took 0.04 sec\n",
            "Epoch 1463, Loss(train/val) 0.26542/0.16930. Took 0.05 sec\n",
            "Epoch 1464, Loss(train/val) 0.25392/0.17037. Took 0.05 sec\n",
            "Epoch 1465, Loss(train/val) 0.24505/0.16966. Took 0.04 sec\n",
            "Epoch 1466, Loss(train/val) 0.24421/0.16915. Took 0.05 sec\n",
            "Epoch 1467, Loss(train/val) 0.27270/0.16955. Took 0.04 sec\n",
            "Epoch 1468, Loss(train/val) 0.25312/0.17010. Took 0.05 sec\n",
            "Epoch 1469, Loss(train/val) 0.24838/0.16927. Took 0.05 sec\n",
            "Epoch 1470, Loss(train/val) 0.24702/0.16894. Took 0.04 sec\n",
            "Epoch 1471, Loss(train/val) 0.24505/0.16918. Took 0.05 sec\n",
            "Epoch 1472, Loss(train/val) 0.24787/0.17152. Took 0.04 sec\n",
            "Epoch 1473, Loss(train/val) 0.24810/0.17052. Took 0.05 sec\n",
            "Epoch 1474, Loss(train/val) 0.23556/0.16944. Took 0.04 sec\n",
            "Epoch 1475, Loss(train/val) 0.24365/0.16909. Took 0.05 sec\n",
            "Epoch 1476, Loss(train/val) 0.24811/0.16923. Took 0.05 sec\n",
            "Epoch 1477, Loss(train/val) 0.24502/0.16905. Took 0.06 sec\n",
            "Epoch 1478, Loss(train/val) 0.25582/0.16897. Took 0.05 sec\n",
            "Epoch 1479, Loss(train/val) 0.25136/0.16901. Took 0.05 sec\n",
            "Epoch 1480, Loss(train/val) 0.25929/0.16883. Took 0.04 sec\n",
            "Epoch 1481, Loss(train/val) 0.24282/0.16895. Took 0.06 sec\n",
            "Epoch 1482, Loss(train/val) 0.24406/0.16976. Took 0.05 sec\n",
            "Epoch 1483, Loss(train/val) 0.25610/0.17006. Took 0.05 sec\n",
            "Epoch 1484, Loss(train/val) 0.24977/0.16895. Took 0.05 sec\n",
            "Epoch 1485, Loss(train/val) 0.24208/0.16863. Took 0.05 sec\n",
            "Epoch 1486, Loss(train/val) 0.24605/0.16904. Took 0.05 sec\n",
            "Epoch 1487, Loss(train/val) 0.24974/0.16865. Took 0.05 sec\n",
            "Epoch 1488, Loss(train/val) 0.27527/0.16905. Took 0.04 sec\n",
            "Epoch 1489, Loss(train/val) 0.25099/0.16913. Took 0.05 sec\n",
            "Epoch 1490, Loss(train/val) 0.25479/0.16898. Took 0.05 sec\n",
            "Epoch 1491, Loss(train/val) 0.24254/0.16858. Took 0.06 sec\n",
            "Epoch 1492, Loss(train/val) 0.25592/0.16932. Took 0.05 sec\n",
            "Epoch 1493, Loss(train/val) 0.24376/0.16877. Took 0.05 sec\n",
            "Epoch 1494, Loss(train/val) 0.24548/0.16884. Took 0.05 sec\n",
            "Epoch 1495, Loss(train/val) 0.25222/0.16916. Took 0.05 sec\n",
            "Epoch 1496, Loss(train/val) 0.25840/0.16864. Took 0.06 sec\n",
            "Epoch 1497, Loss(train/val) 0.24712/0.16861. Took 0.05 sec\n",
            "Epoch 1498, Loss(train/val) 0.24453/0.16848. Took 0.05 sec\n",
            "Epoch 1499, Loss(train/val) 0.24540/0.16836. Took 0.05 sec\n",
            "Epoch 1500, Loss(train/val) 0.24998/0.16893. Took 0.05 sec\n",
            "Epoch 1501, Loss(train/val) 0.24445/0.16892. Took 0.05 sec\n",
            "Epoch 1502, Loss(train/val) 0.24324/0.17052. Took 0.05 sec\n",
            "Epoch 1503, Loss(train/val) 0.25403/0.16896. Took 0.05 sec\n",
            "Epoch 1504, Loss(train/val) 0.26349/0.16865. Took 0.04 sec\n",
            "Epoch 1505, Loss(train/val) 0.23594/0.16847. Took 0.05 sec\n",
            "Epoch 1506, Loss(train/val) 0.24347/0.16835. Took 0.04 sec\n",
            "Epoch 1507, Loss(train/val) 0.24908/0.16823. Took 0.04 sec\n",
            "Epoch 1508, Loss(train/val) 0.25429/0.16822. Took 0.04 sec\n",
            "Epoch 1509, Loss(train/val) 0.26945/0.16836. Took 0.05 sec\n",
            "Epoch 1510, Loss(train/val) 0.25223/0.16814. Took 0.05 sec\n",
            "Epoch 1511, Loss(train/val) 0.24302/0.16811. Took 0.05 sec\n",
            "Epoch 1512, Loss(train/val) 0.25767/0.16837. Took 0.05 sec\n",
            "Epoch 1513, Loss(train/val) 0.25889/0.16837. Took 0.05 sec\n",
            "Epoch 1514, Loss(train/val) 0.24556/0.16824. Took 0.05 sec\n",
            "Epoch 1515, Loss(train/val) 0.25291/0.16879. Took 0.05 sec\n",
            "Epoch 1516, Loss(train/val) 0.23963/0.16922. Took 0.05 sec\n",
            "Epoch 1517, Loss(train/val) 0.24358/0.16860. Took 0.04 sec\n",
            "Epoch 1518, Loss(train/val) 0.24204/0.16811. Took 0.05 sec\n",
            "Epoch 1519, Loss(train/val) 0.24123/0.16813. Took 0.04 sec\n",
            "Epoch 1520, Loss(train/val) 0.25246/0.16815. Took 0.05 sec\n",
            "Epoch 1521, Loss(train/val) 0.23888/0.16842. Took 0.05 sec\n",
            "Epoch 1522, Loss(train/val) 0.26689/0.16873. Took 0.05 sec\n",
            "Epoch 1523, Loss(train/val) 0.24369/0.16879. Took 0.06 sec\n",
            "Epoch 1524, Loss(train/val) 0.25861/0.16824. Took 0.05 sec\n",
            "Epoch 1525, Loss(train/val) 0.24553/0.16765. Took 0.06 sec\n",
            "Epoch 1526, Loss(train/val) 0.23692/0.16779. Took 0.05 sec\n",
            "Epoch 1527, Loss(train/val) 0.25573/0.16826. Took 0.04 sec\n",
            "Epoch 1528, Loss(train/val) 0.24502/0.16914. Took 0.05 sec\n",
            "Epoch 1529, Loss(train/val) 0.24519/0.16888. Took 0.04 sec\n",
            "Epoch 1530, Loss(train/val) 0.25797/0.16942. Took 0.05 sec\n",
            "Epoch 1531, Loss(train/val) 0.24943/0.16813. Took 0.05 sec\n",
            "Epoch 1532, Loss(train/val) 0.24163/0.16812. Took 0.04 sec\n",
            "Epoch 1533, Loss(train/val) 0.24216/0.16853. Took 0.04 sec\n",
            "Epoch 1534, Loss(train/val) 0.25858/0.16980. Took 0.04 sec\n",
            "Epoch 1535, Loss(train/val) 0.24301/0.16889. Took 0.05 sec\n",
            "Epoch 1536, Loss(train/val) 0.25045/0.16787. Took 0.05 sec\n",
            "Epoch 1537, Loss(train/val) 0.24756/0.16768. Took 0.05 sec\n",
            "Epoch 1538, Loss(train/val) 0.24822/0.16773. Took 0.04 sec\n",
            "Epoch 1539, Loss(train/val) 0.25429/0.16853. Took 0.04 sec\n",
            "Epoch 1540, Loss(train/val) 0.24142/0.16895. Took 0.05 sec\n",
            "Epoch 1541, Loss(train/val) 0.24717/0.16942. Took 0.05 sec\n",
            "Epoch 1542, Loss(train/val) 0.23768/0.17045. Took 0.05 sec\n",
            "Epoch 1543, Loss(train/val) 0.25529/0.17081. Took 0.05 sec\n",
            "Epoch 1544, Loss(train/val) 0.24579/0.16912. Took 0.05 sec\n",
            "Epoch 1545, Loss(train/val) 0.26933/0.16794. Took 0.05 sec\n",
            "Epoch 1546, Loss(train/val) 0.24841/0.16764. Took 0.04 sec\n",
            "Epoch 1547, Loss(train/val) 0.24335/0.16769. Took 0.05 sec\n",
            "Epoch 1548, Loss(train/val) 0.26203/0.16778. Took 0.04 sec\n",
            "Epoch 1549, Loss(train/val) 0.23468/0.16908. Took 0.04 sec\n",
            "Epoch 1550, Loss(train/val) 0.24216/0.17125. Took 0.05 sec\n",
            "Epoch 1551, Loss(train/val) 0.23915/0.16981. Took 0.05 sec\n",
            "Epoch 1552, Loss(train/val) 0.24278/0.16735. Took 0.05 sec\n",
            "Epoch 1553, Loss(train/val) 0.23564/0.16763. Took 0.04 sec\n",
            "Epoch 1554, Loss(train/val) 0.25348/0.16749. Took 0.05 sec\n",
            "Epoch 1555, Loss(train/val) 0.24439/0.16763. Took 0.05 sec\n",
            "Epoch 1556, Loss(train/val) 0.25881/0.16752. Took 0.05 sec\n",
            "Epoch 1557, Loss(train/val) 0.24196/0.16758. Took 0.05 sec\n",
            "Epoch 1558, Loss(train/val) 0.24627/0.16785. Took 0.05 sec\n",
            "Epoch 1559, Loss(train/val) 0.24251/0.16754. Took 0.06 sec\n",
            "Epoch 1560, Loss(train/val) 0.25902/0.16752. Took 0.05 sec\n",
            "Epoch 1561, Loss(train/val) 0.23970/0.16763. Took 0.05 sec\n",
            "Epoch 1562, Loss(train/val) 0.24555/0.16728. Took 0.05 sec\n",
            "Epoch 1563, Loss(train/val) 0.25785/0.16733. Took 0.05 sec\n",
            "Epoch 1564, Loss(train/val) 0.24405/0.16735. Took 0.06 sec\n",
            "Epoch 1565, Loss(train/val) 0.24762/0.16734. Took 0.06 sec\n",
            "Epoch 1566, Loss(train/val) 0.24183/0.16721. Took 0.05 sec\n",
            "Epoch 1567, Loss(train/val) 0.23800/0.16723. Took 0.05 sec\n",
            "Epoch 1568, Loss(train/val) 0.24531/0.16765. Took 0.06 sec\n",
            "Epoch 1569, Loss(train/val) 0.24603/0.16789. Took 0.05 sec\n",
            "Epoch 1570, Loss(train/val) 0.26120/0.16930. Took 0.05 sec\n",
            "Epoch 1571, Loss(train/val) 0.24146/0.16771. Took 0.05 sec\n",
            "Epoch 1572, Loss(train/val) 0.24470/0.16719. Took 0.05 sec\n",
            "Epoch 1573, Loss(train/val) 0.23978/0.16725. Took 0.06 sec\n",
            "Epoch 1574, Loss(train/val) 0.27792/0.16760. Took 0.05 sec\n",
            "Epoch 1575, Loss(train/val) 0.25847/0.16764. Took 0.05 sec\n",
            "Epoch 1576, Loss(train/val) 0.23577/0.16711. Took 0.05 sec\n",
            "Epoch 1577, Loss(train/val) 0.25227/0.16701. Took 0.05 sec\n",
            "Epoch 1578, Loss(train/val) 0.25019/0.16729. Took 0.05 sec\n",
            "Epoch 1579, Loss(train/val) 0.24411/0.16771. Took 0.05 sec\n",
            "Epoch 1580, Loss(train/val) 0.25469/0.16695. Took 0.04 sec\n",
            "Epoch 1581, Loss(train/val) 0.24776/0.16667. Took 0.04 sec\n",
            "Epoch 1582, Loss(train/val) 0.24336/0.16699. Took 0.04 sec\n",
            "Epoch 1583, Loss(train/val) 0.25373/0.16721. Took 0.06 sec\n",
            "Epoch 1584, Loss(train/val) 0.24663/0.16828. Took 0.05 sec\n",
            "Epoch 1585, Loss(train/val) 0.25272/0.16699. Took 0.05 sec\n",
            "Epoch 1586, Loss(train/val) 0.24269/0.16689. Took 0.04 sec\n",
            "Epoch 1587, Loss(train/val) 0.26397/0.16782. Took 0.05 sec\n",
            "Epoch 1588, Loss(train/val) 0.24464/0.16797. Took 0.05 sec\n",
            "Epoch 1589, Loss(train/val) 0.24915/0.16838. Took 0.05 sec\n",
            "Epoch 1590, Loss(train/val) 0.25328/0.16683. Took 0.04 sec\n",
            "Epoch 1591, Loss(train/val) 0.24610/0.16664. Took 0.04 sec\n",
            "Epoch 1592, Loss(train/val) 0.25598/0.16687. Took 0.04 sec\n",
            "Epoch 1593, Loss(train/val) 0.24921/0.16848. Took 0.05 sec\n",
            "Epoch 1594, Loss(train/val) 0.24625/0.17169. Took 0.04 sec\n",
            "Epoch 1595, Loss(train/val) 0.25784/0.17038. Took 0.04 sec\n",
            "Epoch 1596, Loss(train/val) 0.24647/0.17013. Took 0.05 sec\n",
            "Epoch 1597, Loss(train/val) 0.25036/0.16718. Took 0.04 sec\n",
            "Epoch 1598, Loss(train/val) 0.24071/0.16745. Took 0.05 sec\n",
            "Epoch 1599, Loss(train/val) 0.25500/0.16839. Took 0.04 sec\n",
            "Epoch 1600, Loss(train/val) 0.24325/0.16791. Took 0.05 sec\n",
            "Epoch 1601, Loss(train/val) 0.24941/0.16643. Took 0.04 sec\n",
            "Epoch 1602, Loss(train/val) 0.25144/0.16674. Took 0.05 sec\n",
            "Epoch 1603, Loss(train/val) 0.23797/0.16656. Took 0.05 sec\n",
            "Epoch 1604, Loss(train/val) 0.24167/0.16672. Took 0.04 sec\n",
            "Epoch 1605, Loss(train/val) 0.24024/0.16645. Took 0.04 sec\n",
            "Epoch 1606, Loss(train/val) 0.25596/0.16655. Took 0.05 sec\n",
            "Epoch 1607, Loss(train/val) 0.23823/0.16629. Took 0.05 sec\n",
            "Epoch 1608, Loss(train/val) 0.24193/0.16761. Took 0.05 sec\n",
            "Epoch 1609, Loss(train/val) 0.25157/0.16694. Took 0.05 sec\n",
            "Epoch 1610, Loss(train/val) 0.24024/0.16664. Took 0.04 sec\n",
            "Epoch 1611, Loss(train/val) 0.24304/0.16671. Took 0.04 sec\n",
            "Epoch 1612, Loss(train/val) 0.24244/0.16725. Took 0.04 sec\n",
            "Epoch 1613, Loss(train/val) 0.24339/0.16639. Took 0.05 sec\n",
            "Epoch 1614, Loss(train/val) 0.24125/0.16620. Took 0.05 sec\n",
            "Epoch 1615, Loss(train/val) 0.25405/0.16640. Took 0.04 sec\n",
            "Epoch 1616, Loss(train/val) 0.26100/0.16918. Took 0.04 sec\n",
            "Epoch 1617, Loss(train/val) 0.24651/0.16859. Took 0.04 sec\n",
            "Epoch 1618, Loss(train/val) 0.25910/0.16625. Took 0.05 sec\n",
            "Epoch 1619, Loss(train/val) 0.25560/0.16680. Took 0.05 sec\n",
            "Epoch 1620, Loss(train/val) 0.25667/0.16921. Took 0.04 sec\n",
            "Epoch 1621, Loss(train/val) 0.24173/0.16701. Took 0.05 sec\n",
            "Epoch 1622, Loss(train/val) 0.24908/0.16609. Took 0.05 sec\n",
            "Epoch 1623, Loss(train/val) 0.24183/0.16733. Took 0.05 sec\n",
            "Epoch 1624, Loss(train/val) 0.24455/0.16626. Took 0.05 sec\n",
            "Epoch 1625, Loss(train/val) 0.23862/0.16601. Took 0.05 sec\n",
            "Epoch 1626, Loss(train/val) 0.23365/0.16582. Took 0.05 sec\n",
            "Epoch 1627, Loss(train/val) 0.24022/0.16595. Took 0.05 sec\n",
            "Epoch 1628, Loss(train/val) 0.24437/0.16653. Took 0.06 sec\n",
            "Epoch 1629, Loss(train/val) 0.23768/0.16773. Took 0.05 sec\n",
            "Epoch 1630, Loss(train/val) 0.25675/0.16694. Took 0.05 sec\n",
            "Epoch 1631, Loss(train/val) 0.24476/0.16688. Took 0.05 sec\n",
            "Epoch 1632, Loss(train/val) 0.23532/0.16734. Took 0.04 sec\n",
            "Epoch 1633, Loss(train/val) 0.23374/0.16731. Took 0.05 sec\n",
            "Epoch 1634, Loss(train/val) 0.25361/0.16623. Took 0.05 sec\n",
            "Epoch 1635, Loss(train/val) 0.24679/0.16603. Took 0.05 sec\n",
            "Epoch 1636, Loss(train/val) 0.24573/0.16602. Took 0.04 sec\n",
            "Epoch 1637, Loss(train/val) 0.24893/0.16661. Took 0.05 sec\n",
            "Epoch 1638, Loss(train/val) 0.24283/0.16606. Took 0.05 sec\n",
            "Epoch 1639, Loss(train/val) 0.24375/0.16553. Took 0.05 sec\n",
            "Epoch 1640, Loss(train/val) 0.24111/0.16746. Took 0.05 sec\n",
            "Epoch 1641, Loss(train/val) 0.23482/0.16606. Took 0.04 sec\n",
            "Epoch 1642, Loss(train/val) 0.24200/0.16568. Took 0.05 sec\n",
            "Epoch 1643, Loss(train/val) 0.25017/0.16667. Took 0.05 sec\n",
            "Epoch 1644, Loss(train/val) 0.24333/0.16660. Took 0.04 sec\n",
            "Epoch 1645, Loss(train/val) 0.25302/0.16694. Took 0.04 sec\n",
            "Epoch 1646, Loss(train/val) 0.23625/0.16648. Took 0.05 sec\n",
            "Epoch 1647, Loss(train/val) 0.24323/0.16576. Took 0.04 sec\n",
            "Epoch 1648, Loss(train/val) 0.24049/0.16579. Took 0.05 sec\n",
            "Epoch 1649, Loss(train/val) 0.24533/0.16638. Took 0.05 sec\n",
            "Epoch 1650, Loss(train/val) 0.24759/0.16682. Took 0.05 sec\n",
            "Epoch 1651, Loss(train/val) 0.24581/0.16553. Took 0.05 sec\n",
            "Epoch 1652, Loss(train/val) 0.25968/0.16655. Took 0.04 sec\n",
            "Epoch 1653, Loss(train/val) 0.24371/0.16599. Took 0.05 sec\n",
            "Epoch 1654, Loss(train/val) 0.24571/0.16547. Took 0.05 sec\n",
            "Epoch 1655, Loss(train/val) 0.23631/0.16616. Took 0.04 sec\n",
            "Epoch 1656, Loss(train/val) 0.24154/0.16834. Took 0.05 sec\n",
            "Epoch 1657, Loss(train/val) 0.24319/0.16877. Took 0.05 sec\n",
            "Epoch 1658, Loss(train/val) 0.24793/0.16800. Took 0.05 sec\n",
            "Epoch 1659, Loss(train/val) 0.24726/0.16662. Took 0.05 sec\n",
            "Epoch 1660, Loss(train/val) 0.23557/0.16529. Took 0.05 sec\n",
            "Epoch 1661, Loss(train/val) 0.25522/0.16540. Took 0.05 sec\n",
            "Epoch 1662, Loss(train/val) 0.24698/0.16750. Took 0.05 sec\n",
            "Epoch 1663, Loss(train/val) 0.24187/0.16777. Took 0.05 sec\n",
            "Epoch 1664, Loss(train/val) 0.24158/0.16672. Took 0.04 sec\n",
            "Epoch 1665, Loss(train/val) 0.24575/0.16498. Took 0.05 sec\n",
            "Epoch 1666, Loss(train/val) 0.25229/0.16528. Took 0.05 sec\n",
            "Epoch 1667, Loss(train/val) 0.24570/0.16578. Took 0.05 sec\n",
            "Epoch 1668, Loss(train/val) 0.24503/0.16666. Took 0.06 sec\n",
            "Epoch 1669, Loss(train/val) 0.24156/0.16588. Took 0.05 sec\n",
            "Epoch 1670, Loss(train/val) 0.23918/0.16690. Took 0.04 sec\n",
            "Epoch 1671, Loss(train/val) 0.25715/0.16801. Took 0.05 sec\n",
            "Epoch 1672, Loss(train/val) 0.24166/0.16815. Took 0.05 sec\n",
            "Epoch 1673, Loss(train/val) 0.23828/0.16627. Took 0.05 sec\n",
            "Epoch 1674, Loss(train/val) 0.25892/0.16515. Took 0.05 sec\n",
            "Epoch 1675, Loss(train/val) 0.23487/0.16548. Took 0.04 sec\n",
            "Epoch 1676, Loss(train/val) 0.23861/0.16500. Took 0.05 sec\n",
            "Epoch 1677, Loss(train/val) 0.25095/0.16571. Took 0.05 sec\n",
            "Epoch 1678, Loss(train/val) 0.23710/0.16586. Took 0.05 sec\n",
            "Epoch 1679, Loss(train/val) 0.23438/0.16602. Took 0.05 sec\n",
            "Epoch 1680, Loss(train/val) 0.23599/0.16528. Took 0.04 sec\n",
            "Epoch 1681, Loss(train/val) 0.25112/0.16483. Took 0.05 sec\n",
            "Epoch 1682, Loss(train/val) 0.24139/0.16475. Took 0.05 sec\n",
            "Epoch 1683, Loss(train/val) 0.23583/0.16496. Took 0.05 sec\n",
            "Epoch 1684, Loss(train/val) 0.26822/0.16569. Took 0.05 sec\n",
            "Epoch 1685, Loss(train/val) 0.24396/0.16607. Took 0.05 sec\n",
            "Epoch 1686, Loss(train/val) 0.23738/0.16531. Took 0.05 sec\n",
            "Epoch 1687, Loss(train/val) 0.24342/0.16480. Took 0.05 sec\n",
            "Epoch 1688, Loss(train/val) 0.23823/0.16460. Took 0.05 sec\n",
            "Epoch 1689, Loss(train/val) 0.24660/0.16595. Took 0.04 sec\n",
            "Epoch 1690, Loss(train/val) 0.23237/0.16642. Took 0.05 sec\n",
            "Epoch 1691, Loss(train/val) 0.23357/0.16582. Took 0.05 sec\n",
            "Epoch 1692, Loss(train/val) 0.24316/0.16689. Took 0.05 sec\n",
            "Epoch 1693, Loss(train/val) 0.24266/0.16515. Took 0.05 sec\n",
            "Epoch 1694, Loss(train/val) 0.24138/0.16452. Took 0.05 sec\n",
            "Epoch 1695, Loss(train/val) 0.23087/0.16479. Took 0.04 sec\n",
            "Epoch 1696, Loss(train/val) 0.24184/0.16460. Took 0.05 sec\n",
            "Epoch 1697, Loss(train/val) 0.23519/0.16442. Took 0.05 sec\n",
            "Epoch 1698, Loss(train/val) 0.24737/0.16455. Took 0.05 sec\n",
            "Epoch 1699, Loss(train/val) 0.24006/0.16444. Took 0.05 sec\n",
            "Epoch 1700, Loss(train/val) 0.26398/0.16457. Took 0.04 sec\n",
            "Epoch 1701, Loss(train/val) 0.25765/0.16504. Took 0.04 sec\n",
            "Epoch 1702, Loss(train/val) 0.23964/0.16547. Took 0.05 sec\n",
            "Epoch 1703, Loss(train/val) 0.25822/0.16468. Took 0.06 sec\n",
            "Epoch 1704, Loss(train/val) 0.24438/0.16440. Took 0.05 sec\n",
            "Epoch 1705, Loss(train/val) 0.24979/0.16433. Took 0.05 sec\n",
            "Epoch 1706, Loss(train/val) 0.24207/0.16412. Took 0.05 sec\n",
            "Epoch 1707, Loss(train/val) 0.24913/0.16425. Took 0.05 sec\n",
            "Epoch 1708, Loss(train/val) 0.23443/0.16429. Took 0.06 sec\n",
            "Epoch 1709, Loss(train/val) 0.23923/0.16707. Took 0.05 sec\n",
            "Epoch 1710, Loss(train/val) 0.23334/0.16892. Took 0.05 sec\n",
            "Epoch 1711, Loss(train/val) 0.23134/0.16905. Took 0.05 sec\n",
            "Epoch 1712, Loss(train/val) 0.24321/0.16675. Took 0.05 sec\n",
            "Epoch 1713, Loss(train/val) 0.22929/0.16428. Took 0.06 sec\n",
            "Epoch 1714, Loss(train/val) 0.24921/0.16574. Took 0.04 sec\n",
            "Epoch 1715, Loss(train/val) 0.24491/0.16625. Took 0.04 sec\n",
            "Epoch 1716, Loss(train/val) 0.24129/0.16497. Took 0.04 sec\n",
            "Epoch 1717, Loss(train/val) 0.25527/0.16450. Took 0.05 sec\n",
            "Epoch 1718, Loss(train/val) 0.23455/0.16434. Took 0.05 sec\n",
            "Epoch 1719, Loss(train/val) 0.24841/0.16428. Took 0.05 sec\n",
            "Epoch 1720, Loss(train/val) 0.23590/0.16443. Took 0.05 sec\n",
            "Epoch 1721, Loss(train/val) 0.23682/0.16508. Took 0.05 sec\n",
            "Epoch 1722, Loss(train/val) 0.23633/0.16626. Took 0.05 sec\n",
            "Epoch 1723, Loss(train/val) 0.23907/0.16442. Took 0.06 sec\n",
            "Epoch 1724, Loss(train/val) 0.23251/0.16375. Took 0.05 sec\n",
            "Epoch 1725, Loss(train/val) 0.25628/0.16391. Took 0.05 sec\n",
            "Epoch 1726, Loss(train/val) 0.23772/0.16401. Took 0.05 sec\n",
            "Epoch 1727, Loss(train/val) 0.24548/0.16398. Took 0.05 sec\n",
            "Epoch 1728, Loss(train/val) 0.25238/0.16379. Took 0.06 sec\n",
            "Epoch 1729, Loss(train/val) 0.24906/0.16369. Took 0.05 sec\n",
            "Epoch 1730, Loss(train/val) 0.22991/0.16348. Took 0.05 sec\n",
            "Epoch 1731, Loss(train/val) 0.24792/0.16394. Took 0.05 sec\n",
            "Epoch 1732, Loss(train/val) 0.25823/0.16436. Took 0.05 sec\n",
            "Epoch 1733, Loss(train/val) 0.23284/0.16404. Took 0.05 sec\n",
            "Epoch 1734, Loss(train/val) 0.24750/0.16512. Took 0.05 sec\n",
            "Epoch 1735, Loss(train/val) 0.25868/0.16487. Took 0.04 sec\n",
            "Epoch 1736, Loss(train/val) 0.23282/0.16396. Took 0.04 sec\n",
            "Epoch 1737, Loss(train/val) 0.23139/0.16372. Took 0.05 sec\n",
            "Epoch 1738, Loss(train/val) 0.24107/0.16358. Took 0.05 sec\n",
            "Epoch 1739, Loss(train/val) 0.23117/0.16343. Took 0.05 sec\n",
            "Epoch 1740, Loss(train/val) 0.23748/0.16421. Took 0.04 sec\n",
            "Epoch 1741, Loss(train/val) 0.24153/0.16505. Took 0.04 sec\n",
            "Epoch 1742, Loss(train/val) 0.25065/0.16547. Took 0.04 sec\n",
            "Epoch 1743, Loss(train/val) 0.26286/0.16439. Took 0.05 sec\n",
            "Epoch 1744, Loss(train/val) 0.23400/0.16407. Took 0.06 sec\n",
            "Epoch 1745, Loss(train/val) 0.23727/0.16358. Took 0.05 sec\n",
            "Epoch 1746, Loss(train/val) 0.23754/0.16359. Took 0.05 sec\n",
            "Epoch 1747, Loss(train/val) 0.23454/0.16377. Took 0.05 sec\n",
            "Epoch 1748, Loss(train/val) 0.25118/0.16514. Took 0.05 sec\n",
            "Epoch 1749, Loss(train/val) 0.24535/0.16493. Took 0.04 sec\n",
            "Epoch 1750, Loss(train/val) 0.23651/0.16325. Took 0.05 sec\n",
            "Epoch 1751, Loss(train/val) 0.25014/0.16470. Took 0.05 sec\n",
            "Epoch 1752, Loss(train/val) 0.24286/0.16481. Took 0.05 sec\n",
            "Epoch 1753, Loss(train/val) 0.23895/0.16456. Took 0.05 sec\n",
            "Epoch 1754, Loss(train/val) 0.23605/0.16453. Took 0.04 sec\n",
            "Epoch 1755, Loss(train/val) 0.24436/0.16394. Took 0.05 sec\n",
            "Epoch 1756, Loss(train/val) 0.22657/0.16331. Took 0.05 sec\n",
            "Epoch 1757, Loss(train/val) 0.25162/0.16395. Took 0.05 sec\n",
            "Epoch 1758, Loss(train/val) 0.24931/0.16456. Took 0.05 sec\n",
            "Epoch 1759, Loss(train/val) 0.23521/0.16464. Took 0.05 sec\n",
            "Epoch 1760, Loss(train/val) 0.24581/0.16361. Took 0.05 sec\n",
            "Epoch 1761, Loss(train/val) 0.23253/0.16282. Took 0.05 sec\n",
            "Epoch 1762, Loss(train/val) 0.24593/0.16319. Took 0.04 sec\n",
            "Epoch 1763, Loss(train/val) 0.24885/0.16317. Took 0.05 sec\n",
            "Epoch 1764, Loss(train/val) 0.23857/0.16314. Took 0.05 sec\n",
            "Epoch 1765, Loss(train/val) 0.23334/0.16452. Took 0.05 sec\n",
            "Epoch 1766, Loss(train/val) 0.24838/0.16504. Took 0.05 sec\n",
            "Epoch 1767, Loss(train/val) 0.24320/0.16752. Took 0.05 sec\n",
            "Epoch 1768, Loss(train/val) 0.23548/0.16539. Took 0.05 sec\n",
            "Epoch 1769, Loss(train/val) 0.24938/0.16340. Took 0.04 sec\n",
            "Epoch 1770, Loss(train/val) 0.24616/0.16288. Took 0.04 sec\n",
            "Epoch 1771, Loss(train/val) 0.23468/0.16278. Took 0.05 sec\n",
            "Epoch 1772, Loss(train/val) 0.23035/0.16292. Took 0.05 sec\n",
            "Epoch 1773, Loss(train/val) 0.22990/0.16281. Took 0.05 sec\n",
            "Epoch 1774, Loss(train/val) 0.24704/0.16294. Took 0.05 sec\n",
            "Epoch 1775, Loss(train/val) 0.24123/0.16327. Took 0.04 sec\n",
            "Epoch 1776, Loss(train/val) 0.24365/0.16256. Took 0.05 sec\n",
            "Epoch 1777, Loss(train/val) 0.23902/0.16255. Took 0.06 sec\n",
            "Epoch 1778, Loss(train/val) 0.26719/0.16260. Took 0.05 sec\n",
            "Epoch 1779, Loss(train/val) 0.22850/0.16272. Took 0.05 sec\n",
            "Epoch 1780, Loss(train/val) 0.22753/0.16343. Took 0.05 sec\n",
            "Epoch 1781, Loss(train/val) 0.23598/0.16382. Took 0.05 sec\n",
            "Epoch 1782, Loss(train/val) 0.25450/0.16389. Took 0.05 sec\n",
            "Epoch 1783, Loss(train/val) 0.23107/0.16328. Took 0.05 sec\n",
            "Epoch 1784, Loss(train/val) 0.23949/0.16262. Took 0.05 sec\n",
            "Epoch 1785, Loss(train/val) 0.24105/0.16256. Took 0.05 sec\n",
            "Epoch 1786, Loss(train/val) 0.23076/0.16261. Took 0.05 sec\n",
            "Epoch 1787, Loss(train/val) 0.24365/0.16247. Took 0.05 sec\n",
            "Epoch 1788, Loss(train/val) 0.23509/0.16235. Took 0.05 sec\n",
            "Epoch 1789, Loss(train/val) 0.23531/0.16235. Took 0.05 sec\n",
            "Epoch 1790, Loss(train/val) 0.23340/0.16270. Took 0.05 sec\n",
            "Epoch 1791, Loss(train/val) 0.23315/0.16316. Took 0.05 sec\n",
            "Epoch 1792, Loss(train/val) 0.24551/0.16485. Took 0.04 sec\n",
            "Epoch 1793, Loss(train/val) 0.23229/0.16587. Took 0.05 sec\n",
            "Epoch 1794, Loss(train/val) 0.23893/0.16558. Took 0.05 sec\n",
            "Epoch 1795, Loss(train/val) 0.24859/0.16223. Took 0.04 sec\n",
            "Epoch 1796, Loss(train/val) 0.23671/0.16294. Took 0.05 sec\n",
            "Epoch 1797, Loss(train/val) 0.25164/0.16517. Took 0.05 sec\n",
            "Epoch 1798, Loss(train/val) 0.23582/0.16324. Took 0.06 sec\n",
            "Epoch 1799, Loss(train/val) 0.23621/0.16245. Took 0.05 sec\n",
            "Epoch 1800, Loss(train/val) 0.23316/0.16220. Took 0.05 sec\n",
            "Epoch 1801, Loss(train/val) 0.22708/0.16239. Took 0.05 sec\n",
            "Epoch 1802, Loss(train/val) 0.24497/0.16294. Took 0.05 sec\n",
            "Epoch 1803, Loss(train/val) 0.24021/0.16317. Took 0.05 sec\n",
            "Epoch 1804, Loss(train/val) 0.24042/0.16323. Took 0.05 sec\n",
            "Epoch 1805, Loss(train/val) 0.25561/0.16321. Took 0.04 sec\n",
            "Epoch 1806, Loss(train/val) 0.24552/0.16243. Took 0.05 sec\n",
            "Epoch 1807, Loss(train/val) 0.23147/0.16276. Took 0.04 sec\n",
            "Epoch 1808, Loss(train/val) 0.23686/0.16296. Took 0.05 sec\n",
            "Epoch 1809, Loss(train/val) 0.23281/0.16211. Took 0.05 sec\n",
            "Epoch 1810, Loss(train/val) 0.24023/0.16248. Took 0.05 sec\n",
            "Epoch 1811, Loss(train/val) 0.23220/0.16308. Took 0.05 sec\n",
            "Epoch 1812, Loss(train/val) 0.22989/0.16291. Took 0.05 sec\n",
            "Epoch 1813, Loss(train/val) 0.24169/0.16267. Took 0.05 sec\n",
            "Epoch 1814, Loss(train/val) 0.23011/0.16350. Took 0.05 sec\n",
            "Epoch 1815, Loss(train/val) 0.23566/0.16375. Took 0.04 sec\n",
            "Epoch 1816, Loss(train/val) 0.23070/0.16408. Took 0.04 sec\n",
            "Epoch 1817, Loss(train/val) 0.23186/0.16483. Took 0.05 sec\n",
            "Epoch 1818, Loss(train/val) 0.24331/0.16239. Took 0.05 sec\n",
            "Epoch 1819, Loss(train/val) 0.24522/0.16192. Took 0.05 sec\n",
            "Epoch 1820, Loss(train/val) 0.26185/0.16203. Took 0.04 sec\n",
            "Epoch 1821, Loss(train/val) 0.23276/0.16368. Took 0.05 sec\n",
            "Epoch 1822, Loss(train/val) 0.23876/0.16315. Took 0.05 sec\n",
            "Epoch 1823, Loss(train/val) 0.23768/0.16256. Took 0.06 sec\n",
            "Epoch 1824, Loss(train/val) 0.23203/0.16258. Took 0.05 sec\n",
            "Epoch 1825, Loss(train/val) 0.24515/0.16219. Took 0.05 sec\n",
            "Epoch 1826, Loss(train/val) 0.23807/0.16179. Took 0.05 sec\n",
            "Epoch 1827, Loss(train/val) 0.23868/0.16235. Took 0.05 sec\n",
            "Epoch 1828, Loss(train/val) 0.26191/0.16256. Took 0.05 sec\n",
            "Epoch 1829, Loss(train/val) 0.24074/0.16374. Took 0.05 sec\n",
            "Epoch 1830, Loss(train/val) 0.23355/0.16399. Took 0.05 sec\n",
            "Epoch 1831, Loss(train/val) 0.25225/0.16285. Took 0.05 sec\n",
            "Epoch 1832, Loss(train/val) 0.25224/0.16171. Took 0.05 sec\n",
            "Epoch 1833, Loss(train/val) 0.23994/0.16202. Took 0.05 sec\n",
            "Epoch 1834, Loss(train/val) 0.22841/0.16219. Took 0.05 sec\n",
            "Epoch 1835, Loss(train/val) 0.23832/0.16162. Took 0.05 sec\n",
            "Epoch 1836, Loss(train/val) 0.25621/0.16158. Took 0.05 sec\n",
            "Epoch 1837, Loss(train/val) 0.23202/0.16160. Took 0.06 sec\n",
            "Epoch 1838, Loss(train/val) 0.23192/0.16201. Took 0.05 sec\n",
            "Epoch 1839, Loss(train/val) 0.23817/0.16181. Took 0.04 sec\n",
            "Epoch 1840, Loss(train/val) 0.23381/0.16220. Took 0.05 sec\n",
            "Epoch 1841, Loss(train/val) 0.23845/0.16153. Took 0.05 sec\n",
            "Epoch 1842, Loss(train/val) 0.24637/0.16148. Took 0.06 sec\n",
            "Epoch 1843, Loss(train/val) 0.23336/0.16154. Took 0.05 sec\n",
            "Epoch 1844, Loss(train/val) 0.23365/0.16173. Took 0.05 sec\n",
            "Epoch 1845, Loss(train/val) 0.22274/0.16270. Took 0.05 sec\n",
            "Epoch 1846, Loss(train/val) 0.26392/0.16297. Took 0.05 sec\n",
            "Epoch 1847, Loss(train/val) 0.23159/0.16328. Took 0.05 sec\n",
            "Epoch 1848, Loss(train/val) 0.25403/0.16223. Took 0.04 sec\n",
            "Epoch 1849, Loss(train/val) 0.23724/0.16193. Took 0.05 sec\n",
            "Epoch 1850, Loss(train/val) 0.25090/0.16150. Took 0.05 sec\n",
            "Epoch 1851, Loss(train/val) 0.23260/0.16141. Took 0.04 sec\n",
            "Epoch 1852, Loss(train/val) 0.23443/0.16124. Took 0.05 sec\n",
            "Epoch 1853, Loss(train/val) 0.24115/0.16147. Took 0.05 sec\n",
            "Epoch 1854, Loss(train/val) 0.28029/0.16425. Took 0.04 sec\n",
            "Epoch 1855, Loss(train/val) 0.24027/0.16602. Took 0.05 sec\n",
            "Epoch 1856, Loss(train/val) 0.25080/0.16357. Took 0.04 sec\n",
            "Epoch 1857, Loss(train/val) 0.23361/0.16133. Took 0.05 sec\n",
            "Epoch 1858, Loss(train/val) 0.23379/0.16135. Took 0.05 sec\n",
            "Epoch 1859, Loss(train/val) 0.23920/0.16150. Took 0.04 sec\n",
            "Epoch 1860, Loss(train/val) 0.23715/0.16307. Took 0.05 sec\n",
            "Epoch 1861, Loss(train/val) 0.24388/0.16258. Took 0.05 sec\n",
            "Epoch 1862, Loss(train/val) 0.24875/0.16294. Took 0.05 sec\n",
            "Epoch 1863, Loss(train/val) 0.23331/0.16303. Took 0.05 sec\n",
            "Epoch 1864, Loss(train/val) 0.23639/0.16415. Took 0.05 sec\n",
            "Epoch 1865, Loss(train/val) 0.23609/0.16155. Took 0.04 sec\n",
            "Epoch 1866, Loss(train/val) 0.22999/0.16109. Took 0.05 sec\n",
            "Epoch 1867, Loss(train/val) 0.25550/0.16133. Took 0.05 sec\n",
            "Epoch 1868, Loss(train/val) 0.23195/0.16147. Took 0.05 sec\n",
            "Epoch 1869, Loss(train/val) 0.23609/0.16139. Took 0.04 sec\n",
            "Epoch 1870, Loss(train/val) 0.24877/0.16194. Took 0.04 sec\n",
            "Epoch 1871, Loss(train/val) 0.23517/0.16168. Took 0.05 sec\n",
            "Epoch 1872, Loss(train/val) 0.23406/0.16170. Took 0.06 sec\n",
            "Epoch 1873, Loss(train/val) 0.23274/0.16191. Took 0.05 sec\n",
            "Epoch 1874, Loss(train/val) 0.23132/0.16125. Took 0.05 sec\n",
            "Epoch 1875, Loss(train/val) 0.24126/0.16057. Took 0.05 sec\n",
            "Epoch 1876, Loss(train/val) 0.24293/0.16068. Took 0.04 sec\n",
            "Epoch 1877, Loss(train/val) 0.23169/0.16149. Took 0.05 sec\n",
            "Epoch 1878, Loss(train/val) 0.23280/0.16114. Took 0.05 sec\n",
            "Epoch 1879, Loss(train/val) 0.23335/0.16109. Took 0.05 sec\n",
            "Epoch 1880, Loss(train/val) 0.23270/0.16072. Took 0.04 sec\n",
            "Epoch 1881, Loss(train/val) 0.24063/0.16075. Took 0.04 sec\n",
            "Epoch 1882, Loss(train/val) 0.23424/0.16075. Took 0.07 sec\n",
            "Epoch 1883, Loss(train/val) 0.24392/0.16069. Took 0.05 sec\n",
            "Epoch 1884, Loss(train/val) 0.25011/0.16086. Took 0.05 sec\n",
            "Epoch 1885, Loss(train/val) 0.23166/0.16095. Took 0.04 sec\n",
            "Epoch 1886, Loss(train/val) 0.24614/0.16073. Took 0.05 sec\n",
            "Epoch 1887, Loss(train/val) 0.22832/0.16054. Took 0.05 sec\n",
            "Epoch 1888, Loss(train/val) 0.24685/0.16117. Took 0.04 sec\n",
            "Epoch 1889, Loss(train/val) 0.22997/0.16157. Took 0.05 sec\n",
            "Epoch 1890, Loss(train/val) 0.23973/0.16239. Took 0.05 sec\n",
            "Epoch 1891, Loss(train/val) 0.23236/0.16071. Took 0.05 sec\n",
            "Epoch 1892, Loss(train/val) 0.23838/0.16053. Took 0.05 sec\n",
            "Epoch 1893, Loss(train/val) 0.23413/0.16059. Took 0.05 sec\n",
            "Epoch 1894, Loss(train/val) 0.23669/0.16056. Took 0.04 sec\n",
            "Epoch 1895, Loss(train/val) 0.23309/0.16201. Took 0.05 sec\n",
            "Epoch 1896, Loss(train/val) 0.22331/0.16296. Took 0.04 sec\n",
            "Epoch 1897, Loss(train/val) 0.23066/0.16222. Took 0.05 sec\n",
            "Epoch 1898, Loss(train/val) 0.23845/0.16106. Took 0.04 sec\n",
            "Epoch 1899, Loss(train/val) 0.24195/0.16041. Took 0.04 sec\n",
            "Epoch 1900, Loss(train/val) 0.23141/0.16054. Took 0.05 sec\n",
            "Epoch 1901, Loss(train/val) 0.24087/0.16051. Took 0.04 sec\n",
            "Epoch 1902, Loss(train/val) 0.22632/0.16061. Took 0.05 sec\n",
            "Epoch 1903, Loss(train/val) 0.23406/0.16029. Took 0.06 sec\n",
            "Epoch 1904, Loss(train/val) 0.23232/0.16056. Took 0.05 sec\n",
            "Epoch 1905, Loss(train/val) 0.23230/0.16250. Took 0.05 sec\n",
            "Epoch 1906, Loss(train/val) 0.23980/0.16256. Took 0.05 sec\n",
            "Epoch 1907, Loss(train/val) 0.24377/0.16055. Took 0.05 sec\n",
            "Epoch 1908, Loss(train/val) 0.23526/0.16022. Took 0.05 sec\n",
            "Epoch 1909, Loss(train/val) 0.23386/0.16331. Took 0.05 sec\n",
            "Epoch 1910, Loss(train/val) 0.23085/0.16244. Took 0.05 sec\n",
            "Epoch 1911, Loss(train/val) 0.23615/0.16145. Took 0.05 sec\n",
            "Epoch 1912, Loss(train/val) 0.23093/0.16102. Took 0.05 sec\n",
            "Epoch 1913, Loss(train/val) 0.24289/0.16037. Took 0.05 sec\n",
            "Epoch 1914, Loss(train/val) 0.23715/0.15984. Took 0.05 sec\n",
            "Epoch 1915, Loss(train/val) 0.23456/0.15974. Took 0.04 sec\n",
            "Epoch 1916, Loss(train/val) 0.22754/0.15988. Took 0.05 sec\n",
            "Epoch 1917, Loss(train/val) 0.23213/0.15985. Took 0.04 sec\n",
            "Epoch 1918, Loss(train/val) 0.24185/0.16000. Took 0.05 sec\n",
            "Epoch 1919, Loss(train/val) 0.24933/0.16028. Took 0.04 sec\n",
            "Epoch 1920, Loss(train/val) 0.24184/0.16035. Took 0.05 sec\n",
            "Epoch 1921, Loss(train/val) 0.24237/0.16000. Took 0.05 sec\n",
            "Epoch 1922, Loss(train/val) 0.25662/0.16006. Took 0.05 sec\n",
            "Epoch 1923, Loss(train/val) 0.24259/0.16102. Took 0.05 sec\n",
            "Epoch 1924, Loss(train/val) 0.23033/0.15991. Took 0.05 sec\n",
            "Epoch 1925, Loss(train/val) 0.24555/0.15977. Took 0.06 sec\n",
            "Epoch 1926, Loss(train/val) 0.22714/0.15964. Took 0.05 sec\n",
            "Epoch 1927, Loss(train/val) 0.26028/0.16008. Took 0.04 sec\n",
            "Epoch 1928, Loss(train/val) 0.23588/0.15999. Took 0.04 sec\n",
            "Epoch 1929, Loss(train/val) 0.22470/0.15999. Took 0.05 sec\n",
            "Epoch 1930, Loss(train/val) 0.23136/0.15994. Took 0.04 sec\n",
            "Epoch 1931, Loss(train/val) 0.23243/0.15987. Took 0.05 sec\n",
            "Epoch 1932, Loss(train/val) 0.24313/0.16131. Took 0.05 sec\n",
            "Epoch 1933, Loss(train/val) 0.22893/0.16081. Took 0.05 sec\n",
            "Epoch 1934, Loss(train/val) 0.23041/0.15978. Took 0.05 sec\n",
            "Epoch 1935, Loss(train/val) 0.24034/0.15975. Took 0.05 sec\n",
            "Epoch 1936, Loss(train/val) 0.22982/0.15962. Took 0.05 sec\n",
            "Epoch 1937, Loss(train/val) 0.24937/0.15993. Took 0.04 sec\n",
            "Epoch 1938, Loss(train/val) 0.24190/0.16043. Took 0.04 sec\n",
            "Epoch 1939, Loss(train/val) 0.23533/0.16061. Took 0.05 sec\n",
            "Epoch 1940, Loss(train/val) 0.23025/0.16148. Took 0.04 sec\n",
            "Epoch 1941, Loss(train/val) 0.25035/0.16063. Took 0.05 sec\n",
            "Epoch 1942, Loss(train/val) 0.24084/0.15967. Took 0.04 sec\n",
            "Epoch 1943, Loss(train/val) 0.24645/0.16172. Took 0.05 sec\n",
            "Epoch 1944, Loss(train/val) 0.22920/0.16059. Took 0.04 sec\n",
            "Epoch 1945, Loss(train/val) 0.22955/0.15988. Took 0.04 sec\n",
            "Epoch 1946, Loss(train/val) 0.21986/0.15938. Took 0.06 sec\n",
            "Epoch 1947, Loss(train/val) 0.24351/0.16007. Took 0.05 sec\n",
            "Epoch 1948, Loss(train/val) 0.23738/0.16116. Took 0.04 sec\n",
            "Epoch 1949, Loss(train/val) 0.23880/0.16132. Took 0.04 sec\n",
            "Epoch 1950, Loss(train/val) 0.23250/0.16006. Took 0.04 sec\n",
            "Epoch 1951, Loss(train/val) 0.23499/0.15983. Took 0.05 sec\n",
            "Epoch 1952, Loss(train/val) 0.22947/0.15961. Took 0.04 sec\n",
            "Epoch 1953, Loss(train/val) 0.23543/0.15935. Took 0.04 sec\n",
            "Epoch 1954, Loss(train/val) 0.23230/0.15917. Took 0.05 sec\n",
            "Epoch 1955, Loss(train/val) 0.23146/0.15927. Took 0.05 sec\n",
            "Epoch 1956, Loss(train/val) 0.23493/0.15929. Took 0.05 sec\n",
            "Epoch 1957, Loss(train/val) 0.22943/0.15985. Took 0.05 sec\n",
            "Epoch 1958, Loss(train/val) 0.23020/0.15938. Took 0.04 sec\n",
            "Epoch 1959, Loss(train/val) 0.23007/0.15923. Took 0.05 sec\n",
            "Epoch 1960, Loss(train/val) 0.26687/0.15998. Took 0.04 sec\n",
            "Epoch 1961, Loss(train/val) 0.23422/0.16099. Took 0.05 sec\n",
            "Epoch 1962, Loss(train/val) 0.24491/0.16025. Took 0.04 sec\n",
            "Epoch 1963, Loss(train/val) 0.22907/0.15933. Took 0.05 sec\n",
            "Epoch 1964, Loss(train/val) 0.24275/0.16139. Took 0.04 sec\n",
            "Epoch 1965, Loss(train/val) 0.24338/0.16355. Took 0.04 sec\n",
            "Epoch 1966, Loss(train/val) 0.22809/0.16087. Took 0.05 sec\n",
            "Epoch 1967, Loss(train/val) 0.23241/0.15893. Took 0.05 sec\n",
            "Epoch 1968, Loss(train/val) 0.23892/0.16051. Took 0.06 sec\n",
            "Epoch 1969, Loss(train/val) 0.23232/0.16075. Took 0.04 sec\n",
            "Epoch 1970, Loss(train/val) 0.22791/0.15985. Took 0.04 sec\n",
            "Epoch 1971, Loss(train/val) 0.23443/0.15956. Took 0.05 sec\n",
            "Epoch 1972, Loss(train/val) 0.23348/0.15891. Took 0.04 sec\n",
            "Epoch 1973, Loss(train/val) 0.23488/0.15891. Took 0.04 sec\n",
            "Epoch 1974, Loss(train/val) 0.22754/0.15897. Took 0.05 sec\n",
            "Epoch 1975, Loss(train/val) 0.23998/0.15892. Took 0.05 sec\n",
            "Epoch 1976, Loss(train/val) 0.23246/0.15877. Took 0.05 sec\n",
            "Epoch 1977, Loss(train/val) 0.22664/0.15868. Took 0.05 sec\n",
            "Epoch 1978, Loss(train/val) 0.23955/0.15859. Took 0.05 sec\n",
            "Epoch 1979, Loss(train/val) 0.25545/0.15867. Took 0.05 sec\n",
            "Epoch 1980, Loss(train/val) 0.23251/0.15887. Took 0.05 sec\n",
            "Epoch 1981, Loss(train/val) 0.23595/0.15923. Took 0.05 sec\n",
            "Epoch 1982, Loss(train/val) 0.22978/0.15937. Took 0.05 sec\n",
            "Epoch 1983, Loss(train/val) 0.24620/0.15861. Took 0.04 sec\n",
            "Epoch 1984, Loss(train/val) 0.23175/0.15883. Took 0.05 sec\n",
            "Epoch 1985, Loss(train/val) 0.23403/0.15928. Took 0.05 sec\n",
            "Epoch 1986, Loss(train/val) 0.24198/0.15869. Took 0.05 sec\n",
            "Epoch 1987, Loss(train/val) 0.25771/0.15940. Took 0.05 sec\n",
            "Epoch 1988, Loss(train/val) 0.22772/0.15906. Took 0.05 sec\n",
            "Epoch 1989, Loss(train/val) 0.23310/0.15851. Took 0.05 sec\n",
            "Epoch 1990, Loss(train/val) 0.23058/0.16022. Took 0.05 sec\n",
            "Epoch 1991, Loss(train/val) 0.24428/0.15951. Took 0.05 sec\n",
            "Epoch 1992, Loss(train/val) 0.23401/0.15875. Took 0.04 sec\n",
            "Epoch 1993, Loss(train/val) 0.22365/0.15859. Took 0.05 sec\n",
            "Epoch 1994, Loss(train/val) 0.23094/0.15862. Took 0.05 sec\n",
            "Epoch 1995, Loss(train/val) 0.23842/0.15854. Took 0.04 sec\n",
            "Epoch 1996, Loss(train/val) 0.23734/0.15853. Took 0.05 sec\n",
            "Epoch 1997, Loss(train/val) 0.23118/0.15924. Took 0.05 sec\n",
            "Epoch 1998, Loss(train/val) 0.22747/0.15932. Took 0.04 sec\n",
            "Epoch 1999, Loss(train/val) 0.23452/0.16081. Took 0.05 sec\n",
            "Epoch 2000, Loss(train/val) 0.23133/0.15932. Took 0.05 sec\n",
            "Epoch 2001, Loss(train/val) 0.23617/0.15855. Took 0.05 sec\n",
            "Epoch 2002, Loss(train/val) 0.22834/0.15849. Took 0.05 sec\n",
            "Epoch 2003, Loss(train/val) 0.23669/0.15845. Took 0.04 sec\n",
            "Epoch 2004, Loss(train/val) 0.23267/0.15866. Took 0.04 sec\n",
            "Epoch 2005, Loss(train/val) 0.23510/0.15853. Took 0.05 sec\n",
            "Epoch 2006, Loss(train/val) 0.22849/0.15940. Took 0.05 sec\n",
            "Epoch 2007, Loss(train/val) 0.22707/0.15843. Took 0.05 sec\n",
            "Epoch 2008, Loss(train/val) 0.24123/0.15836. Took 0.04 sec\n",
            "Epoch 2009, Loss(train/val) 0.22264/0.15812. Took 0.04 sec\n",
            "Epoch 2010, Loss(train/val) 0.22931/0.15907. Took 0.04 sec\n",
            "Epoch 2011, Loss(train/val) 0.24327/0.16027. Took 0.06 sec\n",
            "Epoch 2012, Loss(train/val) 0.26387/0.15990. Took 0.04 sec\n",
            "Epoch 2013, Loss(train/val) 0.23646/0.15876. Took 0.04 sec\n",
            "Epoch 2014, Loss(train/val) 0.24172/0.15856. Took 0.04 sec\n",
            "Epoch 2015, Loss(train/val) 0.22812/0.15848. Took 0.04 sec\n",
            "Epoch 2016, Loss(train/val) 0.23371/0.15904. Took 0.05 sec\n",
            "Epoch 2017, Loss(train/val) 0.23954/0.15840. Took 0.05 sec\n",
            "Epoch 2018, Loss(train/val) 0.24889/0.15827. Took 0.05 sec\n",
            "Epoch 2019, Loss(train/val) 0.21760/0.15822. Took 0.05 sec\n",
            "Epoch 2020, Loss(train/val) 0.23818/0.15826. Took 0.05 sec\n",
            "Epoch 2021, Loss(train/val) 0.23457/0.15845. Took 0.05 sec\n",
            "Epoch 2022, Loss(train/val) 0.22737/0.15947. Took 0.05 sec\n",
            "Epoch 2023, Loss(train/val) 0.24157/0.15960. Took 0.05 sec\n",
            "Epoch 2024, Loss(train/val) 0.23369/0.15941. Took 0.04 sec\n",
            "Epoch 2025, Loss(train/val) 0.22516/0.15960. Took 0.04 sec\n",
            "Epoch 2026, Loss(train/val) 0.23422/0.15996. Took 0.05 sec\n",
            "Epoch 2027, Loss(train/val) 0.22124/0.15898. Took 0.04 sec\n",
            "Epoch 2028, Loss(train/val) 0.24445/0.15794. Took 0.04 sec\n",
            "Epoch 2029, Loss(train/val) 0.22920/0.15837. Took 0.05 sec\n",
            "Epoch 2030, Loss(train/val) 0.23074/0.15853. Took 0.05 sec\n",
            "Epoch 2031, Loss(train/val) 0.23284/0.15808. Took 0.05 sec\n",
            "Epoch 2032, Loss(train/val) 0.23738/0.15769. Took 0.04 sec\n",
            "Epoch 2033, Loss(train/val) 0.23108/0.15774. Took 0.05 sec\n",
            "Epoch 2034, Loss(train/val) 0.23144/0.15828. Took 0.04 sec\n",
            "Epoch 2035, Loss(train/val) 0.24789/0.15816. Took 0.04 sec\n",
            "Epoch 2036, Loss(train/val) 0.22460/0.15782. Took 0.06 sec\n",
            "Epoch 2037, Loss(train/val) 0.23176/0.15781. Took 0.04 sec\n",
            "Epoch 2038, Loss(train/val) 0.22539/0.15781. Took 0.05 sec\n",
            "Epoch 2039, Loss(train/val) 0.23444/0.15774. Took 0.04 sec\n",
            "Epoch 2040, Loss(train/val) 0.23704/0.15773. Took 0.05 sec\n",
            "Epoch 2041, Loss(train/val) 0.24460/0.15790. Took 0.05 sec\n",
            "Epoch 2042, Loss(train/val) 0.23570/0.15820. Took 0.04 sec\n",
            "Epoch 2043, Loss(train/val) 0.22775/0.15827. Took 0.04 sec\n",
            "Epoch 2044, Loss(train/val) 0.22898/0.15778. Took 0.05 sec\n",
            "Epoch 2045, Loss(train/val) 0.24589/0.15768. Took 0.05 sec\n",
            "Epoch 2046, Loss(train/val) 0.23453/0.15832. Took 0.06 sec\n",
            "Epoch 2047, Loss(train/val) 0.22984/0.15855. Took 0.05 sec\n",
            "Epoch 2048, Loss(train/val) 0.22442/0.15824. Took 0.05 sec\n",
            "Epoch 2049, Loss(train/val) 0.25999/0.15822. Took 0.06 sec\n",
            "Epoch 2050, Loss(train/val) 0.23394/0.15772. Took 0.06 sec\n",
            "Epoch 2051, Loss(train/val) 0.23085/0.15774. Took 0.05 sec\n",
            "Epoch 2052, Loss(train/val) 0.22702/0.15765. Took 0.05 sec\n",
            "Epoch 2053, Loss(train/val) 0.23666/0.15758. Took 0.05 sec\n",
            "Epoch 2054, Loss(train/val) 0.23294/0.15736. Took 0.05 sec\n",
            "Epoch 2055, Loss(train/val) 0.22571/0.15722. Took 0.06 sec\n",
            "Epoch 2056, Loss(train/val) 0.24299/0.15749. Took 0.05 sec\n",
            "Epoch 2057, Loss(train/val) 0.23065/0.15781. Took 0.05 sec\n",
            "Epoch 2058, Loss(train/val) 0.23371/0.15769. Took 0.05 sec\n",
            "Epoch 2059, Loss(train/val) 0.24135/0.15783. Took 0.05 sec\n",
            "Epoch 2060, Loss(train/val) 0.22693/0.15734. Took 0.05 sec\n",
            "Epoch 2061, Loss(train/val) 0.24545/0.15746. Took 0.05 sec\n",
            "Epoch 2062, Loss(train/val) 0.24221/0.15742. Took 0.05 sec\n",
            "Epoch 2063, Loss(train/val) 0.23120/0.15745. Took 0.04 sec\n",
            "Epoch 2064, Loss(train/val) 0.24892/0.15746. Took 0.05 sec\n",
            "Epoch 2065, Loss(train/val) 0.24241/0.15783. Took 0.05 sec\n",
            "Epoch 2066, Loss(train/val) 0.24543/0.15980. Took 0.04 sec\n",
            "Epoch 2067, Loss(train/val) 0.23730/0.15843. Took 0.05 sec\n",
            "Epoch 2068, Loss(train/val) 0.22599/0.15760. Took 0.04 sec\n",
            "Epoch 2069, Loss(train/val) 0.24223/0.15776. Took 0.05 sec\n",
            "Epoch 2070, Loss(train/val) 0.23043/0.15884. Took 0.04 sec\n",
            "Epoch 2071, Loss(train/val) 0.23662/0.15774. Took 0.04 sec\n",
            "Epoch 2072, Loss(train/val) 0.24325/0.15762. Took 0.04 sec\n",
            "Epoch 2073, Loss(train/val) 0.24509/0.15898. Took 0.04 sec\n",
            "Epoch 2074, Loss(train/val) 0.22978/0.15991. Took 0.06 sec\n",
            "Epoch 2075, Loss(train/val) 0.23552/0.15917. Took 0.04 sec\n",
            "Epoch 2076, Loss(train/val) 0.22476/0.15830. Took 0.04 sec\n",
            "Epoch 2077, Loss(train/val) 0.23042/0.15742. Took 0.05 sec\n",
            "Epoch 2078, Loss(train/val) 0.23554/0.15750. Took 0.04 sec\n",
            "Epoch 2079, Loss(train/val) 0.24053/0.15801. Took 0.05 sec\n",
            "Epoch 2080, Loss(train/val) 0.22604/0.15741. Took 0.04 sec\n",
            "Epoch 2081, Loss(train/val) 0.23157/0.15741. Took 0.05 sec\n",
            "Epoch 2082, Loss(train/val) 0.22950/0.15745. Took 0.05 sec\n",
            "Epoch 2083, Loss(train/val) 0.23551/0.15735. Took 0.04 sec\n",
            "Epoch 2084, Loss(train/val) 0.24411/0.15735. Took 0.05 sec\n",
            "Epoch 2085, Loss(train/val) 0.22277/0.15729. Took 0.04 sec\n",
            "Epoch 2086, Loss(train/val) 0.23436/0.15714. Took 0.04 sec\n",
            "Epoch 2087, Loss(train/val) 0.24507/0.15716. Took 0.04 sec\n",
            "Epoch 2088, Loss(train/val) 0.22703/0.15811. Took 0.05 sec\n",
            "Epoch 2089, Loss(train/val) 0.23551/0.15765. Took 0.05 sec\n",
            "Epoch 2090, Loss(train/val) 0.23049/0.15698. Took 0.05 sec\n",
            "Epoch 2091, Loss(train/val) 0.24519/0.15699. Took 0.05 sec\n",
            "Epoch 2092, Loss(train/val) 0.23147/0.15710. Took 0.04 sec\n",
            "Epoch 2093, Loss(train/val) 0.23251/0.15694. Took 0.05 sec\n",
            "Epoch 2094, Loss(train/val) 0.23958/0.15732. Took 0.05 sec\n",
            "Epoch 2095, Loss(train/val) 0.22723/0.15734. Took 0.04 sec\n",
            "Epoch 2096, Loss(train/val) 0.22524/0.15711. Took 0.05 sec\n",
            "Epoch 2097, Loss(train/val) 0.25756/0.15717. Took 0.04 sec\n",
            "Epoch 2098, Loss(train/val) 0.23050/0.15726. Took 0.05 sec\n",
            "Epoch 2099, Loss(train/val) 0.22727/0.15718. Took 0.05 sec\n",
            "Epoch 2100, Loss(train/val) 0.22202/0.15695. Took 0.05 sec\n",
            "Epoch 2101, Loss(train/val) 0.22534/0.15693. Took 0.05 sec\n",
            "Epoch 2102, Loss(train/val) 0.23243/0.15682. Took 0.05 sec\n",
            "Epoch 2103, Loss(train/val) 0.22670/0.15679. Took 0.04 sec\n",
            "Epoch 2104, Loss(train/val) 0.22786/0.15802. Took 0.05 sec\n",
            "Epoch 2105, Loss(train/val) 0.24776/0.16043. Took 0.05 sec\n",
            "Epoch 2106, Loss(train/val) 0.22733/0.15971. Took 0.05 sec\n",
            "Epoch 2107, Loss(train/val) 0.23709/0.15931. Took 0.05 sec\n",
            "Epoch 2108, Loss(train/val) 0.23832/0.15728. Took 0.05 sec\n",
            "Epoch 2109, Loss(train/val) 0.23536/0.15694. Took 0.06 sec\n",
            "Epoch 2110, Loss(train/val) 0.25596/0.15707. Took 0.04 sec\n",
            "Epoch 2111, Loss(train/val) 0.24740/0.15731. Took 0.04 sec\n",
            "Epoch 2112, Loss(train/val) 0.23246/0.15699. Took 0.04 sec\n",
            "Epoch 2113, Loss(train/val) 0.22708/0.15675. Took 0.05 sec\n",
            "Epoch 2114, Loss(train/val) 0.23657/0.15670. Took 0.05 sec\n",
            "Epoch 2115, Loss(train/val) 0.23382/0.15674. Took 0.05 sec\n",
            "Epoch 2116, Loss(train/val) 0.23358/0.15691. Took 0.04 sec\n",
            "Epoch 2117, Loss(train/val) 0.23345/0.15679. Took 0.05 sec\n",
            "Epoch 2118, Loss(train/val) 0.23168/0.15665. Took 0.05 sec\n",
            "Epoch 2119, Loss(train/val) 0.21986/0.15750. Took 0.05 sec\n",
            "Epoch 2120, Loss(train/val) 0.22304/0.15761. Took 0.04 sec\n",
            "Epoch 2121, Loss(train/val) 0.23196/0.15754. Took 0.05 sec\n",
            "Epoch 2122, Loss(train/val) 0.23536/0.15689. Took 0.05 sec\n",
            "Epoch 2123, Loss(train/val) 0.22990/0.15677. Took 0.05 sec\n",
            "Epoch 2124, Loss(train/val) 0.23558/0.15683. Took 0.05 sec\n",
            "Epoch 2125, Loss(train/val) 0.24573/0.15750. Took 0.04 sec\n",
            "Epoch 2126, Loss(train/val) 0.23287/0.15702. Took 0.05 sec\n",
            "Epoch 2127, Loss(train/val) 0.23514/0.15663. Took 0.05 sec\n",
            "Epoch 2128, Loss(train/val) 0.22671/0.15668. Took 0.05 sec\n",
            "Epoch 2129, Loss(train/val) 0.22559/0.15748. Took 0.05 sec\n",
            "Epoch 2130, Loss(train/val) 0.22644/0.15726. Took 0.05 sec\n",
            "Epoch 2131, Loss(train/val) 0.22936/0.15687. Took 0.05 sec\n",
            "Epoch 2132, Loss(train/val) 0.23607/0.15680. Took 0.05 sec\n",
            "Epoch 2133, Loss(train/val) 0.24381/0.15662. Took 0.05 sec\n",
            "Epoch 2134, Loss(train/val) 0.23174/0.15654. Took 0.05 sec\n",
            "Epoch 2135, Loss(train/val) 0.26262/0.15668. Took 0.05 sec\n",
            "Epoch 2136, Loss(train/val) 0.22769/0.15662. Took 0.04 sec\n",
            "Epoch 2137, Loss(train/val) 0.22565/0.15653. Took 0.04 sec\n",
            "Epoch 2138, Loss(train/val) 0.22504/0.15643. Took 0.04 sec\n",
            "Epoch 2139, Loss(train/val) 0.22635/0.15639. Took 0.06 sec\n",
            "Epoch 2140, Loss(train/val) 0.22649/0.15649. Took 0.05 sec\n",
            "Epoch 2141, Loss(train/val) 0.22551/0.15649. Took 0.04 sec\n",
            "Epoch 2142, Loss(train/val) 0.22722/0.15775. Took 0.04 sec\n",
            "Epoch 2143, Loss(train/val) 0.22955/0.15654. Took 0.05 sec\n",
            "Epoch 2144, Loss(train/val) 0.22440/0.15662. Took 0.06 sec\n",
            "Epoch 2145, Loss(train/val) 0.23725/0.15658. Took 0.05 sec\n",
            "Epoch 2146, Loss(train/val) 0.23119/0.15692. Took 0.04 sec\n",
            "Epoch 2147, Loss(train/val) 0.24646/0.15732. Took 0.05 sec\n",
            "Epoch 2148, Loss(train/val) 0.24854/0.15721. Took 0.05 sec\n",
            "Epoch 2149, Loss(train/val) 0.22701/0.15661. Took 0.05 sec\n",
            "Epoch 2150, Loss(train/val) 0.24399/0.15639. Took 0.04 sec\n",
            "Epoch 2151, Loss(train/val) 0.22839/0.15636. Took 0.05 sec\n",
            "Epoch 2152, Loss(train/val) 0.23279/0.15643. Took 0.04 sec\n",
            "Epoch 2153, Loss(train/val) 0.23272/0.15656. Took 0.05 sec\n",
            "Epoch 2154, Loss(train/val) 0.22501/0.15710. Took 0.06 sec\n",
            "Epoch 2155, Loss(train/val) 0.22626/0.15650. Took 0.05 sec\n",
            "Epoch 2156, Loss(train/val) 0.23394/0.15650. Took 0.05 sec\n",
            "Epoch 2157, Loss(train/val) 0.22913/0.15651. Took 0.05 sec\n",
            "Epoch 2158, Loss(train/val) 0.23753/0.15660. Took 0.05 sec\n",
            "Epoch 2159, Loss(train/val) 0.25730/0.15784. Took 0.05 sec\n",
            "Epoch 2160, Loss(train/val) 0.22144/0.15732. Took 0.06 sec\n",
            "Epoch 2161, Loss(train/val) 0.23113/0.15640. Took 0.05 sec\n",
            "Epoch 2162, Loss(train/val) 0.23059/0.15647. Took 0.05 sec\n",
            "Epoch 2163, Loss(train/val) 0.23199/0.15659. Took 0.05 sec\n",
            "Epoch 2164, Loss(train/val) 0.22706/0.15603. Took 0.05 sec\n",
            "Epoch 2165, Loss(train/val) 0.22832/0.15611. Took 0.04 sec\n",
            "Epoch 2166, Loss(train/val) 0.23135/0.15618. Took 0.05 sec\n",
            "Epoch 2167, Loss(train/val) 0.22918/0.15604. Took 0.05 sec\n",
            "Epoch 2168, Loss(train/val) 0.25708/0.15626. Took 0.05 sec\n",
            "Epoch 2169, Loss(train/val) 0.23420/0.15625. Took 0.06 sec\n",
            "Epoch 2170, Loss(train/val) 0.22418/0.15600. Took 0.05 sec\n",
            "Epoch 2171, Loss(train/val) 0.24051/0.15607. Took 0.04 sec\n",
            "Epoch 2172, Loss(train/val) 0.22252/0.15689. Took 0.05 sec\n",
            "Epoch 2173, Loss(train/val) 0.22399/0.15699. Took 0.05 sec\n",
            "Epoch 2174, Loss(train/val) 0.22616/0.15983. Took 0.05 sec\n",
            "Epoch 2175, Loss(train/val) 0.23272/0.16064. Took 0.05 sec\n",
            "Epoch 2176, Loss(train/val) 0.23693/0.15888. Took 0.05 sec\n",
            "Epoch 2177, Loss(train/val) 0.23436/0.15699. Took 0.05 sec\n",
            "Epoch 2178, Loss(train/val) 0.23392/0.15617. Took 0.05 sec\n",
            "Epoch 2179, Loss(train/val) 0.22445/0.15671. Took 0.05 sec\n",
            "Epoch 2180, Loss(train/val) 0.22018/0.15689. Took 0.05 sec\n",
            "Epoch 2181, Loss(train/val) 0.23604/0.15676. Took 0.05 sec\n",
            "Epoch 2182, Loss(train/val) 0.23482/0.15670. Took 0.05 sec\n",
            "Epoch 2183, Loss(train/val) 0.23432/0.15627. Took 0.05 sec\n",
            "Epoch 2184, Loss(train/val) 0.22472/0.15573. Took 0.06 sec\n",
            "Epoch 2185, Loss(train/val) 0.22740/0.15618. Took 0.05 sec\n",
            "Epoch 2186, Loss(train/val) 0.23389/0.15669. Took 0.05 sec\n",
            "Epoch 2187, Loss(train/val) 0.22233/0.15701. Took 0.05 sec\n",
            "Epoch 2188, Loss(train/val) 0.24201/0.15639. Took 0.05 sec\n",
            "Epoch 2189, Loss(train/val) 0.24699/0.15598. Took 0.05 sec\n",
            "Epoch 2190, Loss(train/val) 0.24492/0.15595. Took 0.04 sec\n",
            "Epoch 2191, Loss(train/val) 0.22945/0.15590. Took 0.05 sec\n",
            "Epoch 2192, Loss(train/val) 0.23767/0.15581. Took 0.04 sec\n",
            "Epoch 2193, Loss(train/val) 0.22721/0.15578. Took 0.05 sec\n",
            "Epoch 2194, Loss(train/val) 0.22745/0.15575. Took 0.05 sec\n",
            "Epoch 2195, Loss(train/val) 0.22762/0.15573. Took 0.04 sec\n",
            "Epoch 2196, Loss(train/val) 0.22766/0.15585. Took 0.04 sec\n",
            "Epoch 2197, Loss(train/val) 0.22743/0.15587. Took 0.05 sec\n",
            "Epoch 2198, Loss(train/val) 0.23755/0.15610. Took 0.04 sec\n",
            "Epoch 2199, Loss(train/val) 0.23461/0.15606. Took 0.05 sec\n",
            "Epoch 2200, Loss(train/val) 0.22855/0.15580. Took 0.04 sec\n",
            "Epoch 2201, Loss(train/val) 0.23384/0.15581. Took 0.04 sec\n",
            "Epoch 2202, Loss(train/val) 0.22412/0.15581. Took 0.04 sec\n",
            "Epoch 2203, Loss(train/val) 0.23496/0.15580. Took 0.05 sec\n",
            "Epoch 2204, Loss(train/val) 0.23604/0.15571. Took 0.05 sec\n",
            "Epoch 2205, Loss(train/val) 0.23332/0.15586. Took 0.04 sec\n",
            "Epoch 2206, Loss(train/val) 0.23346/0.15558. Took 0.04 sec\n",
            "Epoch 2207, Loss(train/val) 0.24114/0.15557. Took 0.05 sec\n",
            "Epoch 2208, Loss(train/val) 0.22794/0.15557. Took 0.05 sec\n",
            "Epoch 2209, Loss(train/val) 0.23029/0.15562. Took 0.05 sec\n",
            "Epoch 2210, Loss(train/val) 0.22918/0.15550. Took 0.05 sec\n",
            "Epoch 2211, Loss(train/val) 0.24601/0.15549. Took 0.05 sec\n",
            "Epoch 2212, Loss(train/val) 0.22494/0.15574. Took 0.05 sec\n",
            "Epoch 2213, Loss(train/val) 0.23463/0.15574. Took 0.04 sec\n",
            "Epoch 2214, Loss(train/val) 0.23229/0.15561. Took 0.05 sec\n",
            "Epoch 2215, Loss(train/val) 0.22526/0.15710. Took 0.05 sec\n",
            "Epoch 2216, Loss(train/val) 0.23341/0.15820. Took 0.05 sec\n",
            "Epoch 2217, Loss(train/val) 0.22730/0.15688. Took 0.05 sec\n",
            "Epoch 2218, Loss(train/val) 0.23352/0.15558. Took 0.05 sec\n",
            "Epoch 2219, Loss(train/val) 0.24392/0.15583. Took 0.05 sec\n",
            "Epoch 2220, Loss(train/val) 0.24126/0.15816. Took 0.04 sec\n",
            "Epoch 2221, Loss(train/val) 0.22442/0.15900. Took 0.04 sec\n",
            "Epoch 2222, Loss(train/val) 0.24057/0.15994. Took 0.04 sec\n",
            "Epoch 2223, Loss(train/val) 0.22948/0.15847. Took 0.04 sec\n",
            "Epoch 2224, Loss(train/val) 0.23601/0.15620. Took 0.06 sec\n",
            "Epoch 2225, Loss(train/val) 0.22942/0.15559. Took 0.05 sec\n",
            "Epoch 2226, Loss(train/val) 0.21568/0.15549. Took 0.05 sec\n",
            "Epoch 2227, Loss(train/val) 0.22301/0.15550. Took 0.05 sec\n",
            "Epoch 2228, Loss(train/val) 0.22597/0.15685. Took 0.05 sec\n",
            "Epoch 2229, Loss(train/val) 0.22698/0.15718. Took 0.05 sec\n",
            "Epoch 2230, Loss(train/val) 0.24304/0.15598. Took 0.04 sec\n",
            "Epoch 2231, Loss(train/val) 0.22801/0.15552. Took 0.04 sec\n",
            "Epoch 2232, Loss(train/val) 0.22491/0.15582. Took 0.05 sec\n",
            "Epoch 2233, Loss(train/val) 0.22896/0.15596. Took 0.04 sec\n",
            "Epoch 2234, Loss(train/val) 0.23873/0.15649. Took 0.05 sec\n",
            "Epoch 2235, Loss(train/val) 0.22450/0.15614. Took 0.05 sec\n",
            "Epoch 2236, Loss(train/val) 0.23332/0.15537. Took 0.05 sec\n",
            "Epoch 2237, Loss(train/val) 0.24175/0.15537. Took 0.04 sec\n",
            "Epoch 2238, Loss(train/val) 0.23340/0.15536. Took 0.04 sec\n",
            "Epoch 2239, Loss(train/val) 0.23070/0.15545. Took 0.05 sec\n",
            "Epoch 2240, Loss(train/val) 0.24086/0.15603. Took 0.05 sec\n",
            "Epoch 2241, Loss(train/val) 0.22255/0.15564. Took 0.04 sec\n",
            "Epoch 2242, Loss(train/val) 0.22393/0.15537. Took 0.04 sec\n",
            "Epoch 2243, Loss(train/val) 0.22202/0.15534. Took 0.04 sec\n",
            "Epoch 2244, Loss(train/val) 0.23289/0.15520. Took 0.05 sec\n",
            "Epoch 2245, Loss(train/val) 0.22773/0.15517. Took 0.04 sec\n",
            "Epoch 2246, Loss(train/val) 0.22759/0.15540. Took 0.06 sec\n",
            "Epoch 2247, Loss(train/val) 0.22779/0.15554. Took 0.05 sec\n",
            "Epoch 2248, Loss(train/val) 0.22643/0.15548. Took 0.04 sec\n",
            "Epoch 2249, Loss(train/val) 0.23955/0.15547. Took 0.05 sec\n",
            "Epoch 2250, Loss(train/val) 0.22879/0.15551. Took 0.05 sec\n",
            "Epoch 2251, Loss(train/val) 0.23779/0.15538. Took 0.04 sec\n",
            "Epoch 2252, Loss(train/val) 0.23158/0.15542. Took 0.04 sec\n",
            "Epoch 2253, Loss(train/val) 0.23211/0.15549. Took 0.05 sec\n",
            "Epoch 2254, Loss(train/val) 0.23051/0.15549. Took 0.05 sec\n",
            "Epoch 2255, Loss(train/val) 0.23651/0.15551. Took 0.05 sec\n",
            "Epoch 2256, Loss(train/val) 0.22494/0.15529. Took 0.05 sec\n",
            "Epoch 2257, Loss(train/val) 0.22547/0.15517. Took 0.04 sec\n",
            "Epoch 2258, Loss(train/val) 0.22929/0.15671. Took 0.04 sec\n",
            "Epoch 2259, Loss(train/val) 0.23860/0.15625. Took 0.05 sec\n",
            "Epoch 2260, Loss(train/val) 0.22571/0.15627. Took 0.05 sec\n",
            "Epoch 2261, Loss(train/val) 0.23176/0.15565. Took 0.05 sec\n",
            "Epoch 2262, Loss(train/val) 0.23451/0.15515. Took 0.05 sec\n",
            "Epoch 2263, Loss(train/val) 0.23779/0.15537. Took 0.05 sec\n",
            "Epoch 2264, Loss(train/val) 0.22522/0.15540. Took 0.06 sec\n",
            "Epoch 2265, Loss(train/val) 0.23236/0.15542. Took 0.05 sec\n",
            "Epoch 2266, Loss(train/val) 0.22387/0.15527. Took 0.04 sec\n",
            "Epoch 2267, Loss(train/val) 0.23489/0.15529. Took 0.05 sec\n",
            "Epoch 2268, Loss(train/val) 0.23485/0.15514. Took 0.05 sec\n",
            "Epoch 2269, Loss(train/val) 0.22517/0.15570. Took 0.05 sec\n",
            "Epoch 2270, Loss(train/val) 0.23140/0.15577. Took 0.05 sec\n",
            "Epoch 2271, Loss(train/val) 0.23192/0.15517. Took 0.05 sec\n",
            "Epoch 2272, Loss(train/val) 0.22848/0.15565. Took 0.06 sec\n",
            "Epoch 2273, Loss(train/val) 0.24316/0.15700. Took 0.06 sec\n",
            "Epoch 2274, Loss(train/val) 0.25585/0.15715. Took 0.05 sec\n",
            "Epoch 2275, Loss(train/val) 0.22458/0.15540. Took 0.05 sec\n",
            "Epoch 2276, Loss(train/val) 0.21856/0.15525. Took 0.04 sec\n",
            "Epoch 2277, Loss(train/val) 0.22287/0.15541. Took 0.05 sec\n",
            "Epoch 2278, Loss(train/val) 0.23875/0.15584. Took 0.05 sec\n",
            "Epoch 2279, Loss(train/val) 0.22582/0.15607. Took 0.04 sec\n",
            "Epoch 2280, Loss(train/val) 0.23496/0.15568. Took 0.04 sec\n",
            "Epoch 2281, Loss(train/val) 0.23250/0.15488. Took 0.04 sec\n",
            "Epoch 2282, Loss(train/val) 0.24055/0.15479. Took 0.05 sec\n",
            "Epoch 2283, Loss(train/val) 0.22991/0.15481. Took 0.05 sec\n",
            "Epoch 2284, Loss(train/val) 0.23316/0.15472. Took 0.05 sec\n",
            "Epoch 2285, Loss(train/val) 0.22488/0.15482. Took 0.05 sec\n",
            "Epoch 2286, Loss(train/val) 0.22355/0.15570. Took 0.05 sec\n",
            "Epoch 2287, Loss(train/val) 0.23867/0.15549. Took 0.05 sec\n",
            "Epoch 2288, Loss(train/val) 0.23613/0.15645. Took 0.06 sec\n",
            "Epoch 2289, Loss(train/val) 0.22629/0.15538. Took 0.05 sec\n",
            "Epoch 2290, Loss(train/val) 0.22182/0.15487. Took 0.04 sec\n",
            "Epoch 2291, Loss(train/val) 0.23518/0.15532. Took 0.05 sec\n",
            "Epoch 2292, Loss(train/val) 0.23314/0.15550. Took 0.04 sec\n",
            "Epoch 2293, Loss(train/val) 0.24970/0.15500. Took 0.05 sec\n",
            "Epoch 2294, Loss(train/val) 0.22700/0.15506. Took 0.05 sec\n",
            "Epoch 2295, Loss(train/val) 0.24009/0.15505. Took 0.05 sec\n",
            "Epoch 2296, Loss(train/val) 0.22886/0.15501. Took 0.04 sec\n",
            "Epoch 2297, Loss(train/val) 0.22229/0.15503. Took 0.04 sec\n",
            "Epoch 2298, Loss(train/val) 0.24244/0.15472. Took 0.05 sec\n",
            "Epoch 2299, Loss(train/val) 0.21975/0.15472. Took 0.04 sec\n",
            "Epoch 2300, Loss(train/val) 0.22864/0.15468. Took 0.05 sec\n",
            "Epoch 2301, Loss(train/val) 0.22947/0.15473. Took 0.05 sec\n",
            "Epoch 2302, Loss(train/val) 0.23727/0.15464. Took 0.05 sec\n",
            "Epoch 2303, Loss(train/val) 0.22420/0.15474. Took 0.05 sec\n",
            "Epoch 2304, Loss(train/val) 0.23188/0.15473. Took 0.05 sec\n",
            "Epoch 2305, Loss(train/val) 0.22903/0.15473. Took 0.04 sec\n",
            "Epoch 2306, Loss(train/val) 0.22591/0.15482. Took 0.04 sec\n",
            "Epoch 2307, Loss(train/val) 0.22802/0.15523. Took 0.05 sec\n",
            "Epoch 2308, Loss(train/val) 0.24090/0.15489. Took 0.05 sec\n",
            "Epoch 2309, Loss(train/val) 0.22976/0.15456. Took 0.04 sec\n",
            "Epoch 2310, Loss(train/val) 0.23118/0.15456. Took 0.05 sec\n",
            "Epoch 2311, Loss(train/val) 0.23858/0.15485. Took 0.05 sec\n",
            "Epoch 2312, Loss(train/val) 0.22900/0.15547. Took 0.05 sec\n",
            "Epoch 2313, Loss(train/val) 0.22232/0.15575. Took 0.05 sec\n",
            "Epoch 2314, Loss(train/val) 0.24260/0.15507. Took 0.05 sec\n",
            "Epoch 2315, Loss(train/val) 0.23067/0.15450. Took 0.05 sec\n",
            "Epoch 2316, Loss(train/val) 0.23075/0.15458. Took 0.05 sec\n",
            "Epoch 2317, Loss(train/val) 0.22809/0.15491. Took 0.04 sec\n",
            "Epoch 2318, Loss(train/val) 0.22466/0.15478. Took 0.05 sec\n",
            "Epoch 2319, Loss(train/val) 0.22974/0.15458. Took 0.05 sec\n",
            "Epoch 2320, Loss(train/val) 0.23589/0.15451. Took 0.05 sec\n",
            "Epoch 2321, Loss(train/val) 0.22381/0.15451. Took 0.05 sec\n",
            "Epoch 2322, Loss(train/val) 0.24343/0.15479. Took 0.05 sec\n",
            "Epoch 2323, Loss(train/val) 0.22567/0.15558. Took 0.05 sec\n",
            "Epoch 2324, Loss(train/val) 0.24333/0.15476. Took 0.04 sec\n",
            "Epoch 2325, Loss(train/val) 0.22586/0.15447. Took 0.05 sec\n",
            "Epoch 2326, Loss(train/val) 0.23000/0.15454. Took 0.05 sec\n",
            "Epoch 2327, Loss(train/val) 0.22965/0.15487. Took 0.04 sec\n",
            "Epoch 2328, Loss(train/val) 0.22607/0.15474. Took 0.05 sec\n",
            "Epoch 2329, Loss(train/val) 0.22116/0.15450. Took 0.05 sec\n",
            "Epoch 2330, Loss(train/val) 0.23709/0.15460. Took 0.04 sec\n",
            "Epoch 2331, Loss(train/val) 0.22929/0.15451. Took 0.05 sec\n",
            "Epoch 2332, Loss(train/val) 0.22262/0.15464. Took 0.05 sec\n",
            "Epoch 2333, Loss(train/val) 0.22779/0.15460. Took 0.05 sec\n",
            "Epoch 2334, Loss(train/val) 0.22677/0.15471. Took 0.05 sec\n",
            "Epoch 2335, Loss(train/val) 0.23353/0.15478. Took 0.05 sec\n",
            "Epoch 2336, Loss(train/val) 0.24308/0.15467. Took 0.05 sec\n",
            "Epoch 2337, Loss(train/val) 0.22941/0.15466. Took 0.05 sec\n",
            "Epoch 2338, Loss(train/val) 0.22464/0.15491. Took 0.05 sec\n",
            "Epoch 2339, Loss(train/val) 0.23522/0.15469. Took 0.05 sec\n",
            "Epoch 2340, Loss(train/val) 0.22204/0.15463. Took 0.05 sec\n",
            "Epoch 2341, Loss(train/val) 0.22397/0.15446. Took 0.05 sec\n",
            "Epoch 2342, Loss(train/val) 0.22354/0.15473. Took 0.04 sec\n",
            "Epoch 2343, Loss(train/val) 0.23639/0.15479. Took 0.05 sec\n",
            "Epoch 2344, Loss(train/val) 0.22539/0.15446. Took 0.05 sec\n",
            "Epoch 2345, Loss(train/val) 0.21761/0.15456. Took 0.05 sec\n",
            "Epoch 2346, Loss(train/val) 0.23773/0.15482. Took 0.04 sec\n",
            "Epoch 2347, Loss(train/val) 0.23077/0.15496. Took 0.04 sec\n",
            "Epoch 2348, Loss(train/val) 0.21995/0.15497. Took 0.05 sec\n",
            "Epoch 2349, Loss(train/val) 0.23293/0.15492. Took 0.05 sec\n",
            "Epoch 2350, Loss(train/val) 0.22717/0.15475. Took 0.05 sec\n",
            "Epoch 2351, Loss(train/val) 0.22835/0.15448. Took 0.05 sec\n",
            "Epoch 2352, Loss(train/val) 0.25181/0.15456. Took 0.05 sec\n",
            "Epoch 2353, Loss(train/val) 0.23343/0.15447. Took 0.05 sec\n",
            "Epoch 2354, Loss(train/val) 0.22857/0.15457. Took 0.05 sec\n",
            "Epoch 2355, Loss(train/val) 0.21853/0.15463. Took 0.05 sec\n",
            "Epoch 2356, Loss(train/val) 0.23124/0.15460. Took 0.05 sec\n",
            "Epoch 2357, Loss(train/val) 0.22998/0.15467. Took 0.05 sec\n",
            "Epoch 2358, Loss(train/val) 0.21810/0.15470. Took 0.05 sec\n",
            "Epoch 2359, Loss(train/val) 0.23611/0.15467. Took 0.05 sec\n",
            "Epoch 2360, Loss(train/val) 0.23061/0.15451. Took 0.05 sec\n",
            "Epoch 2361, Loss(train/val) 0.22466/0.15442. Took 0.04 sec\n",
            "Epoch 2362, Loss(train/val) 0.22758/0.15433. Took 0.05 sec\n",
            "Epoch 2363, Loss(train/val) 0.22442/0.15469. Took 0.05 sec\n",
            "Epoch 2364, Loss(train/val) 0.22322/0.15436. Took 0.04 sec\n",
            "Epoch 2365, Loss(train/val) 0.23227/0.15423. Took 0.05 sec\n",
            "Epoch 2366, Loss(train/val) 0.23004/0.15446. Took 0.04 sec\n",
            "Epoch 2367, Loss(train/val) 0.23012/0.15458. Took 0.05 sec\n",
            "Epoch 2368, Loss(train/val) 0.24517/0.15463. Took 0.05 sec\n",
            "Epoch 2369, Loss(train/val) 0.23500/0.15625. Took 0.04 sec\n",
            "Epoch 2370, Loss(train/val) 0.23464/0.15612. Took 0.04 sec\n",
            "Epoch 2371, Loss(train/val) 0.22251/0.15499. Took 0.05 sec\n",
            "Epoch 2372, Loss(train/val) 0.23277/0.15414. Took 0.04 sec\n",
            "Epoch 2373, Loss(train/val) 0.22168/0.15445. Took 0.06 sec\n",
            "Epoch 2374, Loss(train/val) 0.23306/0.15538. Took 0.06 sec\n",
            "Epoch 2375, Loss(train/val) 0.22423/0.15570. Took 0.04 sec\n",
            "Epoch 2376, Loss(train/val) 0.21809/0.15497. Took 0.04 sec\n",
            "Epoch 2377, Loss(train/val) 0.23443/0.15508. Took 0.05 sec\n",
            "Epoch 2378, Loss(train/val) 0.22128/0.15436. Took 0.05 sec\n",
            "Epoch 2379, Loss(train/val) 0.23580/0.15455. Took 0.05 sec\n",
            "Epoch 2380, Loss(train/val) 0.23432/0.15466. Took 0.04 sec\n",
            "Epoch 2381, Loss(train/val) 0.23079/0.15423. Took 0.05 sec\n",
            "Epoch 2382, Loss(train/val) 0.23124/0.15429. Took 0.05 sec\n",
            "Epoch 2383, Loss(train/val) 0.22230/0.15438. Took 0.05 sec\n",
            "Epoch 2384, Loss(train/val) 0.24484/0.15415. Took 0.04 sec\n",
            "Epoch 2385, Loss(train/val) 0.22578/0.15414. Took 0.04 sec\n",
            "Epoch 2386, Loss(train/val) 0.22532/0.15481. Took 0.05 sec\n",
            "Epoch 2387, Loss(train/val) 0.22390/0.15451. Took 0.05 sec\n",
            "Epoch 2388, Loss(train/val) 0.21963/0.15459. Took 0.05 sec\n",
            "Epoch 2389, Loss(train/val) 0.22524/0.15508. Took 0.04 sec\n",
            "Epoch 2390, Loss(train/val) 0.22597/0.15508. Took 0.04 sec\n",
            "Epoch 2391, Loss(train/val) 0.23126/0.15490. Took 0.05 sec\n",
            "Epoch 2392, Loss(train/val) 0.22505/0.15417. Took 0.05 sec\n",
            "Epoch 2393, Loss(train/val) 0.22635/0.15564. Took 0.05 sec\n",
            "Epoch 2394, Loss(train/val) 0.22941/0.15633. Took 0.05 sec\n",
            "Epoch 2395, Loss(train/val) 0.23513/0.15529. Took 0.05 sec\n",
            "Epoch 2396, Loss(train/val) 0.23090/0.15464. Took 0.05 sec\n",
            "Epoch 2397, Loss(train/val) 0.22358/0.15426. Took 0.05 sec\n",
            "Epoch 2398, Loss(train/val) 0.25135/0.15447. Took 0.05 sec\n",
            "Epoch 2399, Loss(train/val) 0.22734/0.15582. Took 0.05 sec\n",
            "Epoch 2400, Loss(train/val) 0.22607/0.15504. Took 0.05 sec\n",
            "Epoch 2401, Loss(train/val) 0.23470/0.15488. Took 0.04 sec\n",
            "Epoch 2402, Loss(train/val) 0.23144/0.15402. Took 0.05 sec\n",
            "Epoch 2403, Loss(train/val) 0.24630/0.15430. Took 0.05 sec\n",
            "Epoch 2404, Loss(train/val) 0.23171/0.15455. Took 0.04 sec\n",
            "Epoch 2405, Loss(train/val) 0.22902/0.15458. Took 0.05 sec\n",
            "Epoch 2406, Loss(train/val) 0.22938/0.15428. Took 0.05 sec\n",
            "Epoch 2407, Loss(train/val) 0.24839/0.15436. Took 0.04 sec\n",
            "Epoch 2408, Loss(train/val) 0.23042/0.15433. Took 0.05 sec\n",
            "Epoch 2409, Loss(train/val) 0.22331/0.15429. Took 0.04 sec\n",
            "Epoch 2410, Loss(train/val) 0.23909/0.15409. Took 0.04 sec\n",
            "Epoch 2411, Loss(train/val) 0.23426/0.15425. Took 0.05 sec\n",
            "Epoch 2412, Loss(train/val) 0.21878/0.15401. Took 0.04 sec\n",
            "Epoch 2413, Loss(train/val) 0.22262/0.15388. Took 0.06 sec\n",
            "Epoch 2414, Loss(train/val) 0.23035/0.15387. Took 0.05 sec\n",
            "Epoch 2415, Loss(train/val) 0.22371/0.15448. Took 0.05 sec\n",
            "Epoch 2416, Loss(train/val) 0.24083/0.15474. Took 0.05 sec\n",
            "Epoch 2417, Loss(train/val) 0.23977/0.15405. Took 0.05 sec\n",
            "Epoch 2418, Loss(train/val) 0.22947/0.15428. Took 0.05 sec\n",
            "Epoch 2419, Loss(train/val) 0.22057/0.15424. Took 0.05 sec\n",
            "Epoch 2420, Loss(train/val) 0.22803/0.15427. Took 0.04 sec\n",
            "Epoch 2421, Loss(train/val) 0.22966/0.15500. Took 0.05 sec\n",
            "Epoch 2422, Loss(train/val) 0.24079/0.15652. Took 0.04 sec\n",
            "Epoch 2423, Loss(train/val) 0.22165/0.15754. Took 0.05 sec\n",
            "Epoch 2424, Loss(train/val) 0.23585/0.15792. Took 0.05 sec\n",
            "Epoch 2425, Loss(train/val) 0.23463/0.15884. Took 0.05 sec\n",
            "Epoch 2426, Loss(train/val) 0.24357/0.15805. Took 0.04 sec\n",
            "Epoch 2427, Loss(train/val) 0.22161/0.15757. Took 0.05 sec\n",
            "Epoch 2428, Loss(train/val) 0.22190/0.15460. Took 0.05 sec\n",
            "Epoch 2429, Loss(train/val) 0.22198/0.15424. Took 0.04 sec\n",
            "Epoch 2430, Loss(train/val) 0.22558/0.15608. Took 0.04 sec\n",
            "Epoch 2431, Loss(train/val) 0.22131/0.16127. Took 0.04 sec\n",
            "Epoch 2432, Loss(train/val) 0.23361/0.15968. Took 0.04 sec\n",
            "Epoch 2433, Loss(train/val) 0.22691/0.15583. Took 0.05 sec\n",
            "Epoch 2434, Loss(train/val) 0.23152/0.15434. Took 0.04 sec\n",
            "Epoch 2435, Loss(train/val) 0.21902/0.15640. Took 0.05 sec\n",
            "Epoch 2436, Loss(train/val) 0.23309/0.15805. Took 0.05 sec\n",
            "Epoch 2437, Loss(train/val) 0.22370/0.15625. Took 0.05 sec\n",
            "Epoch 2438, Loss(train/val) 0.22875/0.15466. Took 0.05 sec\n",
            "Epoch 2439, Loss(train/val) 0.23126/0.15394. Took 0.04 sec\n",
            "Epoch 2440, Loss(train/val) 0.25140/0.15443. Took 0.05 sec\n",
            "Epoch 2441, Loss(train/val) 0.23608/0.15410. Took 0.04 sec\n",
            "Epoch 2442, Loss(train/val) 0.22885/0.15400. Took 0.04 sec\n",
            "Epoch 2443, Loss(train/val) 0.23405/0.15391. Took 0.06 sec\n",
            "Epoch 2444, Loss(train/val) 0.23888/0.15395. Took 0.05 sec\n",
            "Epoch 2445, Loss(train/val) 0.23027/0.15391. Took 0.04 sec\n",
            "Epoch 2446, Loss(train/val) 0.22516/0.15408. Took 0.05 sec\n",
            "Epoch 2447, Loss(train/val) 0.22256/0.15429. Took 0.04 sec\n",
            "Epoch 2448, Loss(train/val) 0.22638/0.15404. Took 0.05 sec\n",
            "Epoch 2449, Loss(train/val) 0.22476/0.15406. Took 0.05 sec\n",
            "Epoch 2450, Loss(train/val) 0.22834/0.15405. Took 0.04 sec\n",
            "Epoch 2451, Loss(train/val) 0.22226/0.15412. Took 0.05 sec\n",
            "Epoch 2452, Loss(train/val) 0.24103/0.15413. Took 0.04 sec\n",
            "Epoch 2453, Loss(train/val) 0.23302/0.15428. Took 0.05 sec\n",
            "Epoch 2454, Loss(train/val) 0.22640/0.15439. Took 0.05 sec\n",
            "Epoch 2455, Loss(train/val) 0.24540/0.15461. Took 0.05 sec\n",
            "Epoch 2456, Loss(train/val) 0.23872/0.15461. Took 0.05 sec\n",
            "Epoch 2457, Loss(train/val) 0.22725/0.15412. Took 0.05 sec\n",
            "Epoch 2458, Loss(train/val) 0.23812/0.15455. Took 0.06 sec\n",
            "Epoch 2459, Loss(train/val) 0.23366/0.15484. Took 0.05 sec\n",
            "Epoch 2460, Loss(train/val) 0.23126/0.15509. Took 0.04 sec\n",
            "Epoch 2461, Loss(train/val) 0.22081/0.15435. Took 0.05 sec\n",
            "Epoch 2462, Loss(train/val) 0.22674/0.15414. Took 0.05 sec\n",
            "Epoch 2463, Loss(train/val) 0.22031/0.15400. Took 0.05 sec\n",
            "Epoch 2464, Loss(train/val) 0.22513/0.15396. Took 0.05 sec\n",
            "Epoch 2465, Loss(train/val) 0.21776/0.15412. Took 0.05 sec\n",
            "Epoch 2466, Loss(train/val) 0.23330/0.15403. Took 0.04 sec\n",
            "Epoch 2467, Loss(train/val) 0.22121/0.15418. Took 0.04 sec\n",
            "Epoch 2468, Loss(train/val) 0.22889/0.15406. Took 0.05 sec\n",
            "Epoch 2469, Loss(train/val) 0.22317/0.15418. Took 0.05 sec\n",
            "Epoch 2470, Loss(train/val) 0.23187/0.15414. Took 0.05 sec\n",
            "Epoch 2471, Loss(train/val) 0.25925/0.15461. Took 0.04 sec\n",
            "Epoch 2472, Loss(train/val) 0.24196/0.15628. Took 0.05 sec\n",
            "Epoch 2473, Loss(train/val) 0.23224/0.15699. Took 0.05 sec\n",
            "Epoch 2474, Loss(train/val) 0.25075/0.15615. Took 0.05 sec\n",
            "Epoch 2475, Loss(train/val) 0.22205/0.15475. Took 0.05 sec\n",
            "Epoch 2476, Loss(train/val) 0.22724/0.15399. Took 0.04 sec\n",
            "Epoch 2477, Loss(train/val) 0.23002/0.15443. Took 0.05 sec\n",
            "Epoch 2478, Loss(train/val) 0.22061/0.15454. Took 0.05 sec\n",
            "Epoch 2479, Loss(train/val) 0.22087/0.15522. Took 0.05 sec\n",
            "Epoch 2480, Loss(train/val) 0.23298/0.15446. Took 0.06 sec\n",
            "Epoch 2481, Loss(train/val) 0.22921/0.15410. Took 0.05 sec\n",
            "Epoch 2482, Loss(train/val) 0.22126/0.15423. Took 0.06 sec\n",
            "Epoch 2483, Loss(train/val) 0.22539/0.15471. Took 0.05 sec\n",
            "Epoch 2484, Loss(train/val) 0.22640/0.15429. Took 0.05 sec\n",
            "Epoch 2485, Loss(train/val) 0.22320/0.15425. Took 0.05 sec\n",
            "Epoch 2486, Loss(train/val) 0.22376/0.15432. Took 0.05 sec\n",
            "Epoch 2487, Loss(train/val) 0.23254/0.15411. Took 0.06 sec\n",
            "Epoch 2488, Loss(train/val) 0.22797/0.15411. Took 0.05 sec\n",
            "Epoch 2489, Loss(train/val) 0.22116/0.15422. Took 0.04 sec\n",
            "Epoch 2490, Loss(train/val) 0.22214/0.15406. Took 0.05 sec\n",
            "Epoch 2491, Loss(train/val) 0.22729/0.15406. Took 0.05 sec\n",
            "Epoch 2492, Loss(train/val) 0.23037/0.15415. Took 0.04 sec\n",
            "Epoch 2493, Loss(train/val) 0.23152/0.15419. Took 0.04 sec\n",
            "Epoch 2494, Loss(train/val) 0.23339/0.15413. Took 0.04 sec\n",
            "Epoch 2495, Loss(train/val) 0.23405/0.15408. Took 0.05 sec\n",
            "Epoch 2496, Loss(train/val) 0.24188/0.15416. Took 0.05 sec\n",
            "Epoch 2497, Loss(train/val) 0.22229/0.15472. Took 0.04 sec\n",
            "Epoch 2498, Loss(train/val) 0.21624/0.15484. Took 0.05 sec\n",
            "Epoch 2499, Loss(train/val) 0.22421/0.15413. Took 0.05 sec\n",
            "Epoch 2500, Loss(train/val) 0.22494/0.15378. Took 0.05 sec\n",
            "Epoch 2501, Loss(train/val) 0.22424/0.15452. Took 0.05 sec\n",
            "Epoch 2502, Loss(train/val) 0.22930/0.15491. Took 0.04 sec\n",
            "Epoch 2503, Loss(train/val) 0.23833/0.15403. Took 0.04 sec\n",
            "Epoch 2504, Loss(train/val) 0.25945/0.15588. Took 0.04 sec\n",
            "Epoch 2505, Loss(train/val) 0.22602/0.15718. Took 0.05 sec\n",
            "Epoch 2506, Loss(train/val) 0.22079/0.15448. Took 0.05 sec\n",
            "Epoch 2507, Loss(train/val) 0.24025/0.15490. Took 0.04 sec\n",
            "Epoch 2508, Loss(train/val) 0.22162/0.15450. Took 0.04 sec\n",
            "Epoch 2509, Loss(train/val) 0.22811/0.15530. Took 0.05 sec\n",
            "Epoch 2510, Loss(train/val) 0.22559/0.15391. Took 0.04 sec\n",
            "Epoch 2511, Loss(train/val) 0.22818/0.15406. Took 0.06 sec\n",
            "Epoch 2512, Loss(train/val) 0.22319/0.15390. Took 0.04 sec\n",
            "Epoch 2513, Loss(train/val) 0.23640/0.15444. Took 0.04 sec\n",
            "Epoch 2514, Loss(train/val) 0.22604/0.15556. Took 0.05 sec\n",
            "Epoch 2515, Loss(train/val) 0.23649/0.15484. Took 0.05 sec\n",
            "Epoch 2516, Loss(train/val) 0.23235/0.15486. Took 0.06 sec\n",
            "Epoch 2517, Loss(train/val) 0.24625/0.15463. Took 0.05 sec\n",
            "Epoch 2518, Loss(train/val) 0.22236/0.15458. Took 0.05 sec\n",
            "Epoch 2519, Loss(train/val) 0.23109/0.15378. Took 0.05 sec\n",
            "Epoch 2520, Loss(train/val) 0.22526/0.15379. Took 0.05 sec\n",
            "Epoch 2521, Loss(train/val) 0.23013/0.15439. Took 0.05 sec\n",
            "Epoch 2522, Loss(train/val) 0.21814/0.15487. Took 0.05 sec\n",
            "Epoch 2523, Loss(train/val) 0.23426/0.15445. Took 0.05 sec\n",
            "Epoch 2524, Loss(train/val) 0.23813/0.15529. Took 0.05 sec\n",
            "Epoch 2525, Loss(train/val) 0.23428/0.15401. Took 0.05 sec\n",
            "Epoch 2526, Loss(train/val) 0.21681/0.15386. Took 0.05 sec\n",
            "Epoch 2527, Loss(train/val) 0.23911/0.15390. Took 0.05 sec\n",
            "Epoch 2528, Loss(train/val) 0.22063/0.15391. Took 0.04 sec\n",
            "Epoch 2529, Loss(train/val) 0.23044/0.15492. Took 0.04 sec\n",
            "Epoch 2530, Loss(train/val) 0.23069/0.15437. Took 0.04 sec\n",
            "Epoch 2531, Loss(train/val) 0.22721/0.15373. Took 0.05 sec\n",
            "Epoch 2532, Loss(train/val) 0.22298/0.15424. Took 0.05 sec\n",
            "Epoch 2533, Loss(train/val) 0.22256/0.15364. Took 0.05 sec\n",
            "Epoch 2534, Loss(train/val) 0.22750/0.15433. Took 0.05 sec\n",
            "Epoch 2535, Loss(train/val) 0.22281/0.15610. Took 0.04 sec\n",
            "Epoch 2536, Loss(train/val) 0.23409/0.15474. Took 0.05 sec\n",
            "Epoch 2537, Loss(train/val) 0.22215/0.15447. Took 0.05 sec\n",
            "Epoch 2538, Loss(train/val) 0.22878/0.15407. Took 0.05 sec\n",
            "Epoch 2539, Loss(train/val) 0.23753/0.15387. Took 0.05 sec\n",
            "Epoch 2540, Loss(train/val) 0.22542/0.15387. Took 0.05 sec\n",
            "Epoch 2541, Loss(train/val) 0.23662/0.15381. Took 0.06 sec\n",
            "Epoch 2542, Loss(train/val) 0.22633/0.15387. Took 0.05 sec\n",
            "Epoch 2543, Loss(train/val) 0.22407/0.15377. Took 0.06 sec\n",
            "Epoch 2544, Loss(train/val) 0.23212/0.15379. Took 0.05 sec\n",
            "Epoch 2545, Loss(train/val) 0.23055/0.15394. Took 0.05 sec\n",
            "Epoch 2546, Loss(train/val) 0.24237/0.15398. Took 0.06 sec\n",
            "Epoch 2547, Loss(train/val) 0.22728/0.15583. Took 0.05 sec\n",
            "Epoch 2548, Loss(train/val) 0.22935/0.15888. Took 0.05 sec\n",
            "Epoch 2549, Loss(train/val) 0.22635/0.15586. Took 0.05 sec\n",
            "Epoch 2550, Loss(train/val) 0.22746/0.15395. Took 0.05 sec\n",
            "Epoch 2551, Loss(train/val) 0.23755/0.15370. Took 0.05 sec\n",
            "Epoch 2552, Loss(train/val) 0.23387/0.15369. Took 0.05 sec\n",
            "Epoch 2553, Loss(train/val) 0.22200/0.15391. Took 0.04 sec\n",
            "Epoch 2554, Loss(train/val) 0.21695/0.15412. Took 0.04 sec\n",
            "Epoch 2555, Loss(train/val) 0.22977/0.15466. Took 0.04 sec\n",
            "Epoch 2556, Loss(train/val) 0.23063/0.15469. Took 0.05 sec\n",
            "Epoch 2557, Loss(train/val) 0.22528/0.15426. Took 0.05 sec\n",
            "Epoch 2558, Loss(train/val) 0.23255/0.15357. Took 0.05 sec\n",
            "Epoch 2559, Loss(train/val) 0.22244/0.15397. Took 0.05 sec\n",
            "Epoch 2560, Loss(train/val) 0.25608/0.15808. Took 0.05 sec\n",
            "Epoch 2561, Loss(train/val) 0.22317/0.16063. Took 0.06 sec\n",
            "Epoch 2562, Loss(train/val) 0.23442/0.15878. Took 0.05 sec\n",
            "Epoch 2563, Loss(train/val) 0.21627/0.15459. Took 0.05 sec\n",
            "Epoch 2564, Loss(train/val) 0.23563/0.15514. Took 0.05 sec\n",
            "Epoch 2565, Loss(train/val) 0.23862/0.15376. Took 0.04 sec\n",
            "Epoch 2566, Loss(train/val) 0.22590/0.15428. Took 0.05 sec\n",
            "Epoch 2567, Loss(train/val) 0.21976/0.15516. Took 0.05 sec\n",
            "Epoch 2568, Loss(train/val) 0.24913/0.15488. Took 0.05 sec\n",
            "Epoch 2569, Loss(train/val) 0.22551/0.15381. Took 0.05 sec\n",
            "Epoch 2570, Loss(train/val) 0.23842/0.15365. Took 0.05 sec\n",
            "Epoch 2571, Loss(train/val) 0.22262/0.15362. Took 0.05 sec\n",
            "Epoch 2572, Loss(train/val) 0.22269/0.15475. Took 0.05 sec\n",
            "Epoch 2573, Loss(train/val) 0.24281/0.15519. Took 0.05 sec\n",
            "Epoch 2574, Loss(train/val) 0.22410/0.15506. Took 0.05 sec\n",
            "Epoch 2575, Loss(train/val) 0.22192/0.15471. Took 0.04 sec\n",
            "Epoch 2576, Loss(train/val) 0.23007/0.15423. Took 0.05 sec\n",
            "Epoch 2577, Loss(train/val) 0.23173/0.15375. Took 0.05 sec\n",
            "Epoch 2578, Loss(train/val) 0.22353/0.15380. Took 0.05 sec\n",
            "Epoch 2579, Loss(train/val) 0.23234/0.15402. Took 0.04 sec\n",
            "Epoch 2580, Loss(train/val) 0.23411/0.15487. Took 0.05 sec\n",
            "Epoch 2581, Loss(train/val) 0.23286/0.15464. Took 0.05 sec\n",
            "Epoch 2582, Loss(train/val) 0.22526/0.15402. Took 0.05 sec\n",
            "Epoch 2583, Loss(train/val) 0.22882/0.15368. Took 0.04 sec\n",
            "Epoch 2584, Loss(train/val) 0.22342/0.15378. Took 0.04 sec\n",
            "Epoch 2585, Loss(train/val) 0.21896/0.15358. Took 0.05 sec\n",
            "Epoch 2586, Loss(train/val) 0.23297/0.15357. Took 0.05 sec\n",
            "Epoch 2587, Loss(train/val) 0.23052/0.15353. Took 0.05 sec\n",
            "Epoch 2588, Loss(train/val) 0.22516/0.15389. Took 0.05 sec\n",
            "Epoch 2589, Loss(train/val) 0.24175/0.15360. Took 0.05 sec\n",
            "Epoch 2590, Loss(train/val) 0.23243/0.15377. Took 0.04 sec\n",
            "Epoch 2591, Loss(train/val) 0.22620/0.15359. Took 0.05 sec\n",
            "Epoch 2592, Loss(train/val) 0.23133/0.15356. Took 0.05 sec\n",
            "Epoch 2593, Loss(train/val) 0.22018/0.15350. Took 0.05 sec\n",
            "Epoch 2594, Loss(train/val) 0.23018/0.15356. Took 0.05 sec\n",
            "Epoch 2595, Loss(train/val) 0.22286/0.15364. Took 0.05 sec\n",
            "Epoch 2596, Loss(train/val) 0.23813/0.15355. Took 0.05 sec\n",
            "Epoch 2597, Loss(train/val) 0.22298/0.15345. Took 0.05 sec\n",
            "Epoch 2598, Loss(train/val) 0.23133/0.15351. Took 0.05 sec\n",
            "Epoch 2599, Loss(train/val) 0.22188/0.15343. Took 0.05 sec\n",
            "Epoch 2600, Loss(train/val) 0.24422/0.15338. Took 0.05 sec\n",
            "Epoch 2601, Loss(train/val) 0.22727/0.15419. Took 0.05 sec\n",
            "Epoch 2602, Loss(train/val) 0.22851/0.15367. Took 0.05 sec\n",
            "Epoch 2603, Loss(train/val) 0.21808/0.15348. Took 0.05 sec\n",
            "Epoch 2604, Loss(train/val) 0.23538/0.15385. Took 0.04 sec\n",
            "Epoch 2605, Loss(train/val) 0.23432/0.15405. Took 0.05 sec\n",
            "Epoch 2606, Loss(train/val) 0.23144/0.15365. Took 0.07 sec\n",
            "Epoch 2607, Loss(train/val) 0.22887/0.15377. Took 0.05 sec\n",
            "Epoch 2608, Loss(train/val) 0.22936/0.15406. Took 0.05 sec\n",
            "Epoch 2609, Loss(train/val) 0.22105/0.15369. Took 0.05 sec\n",
            "Epoch 2610, Loss(train/val) 0.23059/0.15382. Took 0.05 sec\n",
            "Epoch 2611, Loss(train/val) 0.22919/0.15373. Took 0.06 sec\n",
            "Epoch 2612, Loss(train/val) 0.25375/0.15381. Took 0.04 sec\n",
            "Epoch 2613, Loss(train/val) 0.22010/0.15387. Took 0.05 sec\n",
            "Epoch 2614, Loss(train/val) 0.23741/0.15392. Took 0.05 sec\n",
            "Epoch 2615, Loss(train/val) 0.24219/0.15404. Took 0.05 sec\n",
            "Epoch 2616, Loss(train/val) 0.23256/0.15363. Took 0.05 sec\n",
            "Epoch 2617, Loss(train/val) 0.24345/0.15372. Took 0.05 sec\n",
            "Epoch 2618, Loss(train/val) 0.23803/0.15477. Took 0.04 sec\n",
            "Epoch 2619, Loss(train/val) 0.23887/0.15495. Took 0.05 sec\n",
            "Epoch 2620, Loss(train/val) 0.23236/0.15358. Took 0.04 sec\n",
            "Epoch 2621, Loss(train/val) 0.23952/0.15372. Took 0.05 sec\n",
            "Epoch 2622, Loss(train/val) 0.22551/0.15432. Took 0.04 sec\n",
            "Epoch 2623, Loss(train/val) 0.23134/0.15514. Took 0.04 sec\n",
            "Epoch 2624, Loss(train/val) 0.23280/0.15402. Took 0.05 sec\n",
            "Epoch 2625, Loss(train/val) 0.21461/0.15353. Took 0.05 sec\n",
            "Epoch 2626, Loss(train/val) 0.21921/0.15443. Took 0.05 sec\n",
            "Epoch 2627, Loss(train/val) 0.23376/0.15501. Took 0.05 sec\n",
            "Epoch 2628, Loss(train/val) 0.23173/0.15340. Took 0.05 sec\n",
            "Epoch 2629, Loss(train/val) 0.22548/0.15327. Took 0.05 sec\n",
            "Epoch 2630, Loss(train/val) 0.23726/0.15374. Took 0.05 sec\n",
            "Epoch 2631, Loss(train/val) 0.22413/0.15332. Took 0.05 sec\n",
            "Epoch 2632, Loss(train/val) 0.22509/0.15340. Took 0.04 sec\n",
            "Epoch 2633, Loss(train/val) 0.22015/0.15374. Took 0.04 sec\n",
            "Epoch 2634, Loss(train/val) 0.22144/0.15355. Took 0.04 sec\n",
            "Epoch 2635, Loss(train/val) 0.22595/0.15401. Took 0.04 sec\n",
            "Epoch 2636, Loss(train/val) 0.22931/0.15406. Took 0.06 sec\n",
            "Epoch 2637, Loss(train/val) 0.22629/0.15363. Took 0.05 sec\n",
            "Epoch 2638, Loss(train/val) 0.23301/0.15344. Took 0.04 sec\n",
            "Epoch 2639, Loss(train/val) 0.21993/0.15357. Took 0.04 sec\n",
            "Epoch 2640, Loss(train/val) 0.23081/0.15344. Took 0.04 sec\n",
            "Epoch 2641, Loss(train/val) 0.22808/0.15352. Took 0.05 sec\n",
            "Epoch 2642, Loss(train/val) 0.23640/0.15331. Took 0.04 sec\n",
            "Epoch 2643, Loss(train/val) 0.22093/0.15363. Took 0.05 sec\n",
            "Epoch 2644, Loss(train/val) 0.23308/0.15354. Took 0.05 sec\n",
            "Epoch 2645, Loss(train/val) 0.22749/0.15329. Took 0.05 sec\n",
            "Epoch 2646, Loss(train/val) 0.22240/0.15338. Took 0.05 sec\n",
            "Epoch 2647, Loss(train/val) 0.24399/0.15364. Took 0.04 sec\n",
            "Epoch 2648, Loss(train/val) 0.23677/0.15434. Took 0.04 sec\n",
            "Epoch 2649, Loss(train/val) 0.22640/0.15356. Took 0.05 sec\n",
            "Epoch 2650, Loss(train/val) 0.22225/0.15328. Took 0.05 sec\n",
            "Epoch 2651, Loss(train/val) 0.22949/0.15327. Took 0.06 sec\n",
            "Epoch 2652, Loss(train/val) 0.22290/0.15317. Took 0.05 sec\n",
            "Epoch 2653, Loss(train/val) 0.22177/0.15325. Took 0.05 sec\n",
            "Epoch 2654, Loss(train/val) 0.23431/0.15327. Took 0.05 sec\n",
            "Epoch 2655, Loss(train/val) 0.23733/0.15346. Took 0.04 sec\n",
            "Epoch 2656, Loss(train/val) 0.22270/0.15328. Took 0.06 sec\n",
            "Epoch 2657, Loss(train/val) 0.22386/0.15341. Took 0.04 sec\n",
            "Epoch 2658, Loss(train/val) 0.24986/0.15432. Took 0.04 sec\n",
            "Epoch 2659, Loss(train/val) 0.22331/0.15530. Took 0.04 sec\n",
            "Epoch 2660, Loss(train/val) 0.22275/0.15535. Took 0.04 sec\n",
            "Epoch 2661, Loss(train/val) 0.22266/0.15508. Took 0.05 sec\n",
            "Epoch 2662, Loss(train/val) 0.22186/0.15329. Took 0.05 sec\n",
            "Epoch 2663, Loss(train/val) 0.22940/0.15395. Took 0.04 sec\n",
            "Epoch 2664, Loss(train/val) 0.22622/0.15487. Took 0.05 sec\n",
            "Epoch 2665, Loss(train/val) 0.23671/0.15530. Took 0.05 sec\n",
            "Epoch 2666, Loss(train/val) 0.22566/0.15423. Took 0.05 sec\n",
            "Epoch 2667, Loss(train/val) 0.23216/0.15351. Took 0.05 sec\n",
            "Epoch 2668, Loss(train/val) 0.23741/0.15340. Took 0.04 sec\n",
            "Epoch 2669, Loss(train/val) 0.21783/0.15349. Took 0.05 sec\n",
            "Epoch 2670, Loss(train/val) 0.22745/0.15405. Took 0.05 sec\n",
            "Epoch 2671, Loss(train/val) 0.21397/0.15345. Took 0.05 sec\n",
            "Epoch 2672, Loss(train/val) 0.23066/0.15448. Took 0.05 sec\n",
            "Epoch 2673, Loss(train/val) 0.22353/0.15456. Took 0.04 sec\n",
            "Epoch 2674, Loss(train/val) 0.22275/0.15368. Took 0.04 sec\n",
            "Epoch 2675, Loss(train/val) 0.23648/0.15304. Took 0.05 sec\n",
            "Epoch 2676, Loss(train/val) 0.22413/0.15308. Took 0.05 sec\n",
            "Epoch 2677, Loss(train/val) 0.22456/0.15363. Took 0.05 sec\n",
            "Epoch 2678, Loss(train/val) 0.23906/0.15322. Took 0.04 sec\n",
            "Epoch 2679, Loss(train/val) 0.22728/0.15512. Took 0.05 sec\n",
            "Epoch 2680, Loss(train/val) 0.22693/0.15583. Took 0.05 sec\n",
            "Epoch 2681, Loss(train/val) 0.22334/0.15543. Took 0.05 sec\n",
            "Epoch 2682, Loss(train/val) 0.23119/0.15370. Took 0.04 sec\n",
            "Epoch 2683, Loss(train/val) 0.22035/0.15379. Took 0.05 sec\n",
            "Epoch 2684, Loss(train/val) 0.24718/0.15367. Took 0.04 sec\n",
            "Epoch 2685, Loss(train/val) 0.22300/0.15362. Took 0.04 sec\n",
            "Epoch 2686, Loss(train/val) 0.23729/0.15341. Took 0.05 sec\n",
            "Epoch 2687, Loss(train/val) 0.22947/0.15335. Took 0.05 sec\n",
            "Epoch 2688, Loss(train/val) 0.22406/0.15407. Took 0.05 sec\n",
            "Epoch 2689, Loss(train/val) 0.22038/0.15396. Took 0.05 sec\n",
            "Epoch 2690, Loss(train/val) 0.22815/0.15355. Took 0.05 sec\n",
            "Epoch 2691, Loss(train/val) 0.21847/0.15344. Took 0.05 sec\n",
            "Epoch 2692, Loss(train/val) 0.23429/0.15333. Took 0.05 sec\n",
            "Epoch 2693, Loss(train/val) 0.23831/0.15354. Took 0.04 sec\n",
            "Epoch 2694, Loss(train/val) 0.21953/0.15338. Took 0.05 sec\n",
            "Epoch 2695, Loss(train/val) 0.22115/0.15319. Took 0.05 sec\n",
            "Epoch 2696, Loss(train/val) 0.23305/0.15505. Took 0.05 sec\n",
            "Epoch 2697, Loss(train/val) 0.22626/0.15759. Took 0.04 sec\n",
            "Epoch 2698, Loss(train/val) 0.23408/0.15674. Took 0.05 sec\n",
            "Epoch 2699, Loss(train/val) 0.22269/0.15423. Took 0.05 sec\n",
            "Epoch 2700, Loss(train/val) 0.22151/0.15300. Took 0.04 sec\n",
            "Epoch 2701, Loss(train/val) 0.22269/0.15449. Took 0.05 sec\n",
            "Epoch 2702, Loss(train/val) 0.22862/0.15440. Took 0.05 sec\n",
            "Epoch 2703, Loss(train/val) 0.23739/0.15386. Took 0.04 sec\n",
            "Epoch 2704, Loss(train/val) 0.22334/0.15368. Took 0.05 sec\n",
            "Epoch 2705, Loss(train/val) 0.22224/0.15330. Took 0.04 sec\n",
            "Epoch 2706, Loss(train/val) 0.21856/0.15322. Took 0.05 sec\n",
            "Epoch 2707, Loss(train/val) 0.23000/0.15336. Took 0.05 sec\n",
            "Epoch 2708, Loss(train/val) 0.22916/0.15348. Took 0.05 sec\n",
            "Epoch 2709, Loss(train/val) 0.22633/0.15487. Took 0.05 sec\n",
            "Epoch 2710, Loss(train/val) 0.23184/0.15452. Took 0.05 sec\n",
            "Epoch 2711, Loss(train/val) 0.22336/0.15336. Took 0.05 sec\n",
            "Epoch 2712, Loss(train/val) 0.22850/0.15308. Took 0.04 sec\n",
            "Epoch 2713, Loss(train/val) 0.22626/0.15378. Took 0.05 sec\n",
            "Epoch 2714, Loss(train/val) 0.23443/0.15621. Took 0.04 sec\n",
            "Epoch 2715, Loss(train/val) 0.22864/0.15406. Took 0.05 sec\n",
            "Epoch 2716, Loss(train/val) 0.23084/0.15308. Took 0.05 sec\n",
            "Epoch 2717, Loss(train/val) 0.22303/0.15330. Took 0.04 sec\n",
            "Epoch 2718, Loss(train/val) 0.23378/0.15398. Took 0.04 sec\n",
            "Epoch 2719, Loss(train/val) 0.22721/0.15366. Took 0.04 sec\n",
            "Epoch 2720, Loss(train/val) 0.22804/0.15354. Took 0.05 sec\n",
            "Epoch 2721, Loss(train/val) 0.22060/0.15351. Took 0.05 sec\n",
            "Epoch 2722, Loss(train/val) 0.22825/0.15366. Took 0.04 sec\n",
            "Epoch 2723, Loss(train/val) 0.21822/0.15334. Took 0.04 sec\n",
            "Epoch 2724, Loss(train/val) 0.22899/0.15303. Took 0.04 sec\n",
            "Epoch 2725, Loss(train/val) 0.23736/0.15297. Took 0.04 sec\n",
            "Epoch 2726, Loss(train/val) 0.22906/0.15292. Took 0.05 sec\n",
            "Epoch 2727, Loss(train/val) 0.22473/0.15297. Took 0.05 sec\n",
            "Epoch 2728, Loss(train/val) 0.22603/0.15331. Took 0.05 sec\n",
            "Epoch 2729, Loss(train/val) 0.23725/0.15431. Took 0.05 sec\n",
            "Epoch 2730, Loss(train/val) 0.23324/0.15492. Took 0.05 sec\n",
            "Epoch 2731, Loss(train/val) 0.24463/0.15403. Took 0.05 sec\n",
            "Epoch 2732, Loss(train/val) 0.22771/0.15392. Took 0.05 sec\n",
            "Epoch 2733, Loss(train/val) 0.22832/0.15340. Took 0.05 sec\n",
            "Epoch 2734, Loss(train/val) 0.22598/0.15356. Took 0.05 sec\n",
            "Epoch 2735, Loss(train/val) 0.23555/0.15344. Took 0.05 sec\n",
            "Epoch 2736, Loss(train/val) 0.22512/0.15336. Took 0.05 sec\n",
            "Epoch 2737, Loss(train/val) 0.24970/0.15312. Took 0.05 sec\n",
            "Epoch 2738, Loss(train/val) 0.23389/0.15300. Took 0.05 sec\n",
            "Epoch 2739, Loss(train/val) 0.22064/0.15337. Took 0.04 sec\n",
            "Epoch 2740, Loss(train/val) 0.22901/0.15362. Took 0.04 sec\n",
            "Epoch 2741, Loss(train/val) 0.23823/0.15356. Took 0.05 sec\n",
            "Epoch 2742, Loss(train/val) 0.24123/0.15367. Took 0.04 sec\n",
            "Epoch 2743, Loss(train/val) 0.23705/0.15525. Took 0.05 sec\n",
            "Epoch 2744, Loss(train/val) 0.22321/0.15679. Took 0.05 sec\n",
            "Epoch 2745, Loss(train/val) 0.22369/0.15456. Took 0.05 sec\n",
            "Epoch 2746, Loss(train/val) 0.22929/0.15370. Took 0.06 sec\n",
            "Epoch 2747, Loss(train/val) 0.22249/0.15308. Took 0.05 sec\n",
            "Epoch 2748, Loss(train/val) 0.22804/0.15334. Took 0.05 sec\n",
            "Epoch 2749, Loss(train/val) 0.22733/0.15311. Took 0.05 sec\n",
            "Epoch 2750, Loss(train/val) 0.23458/0.15299. Took 0.05 sec\n",
            "Epoch 2751, Loss(train/val) 0.23319/0.15298. Took 0.05 sec\n",
            "Epoch 2752, Loss(train/val) 0.22487/0.15332. Took 0.04 sec\n",
            "Epoch 2753, Loss(train/val) 0.22245/0.15325. Took 0.04 sec\n",
            "Epoch 2754, Loss(train/val) 0.22988/0.15301. Took 0.05 sec\n",
            "Epoch 2755, Loss(train/val) 0.22881/0.15305. Took 0.04 sec\n",
            "Epoch 2756, Loss(train/val) 0.23356/0.15323. Took 0.06 sec\n",
            "Epoch 2757, Loss(train/val) 0.22280/0.15441. Took 0.05 sec\n",
            "Epoch 2758, Loss(train/val) 0.24419/0.15382. Took 0.04 sec\n",
            "Epoch 2759, Loss(train/val) 0.22782/0.15348. Took 0.04 sec\n",
            "Epoch 2760, Loss(train/val) 0.22172/0.15312. Took 0.04 sec\n",
            "Epoch 2761, Loss(train/val) 0.22489/0.15316. Took 0.05 sec\n",
            "Epoch 2762, Loss(train/val) 0.22674/0.15303. Took 0.04 sec\n",
            "Epoch 2763, Loss(train/val) 0.22673/0.15302. Took 0.04 sec\n",
            "Epoch 2764, Loss(train/val) 0.22993/0.15350. Took 0.04 sec\n",
            "Epoch 2765, Loss(train/val) 0.24091/0.15521. Took 0.05 sec\n",
            "Epoch 2766, Loss(train/val) 0.23564/0.15580. Took 0.05 sec\n",
            "Epoch 2767, Loss(train/val) 0.22471/0.15551. Took 0.05 sec\n",
            "Epoch 2768, Loss(train/val) 0.22130/0.15309. Took 0.04 sec\n",
            "Epoch 2769, Loss(train/val) 0.22730/0.15274. Took 0.04 sec\n",
            "Epoch 2770, Loss(train/val) 0.22803/0.15343. Took 0.05 sec\n",
            "Epoch 2771, Loss(train/val) 0.22233/0.15392. Took 0.05 sec\n",
            "Epoch 2772, Loss(train/val) 0.21965/0.15342. Took 0.05 sec\n",
            "Epoch 2773, Loss(train/val) 0.23662/0.15313. Took 0.04 sec\n",
            "Epoch 2774, Loss(train/val) 0.23054/0.15320. Took 0.04 sec\n",
            "Epoch 2775, Loss(train/val) 0.23047/0.15325. Took 0.04 sec\n",
            "Epoch 2776, Loss(train/val) 0.23373/0.15339. Took 0.05 sec\n",
            "Epoch 2777, Loss(train/val) 0.23601/0.15372. Took 0.05 sec\n",
            "Epoch 2778, Loss(train/val) 0.22637/0.15406. Took 0.04 sec\n",
            "Epoch 2779, Loss(train/val) 0.22317/0.15544. Took 0.04 sec\n",
            "Epoch 2780, Loss(train/val) 0.22805/0.15341. Took 0.04 sec\n",
            "Epoch 2781, Loss(train/val) 0.24512/0.15293. Took 0.05 sec\n",
            "Epoch 2782, Loss(train/val) 0.21695/0.15414. Took 0.05 sec\n",
            "Epoch 2783, Loss(train/val) 0.22450/0.15515. Took 0.05 sec\n",
            "Epoch 2784, Loss(train/val) 0.21828/0.15294. Took 0.04 sec\n",
            "Epoch 2785, Loss(train/val) 0.23464/0.15283. Took 0.04 sec\n",
            "Epoch 2786, Loss(train/val) 0.23269/0.15283. Took 0.05 sec\n",
            "Epoch 2787, Loss(train/val) 0.22881/0.15277. Took 0.04 sec\n",
            "Epoch 2788, Loss(train/val) 0.22445/0.15303. Took 0.05 sec\n",
            "Epoch 2789, Loss(train/val) 0.25048/0.15390. Took 0.05 sec\n",
            "Epoch 2790, Loss(train/val) 0.22371/0.15611. Took 0.04 sec\n",
            "Epoch 2791, Loss(train/val) 0.25451/0.15565. Took 0.05 sec\n",
            "Epoch 2792, Loss(train/val) 0.22096/0.15350. Took 0.05 sec\n",
            "Epoch 2793, Loss(train/val) 0.22274/0.15290. Took 0.05 sec\n",
            "Epoch 2794, Loss(train/val) 0.22685/0.15298. Took 0.05 sec\n",
            "Epoch 2795, Loss(train/val) 0.22505/0.15365. Took 0.05 sec\n",
            "Epoch 2796, Loss(train/val) 0.23122/0.15382. Took 0.05 sec\n",
            "Epoch 2797, Loss(train/val) 0.24171/0.15355. Took 0.04 sec\n",
            "Epoch 2798, Loss(train/val) 0.22139/0.15377. Took 0.05 sec\n",
            "Epoch 2799, Loss(train/val) 0.23281/0.15357. Took 0.06 sec\n",
            "Epoch 2800, Loss(train/val) 0.23039/0.15331. Took 0.05 sec\n",
            "Epoch 2801, Loss(train/val) 0.22404/0.15313. Took 0.05 sec\n",
            "Epoch 2802, Loss(train/val) 0.22778/0.15317. Took 0.05 sec\n",
            "Epoch 2803, Loss(train/val) 0.23450/0.15433. Took 0.05 sec\n",
            "Epoch 2804, Loss(train/val) 0.23033/0.15621. Took 0.05 sec\n",
            "Epoch 2805, Loss(train/val) 0.21810/0.15398. Took 0.04 sec\n",
            "Epoch 2806, Loss(train/val) 0.21633/0.15325. Took 0.05 sec\n",
            "Epoch 2807, Loss(train/val) 0.22072/0.15286. Took 0.04 sec\n",
            "Epoch 2808, Loss(train/val) 0.23230/0.15299. Took 0.04 sec\n",
            "Epoch 2809, Loss(train/val) 0.22355/0.15346. Took 0.05 sec\n",
            "Epoch 2810, Loss(train/val) 0.22786/0.15343. Took 0.05 sec\n",
            "Epoch 2811, Loss(train/val) 0.23419/0.15306. Took 0.05 sec\n",
            "Epoch 2812, Loss(train/val) 0.23125/0.15296. Took 0.05 sec\n",
            "Epoch 2813, Loss(train/val) 0.22276/0.15300. Took 0.05 sec\n",
            "Epoch 2814, Loss(train/val) 0.22484/0.15296. Took 0.05 sec\n",
            "Epoch 2815, Loss(train/val) 0.22996/0.15429. Took 0.05 sec\n",
            "Epoch 2816, Loss(train/val) 0.22906/0.15574. Took 0.05 sec\n",
            "Epoch 2817, Loss(train/val) 0.23173/0.15624. Took 0.05 sec\n",
            "Epoch 2818, Loss(train/val) 0.22382/0.15366. Took 0.05 sec\n",
            "Epoch 2819, Loss(train/val) 0.22342/0.15320. Took 0.05 sec\n",
            "Epoch 2820, Loss(train/val) 0.23066/0.15287. Took 0.06 sec\n",
            "Epoch 2821, Loss(train/val) 0.22208/0.15288. Took 0.06 sec\n",
            "Epoch 2822, Loss(train/val) 0.23513/0.15378. Took 0.05 sec\n",
            "Epoch 2823, Loss(train/val) 0.22195/0.15362. Took 0.05 sec\n",
            "Epoch 2824, Loss(train/val) 0.22528/0.15387. Took 0.05 sec\n",
            "Epoch 2825, Loss(train/val) 0.22006/0.15351. Took 0.06 sec\n",
            "Epoch 2826, Loss(train/val) 0.22167/0.15315. Took 0.06 sec\n",
            "Epoch 2827, Loss(train/val) 0.23864/0.15299. Took 0.05 sec\n",
            "Epoch 2828, Loss(train/val) 0.22354/0.15286. Took 0.06 sec\n",
            "Epoch 2829, Loss(train/val) 0.21992/0.15268. Took 0.06 sec\n",
            "Epoch 2830, Loss(train/val) 0.23553/0.15277. Took 0.05 sec\n",
            "Epoch 2831, Loss(train/val) 0.22754/0.15334. Took 0.05 sec\n",
            "Epoch 2832, Loss(train/val) 0.22340/0.15296. Took 0.05 sec\n",
            "Epoch 2833, Loss(train/val) 0.22482/0.15277. Took 0.05 sec\n",
            "Epoch 2834, Loss(train/val) 0.22231/0.15269. Took 0.05 sec\n",
            "Epoch 2835, Loss(train/val) 0.22503/0.15283. Took 0.05 sec\n",
            "Epoch 2836, Loss(train/val) 0.22113/0.15288. Took 0.05 sec\n",
            "Epoch 2837, Loss(train/val) 0.22197/0.15293. Took 0.04 sec\n",
            "Epoch 2838, Loss(train/val) 0.23866/0.15314. Took 0.05 sec\n",
            "Epoch 2839, Loss(train/val) 0.22669/0.15337. Took 0.05 sec\n",
            "Epoch 2840, Loss(train/val) 0.23214/0.15316. Took 0.05 sec\n",
            "Epoch 2841, Loss(train/val) 0.21915/0.15280. Took 0.05 sec\n",
            "Epoch 2842, Loss(train/val) 0.22231/0.15286. Took 0.04 sec\n",
            "Epoch 2843, Loss(train/val) 0.22159/0.15311. Took 0.05 sec\n",
            "Epoch 2844, Loss(train/val) 0.22958/0.15315. Took 0.04 sec\n",
            "Epoch 2845, Loss(train/val) 0.23238/0.15288. Took 0.05 sec\n",
            "Epoch 2846, Loss(train/val) 0.23592/0.15274. Took 0.05 sec\n",
            "Epoch 2847, Loss(train/val) 0.22536/0.15320. Took 0.04 sec\n",
            "Epoch 2848, Loss(train/val) 0.22976/0.15314. Took 0.05 sec\n",
            "Epoch 2849, Loss(train/val) 0.22828/0.15278. Took 0.04 sec\n",
            "Epoch 2850, Loss(train/val) 0.21302/0.15299. Took 0.05 sec\n",
            "Epoch 2851, Loss(train/val) 0.23026/0.15268. Took 0.05 sec\n",
            "Epoch 2852, Loss(train/val) 0.21956/0.15263. Took 0.04 sec\n",
            "Epoch 2853, Loss(train/val) 0.22202/0.15275. Took 0.05 sec\n",
            "Epoch 2854, Loss(train/val) 0.22156/0.15318. Took 0.05 sec\n",
            "Epoch 2855, Loss(train/val) 0.22443/0.15351. Took 0.05 sec\n",
            "Epoch 2856, Loss(train/val) 0.22607/0.15320. Took 0.05 sec\n",
            "Epoch 2857, Loss(train/val) 0.22558/0.15360. Took 0.05 sec\n",
            "Epoch 2858, Loss(train/val) 0.22142/0.15364. Took 0.05 sec\n",
            "Epoch 2859, Loss(train/val) 0.25150/0.15316. Took 0.05 sec\n",
            "Epoch 2860, Loss(train/val) 0.21743/0.15523. Took 0.05 sec\n",
            "Epoch 2861, Loss(train/val) 0.22401/0.15829. Took 0.05 sec\n",
            "Epoch 2862, Loss(train/val) 0.22443/0.15668. Took 0.04 sec\n",
            "Epoch 2863, Loss(train/val) 0.23691/0.15622. Took 0.05 sec\n",
            "Epoch 2864, Loss(train/val) 0.23272/0.15418. Took 0.04 sec\n",
            "Epoch 2865, Loss(train/val) 0.22771/0.15292. Took 0.05 sec\n",
            "Epoch 2866, Loss(train/val) 0.22745/0.15300. Took 0.05 sec\n",
            "Epoch 2867, Loss(train/val) 0.22668/0.15337. Took 0.04 sec\n",
            "Epoch 2868, Loss(train/val) 0.22722/0.15300. Took 0.05 sec\n",
            "Epoch 2869, Loss(train/val) 0.23254/0.15295. Took 0.05 sec\n",
            "Epoch 2870, Loss(train/val) 0.23217/0.15356. Took 0.04 sec\n",
            "Epoch 2871, Loss(train/val) 0.23114/0.15505. Took 0.04 sec\n",
            "Epoch 2872, Loss(train/val) 0.22147/0.15467. Took 0.04 sec\n",
            "Epoch 2873, Loss(train/val) 0.21475/0.15309. Took 0.06 sec\n",
            "Epoch 2874, Loss(train/val) 0.22382/0.15289. Took 0.04 sec\n",
            "Epoch 2875, Loss(train/val) 0.24080/0.15318. Took 0.05 sec\n",
            "Epoch 2876, Loss(train/val) 0.22943/0.15312. Took 0.05 sec\n",
            "Epoch 2877, Loss(train/val) 0.21980/0.15336. Took 0.05 sec\n",
            "Epoch 2878, Loss(train/val) 0.22725/0.15289. Took 0.05 sec\n",
            "Epoch 2879, Loss(train/val) 0.23709/0.15295. Took 0.04 sec\n",
            "Epoch 2880, Loss(train/val) 0.23161/0.15307. Took 0.04 sec\n",
            "Epoch 2881, Loss(train/val) 0.24081/0.15486. Took 0.05 sec\n",
            "Epoch 2882, Loss(train/val) 0.22609/0.15393. Took 0.05 sec\n",
            "Epoch 2883, Loss(train/val) 0.22760/0.15289. Took 0.06 sec\n",
            "Epoch 2884, Loss(train/val) 0.22456/0.15262. Took 0.04 sec\n",
            "Epoch 2885, Loss(train/val) 0.22232/0.15267. Took 0.05 sec\n",
            "Epoch 2886, Loss(train/val) 0.21924/0.15257. Took 0.05 sec\n",
            "Epoch 2887, Loss(train/val) 0.22927/0.15267. Took 0.04 sec\n",
            "Epoch 2888, Loss(train/val) 0.21796/0.15260. Took 0.05 sec\n",
            "Epoch 2889, Loss(train/val) 0.21939/0.15265. Took 0.05 sec\n",
            "Epoch 2890, Loss(train/val) 0.22157/0.15262. Took 0.04 sec\n",
            "Epoch 2891, Loss(train/val) 0.22121/0.15267. Took 0.04 sec\n",
            "Epoch 2892, Loss(train/val) 0.21819/0.15262. Took 0.04 sec\n",
            "Epoch 2893, Loss(train/val) 0.23651/0.15259. Took 0.06 sec\n",
            "Epoch 2894, Loss(train/val) 0.22536/0.15265. Took 0.04 sec\n",
            "Epoch 2895, Loss(train/val) 0.23292/0.15268. Took 0.05 sec\n",
            "Epoch 2896, Loss(train/val) 0.22478/0.15326. Took 0.04 sec\n",
            "Epoch 2897, Loss(train/val) 0.22406/0.15343. Took 0.04 sec\n",
            "Epoch 2898, Loss(train/val) 0.22740/0.15338. Took 0.05 sec\n",
            "Epoch 2899, Loss(train/val) 0.22374/0.15390. Took 0.05 sec\n",
            "Epoch 2900, Loss(train/val) 0.22973/0.15353. Took 0.05 sec\n",
            "Epoch 2901, Loss(train/val) 0.22503/0.15298. Took 0.05 sec\n",
            "Epoch 2902, Loss(train/val) 0.22689/0.15256. Took 0.05 sec\n",
            "Epoch 2903, Loss(train/val) 0.21641/0.15393. Took 0.05 sec\n",
            "Epoch 2904, Loss(train/val) 0.22795/0.15396. Took 0.05 sec\n",
            "Epoch 2905, Loss(train/val) 0.22546/0.15271. Took 0.04 sec\n",
            "Epoch 2906, Loss(train/val) 0.22289/0.15260. Took 0.04 sec\n",
            "Epoch 2907, Loss(train/val) 0.22206/0.15275. Took 0.05 sec\n",
            "Epoch 2908, Loss(train/val) 0.22315/0.15259. Took 0.05 sec\n",
            "Epoch 2909, Loss(train/val) 0.22348/0.15274. Took 0.04 sec\n",
            "Epoch 2910, Loss(train/val) 0.21331/0.15351. Took 0.04 sec\n",
            "Epoch 2911, Loss(train/val) 0.22674/0.15295. Took 0.05 sec\n",
            "Epoch 2912, Loss(train/val) 0.22640/0.15287. Took 0.05 sec\n",
            "Epoch 2913, Loss(train/val) 0.22057/0.15270. Took 0.05 sec\n",
            "Epoch 2914, Loss(train/val) 0.22885/0.15305. Took 0.05 sec\n",
            "Epoch 2915, Loss(train/val) 0.24451/0.15320. Took 0.05 sec\n",
            "Epoch 2916, Loss(train/val) 0.23520/0.15323. Took 0.04 sec\n",
            "Epoch 2917, Loss(train/val) 0.22978/0.15300. Took 0.05 sec\n",
            "Epoch 2918, Loss(train/val) 0.24190/0.15335. Took 0.05 sec\n",
            "Epoch 2919, Loss(train/val) 0.23608/0.15305. Took 0.05 sec\n",
            "Epoch 2920, Loss(train/val) 0.26125/0.15264. Took 0.04 sec\n",
            "Epoch 2921, Loss(train/val) 0.21693/0.15275. Took 0.05 sec\n",
            "Epoch 2922, Loss(train/val) 0.23351/0.15326. Took 0.05 sec\n",
            "Epoch 2923, Loss(train/val) 0.22970/0.15448. Took 0.06 sec\n",
            "Epoch 2924, Loss(train/val) 0.24529/0.15330. Took 0.04 sec\n",
            "Epoch 2925, Loss(train/val) 0.22898/0.15275. Took 0.05 sec\n",
            "Epoch 2926, Loss(train/val) 0.22964/0.15298. Took 0.05 sec\n",
            "Epoch 2927, Loss(train/val) 0.20950/0.15466. Took 0.05 sec\n",
            "Epoch 2928, Loss(train/val) 0.22241/0.15470. Took 0.05 sec\n",
            "Epoch 2929, Loss(train/val) 0.21960/0.15353. Took 0.04 sec\n",
            "Epoch 2930, Loss(train/val) 0.22552/0.15302. Took 0.04 sec\n",
            "Epoch 2931, Loss(train/val) 0.22231/0.15266. Took 0.04 sec\n",
            "Epoch 2932, Loss(train/val) 0.22593/0.15263. Took 0.04 sec\n",
            "Epoch 2933, Loss(train/val) 0.22249/0.15277. Took 0.05 sec\n",
            "Epoch 2934, Loss(train/val) 0.22377/0.15280. Took 0.04 sec\n",
            "Epoch 2935, Loss(train/val) 0.23014/0.15300. Took 0.04 sec\n",
            "Epoch 2936, Loss(train/val) 0.24434/0.15361. Took 0.04 sec\n",
            "Epoch 2937, Loss(train/val) 0.22893/0.15630. Took 0.04 sec\n",
            "Epoch 2938, Loss(train/val) 0.23177/0.15613. Took 0.05 sec\n",
            "Epoch 2939, Loss(train/val) 0.22096/0.15385. Took 0.04 sec\n",
            "Epoch 2940, Loss(train/val) 0.22878/0.15270. Took 0.05 sec\n",
            "Epoch 2941, Loss(train/val) 0.24151/0.15291. Took 0.04 sec\n",
            "Epoch 2942, Loss(train/val) 0.22897/0.15345. Took 0.04 sec\n",
            "Epoch 2943, Loss(train/val) 0.23075/0.15262. Took 0.05 sec\n",
            "Epoch 2944, Loss(train/val) 0.21785/0.15271. Took 0.05 sec\n",
            "Epoch 2945, Loss(train/val) 0.23286/0.15264. Took 0.04 sec\n",
            "Epoch 2946, Loss(train/val) 0.23221/0.15289. Took 0.04 sec\n",
            "Epoch 2947, Loss(train/val) 0.21714/0.15260. Took 0.05 sec\n",
            "Epoch 2948, Loss(train/val) 0.21612/0.15269. Took 0.05 sec\n",
            "Epoch 2949, Loss(train/val) 0.23490/0.15290. Took 0.05 sec\n",
            "Epoch 2950, Loss(train/val) 0.22767/0.15278. Took 0.04 sec\n",
            "Epoch 2951, Loss(train/val) 0.22255/0.15307. Took 0.04 sec\n",
            "Epoch 2952, Loss(train/val) 0.23084/0.15347. Took 0.04 sec\n",
            "Epoch 2953, Loss(train/val) 0.21802/0.15429. Took 0.05 sec\n",
            "Epoch 2954, Loss(train/val) 0.23042/0.15465. Took 0.04 sec\n",
            "Epoch 2955, Loss(train/val) 0.22229/0.15333. Took 0.04 sec\n",
            "Epoch 2956, Loss(train/val) 0.22223/0.15261. Took 0.04 sec\n",
            "Epoch 2957, Loss(train/val) 0.23400/0.15412. Took 0.05 sec\n",
            "Epoch 2958, Loss(train/val) 0.22746/0.15584. Took 0.05 sec\n",
            "Epoch 2959, Loss(train/val) 0.23127/0.15652. Took 0.05 sec\n",
            "Epoch 2960, Loss(train/val) 0.22072/0.15526. Took 0.05 sec\n",
            "Epoch 2961, Loss(train/val) 0.22234/0.15362. Took 0.05 sec\n",
            "Epoch 2962, Loss(train/val) 0.24552/0.15300. Took 0.04 sec\n",
            "Epoch 2963, Loss(train/val) 0.22398/0.15473. Took 0.05 sec\n",
            "Epoch 2964, Loss(train/val) 0.21699/0.15504. Took 0.04 sec\n",
            "Epoch 2965, Loss(train/val) 0.23253/0.15397. Took 0.05 sec\n",
            "Epoch 2966, Loss(train/val) 0.23295/0.15303. Took 0.05 sec\n",
            "Epoch 2967, Loss(train/val) 0.23370/0.15321. Took 0.05 sec\n",
            "Epoch 2968, Loss(train/val) 0.21858/0.15413. Took 0.05 sec\n",
            "Epoch 2969, Loss(train/val) 0.22190/0.15296. Took 0.05 sec\n",
            "Epoch 2970, Loss(train/val) 0.22472/0.15247. Took 0.05 sec\n",
            "Epoch 2971, Loss(train/val) 0.22952/0.15270. Took 0.04 sec\n",
            "Epoch 2972, Loss(train/val) 0.22566/0.15420. Took 0.04 sec\n",
            "Epoch 2973, Loss(train/val) 0.21942/0.15382. Took 0.05 sec\n",
            "Epoch 2974, Loss(train/val) 0.23374/0.15299. Took 0.04 sec\n",
            "Epoch 2975, Loss(train/val) 0.23451/0.15357. Took 0.04 sec\n",
            "Epoch 2976, Loss(train/val) 0.22554/0.15506. Took 0.05 sec\n",
            "Epoch 2977, Loss(train/val) 0.22643/0.15355. Took 0.05 sec\n",
            "Epoch 2978, Loss(train/val) 0.22125/0.15273. Took 0.05 sec\n",
            "Epoch 2979, Loss(train/val) 0.21839/0.15251. Took 0.05 sec\n",
            "Epoch 2980, Loss(train/val) 0.23006/0.15249. Took 0.04 sec\n",
            "Epoch 2981, Loss(train/val) 0.22499/0.15251. Took 0.05 sec\n",
            "Epoch 2982, Loss(train/val) 0.21959/0.15312. Took 0.05 sec\n",
            "Epoch 2983, Loss(train/val) 0.23205/0.15291. Took 0.05 sec\n",
            "Epoch 2984, Loss(train/val) 0.22492/0.15253. Took 0.05 sec\n",
            "Epoch 2985, Loss(train/val) 0.22059/0.15231. Took 0.04 sec\n",
            "Epoch 2986, Loss(train/val) 0.22029/0.15262. Took 0.04 sec\n",
            "Epoch 2987, Loss(train/val) 0.23731/0.15276. Took 0.05 sec\n",
            "Epoch 2988, Loss(train/val) 0.22658/0.15283. Took 0.05 sec\n",
            "Epoch 2989, Loss(train/val) 0.22718/0.15249. Took 0.05 sec\n",
            "Epoch 2990, Loss(train/val) 0.21789/0.15219. Took 0.05 sec\n",
            "Epoch 2991, Loss(train/val) 0.24620/0.15275. Took 0.05 sec\n",
            "Epoch 2992, Loss(train/val) 0.23581/0.15291. Took 0.04 sec\n",
            "Epoch 2993, Loss(train/val) 0.22208/0.15277. Took 0.05 sec\n",
            "Epoch 2994, Loss(train/val) 0.22942/0.15254. Took 0.04 sec\n",
            "Epoch 2995, Loss(train/val) 0.21758/0.15297. Took 0.05 sec\n",
            "Epoch 2996, Loss(train/val) 0.22336/0.15279. Took 0.05 sec\n",
            "Epoch 2997, Loss(train/val) 0.22734/0.15287. Took 0.05 sec\n",
            "Epoch 2998, Loss(train/val) 0.24306/0.15285. Took 0.05 sec\n",
            "Epoch 2999, Loss(train/val) 0.22800/0.15246. Took 0.04 sec\n",
            "Epoch 3000, Loss(train/val) 0.23707/0.15238. Took 0.04 sec\n",
            "Epoch 3001, Loss(train/val) 0.22751/0.15234. Took 0.05 sec\n",
            "Epoch 3002, Loss(train/val) 0.22306/0.15242. Took 0.05 sec\n",
            "Epoch 3003, Loss(train/val) 0.22724/0.15243. Took 0.05 sec\n",
            "Epoch 3004, Loss(train/val) 0.22069/0.15237. Took 0.05 sec\n",
            "Epoch 3005, Loss(train/val) 0.22333/0.15246. Took 0.05 sec\n",
            "Epoch 3006, Loss(train/val) 0.23970/0.15237. Took 0.04 sec\n",
            "Epoch 3007, Loss(train/val) 0.22890/0.15238. Took 0.05 sec\n",
            "Epoch 3008, Loss(train/val) 0.22503/0.15230. Took 0.05 sec\n",
            "Epoch 3009, Loss(train/val) 0.23717/0.15242. Took 0.05 sec\n",
            "Epoch 3010, Loss(train/val) 0.21494/0.15320. Took 0.05 sec\n",
            "Epoch 3011, Loss(train/val) 0.22493/0.15471. Took 0.05 sec\n",
            "Epoch 3012, Loss(train/val) 0.22352/0.15337. Took 0.05 sec\n",
            "Epoch 3013, Loss(train/val) 0.21880/0.15240. Took 0.05 sec\n",
            "Epoch 3014, Loss(train/val) 0.22074/0.15270. Took 0.05 sec\n",
            "Epoch 3015, Loss(train/val) 0.22836/0.15321. Took 0.04 sec\n",
            "Epoch 3016, Loss(train/val) 0.22200/0.15292. Took 0.05 sec\n",
            "Epoch 3017, Loss(train/val) 0.22876/0.15251. Took 0.05 sec\n",
            "Epoch 3018, Loss(train/val) 0.22557/0.15315. Took 0.05 sec\n",
            "Epoch 3019, Loss(train/val) 0.21784/0.15276. Took 0.05 sec\n",
            "Epoch 3020, Loss(train/val) 0.22509/0.15278. Took 0.05 sec\n",
            "Epoch 3021, Loss(train/val) 0.22058/0.15285. Took 0.04 sec\n",
            "Epoch 3022, Loss(train/val) 0.22931/0.15255. Took 0.05 sec\n",
            "Epoch 3023, Loss(train/val) 0.22740/0.15226. Took 0.05 sec\n",
            "Epoch 3024, Loss(train/val) 0.22651/0.15225. Took 0.05 sec\n",
            "Epoch 3025, Loss(train/val) 0.21902/0.15233. Took 0.05 sec\n",
            "Epoch 3026, Loss(train/val) 0.22533/0.15338. Took 0.04 sec\n",
            "Epoch 3027, Loss(train/val) 0.22750/0.15319. Took 0.04 sec\n",
            "Epoch 3028, Loss(train/val) 0.24059/0.15268. Took 0.06 sec\n",
            "Epoch 3029, Loss(train/val) 0.21757/0.15363. Took 0.05 sec\n",
            "Epoch 3030, Loss(train/val) 0.22214/0.15361. Took 0.04 sec\n",
            "Epoch 3031, Loss(train/val) 0.23413/0.15313. Took 0.04 sec\n",
            "Epoch 3032, Loss(train/val) 0.21879/0.15280. Took 0.05 sec\n",
            "Epoch 3033, Loss(train/val) 0.22250/0.15279. Took 0.06 sec\n",
            "Epoch 3034, Loss(train/val) 0.23793/0.15268. Took 0.05 sec\n",
            "Epoch 3035, Loss(train/val) 0.22070/0.15249. Took 0.05 sec\n",
            "Epoch 3036, Loss(train/val) 0.22121/0.15278. Took 0.05 sec\n",
            "Epoch 3037, Loss(train/val) 0.22487/0.15261. Took 0.05 sec\n",
            "Epoch 3038, Loss(train/val) 0.22714/0.15261. Took 0.05 sec\n",
            "Epoch 3039, Loss(train/val) 0.22555/0.15297. Took 0.04 sec\n",
            "Epoch 3040, Loss(train/val) 0.23540/0.15347. Took 0.05 sec\n",
            "Epoch 3041, Loss(train/val) 0.21603/0.15391. Took 0.05 sec\n",
            "Epoch 3042, Loss(train/val) 0.23006/0.15285. Took 0.04 sec\n",
            "Epoch 3043, Loss(train/val) 0.22650/0.15267. Took 0.05 sec\n",
            "Epoch 3044, Loss(train/val) 0.23994/0.15306. Took 0.05 sec\n",
            "Epoch 3045, Loss(train/val) 0.22348/0.15283. Took 0.05 sec\n",
            "Epoch 3046, Loss(train/val) 0.23016/0.15258. Took 0.05 sec\n",
            "Epoch 3047, Loss(train/val) 0.23571/0.15259. Took 0.05 sec\n",
            "Epoch 3048, Loss(train/val) 0.22509/0.15263. Took 0.06 sec\n",
            "Epoch 3049, Loss(train/val) 0.21989/0.15274. Took 0.05 sec\n",
            "Epoch 3050, Loss(train/val) 0.21696/0.15321. Took 0.05 sec\n",
            "Epoch 3051, Loss(train/val) 0.22232/0.15290. Took 0.05 sec\n",
            "Epoch 3052, Loss(train/val) 0.22115/0.15294. Took 0.04 sec\n",
            "Epoch 3053, Loss(train/val) 0.25263/0.15251. Took 0.05 sec\n",
            "Epoch 3054, Loss(train/val) 0.22854/0.15246. Took 0.06 sec\n",
            "Epoch 3055, Loss(train/val) 0.22478/0.15230. Took 0.04 sec\n",
            "Epoch 3056, Loss(train/val) 0.22113/0.15250. Took 0.04 sec\n",
            "Epoch 3057, Loss(train/val) 0.22072/0.15284. Took 0.05 sec\n",
            "Epoch 3058, Loss(train/val) 0.21789/0.15329. Took 0.05 sec\n",
            "Epoch 3059, Loss(train/val) 0.22116/0.15334. Took 0.05 sec\n",
            "Epoch 3060, Loss(train/val) 0.22495/0.15304. Took 0.05 sec\n",
            "Epoch 3061, Loss(train/val) 0.22788/0.15376. Took 0.04 sec\n",
            "Epoch 3062, Loss(train/val) 0.22455/0.15440. Took 0.04 sec\n",
            "Epoch 3063, Loss(train/val) 0.24025/0.15423. Took 0.05 sec\n",
            "Epoch 3064, Loss(train/val) 0.21881/0.15327. Took 0.05 sec\n",
            "Epoch 3065, Loss(train/val) 0.22155/0.15230. Took 0.05 sec\n",
            "Epoch 3066, Loss(train/val) 0.23421/0.15249. Took 0.05 sec\n",
            "Epoch 3067, Loss(train/val) 0.24299/0.15289. Took 0.04 sec\n",
            "Epoch 3068, Loss(train/val) 0.22530/0.15314. Took 0.05 sec\n",
            "Epoch 3069, Loss(train/val) 0.22515/0.15251. Took 0.05 sec\n",
            "Epoch 3070, Loss(train/val) 0.21863/0.15247. Took 0.05 sec\n",
            "Epoch 3071, Loss(train/val) 0.23349/0.15306. Took 0.05 sec\n",
            "Epoch 3072, Loss(train/val) 0.21991/0.15402. Took 0.04 sec\n",
            "Epoch 3073, Loss(train/val) 0.22090/0.15296. Took 0.05 sec\n",
            "Epoch 3074, Loss(train/val) 0.23644/0.15242. Took 0.05 sec\n",
            "Epoch 3075, Loss(train/val) 0.22643/0.15220. Took 0.05 sec\n",
            "Epoch 3076, Loss(train/val) 0.22568/0.15211. Took 0.04 sec\n",
            "Epoch 3077, Loss(train/val) 0.22756/0.15203. Took 0.05 sec\n",
            "Epoch 3078, Loss(train/val) 0.22614/0.15219. Took 0.05 sec\n",
            "Epoch 3079, Loss(train/val) 0.22096/0.15220. Took 0.05 sec\n",
            "Epoch 3080, Loss(train/val) 0.21760/0.15220. Took 0.05 sec\n",
            "Epoch 3081, Loss(train/val) 0.22202/0.15233. Took 0.04 sec\n",
            "Epoch 3082, Loss(train/val) 0.22328/0.15248. Took 0.05 sec\n",
            "Epoch 3083, Loss(train/val) 0.22869/0.15229. Took 0.05 sec\n",
            "Epoch 3084, Loss(train/val) 0.23285/0.15235. Took 0.05 sec\n",
            "Epoch 3085, Loss(train/val) 0.22432/0.15242. Took 0.05 sec\n",
            "Epoch 3086, Loss(train/val) 0.22339/0.15216. Took 0.05 sec\n",
            "Epoch 3087, Loss(train/val) 0.21895/0.15212. Took 0.04 sec\n",
            "Epoch 3088, Loss(train/val) 0.23897/0.15223. Took 0.06 sec\n",
            "Epoch 3089, Loss(train/val) 0.21628/0.15220. Took 0.04 sec\n",
            "Epoch 3090, Loss(train/val) 0.23390/0.15208. Took 0.04 sec\n",
            "Epoch 3091, Loss(train/val) 0.22684/0.15218. Took 0.06 sec\n",
            "Epoch 3092, Loss(train/val) 0.23405/0.15230. Took 0.05 sec\n",
            "Epoch 3093, Loss(train/val) 0.22483/0.15217. Took 0.05 sec\n",
            "Epoch 3094, Loss(train/val) 0.24530/0.15220. Took 0.04 sec\n",
            "Epoch 3095, Loss(train/val) 0.22447/0.15235. Took 0.05 sec\n",
            "Epoch 3096, Loss(train/val) 0.21872/0.15298. Took 0.05 sec\n",
            "Epoch 3097, Loss(train/val) 0.23610/0.15259. Took 0.04 sec\n",
            "Epoch 3098, Loss(train/val) 0.22066/0.15247. Took 0.05 sec\n",
            "Epoch 3099, Loss(train/val) 0.22920/0.15217. Took 0.05 sec\n",
            "Epoch 3100, Loss(train/val) 0.22128/0.15251. Took 0.05 sec\n",
            "Epoch 3101, Loss(train/val) 0.22879/0.15244. Took 0.05 sec\n",
            "Epoch 3102, Loss(train/val) 0.24705/0.15222. Took 0.05 sec\n",
            "Epoch 3103, Loss(train/val) 0.22303/0.15252. Took 0.05 sec\n",
            "Epoch 3104, Loss(train/val) 0.23345/0.15269. Took 0.05 sec\n",
            "Epoch 3105, Loss(train/val) 0.23015/0.15309. Took 0.05 sec\n",
            "Epoch 3106, Loss(train/val) 0.24071/0.15474. Took 0.05 sec\n",
            "Epoch 3107, Loss(train/val) 0.23084/0.15519. Took 0.04 sec\n",
            "Epoch 3108, Loss(train/val) 0.22096/0.15259. Took 0.05 sec\n",
            "Epoch 3109, Loss(train/val) 0.22955/0.15242. Took 0.04 sec\n",
            "Epoch 3110, Loss(train/val) 0.22893/0.15348. Took 0.05 sec\n",
            "Epoch 3111, Loss(train/val) 0.25226/0.15341. Took 0.04 sec\n",
            "Epoch 3112, Loss(train/val) 0.21687/0.15301. Took 0.05 sec\n",
            "Epoch 3113, Loss(train/val) 0.21651/0.15247. Took 0.05 sec\n",
            "Epoch 3114, Loss(train/val) 0.23396/0.15230. Took 0.05 sec\n",
            "Epoch 3115, Loss(train/val) 0.22777/0.15225. Took 0.05 sec\n",
            "Epoch 3116, Loss(train/val) 0.24188/0.15233. Took 0.05 sec\n",
            "Epoch 3117, Loss(train/val) 0.22823/0.15230. Took 0.05 sec\n",
            "Epoch 3118, Loss(train/val) 0.22086/0.15233. Took 0.06 sec\n",
            "Epoch 3119, Loss(train/val) 0.22195/0.15209. Took 0.05 sec\n",
            "Epoch 3120, Loss(train/val) 0.22292/0.15184. Took 0.05 sec\n",
            "Epoch 3121, Loss(train/val) 0.21695/0.15195. Took 0.05 sec\n",
            "Epoch 3122, Loss(train/val) 0.22519/0.15194. Took 0.05 sec\n",
            "Epoch 3123, Loss(train/val) 0.22218/0.15199. Took 0.06 sec\n",
            "Epoch 3124, Loss(train/val) 0.22168/0.15204. Took 0.05 sec\n",
            "Epoch 3125, Loss(train/val) 0.22958/0.15223. Took 0.04 sec\n",
            "Epoch 3126, Loss(train/val) 0.22328/0.15211. Took 0.04 sec\n",
            "Epoch 3127, Loss(train/val) 0.22800/0.15216. Took 0.05 sec\n",
            "Epoch 3128, Loss(train/val) 0.21790/0.15216. Took 0.05 sec\n",
            "Epoch 3129, Loss(train/val) 0.21042/0.15221. Took 0.05 sec\n",
            "Epoch 3130, Loss(train/val) 0.21556/0.15234. Took 0.05 sec\n",
            "Epoch 3131, Loss(train/val) 0.23143/0.15245. Took 0.04 sec\n",
            "Epoch 3132, Loss(train/val) 0.22063/0.15224. Took 0.04 sec\n",
            "Epoch 3133, Loss(train/val) 0.21846/0.15227. Took 0.05 sec\n",
            "Epoch 3134, Loss(train/val) 0.22649/0.15230. Took 0.04 sec\n",
            "Epoch 3135, Loss(train/val) 0.21784/0.15228. Took 0.04 sec\n",
            "Epoch 3136, Loss(train/val) 0.22226/0.15246. Took 0.04 sec\n",
            "Epoch 3137, Loss(train/val) 0.22458/0.15369. Took 0.05 sec\n",
            "Epoch 3138, Loss(train/val) 0.22147/0.15482. Took 0.05 sec\n",
            "Epoch 3139, Loss(train/val) 0.23129/0.15442. Took 0.05 sec\n",
            "Epoch 3140, Loss(train/val) 0.21541/0.15356. Took 0.04 sec\n",
            "Epoch 3141, Loss(train/val) 0.22040/0.15233. Took 0.05 sec\n",
            "Epoch 3142, Loss(train/val) 0.24408/0.15227. Took 0.06 sec\n",
            "Epoch 3143, Loss(train/val) 0.23812/0.15271. Took 0.06 sec\n",
            "Epoch 3144, Loss(train/val) 0.22595/0.15271. Took 0.04 sec\n",
            "Epoch 3145, Loss(train/val) 0.22541/0.15241. Took 0.05 sec\n",
            "Epoch 3146, Loss(train/val) 0.22027/0.15230. Took 0.05 sec\n",
            "Epoch 3147, Loss(train/val) 0.22512/0.15224. Took 0.04 sec\n",
            "Epoch 3148, Loss(train/val) 0.23903/0.15230. Took 0.06 sec\n",
            "Epoch 3149, Loss(train/val) 0.21875/0.15238. Took 0.05 sec\n",
            "Epoch 3150, Loss(train/val) 0.22265/0.15234. Took 0.04 sec\n",
            "Epoch 3151, Loss(train/val) 0.23966/0.15241. Took 0.05 sec\n",
            "Epoch 3152, Loss(train/val) 0.22123/0.15254. Took 0.05 sec\n",
            "Epoch 3153, Loss(train/val) 0.23516/0.15428. Took 0.05 sec\n",
            "Epoch 3154, Loss(train/val) 0.21803/0.15472. Took 0.04 sec\n",
            "Epoch 3155, Loss(train/val) 0.22525/0.15350. Took 0.04 sec\n",
            "Epoch 3156, Loss(train/val) 0.22103/0.15313. Took 0.04 sec\n",
            "Epoch 3157, Loss(train/val) 0.21944/0.15208. Took 0.04 sec\n",
            "Epoch 3158, Loss(train/val) 0.23138/0.15353. Took 0.06 sec\n",
            "Epoch 3159, Loss(train/val) 0.22350/0.15685. Took 0.04 sec\n",
            "Epoch 3160, Loss(train/val) 0.21971/0.15720. Took 0.05 sec\n",
            "Epoch 3161, Loss(train/val) 0.22688/0.15398. Took 0.04 sec\n",
            "Epoch 3162, Loss(train/val) 0.21600/0.15244. Took 0.04 sec\n",
            "Epoch 3163, Loss(train/val) 0.22461/0.15193. Took 0.05 sec\n",
            "Epoch 3164, Loss(train/val) 0.22280/0.15279. Took 0.05 sec\n",
            "Epoch 3165, Loss(train/val) 0.21923/0.15369. Took 0.04 sec\n",
            "Epoch 3166, Loss(train/val) 0.22624/0.15360. Took 0.05 sec\n",
            "Epoch 3167, Loss(train/val) 0.23152/0.15305. Took 0.05 sec\n",
            "Epoch 3168, Loss(train/val) 0.21864/0.15287. Took 0.06 sec\n",
            "Epoch 3169, Loss(train/val) 0.22027/0.15217. Took 0.05 sec\n",
            "Epoch 3170, Loss(train/val) 0.22356/0.15214. Took 0.05 sec\n",
            "Epoch 3171, Loss(train/val) 0.22982/0.15227. Took 0.05 sec\n",
            "Epoch 3172, Loss(train/val) 0.22011/0.15239. Took 0.05 sec\n",
            "Epoch 3173, Loss(train/val) 0.22157/0.15217. Took 0.06 sec\n",
            "Epoch 3174, Loss(train/val) 0.23555/0.15231. Took 0.05 sec\n",
            "Epoch 3175, Loss(train/val) 0.25292/0.15260. Took 0.05 sec\n",
            "Epoch 3176, Loss(train/val) 0.22901/0.15319. Took 0.05 sec\n",
            "Epoch 3177, Loss(train/val) 0.21779/0.15398. Took 0.05 sec\n",
            "Epoch 3178, Loss(train/val) 0.22941/0.15401. Took 0.06 sec\n",
            "Epoch 3179, Loss(train/val) 0.21994/0.15259. Took 0.08 sec\n",
            "Epoch 3180, Loss(train/val) 0.21964/0.15254. Took 0.09 sec\n",
            "Epoch 3181, Loss(train/val) 0.21655/0.15226. Took 0.09 sec\n",
            "Epoch 3182, Loss(train/val) 0.22039/0.15213. Took 0.08 sec\n",
            "Epoch 3183, Loss(train/val) 0.21376/0.15278. Took 0.08 sec\n",
            "Epoch 3184, Loss(train/val) 0.22482/0.15230. Took 0.09 sec\n",
            "Epoch 3185, Loss(train/val) 0.22014/0.15214. Took 0.08 sec\n",
            "Epoch 3186, Loss(train/val) 0.21745/0.15214. Took 0.08 sec\n",
            "Epoch 3187, Loss(train/val) 0.21679/0.15208. Took 0.08 sec\n",
            "Epoch 3188, Loss(train/val) 0.25325/0.15234. Took 0.09 sec\n",
            "Epoch 3189, Loss(train/val) 0.22903/0.15226. Took 0.08 sec\n",
            "Epoch 3190, Loss(train/val) 0.22249/0.15258. Took 0.09 sec\n",
            "Epoch 3191, Loss(train/val) 0.22382/0.15458. Took 0.07 sec\n",
            "Epoch 3192, Loss(train/val) 0.22643/0.15344. Took 0.09 sec\n",
            "Epoch 3193, Loss(train/val) 0.23707/0.15198. Took 0.08 sec\n",
            "Epoch 3194, Loss(train/val) 0.22738/0.15404. Took 0.07 sec\n",
            "Epoch 3195, Loss(train/val) 0.21811/0.15412. Took 0.08 sec\n",
            "Epoch 3196, Loss(train/val) 0.23215/0.15510. Took 0.08 sec\n",
            "Epoch 3197, Loss(train/val) 0.23219/0.15575. Took 0.07 sec\n",
            "Epoch 3198, Loss(train/val) 0.22339/0.15232. Took 0.05 sec\n",
            "Epoch 3199, Loss(train/val) 0.22225/0.15193. Took 0.05 sec\n",
            "Epoch 3200, Loss(train/val) 0.22046/0.15203. Took 0.05 sec\n",
            "Epoch 3201, Loss(train/val) 0.22342/0.15198. Took 0.05 sec\n",
            "Epoch 3202, Loss(train/val) 0.21823/0.15196. Took 0.05 sec\n",
            "Epoch 3203, Loss(train/val) 0.21889/0.15195. Took 0.05 sec\n",
            "Epoch 3204, Loss(train/val) 0.22455/0.15200. Took 0.05 sec\n",
            "Epoch 3205, Loss(train/val) 0.22494/0.15195. Took 0.06 sec\n",
            "Epoch 3206, Loss(train/val) 0.23620/0.15210. Took 0.05 sec\n",
            "Epoch 3207, Loss(train/val) 0.22820/0.15232. Took 0.04 sec\n",
            "Epoch 3208, Loss(train/val) 0.22463/0.15215. Took 0.05 sec\n",
            "Epoch 3209, Loss(train/val) 0.21656/0.15216. Took 0.04 sec\n",
            "Epoch 3210, Loss(train/val) 0.22319/0.15238. Took 0.07 sec\n",
            "Epoch 3211, Loss(train/val) 0.21847/0.15199. Took 0.08 sec\n",
            "Epoch 3212, Loss(train/val) 0.21994/0.15192. Took 0.08 sec\n",
            "Epoch 3213, Loss(train/val) 0.22540/0.15229. Took 0.08 sec\n",
            "Epoch 3214, Loss(train/val) 0.24102/0.15373. Took 0.08 sec\n",
            "Epoch 3215, Loss(train/val) 0.22338/0.15281. Took 0.08 sec\n",
            "Epoch 3216, Loss(train/val) 0.23114/0.15195. Took 0.08 sec\n",
            "Epoch 3217, Loss(train/val) 0.24805/0.15213. Took 0.09 sec\n",
            "Epoch 3218, Loss(train/val) 0.23437/0.15279. Took 0.09 sec\n",
            "Epoch 3219, Loss(train/val) 0.21795/0.15244. Took 0.09 sec\n",
            "Epoch 3220, Loss(train/val) 0.21876/0.15223. Took 0.09 sec\n",
            "Epoch 3221, Loss(train/val) 0.21657/0.15186. Took 0.08 sec\n",
            "Epoch 3222, Loss(train/val) 0.22881/0.15189. Took 0.10 sec\n",
            "Epoch 3223, Loss(train/val) 0.22826/0.15245. Took 0.08 sec\n",
            "Epoch 3224, Loss(train/val) 0.22422/0.15363. Took 0.08 sec\n",
            "Epoch 3225, Loss(train/val) 0.23500/0.15456. Took 0.08 sec\n",
            "Epoch 3226, Loss(train/val) 0.22631/0.15348. Took 0.07 sec\n",
            "Epoch 3227, Loss(train/val) 0.22306/0.15297. Took 0.07 sec\n",
            "Epoch 3228, Loss(train/val) 0.21818/0.15198. Took 0.14 sec\n",
            "Epoch 3229, Loss(train/val) 0.21599/0.15195. Took 0.18 sec\n",
            "Epoch 3230, Loss(train/val) 0.21894/0.15189. Took 0.20 sec\n",
            "Epoch 3231, Loss(train/val) 0.22479/0.15215. Took 0.16 sec\n",
            "Epoch 3232, Loss(train/val) 0.22114/0.15191. Took 0.12 sec\n",
            "Epoch 3233, Loss(train/val) 0.22985/0.15200. Took 0.06 sec\n",
            "Epoch 3234, Loss(train/val) 0.22392/0.15199. Took 0.04 sec\n",
            "Epoch 3235, Loss(train/val) 0.22288/0.15193. Took 0.05 sec\n",
            "Epoch 3236, Loss(train/val) 0.22057/0.15221. Took 0.05 sec\n",
            "Epoch 3237, Loss(train/val) 0.22477/0.15208. Took 0.05 sec\n",
            "Epoch 3238, Loss(train/val) 0.22994/0.15232. Took 0.04 sec\n",
            "Epoch 3239, Loss(train/val) 0.22362/0.15235. Took 0.05 sec\n",
            "Epoch 3240, Loss(train/val) 0.22923/0.15305. Took 0.04 sec\n",
            "Epoch 3241, Loss(train/val) 0.22824/0.15419. Took 0.05 sec\n",
            "Epoch 3242, Loss(train/val) 0.22783/0.15556. Took 0.06 sec\n",
            "Epoch 3243, Loss(train/val) 0.22441/0.15487. Took 0.05 sec\n",
            "Epoch 3244, Loss(train/val) 0.23429/0.15517. Took 0.04 sec\n",
            "Epoch 3245, Loss(train/val) 0.22689/0.15310. Took 0.04 sec\n",
            "Epoch 3246, Loss(train/val) 0.21347/0.15180. Took 0.05 sec\n",
            "Epoch 3247, Loss(train/val) 0.22547/0.15273. Took 0.05 sec\n",
            "Epoch 3248, Loss(train/val) 0.22300/0.15340. Took 0.05 sec\n",
            "Epoch 3249, Loss(train/val) 0.23564/0.15368. Took 0.05 sec\n",
            "Epoch 3250, Loss(train/val) 0.22949/0.15355. Took 0.04 sec\n",
            "Epoch 3251, Loss(train/val) 0.23019/0.15223. Took 0.05 sec\n",
            "Epoch 3252, Loss(train/val) 0.21974/0.15188. Took 0.04 sec\n",
            "Epoch 3253, Loss(train/val) 0.22874/0.15190. Took 0.05 sec\n",
            "Epoch 3254, Loss(train/val) 0.21730/0.15223. Took 0.05 sec\n",
            "Epoch 3255, Loss(train/val) 0.21822/0.15273. Took 0.04 sec\n",
            "Epoch 3256, Loss(train/val) 0.21501/0.15263. Took 0.05 sec\n",
            "Epoch 3257, Loss(train/val) 0.23152/0.15231. Took 0.05 sec\n",
            "Epoch 3258, Loss(train/val) 0.22083/0.15275. Took 0.05 sec\n",
            "Epoch 3259, Loss(train/val) 0.22703/0.15215. Took 0.04 sec\n",
            "Epoch 3260, Loss(train/val) 0.22042/0.15181. Took 0.05 sec\n",
            "Epoch 3261, Loss(train/val) 0.22736/0.15188. Took 0.05 sec\n",
            "Epoch 3262, Loss(train/val) 0.23659/0.15189. Took 0.05 sec\n",
            "Epoch 3263, Loss(train/val) 0.22396/0.15215. Took 0.04 sec\n",
            "Epoch 3264, Loss(train/val) 0.21948/0.15204. Took 0.05 sec\n",
            "Epoch 3265, Loss(train/val) 0.21254/0.15246. Took 0.05 sec\n",
            "Epoch 3266, Loss(train/val) 0.24346/0.15237. Took 0.06 sec\n",
            "Epoch 3267, Loss(train/val) 0.21643/0.15256. Took 0.11 sec\n",
            "Epoch 3268, Loss(train/val) 0.21993/0.15233. Took 0.09 sec\n",
            "Epoch 3269, Loss(train/val) 0.23672/0.15203. Took 0.07 sec\n",
            "Epoch 3270, Loss(train/val) 0.21673/0.15203. Took 0.10 sec\n",
            "Epoch 3271, Loss(train/val) 0.22066/0.15271. Took 0.12 sec\n",
            "Epoch 3272, Loss(train/val) 0.22456/0.15288. Took 0.12 sec\n",
            "Epoch 3273, Loss(train/val) 0.22007/0.15277. Took 0.13 sec\n",
            "Epoch 3274, Loss(train/val) 0.21998/0.15182. Took 0.09 sec\n",
            "Epoch 3275, Loss(train/val) 0.22563/0.15288. Took 0.09 sec\n",
            "Epoch 3276, Loss(train/val) 0.22583/0.15436. Took 0.10 sec\n",
            "Epoch 3277, Loss(train/val) 0.21974/0.15411. Took 0.09 sec\n",
            "Epoch 3278, Loss(train/val) 0.22753/0.15306. Took 0.16 sec\n",
            "Epoch 3279, Loss(train/val) 0.21808/0.15257. Took 0.11 sec\n",
            "Epoch 3280, Loss(train/val) 0.22178/0.15222. Took 0.21 sec\n",
            "Epoch 3281, Loss(train/val) 0.22405/0.15206. Took 0.15 sec\n",
            "Epoch 3282, Loss(train/val) 0.22661/0.15177. Took 0.11 sec\n",
            "Epoch 3283, Loss(train/val) 0.22090/0.15198. Took 0.08 sec\n",
            "Epoch 3284, Loss(train/val) 0.22051/0.15254. Took 0.04 sec\n",
            "Epoch 3285, Loss(train/val) 0.23140/0.15277. Took 0.05 sec\n",
            "Epoch 3286, Loss(train/val) 0.23658/0.15303. Took 0.06 sec\n",
            "Epoch 3287, Loss(train/val) 0.22517/0.15263. Took 0.05 sec\n",
            "Epoch 3288, Loss(train/val) 0.22649/0.15179. Took 0.05 sec\n",
            "Epoch 3289, Loss(train/val) 0.22568/0.15208. Took 0.05 sec\n",
            "Epoch 3290, Loss(train/val) 0.22697/0.15173. Took 0.04 sec\n",
            "Epoch 3291, Loss(train/val) 0.21940/0.15193. Took 0.05 sec\n",
            "Epoch 3292, Loss(train/val) 0.22532/0.15240. Took 0.05 sec\n",
            "Epoch 3293, Loss(train/val) 0.22364/0.15242. Took 0.05 sec\n",
            "Epoch 3294, Loss(train/val) 0.22761/0.15267. Took 0.05 sec\n",
            "Epoch 3295, Loss(train/val) 0.22017/0.15226. Took 0.04 sec\n",
            "Epoch 3296, Loss(train/val) 0.21724/0.15186. Took 0.05 sec\n",
            "Epoch 3297, Loss(train/val) 0.21896/0.15177. Took 0.04 sec\n",
            "Epoch 3298, Loss(train/val) 0.21765/0.15242. Took 0.05 sec\n",
            "Epoch 3299, Loss(train/val) 0.22143/0.15271. Took 0.05 sec\n",
            "Epoch 3300, Loss(train/val) 0.23535/0.15288. Took 0.04 sec\n",
            "Epoch 3301, Loss(train/val) 0.21670/0.15271. Took 0.05 sec\n",
            "Epoch 3302, Loss(train/val) 0.23090/0.15184. Took 0.05 sec\n",
            "Epoch 3303, Loss(train/val) 0.22202/0.15346. Took 0.05 sec\n",
            "Epoch 3304, Loss(train/val) 0.22216/0.15348. Took 0.04 sec\n",
            "Epoch 3305, Loss(train/val) 0.21894/0.15227. Took 0.04 sec\n",
            "Epoch 3306, Loss(train/val) 0.21697/0.15170. Took 0.05 sec\n",
            "Epoch 3307, Loss(train/val) 0.21764/0.15299. Took 0.05 sec\n",
            "Epoch 3308, Loss(train/val) 0.22800/0.15337. Took 0.05 sec\n",
            "Epoch 3309, Loss(train/val) 0.22033/0.15280. Took 0.04 sec\n",
            "Epoch 3310, Loss(train/val) 0.23301/0.15193. Took 0.05 sec\n",
            "Epoch 3311, Loss(train/val) 0.23758/0.15268. Took 0.05 sec\n",
            "Epoch 3312, Loss(train/val) 0.22060/0.15380. Took 0.05 sec\n",
            "Epoch 3313, Loss(train/val) 0.21687/0.15210. Took 0.05 sec\n",
            "Epoch 3314, Loss(train/val) 0.22225/0.15179. Took 0.04 sec\n",
            "Epoch 3315, Loss(train/val) 0.22553/0.15195. Took 0.04 sec\n",
            "Epoch 3316, Loss(train/val) 0.24029/0.15248. Took 0.05 sec\n",
            "Epoch 3317, Loss(train/val) 0.22993/0.15235. Took 0.04 sec\n",
            "Epoch 3318, Loss(train/val) 0.23052/0.15203. Took 0.04 sec\n",
            "Epoch 3319, Loss(train/val) 0.22709/0.15244. Took 0.05 sec\n",
            "Epoch 3320, Loss(train/val) 0.23030/0.15260. Took 0.04 sec\n",
            "Epoch 3321, Loss(train/val) 0.22356/0.15198. Took 0.05 sec\n",
            "Epoch 3322, Loss(train/val) 0.23561/0.15172. Took 0.04 sec\n",
            "Epoch 3323, Loss(train/val) 0.22422/0.15204. Took 0.05 sec\n",
            "Epoch 3324, Loss(train/val) 0.21433/0.15181. Took 0.05 sec\n",
            "Epoch 3325, Loss(train/val) 0.23059/0.15170. Took 0.04 sec\n",
            "Epoch 3326, Loss(train/val) 0.22076/0.15166. Took 0.05 sec\n",
            "Epoch 3327, Loss(train/val) 0.23131/0.15198. Took 0.05 sec\n",
            "Epoch 3328, Loss(train/val) 0.21335/0.15217. Took 0.04 sec\n",
            "Epoch 3329, Loss(train/val) 0.22449/0.15259. Took 0.04 sec\n",
            "Epoch 3330, Loss(train/val) 0.22356/0.15198. Took 0.04 sec\n",
            "Epoch 3331, Loss(train/val) 0.22651/0.15164. Took 0.06 sec\n",
            "Epoch 3332, Loss(train/val) 0.22253/0.15308. Took 0.05 sec\n",
            "Epoch 3333, Loss(train/val) 0.22596/0.15325. Took 0.04 sec\n",
            "Epoch 3334, Loss(train/val) 0.21810/0.15334. Took 0.04 sec\n",
            "Epoch 3335, Loss(train/val) 0.22146/0.15231. Took 0.04 sec\n",
            "Epoch 3336, Loss(train/val) 0.24172/0.15247. Took 0.05 sec\n",
            "Epoch 3337, Loss(train/val) 0.22884/0.15338. Took 0.04 sec\n",
            "Epoch 3338, Loss(train/val) 0.23513/0.15364. Took 0.04 sec\n",
            "Epoch 3339, Loss(train/val) 0.21964/0.15301. Took 0.05 sec\n",
            "Epoch 3340, Loss(train/val) 0.23836/0.15260. Took 0.05 sec\n",
            "Epoch 3341, Loss(train/val) 0.21712/0.15188. Took 0.05 sec\n",
            "Epoch 3342, Loss(train/val) 0.22196/0.15206. Took 0.05 sec\n",
            "Epoch 3343, Loss(train/val) 0.22224/0.15218. Took 0.04 sec\n",
            "Epoch 3344, Loss(train/val) 0.23241/0.15198. Took 0.04 sec\n",
            "Epoch 3345, Loss(train/val) 0.22668/0.15207. Took 0.05 sec\n",
            "Epoch 3346, Loss(train/val) 0.22664/0.15180. Took 0.05 sec\n",
            "Epoch 3347, Loss(train/val) 0.22011/0.15176. Took 0.04 sec\n",
            "Epoch 3348, Loss(train/val) 0.21706/0.15177. Took 0.05 sec\n",
            "Epoch 3349, Loss(train/val) 0.22372/0.15200. Took 0.05 sec\n",
            "Epoch 3350, Loss(train/val) 0.21905/0.15404. Took 0.04 sec\n",
            "Epoch 3351, Loss(train/val) 0.22433/0.15581. Took 0.06 sec\n",
            "Epoch 3352, Loss(train/val) 0.22268/0.15652. Took 0.05 sec\n",
            "Epoch 3353, Loss(train/val) 0.22161/0.15288. Took 0.05 sec\n",
            "Epoch 3354, Loss(train/val) 0.22272/0.15146. Took 0.04 sec\n",
            "Epoch 3355, Loss(train/val) 0.22753/0.15225. Took 0.04 sec\n",
            "Epoch 3356, Loss(train/val) 0.22469/0.15367. Took 0.05 sec\n",
            "Epoch 3357, Loss(train/val) 0.21888/0.15642. Took 0.05 sec\n",
            "Epoch 3358, Loss(train/val) 0.21815/0.15515. Took 0.05 sec\n",
            "Epoch 3359, Loss(train/val) 0.21640/0.15278. Took 0.05 sec\n",
            "Epoch 3360, Loss(train/val) 0.21704/0.15175. Took 0.05 sec\n",
            "Epoch 3361, Loss(train/val) 0.22490/0.15374. Took 0.05 sec\n",
            "Epoch 3362, Loss(train/val) 0.21913/0.15411. Took 0.04 sec\n",
            "Epoch 3363, Loss(train/val) 0.22294/0.15443. Took 0.05 sec\n",
            "Epoch 3364, Loss(train/val) 0.22697/0.15340. Took 0.04 sec\n",
            "Epoch 3365, Loss(train/val) 0.21606/0.15245. Took 0.04 sec\n",
            "Epoch 3366, Loss(train/val) 0.22385/0.15176. Took 0.05 sec\n",
            "Epoch 3367, Loss(train/val) 0.21626/0.15188. Took 0.05 sec\n",
            "Epoch 3368, Loss(train/val) 0.22230/0.15179. Took 0.04 sec\n",
            "Epoch 3369, Loss(train/val) 0.23349/0.15194. Took 0.05 sec\n",
            "Epoch 3370, Loss(train/val) 0.23513/0.15190. Took 0.04 sec\n",
            "Epoch 3371, Loss(train/val) 0.23122/0.15308. Took 0.06 sec\n",
            "Epoch 3372, Loss(train/val) 0.23827/0.15296. Took 0.05 sec\n",
            "Epoch 3373, Loss(train/val) 0.22503/0.15479. Took 0.05 sec\n",
            "Epoch 3374, Loss(train/val) 0.21910/0.15364. Took 0.05 sec\n",
            "Epoch 3375, Loss(train/val) 0.24282/0.15241. Took 0.05 sec\n",
            "Epoch 3376, Loss(train/val) 0.22247/0.15158. Took 0.06 sec\n",
            "Epoch 3377, Loss(train/val) 0.22123/0.15165. Took 0.05 sec\n",
            "Epoch 3378, Loss(train/val) 0.24757/0.15180. Took 0.04 sec\n",
            "Epoch 3379, Loss(train/val) 0.22795/0.15232. Took 0.05 sec\n",
            "Epoch 3380, Loss(train/val) 0.21112/0.15208. Took 0.04 sec\n",
            "Epoch 3381, Loss(train/val) 0.22009/0.15179. Took 0.05 sec\n",
            "Epoch 3382, Loss(train/val) 0.22258/0.15216. Took 0.05 sec\n",
            "Epoch 3383, Loss(train/val) 0.22171/0.15215. Took 0.05 sec\n",
            "Epoch 3384, Loss(train/val) 0.21828/0.15177. Took 0.05 sec\n",
            "Epoch 3385, Loss(train/val) 0.22285/0.15155. Took 0.05 sec\n",
            "Epoch 3386, Loss(train/val) 0.22043/0.15151. Took 0.06 sec\n",
            "Epoch 3387, Loss(train/val) 0.21878/0.15152. Took 0.05 sec\n",
            "Epoch 3388, Loss(train/val) 0.24028/0.15165. Took 0.07 sec\n",
            "Epoch 3389, Loss(train/val) 0.21516/0.15146. Took 0.07 sec\n",
            "Epoch 3390, Loss(train/val) 0.22014/0.15210. Took 0.08 sec\n",
            "Epoch 3391, Loss(train/val) 0.22508/0.15164. Took 0.07 sec\n",
            "Epoch 3392, Loss(train/val) 0.23624/0.15165. Took 0.07 sec\n",
            "Epoch 3393, Loss(train/val) 0.23134/0.15157. Took 0.08 sec\n",
            "Epoch 3394, Loss(train/val) 0.22218/0.15156. Took 0.07 sec\n",
            "Epoch 3395, Loss(train/val) 0.22406/0.15150. Took 0.07 sec\n",
            "Epoch 3396, Loss(train/val) 0.23325/0.15185. Took 0.07 sec\n",
            "Epoch 3397, Loss(train/val) 0.21032/0.15190. Took 0.07 sec\n",
            "Epoch 3398, Loss(train/val) 0.22690/0.15204. Took 0.07 sec\n",
            "Epoch 3399, Loss(train/val) 0.22865/0.15197. Took 0.08 sec\n",
            "Epoch 3400, Loss(train/val) 0.22304/0.15185. Took 0.08 sec\n",
            "Epoch 3401, Loss(train/val) 0.21567/0.15176. Took 0.07 sec\n",
            "Epoch 3402, Loss(train/val) 0.22792/0.15202. Took 0.08 sec\n",
            "Epoch 3403, Loss(train/val) 0.21859/0.15182. Took 0.07 sec\n",
            "Epoch 3404, Loss(train/val) 0.22349/0.15258. Took 0.07 sec\n",
            "Epoch 3405, Loss(train/val) 0.22321/0.15196. Took 0.08 sec\n",
            "Epoch 3406, Loss(train/val) 0.24614/0.15182. Took 0.08 sec\n",
            "Epoch 3407, Loss(train/val) 0.22066/0.15295. Took 0.07 sec\n",
            "Epoch 3408, Loss(train/val) 0.21977/0.15268. Took 0.08 sec\n",
            "Epoch 3409, Loss(train/val) 0.22806/0.15200. Took 0.07 sec\n",
            "Epoch 3410, Loss(train/val) 0.21979/0.15151. Took 0.07 sec\n",
            "Epoch 3411, Loss(train/val) 0.21795/0.15151. Took 0.08 sec\n",
            "Epoch 3412, Loss(train/val) 0.23632/0.15150. Took 0.08 sec\n",
            "Epoch 3413, Loss(train/val) 0.22392/0.15148. Took 0.07 sec\n",
            "Epoch 3414, Loss(train/val) 0.22982/0.15155. Took 0.08 sec\n",
            "Epoch 3415, Loss(train/val) 0.22138/0.15154. Took 0.07 sec\n",
            "Epoch 3416, Loss(train/val) 0.21634/0.15154. Took 0.07 sec\n",
            "Epoch 3417, Loss(train/val) 0.21660/0.15171. Took 0.08 sec\n",
            "Epoch 3418, Loss(train/val) 0.23373/0.15167. Took 0.06 sec\n",
            "Epoch 3419, Loss(train/val) 0.22416/0.15165. Took 0.05 sec\n",
            "Epoch 3420, Loss(train/val) 0.21338/0.15191. Took 0.05 sec\n",
            "Epoch 3421, Loss(train/val) 0.22453/0.15380. Took 0.05 sec\n",
            "Epoch 3422, Loss(train/val) 0.23452/0.15533. Took 0.06 sec\n",
            "Epoch 3423, Loss(train/val) 0.23803/0.15406. Took 0.05 sec\n",
            "Epoch 3424, Loss(train/val) 0.23214/0.15187. Took 0.05 sec\n",
            "Epoch 3425, Loss(train/val) 0.21934/0.15151. Took 0.05 sec\n",
            "Epoch 3426, Loss(train/val) 0.21279/0.15166. Took 0.06 sec\n",
            "Epoch 3427, Loss(train/val) 0.23274/0.15161. Took 0.05 sec\n",
            "Epoch 3428, Loss(train/val) 0.21768/0.15160. Took 0.04 sec\n",
            "Epoch 3429, Loss(train/val) 0.21707/0.15191. Took 0.04 sec\n",
            "Epoch 3430, Loss(train/val) 0.22075/0.15196. Took 0.05 sec\n",
            "Epoch 3431, Loss(train/val) 0.21921/0.15183. Took 0.05 sec\n",
            "Epoch 3432, Loss(train/val) 0.21829/0.15160. Took 0.05 sec\n",
            "Epoch 3433, Loss(train/val) 0.23118/0.15152. Took 0.04 sec\n",
            "Epoch 3434, Loss(train/val) 0.21786/0.15145. Took 0.04 sec\n",
            "Epoch 3435, Loss(train/val) 0.21951/0.15279. Took 0.04 sec\n",
            "Epoch 3436, Loss(train/val) 0.21881/0.15255. Took 0.05 sec\n",
            "Epoch 3437, Loss(train/val) 0.21815/0.15290. Took 0.05 sec\n",
            "Epoch 3438, Loss(train/val) 0.22830/0.15195. Took 0.05 sec\n",
            "Epoch 3439, Loss(train/val) 0.21238/0.15145. Took 0.04 sec\n",
            "Epoch 3440, Loss(train/val) 0.22177/0.15136. Took 0.05 sec\n",
            "Epoch 3441, Loss(train/val) 0.21324/0.15156. Took 0.05 sec\n",
            "Epoch 3442, Loss(train/val) 0.22087/0.15170. Took 0.05 sec\n",
            "Epoch 3443, Loss(train/val) 0.22782/0.15149. Took 0.05 sec\n",
            "Epoch 3444, Loss(train/val) 0.22183/0.15186. Took 0.05 sec\n",
            "Epoch 3445, Loss(train/val) 0.22711/0.15169. Took 0.05 sec\n",
            "Epoch 3446, Loss(train/val) 0.23316/0.15295. Took 0.05 sec\n",
            "Epoch 3447, Loss(train/val) 0.22230/0.15348. Took 0.05 sec\n",
            "Epoch 3448, Loss(train/val) 0.22129/0.15234. Took 0.05 sec\n",
            "Epoch 3449, Loss(train/val) 0.21916/0.15196. Took 0.05 sec\n",
            "Epoch 3450, Loss(train/val) 0.22998/0.15179. Took 0.04 sec\n",
            "Epoch 3451, Loss(train/val) 0.22991/0.15161. Took 0.05 sec\n",
            "Epoch 3452, Loss(train/val) 0.21933/0.15145. Took 0.05 sec\n",
            "Epoch 3453, Loss(train/val) 0.22191/0.15275. Took 0.05 sec\n",
            "Epoch 3454, Loss(train/val) 0.21005/0.15486. Took 0.05 sec\n",
            "Epoch 3455, Loss(train/val) 0.21718/0.15327. Took 0.05 sec\n",
            "Epoch 3456, Loss(train/val) 0.22659/0.15188. Took 0.05 sec\n",
            "Epoch 3457, Loss(train/val) 0.22295/0.15161. Took 0.05 sec\n",
            "Epoch 3458, Loss(train/val) 0.21995/0.15163. Took 0.05 sec\n",
            "Epoch 3459, Loss(train/val) 0.23978/0.15166. Took 0.04 sec\n",
            "Epoch 3460, Loss(train/val) 0.23539/0.15158. Took 0.05 sec\n",
            "Epoch 3461, Loss(train/val) 0.21964/0.15163. Took 0.05 sec\n",
            "Epoch 3462, Loss(train/val) 0.21251/0.15231. Took 0.04 sec\n",
            "Epoch 3463, Loss(train/val) 0.22652/0.15348. Took 0.04 sec\n",
            "Epoch 3464, Loss(train/val) 0.23815/0.15391. Took 0.05 sec\n",
            "Epoch 3465, Loss(train/val) 0.23329/0.15348. Took 0.04 sec\n",
            "Epoch 3466, Loss(train/val) 0.21522/0.15273. Took 0.05 sec\n",
            "Epoch 3467, Loss(train/val) 0.23415/0.15173. Took 0.04 sec\n",
            "Epoch 3468, Loss(train/val) 0.22480/0.15174. Took 0.04 sec\n",
            "Epoch 3469, Loss(train/val) 0.21925/0.15175. Took 0.05 sec\n",
            "Epoch 3470, Loss(train/val) 0.22726/0.15173. Took 0.04 sec\n",
            "Epoch 3471, Loss(train/val) 0.21686/0.15179. Took 0.05 sec\n",
            "Epoch 3472, Loss(train/val) 0.22341/0.15213. Took 0.04 sec\n",
            "Epoch 3473, Loss(train/val) 0.22998/0.15357. Took 0.04 sec\n",
            "Epoch 3474, Loss(train/val) 0.22498/0.15177. Took 0.05 sec\n",
            "Epoch 3475, Loss(train/val) 0.21876/0.15146. Took 0.05 sec\n",
            "Epoch 3476, Loss(train/val) 0.22761/0.15148. Took 0.05 sec\n",
            "Epoch 3477, Loss(train/val) 0.23601/0.15159. Took 0.04 sec\n",
            "Epoch 3478, Loss(train/val) 0.22320/0.15187. Took 0.04 sec\n",
            "Epoch 3479, Loss(train/val) 0.24149/0.15192. Took 0.04 sec\n",
            "Epoch 3480, Loss(train/val) 0.22401/0.15149. Took 0.05 sec\n",
            "Epoch 3481, Loss(train/val) 0.22526/0.15142. Took 0.05 sec\n",
            "Epoch 3482, Loss(train/val) 0.22868/0.15140. Took 0.04 sec\n",
            "Epoch 3483, Loss(train/val) 0.23324/0.15152. Took 0.04 sec\n",
            "Epoch 3484, Loss(train/val) 0.22003/0.15140. Took 0.04 sec\n",
            "Epoch 3485, Loss(train/val) 0.22522/0.15147. Took 0.05 sec\n",
            "Epoch 3486, Loss(train/val) 0.22052/0.15201. Took 0.07 sec\n",
            "Epoch 3487, Loss(train/val) 0.22407/0.15186. Took 0.05 sec\n",
            "Epoch 3488, Loss(train/val) 0.23156/0.15159. Took 0.05 sec\n",
            "Epoch 3489, Loss(train/val) 0.22553/0.15195. Took 0.05 sec\n",
            "Epoch 3490, Loss(train/val) 0.23150/0.15383. Took 0.05 sec\n",
            "Epoch 3491, Loss(train/val) 0.22740/0.15332. Took 0.05 sec\n",
            "Epoch 3492, Loss(train/val) 0.22140/0.15287. Took 0.05 sec\n",
            "Epoch 3493, Loss(train/val) 0.22253/0.15188. Took 0.04 sec\n",
            "Epoch 3494, Loss(train/val) 0.23562/0.15153. Took 0.05 sec\n",
            "Epoch 3495, Loss(train/val) 0.23085/0.15143. Took 0.05 sec\n",
            "Epoch 3496, Loss(train/val) 0.23293/0.15200. Took 0.05 sec\n",
            "Epoch 3497, Loss(train/val) 0.22708/0.15288. Took 0.05 sec\n",
            "Epoch 3498, Loss(train/val) 0.23060/0.15411. Took 0.05 sec\n",
            "Epoch 3499, Loss(train/val) 0.23062/0.15269. Took 0.05 sec\n",
            "Epoch 3500, Loss(train/val) 0.21610/0.15166. Took 0.05 sec\n",
            "Epoch 3501, Loss(train/val) 0.21484/0.15150. Took 0.05 sec\n",
            "Epoch 3502, Loss(train/val) 0.22602/0.15193. Took 0.05 sec\n",
            "Epoch 3503, Loss(train/val) 0.21754/0.15173. Took 0.05 sec\n",
            "Epoch 3504, Loss(train/val) 0.22484/0.15156. Took 0.04 sec\n",
            "Epoch 3505, Loss(train/val) 0.21560/0.15148. Took 0.05 sec\n",
            "Epoch 3506, Loss(train/val) 0.24233/0.15137. Took 0.05 sec\n",
            "Epoch 3507, Loss(train/val) 0.21672/0.15137. Took 0.05 sec\n",
            "Epoch 3508, Loss(train/val) 0.22124/0.15178. Took 0.05 sec\n",
            "Epoch 3509, Loss(train/val) 0.23748/0.15225. Took 0.05 sec\n",
            "Epoch 3510, Loss(train/val) 0.21799/0.15331. Took 0.05 sec\n",
            "Epoch 3511, Loss(train/val) 0.22796/0.15378. Took 0.05 sec\n",
            "Epoch 3512, Loss(train/val) 0.21880/0.15253. Took 0.04 sec\n",
            "Epoch 3513, Loss(train/val) 0.21088/0.15178. Took 0.06 sec\n",
            "Epoch 3514, Loss(train/val) 0.22945/0.15154. Took 0.05 sec\n",
            "Epoch 3515, Loss(train/val) 0.21870/0.15220. Took 0.04 sec\n",
            "Epoch 3516, Loss(train/val) 0.22041/0.15267. Took 0.05 sec\n",
            "Epoch 3517, Loss(train/val) 0.22504/0.15223. Took 0.04 sec\n",
            "Epoch 3518, Loss(train/val) 0.22955/0.15152. Took 0.04 sec\n",
            "Epoch 3519, Loss(train/val) 0.22509/0.15180. Took 0.04 sec\n",
            "Epoch 3520, Loss(train/val) 0.22704/0.15241. Took 0.04 sec\n",
            "Epoch 3521, Loss(train/val) 0.22721/0.15517. Took 0.05 sec\n",
            "Epoch 3522, Loss(train/val) 0.22200/0.15265. Took 0.04 sec\n",
            "Epoch 3523, Loss(train/val) 0.21869/0.15162. Took 0.04 sec\n",
            "Epoch 3524, Loss(train/val) 0.21939/0.15183. Took 0.05 sec\n",
            "Epoch 3525, Loss(train/val) 0.22342/0.15328. Took 0.04 sec\n",
            "Epoch 3526, Loss(train/val) 0.21958/0.15551. Took 0.05 sec\n",
            "Epoch 3527, Loss(train/val) 0.22799/0.15537. Took 0.05 sec\n",
            "Epoch 3528, Loss(train/val) 0.21426/0.15382. Took 0.05 sec\n",
            "Epoch 3529, Loss(train/val) 0.21312/0.15189. Took 0.04 sec\n",
            "Epoch 3530, Loss(train/val) 0.24616/0.15154. Took 0.04 sec\n",
            "Epoch 3531, Loss(train/val) 0.23195/0.15231. Took 0.05 sec\n",
            "Epoch 3532, Loss(train/val) 0.22195/0.15539. Took 0.05 sec\n",
            "Epoch 3533, Loss(train/val) 0.21811/0.15440. Took 0.04 sec\n",
            "Epoch 3534, Loss(train/val) 0.22456/0.15245. Took 0.05 sec\n",
            "Epoch 3535, Loss(train/val) 0.23122/0.15135. Took 0.05 sec\n",
            "Epoch 3536, Loss(train/val) 0.22181/0.15133. Took 0.06 sec\n",
            "Epoch 3537, Loss(train/val) 0.22462/0.15172. Took 0.05 sec\n",
            "Epoch 3538, Loss(train/val) 0.23240/0.15186. Took 0.05 sec\n",
            "Epoch 3539, Loss(train/val) 0.24618/0.15140. Took 0.05 sec\n",
            "Epoch 3540, Loss(train/val) 0.21667/0.15136. Took 0.04 sec\n",
            "Epoch 3541, Loss(train/val) 0.22197/0.15165. Took 0.05 sec\n",
            "Epoch 3542, Loss(train/val) 0.22601/0.15175. Took 0.05 sec\n",
            "Epoch 3543, Loss(train/val) 0.21974/0.15168. Took 0.05 sec\n",
            "Epoch 3544, Loss(train/val) 0.23092/0.15198. Took 0.05 sec\n",
            "Epoch 3545, Loss(train/val) 0.22164/0.15171. Took 0.05 sec\n",
            "Epoch 3546, Loss(train/val) 0.22082/0.15169. Took 0.05 sec\n",
            "Epoch 3547, Loss(train/val) 0.22131/0.15199. Took 0.05 sec\n",
            "Epoch 3548, Loss(train/val) 0.21741/0.15251. Took 0.05 sec\n",
            "Epoch 3549, Loss(train/val) 0.21833/0.15232. Took 0.05 sec\n",
            "Epoch 3550, Loss(train/val) 0.23154/0.15196. Took 0.05 sec\n",
            "Epoch 3551, Loss(train/val) 0.26663/0.15247. Took 0.05 sec\n",
            "Epoch 3552, Loss(train/val) 0.22309/0.15258. Took 0.05 sec\n",
            "Epoch 3553, Loss(train/val) 0.23918/0.15223. Took 0.05 sec\n",
            "Epoch 3554, Loss(train/val) 0.22286/0.15170. Took 0.04 sec\n",
            "Epoch 3555, Loss(train/val) 0.23154/0.15179. Took 0.05 sec\n",
            "Epoch 3556, Loss(train/val) 0.22218/0.15275. Took 0.05 sec\n",
            "Epoch 3557, Loss(train/val) 0.22527/0.15273. Took 0.05 sec\n",
            "Epoch 3558, Loss(train/val) 0.21568/0.15321. Took 0.05 sec\n",
            "Epoch 3559, Loss(train/val) 0.21540/0.15289. Took 0.05 sec\n",
            "Epoch 3560, Loss(train/val) 0.22051/0.15234. Took 0.05 sec\n",
            "Epoch 3561, Loss(train/val) 0.21779/0.15171. Took 0.05 sec\n",
            "Epoch 3562, Loss(train/val) 0.22254/0.15165. Took 0.04 sec\n",
            "Epoch 3563, Loss(train/val) 0.22489/0.15244. Took 0.05 sec\n",
            "Epoch 3564, Loss(train/val) 0.21917/0.15206. Took 0.04 sec\n",
            "Epoch 3565, Loss(train/val) 0.22063/0.15170. Took 0.04 sec\n",
            "Epoch 3566, Loss(train/val) 0.22341/0.15195. Took 0.05 sec\n",
            "Epoch 3567, Loss(train/val) 0.21862/0.15225. Took 0.05 sec\n",
            "Epoch 3568, Loss(train/val) 0.23160/0.15218. Took 0.05 sec\n",
            "Epoch 3569, Loss(train/val) 0.22116/0.15198. Took 0.05 sec\n",
            "Epoch 3570, Loss(train/val) 0.22034/0.15164. Took 0.04 sec\n",
            "Epoch 3571, Loss(train/val) 0.21915/0.15148. Took 0.06 sec\n",
            "Epoch 3572, Loss(train/val) 0.22940/0.15141. Took 0.05 sec\n",
            "Epoch 3573, Loss(train/val) 0.24749/0.15167. Took 0.04 sec\n",
            "Epoch 3574, Loss(train/val) 0.21653/0.15167. Took 0.05 sec\n",
            "Epoch 3575, Loss(train/val) 0.21766/0.15248. Took 0.04 sec\n",
            "Epoch 3576, Loss(train/val) 0.22258/0.15188. Took 0.05 sec\n",
            "Epoch 3577, Loss(train/val) 0.22264/0.15154. Took 0.04 sec\n",
            "Epoch 3578, Loss(train/val) 0.23829/0.15159. Took 0.04 sec\n",
            "Epoch 3579, Loss(train/val) 0.22363/0.15148. Took 0.05 sec\n",
            "Epoch 3580, Loss(train/val) 0.21606/0.15157. Took 0.05 sec\n",
            "Epoch 3581, Loss(train/val) 0.23240/0.15175. Took 0.05 sec\n",
            "Epoch 3582, Loss(train/val) 0.21498/0.15174. Took 0.05 sec\n",
            "Epoch 3583, Loss(train/val) 0.21868/0.15151. Took 0.05 sec\n",
            "Epoch 3584, Loss(train/val) 0.22499/0.15171. Took 0.05 sec\n",
            "Epoch 3585, Loss(train/val) 0.22663/0.15179. Took 0.05 sec\n",
            "Epoch 3586, Loss(train/val) 0.25483/0.15170. Took 0.05 sec\n",
            "Epoch 3587, Loss(train/val) 0.22292/0.15130. Took 0.05 sec\n",
            "Epoch 3588, Loss(train/val) 0.22313/0.15126. Took 0.04 sec\n",
            "Epoch 3589, Loss(train/val) 0.23074/0.15139. Took 0.04 sec\n",
            "Epoch 3590, Loss(train/val) 0.23155/0.15156. Took 0.04 sec\n",
            "Epoch 3591, Loss(train/val) 0.22605/0.15127. Took 0.05 sec\n",
            "Epoch 3592, Loss(train/val) 0.22676/0.15167. Took 0.06 sec\n",
            "Epoch 3593, Loss(train/val) 0.21550/0.15133. Took 0.04 sec\n",
            "Epoch 3594, Loss(train/val) 0.22368/0.15128. Took 0.04 sec\n",
            "Epoch 3595, Loss(train/val) 0.21520/0.15124. Took 0.05 sec\n",
            "Epoch 3596, Loss(train/val) 0.22761/0.15180. Took 0.05 sec\n",
            "Epoch 3597, Loss(train/val) 0.22505/0.15149. Took 0.05 sec\n",
            "Epoch 3598, Loss(train/val) 0.22288/0.15167. Took 0.04 sec\n",
            "Epoch 3599, Loss(train/val) 0.20810/0.15142. Took 0.04 sec\n",
            "Epoch 3600, Loss(train/val) 0.23895/0.15126. Took 0.05 sec\n",
            "Epoch 3601, Loss(train/val) 0.22062/0.15125. Took 0.06 sec\n",
            "Epoch 3602, Loss(train/val) 0.21976/0.15130. Took 0.04 sec\n",
            "Epoch 3603, Loss(train/val) 0.22182/0.15138. Took 0.04 sec\n",
            "Epoch 3604, Loss(train/val) 0.22666/0.15140. Took 0.04 sec\n",
            "Epoch 3605, Loss(train/val) 0.21666/0.15155. Took 0.05 sec\n",
            "Epoch 3606, Loss(train/val) 0.21909/0.15128. Took 0.05 sec\n",
            "Epoch 3607, Loss(train/val) 0.21972/0.15121. Took 0.04 sec\n",
            "Epoch 3608, Loss(train/val) 0.21659/0.15180. Took 0.04 sec\n",
            "Epoch 3609, Loss(train/val) 0.22560/0.15132. Took 0.05 sec\n",
            "Epoch 3610, Loss(train/val) 0.22107/0.15125. Took 0.05 sec\n",
            "Epoch 3611, Loss(train/val) 0.22389/0.15150. Took 0.06 sec\n",
            "Epoch 3612, Loss(train/val) 0.22148/0.15136. Took 0.05 sec\n",
            "Epoch 3613, Loss(train/val) 0.22068/0.15150. Took 0.05 sec\n",
            "Epoch 3614, Loss(train/val) 0.22742/0.15146. Took 0.05 sec\n",
            "Epoch 3615, Loss(train/val) 0.21561/0.15162. Took 0.05 sec\n",
            "Epoch 3616, Loss(train/val) 0.22575/0.15145. Took 0.06 sec\n",
            "Epoch 3617, Loss(train/val) 0.22245/0.15133. Took 0.04 sec\n",
            "Epoch 3618, Loss(train/val) 0.23841/0.15203. Took 0.04 sec\n",
            "Epoch 3619, Loss(train/val) 0.21647/0.15232. Took 0.05 sec\n",
            "Epoch 3620, Loss(train/val) 0.23372/0.15250. Took 0.04 sec\n",
            "Epoch 3621, Loss(train/val) 0.22072/0.15209. Took 0.05 sec\n",
            "Epoch 3622, Loss(train/val) 0.23819/0.15194. Took 0.04 sec\n",
            "Epoch 3623, Loss(train/val) 0.23169/0.15146. Took 0.04 sec\n",
            "Epoch 3624, Loss(train/val) 0.23838/0.15125. Took 0.04 sec\n",
            "Epoch 3625, Loss(train/val) 0.22200/0.15120. Took 0.05 sec\n",
            "Epoch 3626, Loss(train/val) 0.23162/0.15137. Took 0.05 sec\n",
            "Epoch 3627, Loss(train/val) 0.21529/0.15133. Took 0.05 sec\n",
            "Epoch 3628, Loss(train/val) 0.22345/0.15179. Took 0.05 sec\n",
            "Epoch 3629, Loss(train/val) 0.24731/0.15250. Took 0.05 sec\n",
            "Epoch 3630, Loss(train/val) 0.22803/0.15204. Took 0.05 sec\n",
            "Epoch 3631, Loss(train/val) 0.22506/0.15225. Took 0.06 sec\n",
            "Epoch 3632, Loss(train/val) 0.22869/0.15192. Took 0.05 sec\n",
            "Epoch 3633, Loss(train/val) 0.22904/0.15111. Took 0.05 sec\n",
            "Epoch 3634, Loss(train/val) 0.21832/0.15137. Took 0.05 sec\n",
            "Epoch 3635, Loss(train/val) 0.22110/0.15141. Took 0.05 sec\n",
            "Epoch 3636, Loss(train/val) 0.21792/0.15174. Took 0.05 sec\n",
            "Epoch 3637, Loss(train/val) 0.22344/0.15170. Took 0.05 sec\n",
            "Epoch 3638, Loss(train/val) 0.22974/0.15167. Took 0.05 sec\n",
            "Epoch 3639, Loss(train/val) 0.21799/0.15214. Took 0.04 sec\n",
            "Epoch 3640, Loss(train/val) 0.21558/0.15244. Took 0.04 sec\n",
            "Epoch 3641, Loss(train/val) 0.22393/0.15140. Took 0.06 sec\n",
            "Epoch 3642, Loss(train/val) 0.21556/0.15122. Took 0.05 sec\n",
            "Epoch 3643, Loss(train/val) 0.23067/0.15118. Took 0.04 sec\n",
            "Epoch 3644, Loss(train/val) 0.22472/0.15122. Took 0.04 sec\n",
            "Epoch 3645, Loss(train/val) 0.22628/0.15145. Took 0.04 sec\n",
            "Epoch 3646, Loss(train/val) 0.22256/0.15143. Took 0.05 sec\n",
            "Epoch 3647, Loss(train/val) 0.22967/0.15157. Took 0.05 sec\n",
            "Epoch 3648, Loss(train/val) 0.22664/0.15140. Took 0.05 sec\n",
            "Epoch 3649, Loss(train/val) 0.22548/0.15120. Took 0.05 sec\n",
            "Epoch 3650, Loss(train/val) 0.21818/0.15158. Took 0.05 sec\n",
            "Epoch 3651, Loss(train/val) 0.22553/0.15154. Took 0.05 sec\n",
            "Epoch 3652, Loss(train/val) 0.22663/0.15134. Took 0.05 sec\n",
            "Epoch 3653, Loss(train/val) 0.24581/0.15118. Took 0.05 sec\n",
            "Epoch 3654, Loss(train/val) 0.22890/0.15138. Took 0.04 sec\n",
            "Epoch 3655, Loss(train/val) 0.22271/0.15102. Took 0.05 sec\n",
            "Epoch 3656, Loss(train/val) 0.22386/0.15129. Took 0.06 sec\n",
            "Epoch 3657, Loss(train/val) 0.22389/0.15145. Took 0.05 sec\n",
            "Epoch 3658, Loss(train/val) 0.21822/0.15134. Took 0.05 sec\n",
            "Epoch 3659, Loss(train/val) 0.21832/0.15140. Took 0.04 sec\n",
            "Epoch 3660, Loss(train/val) 0.22029/0.15121. Took 0.04 sec\n",
            "Epoch 3661, Loss(train/val) 0.23900/0.15341. Took 0.05 sec\n",
            "Epoch 3662, Loss(train/val) 0.23674/0.15485. Took 0.05 sec\n",
            "Epoch 3663, Loss(train/val) 0.21899/0.15790. Took 0.05 sec\n",
            "Epoch 3664, Loss(train/val) 0.21552/0.15480. Took 0.05 sec\n",
            "Epoch 3665, Loss(train/val) 0.21418/0.15147. Took 0.04 sec\n",
            "Epoch 3666, Loss(train/val) 0.21901/0.15220. Took 0.05 sec\n",
            "Epoch 3667, Loss(train/val) 0.21939/0.15424. Took 0.05 sec\n",
            "Epoch 3668, Loss(train/val) 0.21170/0.15619. Took 0.05 sec\n",
            "Epoch 3669, Loss(train/val) 0.22234/0.15460. Took 0.05 sec\n",
            "Epoch 3670, Loss(train/val) 0.22933/0.15354. Took 0.05 sec\n",
            "Epoch 3671, Loss(train/val) 0.21742/0.15328. Took 0.06 sec\n",
            "Epoch 3672, Loss(train/val) 0.23242/0.15209. Took 0.04 sec\n",
            "Epoch 3673, Loss(train/val) 0.25550/0.15167. Took 0.04 sec\n",
            "Epoch 3674, Loss(train/val) 0.22477/0.15317. Took 0.05 sec\n",
            "Epoch 3675, Loss(train/val) 0.22194/0.15448. Took 0.05 sec\n",
            "Epoch 3676, Loss(train/val) 0.22011/0.15393. Took 0.05 sec\n",
            "Epoch 3677, Loss(train/val) 0.21891/0.15242. Took 0.04 sec\n",
            "Epoch 3678, Loss(train/val) 0.23153/0.15146. Took 0.05 sec\n",
            "Epoch 3679, Loss(train/val) 0.22683/0.15142. Took 0.05 sec\n",
            "Epoch 3680, Loss(train/val) 0.22075/0.15119. Took 0.05 sec\n",
            "Epoch 3681, Loss(train/val) 0.22095/0.15112. Took 0.05 sec\n",
            "Epoch 3682, Loss(train/val) 0.21866/0.15123. Took 0.05 sec\n",
            "Epoch 3683, Loss(train/val) 0.22222/0.15117. Took 0.05 sec\n",
            "Epoch 3684, Loss(train/val) 0.21891/0.15122. Took 0.05 sec\n",
            "Epoch 3685, Loss(train/val) 0.23109/0.15146. Took 0.05 sec\n",
            "Epoch 3686, Loss(train/val) 0.21866/0.15194. Took 0.06 sec\n",
            "Epoch 3687, Loss(train/val) 0.25652/0.15106. Took 0.05 sec\n",
            "Epoch 3688, Loss(train/val) 0.23005/0.15127. Took 0.05 sec\n",
            "Epoch 3689, Loss(train/val) 0.22260/0.15131. Took 0.05 sec\n",
            "Epoch 3690, Loss(train/val) 0.21856/0.15108. Took 0.05 sec\n",
            "Epoch 3691, Loss(train/val) 0.21749/0.15117. Took 0.06 sec\n",
            "Epoch 3692, Loss(train/val) 0.22700/0.15142. Took 0.05 sec\n",
            "Epoch 3693, Loss(train/val) 0.21816/0.15171. Took 0.05 sec\n",
            "Epoch 3694, Loss(train/val) 0.22862/0.15209. Took 0.05 sec\n",
            "Epoch 3695, Loss(train/val) 0.22545/0.15183. Took 0.05 sec\n",
            "Epoch 3696, Loss(train/val) 0.21848/0.15128. Took 0.05 sec\n",
            "Epoch 3697, Loss(train/val) 0.22081/0.15121. Took 0.05 sec\n",
            "Epoch 3698, Loss(train/val) 0.21763/0.15122. Took 0.05 sec\n",
            "Epoch 3699, Loss(train/val) 0.22465/0.15119. Took 0.05 sec\n",
            "Epoch 3700, Loss(train/val) 0.22327/0.15135. Took 0.04 sec\n",
            "Epoch 3701, Loss(train/val) 0.21548/0.15217. Took 0.05 sec\n",
            "Epoch 3702, Loss(train/val) 0.22020/0.15290. Took 0.05 sec\n",
            "Epoch 3703, Loss(train/val) 0.21509/0.15261. Took 0.04 sec\n",
            "Epoch 3704, Loss(train/val) 0.21343/0.15175. Took 0.05 sec\n",
            "Epoch 3705, Loss(train/val) 0.22485/0.15111. Took 0.04 sec\n",
            "Epoch 3706, Loss(train/val) 0.23232/0.15141. Took 0.05 sec\n",
            "Epoch 3707, Loss(train/val) 0.21965/0.15204. Took 0.05 sec\n",
            "Epoch 3708, Loss(train/val) 0.22176/0.15151. Took 0.04 sec\n",
            "Epoch 3709, Loss(train/val) 0.22142/0.15131. Took 0.05 sec\n",
            "Epoch 3710, Loss(train/val) 0.23467/0.15140. Took 0.05 sec\n",
            "Epoch 3711, Loss(train/val) 0.23334/0.15160. Took 0.05 sec\n",
            "Epoch 3712, Loss(train/val) 0.22433/0.15129. Took 0.05 sec\n",
            "Epoch 3713, Loss(train/val) 0.21926/0.15135. Took 0.05 sec\n",
            "Epoch 3714, Loss(train/val) 0.21654/0.15175. Took 0.04 sec\n",
            "Epoch 3715, Loss(train/val) 0.22724/0.15203. Took 0.04 sec\n",
            "Epoch 3716, Loss(train/val) 0.22152/0.15112. Took 0.05 sec\n",
            "Epoch 3717, Loss(train/val) 0.21744/0.15116. Took 0.04 sec\n",
            "Epoch 3718, Loss(train/val) 0.23509/0.15122. Took 0.04 sec\n",
            "Epoch 3719, Loss(train/val) 0.21340/0.15175. Took 0.05 sec\n",
            "Epoch 3720, Loss(train/val) 0.22171/0.15156. Took 0.05 sec\n",
            "Epoch 3721, Loss(train/val) 0.22075/0.15168. Took 0.05 sec\n",
            "Epoch 3722, Loss(train/val) 0.22218/0.15115. Took 0.05 sec\n",
            "Epoch 3723, Loss(train/val) 0.22757/0.15108. Took 0.04 sec\n",
            "Epoch 3724, Loss(train/val) 0.22047/0.15103. Took 0.05 sec\n",
            "Epoch 3725, Loss(train/val) 0.23908/0.15165. Took 0.05 sec\n",
            "Epoch 3726, Loss(train/val) 0.23456/0.15120. Took 0.05 sec\n",
            "Epoch 3727, Loss(train/val) 0.22786/0.15322. Took 0.05 sec\n",
            "Epoch 3728, Loss(train/val) 0.22462/0.15438. Took 0.04 sec\n",
            "Epoch 3729, Loss(train/val) 0.22844/0.15339. Took 0.04 sec\n",
            "Epoch 3730, Loss(train/val) 0.21594/0.15146. Took 0.05 sec\n",
            "Epoch 3731, Loss(train/val) 0.21808/0.15133. Took 0.06 sec\n",
            "Epoch 3732, Loss(train/val) 0.22101/0.15115. Took 0.05 sec\n",
            "Epoch 3733, Loss(train/val) 0.22655/0.15116. Took 0.05 sec\n",
            "Epoch 3734, Loss(train/val) 0.22518/0.15131. Took 0.05 sec\n",
            "Epoch 3735, Loss(train/val) 0.21692/0.15221. Took 0.05 sec\n",
            "Epoch 3736, Loss(train/val) 0.22053/0.15189. Took 0.05 sec\n",
            "Epoch 3737, Loss(train/val) 0.22218/0.15133. Took 0.05 sec\n",
            "Epoch 3738, Loss(train/val) 0.22874/0.15127. Took 0.04 sec\n",
            "Epoch 3739, Loss(train/val) 0.22374/0.15123. Took 0.05 sec\n",
            "Epoch 3740, Loss(train/val) 0.23297/0.15127. Took 0.04 sec\n",
            "Epoch 3741, Loss(train/val) 0.23005/0.15125. Took 0.06 sec\n",
            "Epoch 3742, Loss(train/val) 0.21827/0.15123. Took 0.05 sec\n",
            "Epoch 3743, Loss(train/val) 0.22193/0.15141. Took 0.04 sec\n",
            "Epoch 3744, Loss(train/val) 0.22278/0.15113. Took 0.04 sec\n",
            "Epoch 3745, Loss(train/val) 0.22748/0.15133. Took 0.05 sec\n",
            "Epoch 3746, Loss(train/val) 0.21725/0.15142. Took 0.05 sec\n",
            "Epoch 3747, Loss(train/val) 0.21643/0.15127. Took 0.04 sec\n",
            "Epoch 3748, Loss(train/val) 0.22105/0.15097. Took 0.04 sec\n",
            "Epoch 3749, Loss(train/val) 0.22048/0.15111. Took 0.04 sec\n",
            "Epoch 3750, Loss(train/val) 0.21703/0.15125. Took 0.04 sec\n",
            "Epoch 3751, Loss(train/val) 0.22075/0.15152. Took 0.06 sec\n",
            "Epoch 3752, Loss(train/val) 0.23158/0.15157. Took 0.05 sec\n",
            "Epoch 3753, Loss(train/val) 0.22594/0.15121. Took 0.04 sec\n",
            "Epoch 3754, Loss(train/val) 0.22036/0.15128. Took 0.04 sec\n",
            "Epoch 3755, Loss(train/val) 0.22619/0.15218. Took 0.04 sec\n",
            "Epoch 3756, Loss(train/val) 0.21334/0.15188. Took 0.05 sec\n",
            "Epoch 3757, Loss(train/val) 0.22650/0.15368. Took 0.05 sec\n",
            "Epoch 3758, Loss(train/val) 0.23300/0.15504. Took 0.05 sec\n",
            "Epoch 3759, Loss(train/val) 0.22184/0.15242. Took 0.04 sec\n",
            "Epoch 3760, Loss(train/val) 0.22319/0.15095. Took 0.04 sec\n",
            "Epoch 3761, Loss(train/val) 0.22978/0.15114. Took 0.05 sec\n",
            "Epoch 3762, Loss(train/val) 0.22583/0.15140. Took 0.05 sec\n",
            "Epoch 3763, Loss(train/val) 0.21341/0.15150. Took 0.05 sec\n",
            "Epoch 3764, Loss(train/val) 0.21185/0.15186. Took 0.04 sec\n",
            "Epoch 3765, Loss(train/val) 0.21817/0.15228. Took 0.04 sec\n",
            "Epoch 3766, Loss(train/val) 0.21976/0.15209. Took 0.05 sec\n",
            "Epoch 3767, Loss(train/val) 0.22265/0.15201. Took 0.05 sec\n",
            "Epoch 3768, Loss(train/val) 0.22765/0.15117. Took 0.05 sec\n",
            "Epoch 3769, Loss(train/val) 0.22272/0.15135. Took 0.05 sec\n",
            "Epoch 3770, Loss(train/val) 0.22705/0.15173. Took 0.05 sec\n",
            "Epoch 3771, Loss(train/val) 0.22959/0.15108. Took 0.05 sec\n",
            "Epoch 3772, Loss(train/val) 0.22920/0.15114. Took 0.05 sec\n",
            "Epoch 3773, Loss(train/val) 0.22823/0.15109. Took 0.05 sec\n",
            "Epoch 3774, Loss(train/val) 0.22059/0.15139. Took 0.04 sec\n",
            "Epoch 3775, Loss(train/val) 0.22110/0.15137. Took 0.04 sec\n",
            "Epoch 3776, Loss(train/val) 0.24012/0.15139. Took 0.05 sec\n",
            "Epoch 3777, Loss(train/val) 0.21306/0.15125. Took 0.05 sec\n",
            "Epoch 3778, Loss(train/val) 0.22709/0.15139. Took 0.05 sec\n",
            "Epoch 3779, Loss(train/val) 0.21800/0.15247. Took 0.05 sec\n",
            "Epoch 3780, Loss(train/val) 0.21962/0.15328. Took 0.04 sec\n",
            "Epoch 3781, Loss(train/val) 0.21249/0.15206. Took 0.05 sec\n",
            "Epoch 3782, Loss(train/val) 0.21713/0.15149. Took 0.04 sec\n",
            "Epoch 3783, Loss(train/val) 0.21927/0.15120. Took 0.05 sec\n",
            "Epoch 3784, Loss(train/val) 0.22441/0.15146. Took 0.05 sec\n",
            "Epoch 3785, Loss(train/val) 0.23398/0.15148. Took 0.04 sec\n",
            "Epoch 3786, Loss(train/val) 0.22074/0.15122. Took 0.05 sec\n",
            "Epoch 3787, Loss(train/val) 0.21726/0.15122. Took 0.04 sec\n",
            "Epoch 3788, Loss(train/val) 0.22106/0.15099. Took 0.05 sec\n",
            "Epoch 3789, Loss(train/val) 0.22187/0.15121. Took 0.05 sec\n",
            "Epoch 3790, Loss(train/val) 0.25872/0.15203. Took 0.05 sec\n",
            "Epoch 3791, Loss(train/val) 0.22079/0.15208. Took 0.05 sec\n",
            "Epoch 3792, Loss(train/val) 0.22623/0.15222. Took 0.05 sec\n",
            "Epoch 3793, Loss(train/val) 0.23522/0.15122. Took 0.04 sec\n",
            "Epoch 3794, Loss(train/val) 0.21504/0.15181. Took 0.05 sec\n",
            "Epoch 3795, Loss(train/val) 0.22256/0.15259. Took 0.04 sec\n",
            "Epoch 3796, Loss(train/val) 0.23731/0.15230. Took 0.05 sec\n",
            "Epoch 3797, Loss(train/val) 0.22138/0.15133. Took 0.05 sec\n",
            "Epoch 3798, Loss(train/val) 0.22788/0.15148. Took 0.04 sec\n",
            "Epoch 3799, Loss(train/val) 0.24801/0.15154. Took 0.05 sec\n",
            "Epoch 3800, Loss(train/val) 0.22355/0.15167. Took 0.05 sec\n",
            "Epoch 3801, Loss(train/val) 0.22124/0.15129. Took 0.05 sec\n",
            "Epoch 3802, Loss(train/val) 0.22029/0.15122. Took 0.04 sec\n",
            "Epoch 3803, Loss(train/val) 0.21678/0.15120. Took 0.04 sec\n",
            "Epoch 3804, Loss(train/val) 0.21968/0.15145. Took 0.04 sec\n",
            "Epoch 3805, Loss(train/val) 0.22842/0.15117. Took 0.05 sec\n",
            "Epoch 3806, Loss(train/val) 0.21907/0.15096. Took 0.06 sec\n",
            "Epoch 3807, Loss(train/val) 0.22067/0.15119. Took 0.04 sec\n",
            "Epoch 3808, Loss(train/val) 0.21603/0.15235. Took 0.04 sec\n",
            "Epoch 3809, Loss(train/val) 0.21608/0.15210. Took 0.04 sec\n",
            "Epoch 3810, Loss(train/val) 0.22789/0.15172. Took 0.05 sec\n",
            "Epoch 3811, Loss(train/val) 0.21726/0.15170. Took 0.05 sec\n",
            "Epoch 3812, Loss(train/val) 0.23042/0.15109. Took 0.05 sec\n",
            "Epoch 3813, Loss(train/val) 0.22146/0.15160. Took 0.05 sec\n",
            "Epoch 3814, Loss(train/val) 0.22072/0.15195. Took 0.04 sec\n",
            "Epoch 3815, Loss(train/val) 0.22187/0.15273. Took 0.05 sec\n",
            "Epoch 3816, Loss(train/val) 0.20707/0.15145. Took 0.05 sec\n",
            "Epoch 3817, Loss(train/val) 0.23153/0.15098. Took 0.05 sec\n",
            "Epoch 3818, Loss(train/val) 0.21378/0.15098. Took 0.05 sec\n",
            "Epoch 3819, Loss(train/val) 0.21562/0.15100. Took 0.05 sec\n",
            "Epoch 3820, Loss(train/val) 0.22096/0.15098. Took 0.05 sec\n",
            "Epoch 3821, Loss(train/val) 0.21882/0.15093. Took 0.05 sec\n",
            "Epoch 3822, Loss(train/val) 0.21908/0.15104. Took 0.05 sec\n",
            "Epoch 3823, Loss(train/val) 0.25015/0.15125. Took 0.05 sec\n",
            "Epoch 3824, Loss(train/val) 0.23550/0.15121. Took 0.04 sec\n",
            "Epoch 3825, Loss(train/val) 0.23285/0.15121. Took 0.05 sec\n",
            "Epoch 3826, Loss(train/val) 0.22005/0.15119. Took 0.05 sec\n",
            "Epoch 3827, Loss(train/val) 0.22497/0.15110. Took 0.05 sec\n",
            "Epoch 3828, Loss(train/val) 0.22959/0.15105. Took 0.04 sec\n",
            "Epoch 3829, Loss(train/val) 0.23490/0.15107. Took 0.05 sec\n",
            "Epoch 3830, Loss(train/val) 0.22232/0.15094. Took 0.05 sec\n",
            "Epoch 3831, Loss(train/val) 0.22176/0.15089. Took 0.05 sec\n",
            "Epoch 3832, Loss(train/val) 0.22577/0.15105. Took 0.05 sec\n",
            "Epoch 3833, Loss(train/val) 0.22481/0.15091. Took 0.05 sec\n",
            "Epoch 3834, Loss(train/val) 0.21965/0.15085. Took 0.05 sec\n",
            "Epoch 3835, Loss(train/val) 0.24422/0.15089. Took 0.05 sec\n",
            "Epoch 3836, Loss(train/val) 0.21988/0.15086. Took 0.05 sec\n",
            "Epoch 3837, Loss(train/val) 0.22101/0.15103. Took 0.05 sec\n",
            "Epoch 3838, Loss(train/val) 0.21477/0.15087. Took 0.05 sec\n",
            "Epoch 3839, Loss(train/val) 0.22221/0.15080. Took 0.05 sec\n",
            "Epoch 3840, Loss(train/val) 0.22307/0.15068. Took 0.05 sec\n",
            "Epoch 3841, Loss(train/val) 0.22510/0.15089. Took 0.06 sec\n",
            "Epoch 3842, Loss(train/val) 0.21836/0.15084. Took 0.05 sec\n",
            "Epoch 3843, Loss(train/val) 0.21832/0.15075. Took 0.04 sec\n",
            "Epoch 3844, Loss(train/val) 0.23214/0.15079. Took 0.04 sec\n",
            "Epoch 3845, Loss(train/val) 0.21534/0.15130. Took 0.04 sec\n",
            "Epoch 3846, Loss(train/val) 0.22180/0.15171. Took 0.06 sec\n",
            "Epoch 3847, Loss(train/val) 0.22862/0.15103. Took 0.04 sec\n",
            "Epoch 3848, Loss(train/val) 0.21895/0.15092. Took 0.05 sec\n",
            "Epoch 3849, Loss(train/val) 0.21427/0.15093. Took 0.05 sec\n",
            "Epoch 3850, Loss(train/val) 0.22953/0.15111. Took 0.04 sec\n",
            "Epoch 3851, Loss(train/val) 0.22361/0.15109. Took 0.05 sec\n",
            "Epoch 3852, Loss(train/val) 0.22160/0.15147. Took 0.05 sec\n",
            "Epoch 3853, Loss(train/val) 0.22011/0.15168. Took 0.04 sec\n",
            "Epoch 3854, Loss(train/val) 0.22141/0.15109. Took 0.04 sec\n",
            "Epoch 3855, Loss(train/val) 0.22969/0.15144. Took 0.04 sec\n",
            "Epoch 3856, Loss(train/val) 0.22520/0.15139. Took 0.06 sec\n",
            "Epoch 3857, Loss(train/val) 0.22380/0.15185. Took 0.05 sec\n",
            "Epoch 3858, Loss(train/val) 0.21527/0.15126. Took 0.05 sec\n",
            "Epoch 3859, Loss(train/val) 0.21653/0.15104. Took 0.05 sec\n",
            "Epoch 3860, Loss(train/val) 0.22472/0.15094. Took 0.05 sec\n",
            "Epoch 3861, Loss(train/val) 0.22940/0.15136. Took 0.05 sec\n",
            "Epoch 3862, Loss(train/val) 0.21414/0.15208. Took 0.05 sec\n",
            "Epoch 3863, Loss(train/val) 0.21960/0.15220. Took 0.05 sec\n",
            "Epoch 3864, Loss(train/val) 0.21514/0.15176. Took 0.05 sec\n",
            "Epoch 3865, Loss(train/val) 0.20903/0.15138. Took 0.05 sec\n",
            "Epoch 3866, Loss(train/val) 0.22152/0.15112. Took 0.05 sec\n",
            "Epoch 3867, Loss(train/val) 0.21704/0.15091. Took 0.05 sec\n",
            "Epoch 3868, Loss(train/val) 0.21487/0.15154. Took 0.04 sec\n",
            "Epoch 3869, Loss(train/val) 0.21604/0.15148. Took 0.05 sec\n",
            "Epoch 3870, Loss(train/val) 0.21799/0.15104. Took 0.05 sec\n",
            "Epoch 3871, Loss(train/val) 0.21732/0.15114. Took 0.05 sec\n",
            "Epoch 3872, Loss(train/val) 0.22479/0.15092. Took 0.05 sec\n",
            "Epoch 3873, Loss(train/val) 0.21379/0.15097. Took 0.05 sec\n",
            "Epoch 3874, Loss(train/val) 0.22343/0.15180. Took 0.05 sec\n",
            "Epoch 3875, Loss(train/val) 0.22453/0.15178. Took 0.05 sec\n",
            "Epoch 3876, Loss(train/val) 0.22776/0.15117. Took 0.05 sec\n",
            "Epoch 3877, Loss(train/val) 0.22208/0.15097. Took 0.05 sec\n",
            "Epoch 3878, Loss(train/val) 0.22961/0.15087. Took 0.05 sec\n",
            "Epoch 3879, Loss(train/val) 0.21501/0.15179. Took 0.05 sec\n",
            "Epoch 3880, Loss(train/val) 0.21947/0.15340. Took 0.04 sec\n",
            "Epoch 3881, Loss(train/val) 0.22835/0.15332. Took 0.05 sec\n",
            "Epoch 3882, Loss(train/val) 0.22629/0.15134. Took 0.05 sec\n",
            "Epoch 3883, Loss(train/val) 0.22017/0.15101. Took 0.05 sec\n",
            "Epoch 3884, Loss(train/val) 0.21654/0.15093. Took 0.05 sec\n",
            "Epoch 3885, Loss(train/val) 0.22095/0.15087. Took 0.05 sec\n",
            "Epoch 3886, Loss(train/val) 0.21718/0.15142. Took 0.05 sec\n",
            "Epoch 3887, Loss(train/val) 0.22627/0.15118. Took 0.04 sec\n",
            "Epoch 3888, Loss(train/val) 0.21914/0.15123. Took 0.05 sec\n",
            "Epoch 3889, Loss(train/val) 0.22072/0.15081. Took 0.04 sec\n",
            "Epoch 3890, Loss(train/val) 0.21462/0.15082. Took 0.04 sec\n",
            "Epoch 3891, Loss(train/val) 0.22702/0.15098. Took 0.06 sec\n",
            "Epoch 3892, Loss(train/val) 0.23303/0.15094. Took 0.05 sec\n",
            "Epoch 3893, Loss(train/val) 0.22223/0.15104. Took 0.05 sec\n",
            "Epoch 3894, Loss(train/val) 0.22875/0.15113. Took 0.05 sec\n",
            "Epoch 3895, Loss(train/val) 0.23124/0.15153. Took 0.04 sec\n",
            "Epoch 3896, Loss(train/val) 0.22658/0.15114. Took 0.05 sec\n",
            "Epoch 3897, Loss(train/val) 0.23901/0.15117. Took 0.04 sec\n",
            "Epoch 3898, Loss(train/val) 0.23084/0.15111. Took 0.05 sec\n",
            "Epoch 3899, Loss(train/val) 0.23995/0.15165. Took 0.05 sec\n",
            "Epoch 3900, Loss(train/val) 0.22310/0.15202. Took 0.05 sec\n",
            "Epoch 3901, Loss(train/val) 0.25625/0.15270. Took 0.05 sec\n",
            "Epoch 3902, Loss(train/val) 0.23481/0.15218. Took 0.05 sec\n",
            "Epoch 3903, Loss(train/val) 0.21538/0.15179. Took 0.05 sec\n",
            "Epoch 3904, Loss(train/val) 0.23915/0.15141. Took 0.05 sec\n",
            "Epoch 3905, Loss(train/val) 0.22928/0.15174. Took 0.05 sec\n",
            "Epoch 3906, Loss(train/val) 0.22415/0.15170. Took 0.05 sec\n",
            "Epoch 3907, Loss(train/val) 0.21842/0.15143. Took 0.05 sec\n",
            "Epoch 3908, Loss(train/val) 0.23991/0.15150. Took 0.04 sec\n",
            "Epoch 3909, Loss(train/val) 0.22506/0.15160. Took 0.05 sec\n",
            "Epoch 3910, Loss(train/val) 0.21885/0.15304. Took 0.05 sec\n",
            "Epoch 3911, Loss(train/val) 0.21848/0.15422. Took 0.05 sec\n",
            "Epoch 3912, Loss(train/val) 0.26199/0.15545. Took 0.05 sec\n",
            "Epoch 3913, Loss(train/val) 0.24189/0.15587. Took 0.05 sec\n",
            "Epoch 3914, Loss(train/val) 0.22182/0.15319. Took 0.04 sec\n",
            "Epoch 3915, Loss(train/val) 0.21980/0.15181. Took 0.05 sec\n",
            "Epoch 3916, Loss(train/val) 0.23345/0.15095. Took 0.05 sec\n",
            "Epoch 3917, Loss(train/val) 0.22776/0.15158. Took 0.04 sec\n",
            "Epoch 3918, Loss(train/val) 0.23390/0.15311. Took 0.05 sec\n",
            "Epoch 3919, Loss(train/val) 0.21429/0.15591. Took 0.05 sec\n",
            "Epoch 3920, Loss(train/val) 0.22025/0.15457. Took 0.05 sec\n",
            "Epoch 3921, Loss(train/val) 0.22245/0.15234. Took 0.06 sec\n",
            "Epoch 3922, Loss(train/val) 0.23182/0.15089. Took 0.05 sec\n",
            "Epoch 3923, Loss(train/val) 0.22762/0.15089. Took 0.05 sec\n",
            "Epoch 3924, Loss(train/val) 0.23033/0.15098. Took 0.04 sec\n",
            "Epoch 3925, Loss(train/val) 0.21780/0.15090. Took 0.05 sec\n",
            "Epoch 3926, Loss(train/val) 0.21407/0.15079. Took 0.05 sec\n",
            "Epoch 3927, Loss(train/val) 0.22478/0.15088. Took 0.04 sec\n",
            "Epoch 3928, Loss(train/val) 0.21268/0.15092. Took 0.04 sec\n",
            "Epoch 3929, Loss(train/val) 0.21923/0.15159. Took 0.05 sec\n",
            "Epoch 3930, Loss(train/val) 0.21676/0.15159. Took 0.04 sec\n",
            "Epoch 3931, Loss(train/val) 0.22486/0.15129. Took 0.05 sec\n",
            "Epoch 3932, Loss(train/val) 0.22575/0.15182. Took 0.04 sec\n",
            "Epoch 3933, Loss(train/val) 0.22206/0.15156. Took 0.05 sec\n",
            "Epoch 3934, Loss(train/val) 0.22689/0.15141. Took 0.05 sec\n",
            "Epoch 3935, Loss(train/val) 0.24245/0.15080. Took 0.05 sec\n",
            "Epoch 3936, Loss(train/val) 0.21487/0.15076. Took 0.05 sec\n",
            "Epoch 3937, Loss(train/val) 0.23090/0.15151. Took 0.04 sec\n",
            "Epoch 3938, Loss(train/val) 0.24419/0.15404. Took 0.04 sec\n",
            "Epoch 3939, Loss(train/val) 0.22220/0.15540. Took 0.05 sec\n",
            "Epoch 3940, Loss(train/val) 0.22586/0.15332. Took 0.05 sec\n",
            "Epoch 3941, Loss(train/val) 0.23832/0.15142. Took 0.06 sec\n",
            "Epoch 3942, Loss(train/val) 0.21953/0.15082. Took 0.05 sec\n",
            "Epoch 3943, Loss(train/val) 0.22178/0.15248. Took 0.05 sec\n",
            "Epoch 3944, Loss(train/val) 0.21836/0.15447. Took 0.04 sec\n",
            "Epoch 3945, Loss(train/val) 0.23292/0.15391. Took 0.04 sec\n",
            "Epoch 3946, Loss(train/val) 0.22208/0.15176. Took 0.06 sec\n",
            "Epoch 3947, Loss(train/val) 0.21958/0.15102. Took 0.05 sec\n",
            "Epoch 3948, Loss(train/val) 0.20943/0.15098. Took 0.05 sec\n",
            "Epoch 3949, Loss(train/val) 0.21677/0.15152. Took 0.05 sec\n",
            "Epoch 3950, Loss(train/val) 0.21879/0.15206. Took 0.05 sec\n",
            "Epoch 3951, Loss(train/val) 0.24107/0.15253. Took 0.05 sec\n",
            "Epoch 3952, Loss(train/val) 0.22075/0.15217. Took 0.04 sec\n",
            "Epoch 3953, Loss(train/val) 0.21617/0.15142. Took 0.06 sec\n",
            "Epoch 3954, Loss(train/val) 0.22336/0.15086. Took 0.05 sec\n",
            "Epoch 3955, Loss(train/val) 0.22268/0.15071. Took 0.05 sec\n",
            "Epoch 3956, Loss(train/val) 0.22084/0.15092. Took 0.05 sec\n",
            "Epoch 3957, Loss(train/val) 0.22372/0.15095. Took 0.04 sec\n",
            "Epoch 3958, Loss(train/val) 0.22965/0.15134. Took 0.04 sec\n",
            "Epoch 3959, Loss(train/val) 0.21780/0.15210. Took 0.04 sec\n",
            "Epoch 3960, Loss(train/val) 0.22987/0.15165. Took 0.05 sec\n",
            "Epoch 3961, Loss(train/val) 0.22176/0.15112. Took 0.04 sec\n",
            "Epoch 3962, Loss(train/val) 0.22530/0.15087. Took 0.05 sec\n",
            "Epoch 3963, Loss(train/val) 0.23528/0.15090. Took 0.04 sec\n",
            "Epoch 3964, Loss(train/val) 0.22553/0.15091. Took 0.05 sec\n",
            "Epoch 3965, Loss(train/val) 0.21528/0.15096. Took 0.05 sec\n",
            "Epoch 3966, Loss(train/val) 0.22738/0.15112. Took 0.05 sec\n",
            "Epoch 3967, Loss(train/val) 0.22195/0.15179. Took 0.05 sec\n",
            "Epoch 3968, Loss(train/val) 0.21529/0.15318. Took 0.05 sec\n",
            "Epoch 3969, Loss(train/val) 0.23384/0.15187. Took 0.04 sec\n",
            "Epoch 3970, Loss(train/val) 0.23574/0.15129. Took 0.05 sec\n",
            "Epoch 3971, Loss(train/val) 0.21923/0.15115. Took 0.05 sec\n",
            "Epoch 3972, Loss(train/val) 0.25301/0.15099. Took 0.04 sec\n",
            "Epoch 3973, Loss(train/val) 0.21847/0.15090. Took 0.04 sec\n",
            "Epoch 3974, Loss(train/val) 0.22228/0.15096. Took 0.04 sec\n",
            "Epoch 3975, Loss(train/val) 0.22606/0.15095. Took 0.06 sec\n",
            "Epoch 3976, Loss(train/val) 0.21656/0.15093. Took 0.05 sec\n",
            "Epoch 3977, Loss(train/val) 0.21752/0.15094. Took 0.04 sec\n",
            "Epoch 3978, Loss(train/val) 0.22112/0.15083. Took 0.04 sec\n",
            "Epoch 3979, Loss(train/val) 0.22400/0.15099. Took 0.04 sec\n",
            "Epoch 3980, Loss(train/val) 0.21267/0.15110. Took 0.05 sec\n",
            "Epoch 3981, Loss(train/val) 0.21690/0.15102. Took 0.05 sec\n",
            "Epoch 3982, Loss(train/val) 0.21549/0.15098. Took 0.05 sec\n",
            "Epoch 3983, Loss(train/val) 0.21195/0.15095. Took 0.05 sec\n",
            "Epoch 3984, Loss(train/val) 0.23236/0.15119. Took 0.05 sec\n",
            "Epoch 3985, Loss(train/val) 0.23059/0.15103. Took 0.06 sec\n",
            "Epoch 3986, Loss(train/val) 0.21835/0.15083. Took 0.05 sec\n",
            "Epoch 3987, Loss(train/val) 0.21444/0.15103. Took 0.05 sec\n",
            "Epoch 3988, Loss(train/val) 0.22632/0.15083. Took 0.05 sec\n",
            "Epoch 3989, Loss(train/val) 0.22067/0.15093. Took 0.05 sec\n",
            "Epoch 3990, Loss(train/val) 0.22142/0.15085. Took 0.06 sec\n",
            "Epoch 3991, Loss(train/val) 0.22484/0.15092. Took 0.05 sec\n",
            "Epoch 3992, Loss(train/val) 0.21974/0.15088. Took 0.05 sec\n",
            "Epoch 3993, Loss(train/val) 0.22493/0.15081. Took 0.05 sec\n",
            "Epoch 3994, Loss(train/val) 0.22796/0.15111. Took 0.05 sec\n",
            "Epoch 3995, Loss(train/val) 0.21706/0.15117. Took 0.06 sec\n",
            "Epoch 3996, Loss(train/val) 0.23044/0.15091. Took 0.06 sec\n",
            "Epoch 3997, Loss(train/val) 0.24238/0.15120. Took 0.05 sec\n",
            "Epoch 3998, Loss(train/val) 0.23718/0.15184. Took 0.05 sec\n",
            "Epoch 3999, Loss(train/val) 0.22864/0.15163. Took 0.05 sec\n",
            "Epoch 4000, Loss(train/val) 0.23330/0.15188. Took 0.05 sec\n",
            "Epoch 4001, Loss(train/val) 0.21328/0.15137. Took 0.05 sec\n",
            "Epoch 4002, Loss(train/val) 0.21942/0.15118. Took 0.05 sec\n",
            "Epoch 4003, Loss(train/val) 0.21492/0.15087. Took 0.06 sec\n",
            "Epoch 4004, Loss(train/val) 0.22365/0.15143. Took 0.06 sec\n",
            "Epoch 4005, Loss(train/val) 0.23478/0.15130. Took 0.05 sec\n",
            "Epoch 4006, Loss(train/val) 0.22713/0.15138. Took 0.05 sec\n",
            "Epoch 4007, Loss(train/val) 0.21381/0.15076. Took 0.06 sec\n",
            "Epoch 4008, Loss(train/val) 0.21561/0.15071. Took 0.05 sec\n",
            "Epoch 4009, Loss(train/val) 0.21822/0.15066. Took 0.04 sec\n",
            "Epoch 4010, Loss(train/val) 0.22095/0.15079. Took 0.05 sec\n",
            "Epoch 4011, Loss(train/val) 0.22099/0.15143. Took 0.05 sec\n",
            "Epoch 4012, Loss(train/val) 0.23262/0.15231. Took 0.05 sec\n",
            "Epoch 4013, Loss(train/val) 0.22363/0.15187. Took 0.04 sec\n",
            "Epoch 4014, Loss(train/val) 0.21462/0.15132. Took 0.04 sec\n",
            "Epoch 4015, Loss(train/val) 0.21769/0.15136. Took 0.05 sec\n",
            "Epoch 4016, Loss(train/val) 0.22728/0.15155. Took 0.05 sec\n",
            "Epoch 4017, Loss(train/val) 0.22201/0.15080. Took 0.05 sec\n",
            "Epoch 4018, Loss(train/val) 0.22848/0.15111. Took 0.04 sec\n",
            "Epoch 4019, Loss(train/val) 0.23157/0.15166. Took 0.04 sec\n",
            "Epoch 4020, Loss(train/val) 0.21634/0.15162. Took 0.04 sec\n",
            "Epoch 4021, Loss(train/val) 0.22611/0.15085. Took 0.05 sec\n",
            "Epoch 4022, Loss(train/val) 0.23738/0.15077. Took 0.05 sec\n",
            "Epoch 4023, Loss(train/val) 0.25298/0.15081. Took 0.05 sec\n",
            "Epoch 4024, Loss(train/val) 0.21798/0.15085. Took 0.04 sec\n",
            "Epoch 4025, Loss(train/val) 0.23588/0.15119. Took 0.04 sec\n",
            "Epoch 4026, Loss(train/val) 0.21885/0.15115. Took 0.05 sec\n",
            "Epoch 4027, Loss(train/val) 0.21661/0.15083. Took 0.05 sec\n",
            "Epoch 4028, Loss(train/val) 0.22006/0.15104. Took 0.05 sec\n",
            "Epoch 4029, Loss(train/val) 0.23260/0.15147. Took 0.04 sec\n",
            "Epoch 4030, Loss(train/val) 0.22207/0.15120. Took 0.05 sec\n",
            "Epoch 4031, Loss(train/val) 0.21719/0.15175. Took 0.04 sec\n",
            "Epoch 4032, Loss(train/val) 0.23552/0.15090. Took 0.05 sec\n",
            "Epoch 4033, Loss(train/val) 0.23466/0.15129. Took 0.05 sec\n",
            "Epoch 4034, Loss(train/val) 0.22160/0.15073. Took 0.05 sec\n",
            "Epoch 4035, Loss(train/val) 0.21573/0.15094. Took 0.04 sec\n",
            "Epoch 4036, Loss(train/val) 0.21920/0.15110. Took 0.05 sec\n",
            "Epoch 4037, Loss(train/val) 0.23361/0.15089. Took 0.05 sec\n",
            "Epoch 4038, Loss(train/val) 0.23047/0.15090. Took 0.05 sec\n",
            "Epoch 4039, Loss(train/val) 0.21395/0.15070. Took 0.04 sec\n",
            "Epoch 4040, Loss(train/val) 0.23854/0.15074. Took 0.04 sec\n",
            "Epoch 4041, Loss(train/val) 0.22207/0.15116. Took 0.04 sec\n",
            "Epoch 4042, Loss(train/val) 0.22182/0.15169. Took 0.06 sec\n",
            "Epoch 4043, Loss(train/val) 0.22708/0.15181. Took 0.04 sec\n",
            "Epoch 4044, Loss(train/val) 0.22308/0.15106. Took 0.04 sec\n",
            "Epoch 4045, Loss(train/val) 0.22590/0.15068. Took 0.05 sec\n",
            "Epoch 4046, Loss(train/val) 0.22495/0.15115. Took 0.04 sec\n",
            "Epoch 4047, Loss(train/val) 0.22130/0.15132. Took 0.05 sec\n",
            "Epoch 4048, Loss(train/val) 0.22122/0.15132. Took 0.05 sec\n",
            "Epoch 4049, Loss(train/val) 0.21113/0.15098. Took 0.05 sec\n",
            "Epoch 4050, Loss(train/val) 0.22366/0.15106. Took 0.05 sec\n",
            "Epoch 4051, Loss(train/val) 0.22753/0.15105. Took 0.05 sec\n",
            "Epoch 4052, Loss(train/val) 0.22301/0.15111. Took 0.05 sec\n",
            "Epoch 4053, Loss(train/val) 0.24409/0.15138. Took 0.05 sec\n",
            "Epoch 4054, Loss(train/val) 0.22356/0.15095. Took 0.04 sec\n",
            "Epoch 4055, Loss(train/val) 0.21683/0.15089. Took 0.05 sec\n",
            "Epoch 4056, Loss(train/val) 0.23335/0.15098. Took 0.04 sec\n",
            "Epoch 4057, Loss(train/val) 0.22017/0.15109. Took 0.05 sec\n",
            "Epoch 4058, Loss(train/val) 0.22094/0.15095. Took 0.05 sec\n",
            "Epoch 4059, Loss(train/val) 0.22233/0.15092. Took 0.05 sec\n",
            "Epoch 4060, Loss(train/val) 0.22460/0.15091. Took 0.04 sec\n",
            "Epoch 4061, Loss(train/val) 0.22261/0.15073. Took 0.05 sec\n",
            "Epoch 4062, Loss(train/val) 0.21499/0.15076. Took 0.05 sec\n",
            "Epoch 4063, Loss(train/val) 0.22000/0.15071. Took 0.05 sec\n",
            "Epoch 4064, Loss(train/val) 0.23216/0.15122. Took 0.05 sec\n",
            "Epoch 4065, Loss(train/val) 0.22011/0.15168. Took 0.05 sec\n",
            "Epoch 4066, Loss(train/val) 0.22126/0.15148. Took 0.05 sec\n",
            "Epoch 4067, Loss(train/val) 0.21033/0.15088. Took 0.05 sec\n",
            "Epoch 4068, Loss(train/val) 0.22665/0.15109. Took 0.05 sec\n",
            "Epoch 4069, Loss(train/val) 0.22001/0.15106. Took 0.04 sec\n",
            "Epoch 4070, Loss(train/val) 0.21826/0.15102. Took 0.05 sec\n",
            "Epoch 4071, Loss(train/val) 0.22817/0.15142. Took 0.04 sec\n",
            "Epoch 4072, Loss(train/val) 0.21367/0.15153. Took 0.06 sec\n",
            "Epoch 4073, Loss(train/val) 0.22212/0.15138. Took 0.04 sec\n",
            "Epoch 4074, Loss(train/val) 0.22375/0.15085. Took 0.05 sec\n",
            "Epoch 4075, Loss(train/val) 0.21739/0.15088. Took 0.05 sec\n",
            "Epoch 4076, Loss(train/val) 0.22687/0.15098. Took 0.04 sec\n",
            "Epoch 4077, Loss(train/val) 0.22132/0.15089. Took 0.05 sec\n",
            "Epoch 4078, Loss(train/val) 0.24076/0.15105. Took 0.04 sec\n",
            "Epoch 4079, Loss(train/val) 0.21163/0.15128. Took 0.04 sec\n",
            "Epoch 4080, Loss(train/val) 0.22445/0.15164. Took 0.05 sec\n",
            "Epoch 4081, Loss(train/val) 0.24641/0.15117. Took 0.05 sec\n",
            "Epoch 4082, Loss(train/val) 0.22070/0.15076. Took 0.06 sec\n",
            "Epoch 4083, Loss(train/val) 0.22654/0.15073. Took 0.05 sec\n",
            "Epoch 4084, Loss(train/val) 0.21825/0.15077. Took 0.05 sec\n",
            "Epoch 4085, Loss(train/val) 0.22152/0.15081. Took 0.05 sec\n",
            "Epoch 4086, Loss(train/val) 0.21329/0.15071. Took 0.05 sec\n",
            "Epoch 4087, Loss(train/val) 0.21593/0.15117. Took 0.06 sec\n",
            "Epoch 4088, Loss(train/val) 0.22634/0.15087. Took 0.05 sec\n",
            "Epoch 4089, Loss(train/val) 0.21560/0.15090. Took 0.05 sec\n",
            "Epoch 4090, Loss(train/val) 0.22291/0.15068. Took 0.05 sec\n",
            "Epoch 4091, Loss(train/val) 0.23642/0.15058. Took 0.05 sec\n",
            "Epoch 4092, Loss(train/val) 0.22979/0.15057. Took 0.06 sec\n",
            "Epoch 4093, Loss(train/val) 0.22006/0.15056. Took 0.05 sec\n",
            "Epoch 4094, Loss(train/val) 0.22123/0.15086. Took 0.05 sec\n",
            "Epoch 4095, Loss(train/val) 0.22463/0.15077. Took 0.05 sec\n",
            "Epoch 4096, Loss(train/val) 0.21630/0.15072. Took 0.04 sec\n",
            "Epoch 4097, Loss(train/val) 0.21388/0.15060. Took 0.05 sec\n",
            "Epoch 4098, Loss(train/val) 0.20914/0.15049. Took 0.05 sec\n",
            "Epoch 4099, Loss(train/val) 0.21998/0.15060. Took 0.05 sec\n",
            "Epoch 4100, Loss(train/val) 0.21701/0.15076. Took 0.05 sec\n",
            "Epoch 4101, Loss(train/val) 0.22400/0.15081. Took 0.06 sec\n",
            "Epoch 4102, Loss(train/val) 0.21527/0.15069. Took 0.05 sec\n",
            "Epoch 4103, Loss(train/val) 0.21985/0.15090. Took 0.05 sec\n",
            "Epoch 4104, Loss(train/val) 0.21614/0.15069. Took 0.05 sec\n",
            "Epoch 4105, Loss(train/val) 0.22149/0.15069. Took 0.05 sec\n",
            "Epoch 4106, Loss(train/val) 0.21588/0.15069. Took 0.05 sec\n",
            "Epoch 4107, Loss(train/val) 0.22349/0.15072. Took 0.05 sec\n",
            "Epoch 4108, Loss(train/val) 0.22991/0.15065. Took 0.04 sec\n",
            "Epoch 4109, Loss(train/val) 0.21521/0.15070. Took 0.05 sec\n",
            "Epoch 4110, Loss(train/val) 0.21573/0.15070. Took 0.04 sec\n",
            "Epoch 4111, Loss(train/val) 0.21980/0.15055. Took 0.05 sec\n",
            "Epoch 4112, Loss(train/val) 0.21477/0.15049. Took 0.05 sec\n",
            "Epoch 4113, Loss(train/val) 0.21627/0.15057. Took 0.04 sec\n",
            "Epoch 4114, Loss(train/val) 0.21561/0.15072. Took 0.04 sec\n",
            "Epoch 4115, Loss(train/val) 0.22023/0.15082. Took 0.05 sec\n",
            "Epoch 4116, Loss(train/val) 0.21689/0.15075. Took 0.04 sec\n",
            "Epoch 4117, Loss(train/val) 0.21530/0.15080. Took 0.05 sec\n",
            "Epoch 4118, Loss(train/val) 0.21855/0.15089. Took 0.05 sec\n",
            "Epoch 4119, Loss(train/val) 0.21904/0.15086. Took 0.04 sec\n",
            "Epoch 4120, Loss(train/val) 0.21392/0.15086. Took 0.05 sec\n",
            "Epoch 4121, Loss(train/val) 0.21512/0.15081. Took 0.05 sec\n",
            "Epoch 4122, Loss(train/val) 0.22658/0.15082. Took 0.06 sec\n",
            "Epoch 4123, Loss(train/val) 0.22577/0.15119. Took 0.05 sec\n",
            "Epoch 4124, Loss(train/val) 0.21835/0.15080. Took 0.04 sec\n",
            "Epoch 4125, Loss(train/val) 0.22578/0.15080. Took 0.05 sec\n",
            "Epoch 4126, Loss(train/val) 0.23018/0.15071. Took 0.04 sec\n",
            "Epoch 4127, Loss(train/val) 0.22825/0.15058. Took 0.05 sec\n",
            "Epoch 4128, Loss(train/val) 0.21849/0.15087. Took 0.05 sec\n",
            "Epoch 4129, Loss(train/val) 0.22114/0.15075. Took 0.04 sec\n",
            "Epoch 4130, Loss(train/val) 0.23602/0.15069. Took 0.05 sec\n",
            "Epoch 4131, Loss(train/val) 0.21552/0.15081. Took 0.05 sec\n",
            "Epoch 4132, Loss(train/val) 0.24427/0.15096. Took 0.05 sec\n",
            "Epoch 4133, Loss(train/val) 0.21585/0.15077. Took 0.04 sec\n",
            "Epoch 4134, Loss(train/val) 0.21591/0.15084. Took 0.05 sec\n",
            "Epoch 4135, Loss(train/val) 0.21568/0.15091. Took 0.05 sec\n",
            "Epoch 4136, Loss(train/val) 0.21801/0.15079. Took 0.05 sec\n",
            "Epoch 4137, Loss(train/val) 0.21663/0.15074. Took 0.06 sec\n",
            "Epoch 4138, Loss(train/val) 0.22441/0.15085. Took 0.04 sec\n",
            "Epoch 4139, Loss(train/val) 0.21581/0.15150. Took 0.04 sec\n",
            "Epoch 4140, Loss(train/val) 0.21567/0.15114. Took 0.04 sec\n",
            "Epoch 4141, Loss(train/val) 0.22276/0.15134. Took 0.05 sec\n",
            "Epoch 4142, Loss(train/val) 0.22196/0.15092. Took 0.05 sec\n",
            "Epoch 4143, Loss(train/val) 0.21797/0.15083. Took 0.05 sec\n",
            "Epoch 4144, Loss(train/val) 0.22409/0.15076. Took 0.05 sec\n",
            "Epoch 4145, Loss(train/val) 0.21495/0.15092. Took 0.04 sec\n",
            "Epoch 4146, Loss(train/val) 0.23551/0.15169. Took 0.05 sec\n",
            "Epoch 4147, Loss(train/val) 0.22230/0.15101. Took 0.06 sec\n",
            "Epoch 4148, Loss(train/val) 0.21510/0.15091. Took 0.05 sec\n",
            "Epoch 4149, Loss(train/val) 0.21501/0.15096. Took 0.05 sec\n",
            "Epoch 4150, Loss(train/val) 0.22678/0.15098. Took 0.04 sec\n",
            "Epoch 4151, Loss(train/val) 0.22603/0.15098. Took 0.05 sec\n",
            "Epoch 4152, Loss(train/val) 0.22582/0.15100. Took 0.06 sec\n",
            "Epoch 4153, Loss(train/val) 0.24065/0.15159. Took 0.05 sec\n",
            "Epoch 4154, Loss(train/val) 0.22915/0.15145. Took 0.05 sec\n",
            "Epoch 4155, Loss(train/val) 0.21310/0.15172. Took 0.05 sec\n",
            "Epoch 4156, Loss(train/val) 0.22109/0.15155. Took 0.05 sec\n",
            "Epoch 4157, Loss(train/val) 0.21815/0.15089. Took 0.06 sec\n",
            "Epoch 4158, Loss(train/val) 0.21655/0.15076. Took 0.04 sec\n",
            "Epoch 4159, Loss(train/val) 0.21734/0.15116. Took 0.04 sec\n",
            "Epoch 4160, Loss(train/val) 0.22081/0.15165. Took 0.05 sec\n",
            "Epoch 4161, Loss(train/val) 0.22424/0.15121. Took 0.05 sec\n",
            "Epoch 4162, Loss(train/val) 0.21800/0.15103. Took 0.05 sec\n",
            "Epoch 4163, Loss(train/val) 0.21089/0.15069. Took 0.04 sec\n",
            "Epoch 4164, Loss(train/val) 0.23196/0.15113. Took 0.05 sec\n",
            "Epoch 4165, Loss(train/val) 0.21204/0.15182. Took 0.05 sec\n",
            "Epoch 4166, Loss(train/val) 0.22312/0.15150. Took 0.04 sec\n",
            "Epoch 4167, Loss(train/val) 0.22321/0.15145. Took 0.05 sec\n",
            "Epoch 4168, Loss(train/val) 0.24146/0.15094. Took 0.06 sec\n",
            "Epoch 4169, Loss(train/val) 0.21551/0.15090. Took 0.04 sec\n",
            "Epoch 4170, Loss(train/val) 0.22769/0.15068. Took 0.04 sec\n",
            "Epoch 4171, Loss(train/val) 0.21497/0.15054. Took 0.05 sec\n",
            "Epoch 4172, Loss(train/val) 0.22422/0.15046. Took 0.06 sec\n",
            "Epoch 4173, Loss(train/val) 0.21329/0.15063. Took 0.05 sec\n",
            "Epoch 4174, Loss(train/val) 0.22142/0.15065. Took 0.05 sec\n",
            "Epoch 4175, Loss(train/val) 0.22187/0.15064. Took 0.05 sec\n",
            "Epoch 4176, Loss(train/val) 0.22559/0.15072. Took 0.05 sec\n",
            "Epoch 4177, Loss(train/val) 0.21796/0.15067. Took 0.06 sec\n",
            "Epoch 4178, Loss(train/val) 0.22970/0.15075. Took 0.05 sec\n",
            "Epoch 4179, Loss(train/val) 0.22309/0.15070. Took 0.05 sec\n",
            "Epoch 4180, Loss(train/val) 0.21261/0.15076. Took 0.04 sec\n",
            "Epoch 4181, Loss(train/val) 0.22562/0.15158. Took 0.05 sec\n",
            "Epoch 4182, Loss(train/val) 0.21440/0.15210. Took 0.05 sec\n",
            "Epoch 4183, Loss(train/val) 0.23313/0.15188. Took 0.05 sec\n",
            "Epoch 4184, Loss(train/val) 0.22971/0.15129. Took 0.05 sec\n",
            "Epoch 4185, Loss(train/val) 0.22822/0.15079. Took 0.05 sec\n",
            "Epoch 4186, Loss(train/val) 0.22188/0.15096. Took 0.05 sec\n",
            "Epoch 4187, Loss(train/val) 0.24061/0.15071. Took 0.05 sec\n",
            "Epoch 4188, Loss(train/val) 0.22616/0.15068. Took 0.04 sec\n",
            "Epoch 4189, Loss(train/val) 0.21937/0.15142. Took 0.05 sec\n",
            "Epoch 4190, Loss(train/val) 0.22392/0.15177. Took 0.05 sec\n",
            "Epoch 4191, Loss(train/val) 0.22489/0.15125. Took 0.05 sec\n",
            "Epoch 4192, Loss(train/val) 0.22730/0.15119. Took 0.05 sec\n",
            "Epoch 4193, Loss(train/val) 0.22077/0.15136. Took 0.05 sec\n",
            "Epoch 4194, Loss(train/val) 0.22465/0.15131. Took 0.05 sec\n",
            "Epoch 4195, Loss(train/val) 0.21437/0.15154. Took 0.05 sec\n",
            "Epoch 4196, Loss(train/val) 0.21349/0.15108. Took 0.04 sec\n",
            "Epoch 4197, Loss(train/val) 0.22493/0.15072. Took 0.05 sec\n",
            "Epoch 4198, Loss(train/val) 0.23353/0.15055. Took 0.05 sec\n",
            "Epoch 4199, Loss(train/val) 0.22003/0.15045. Took 0.04 sec\n",
            "Epoch 4200, Loss(train/val) 0.22059/0.15067. Took 0.05 sec\n",
            "Epoch 4201, Loss(train/val) 0.22534/0.15067. Took 0.04 sec\n",
            "Epoch 4202, Loss(train/val) 0.21492/0.15082. Took 0.05 sec\n",
            "Epoch 4203, Loss(train/val) 0.21734/0.15083. Took 0.05 sec\n",
            "Epoch 4204, Loss(train/val) 0.22452/0.15106. Took 0.05 sec\n",
            "Epoch 4205, Loss(train/val) 0.22837/0.15137. Took 0.05 sec\n",
            "Epoch 4206, Loss(train/val) 0.22005/0.15189. Took 0.05 sec\n",
            "Epoch 4207, Loss(train/val) 0.21399/0.15153. Took 0.06 sec\n",
            "Epoch 4208, Loss(train/val) 0.21777/0.15083. Took 0.05 sec\n",
            "Epoch 4209, Loss(train/val) 0.22214/0.15060. Took 0.05 sec\n",
            "Epoch 4210, Loss(train/val) 0.21973/0.15059. Took 0.04 sec\n",
            "Epoch 4211, Loss(train/val) 0.22205/0.15135. Took 0.04 sec\n",
            "Epoch 4212, Loss(train/val) 0.21992/0.15177. Took 0.05 sec\n",
            "Epoch 4213, Loss(train/val) 0.21042/0.15153. Took 0.05 sec\n",
            "Epoch 4214, Loss(train/val) 0.21622/0.15112. Took 0.05 sec\n",
            "Epoch 4215, Loss(train/val) 0.21731/0.15058. Took 0.05 sec\n",
            "Epoch 4216, Loss(train/val) 0.21910/0.15058. Took 0.05 sec\n",
            "Epoch 4217, Loss(train/val) 0.22395/0.15102. Took 0.05 sec\n",
            "Epoch 4218, Loss(train/val) 0.23344/0.15158. Took 0.05 sec\n",
            "Epoch 4219, Loss(train/val) 0.21402/0.15134. Took 0.05 sec\n",
            "Epoch 4220, Loss(train/val) 0.22223/0.15064. Took 0.05 sec\n",
            "Epoch 4221, Loss(train/val) 0.21240/0.15071. Took 0.05 sec\n",
            "Epoch 4222, Loss(train/val) 0.22464/0.15071. Took 0.05 sec\n",
            "Epoch 4223, Loss(train/val) 0.23480/0.15094. Took 0.05 sec\n",
            "Epoch 4224, Loss(train/val) 0.21926/0.15100. Took 0.05 sec\n",
            "Epoch 4225, Loss(train/val) 0.22455/0.15049. Took 0.04 sec\n",
            "Epoch 4226, Loss(train/val) 0.23142/0.15048. Took 0.04 sec\n",
            "Epoch 4227, Loss(train/val) 0.21362/0.15058. Took 0.05 sec\n",
            "Epoch 4228, Loss(train/val) 0.22009/0.15071. Took 0.06 sec\n",
            "Epoch 4229, Loss(train/val) 0.23341/0.15092. Took 0.05 sec\n",
            "Epoch 4230, Loss(train/val) 0.23005/0.15052. Took 0.04 sec\n",
            "Epoch 4231, Loss(train/val) 0.22048/0.15081. Took 0.05 sec\n",
            "Epoch 4232, Loss(train/val) 0.22521/0.15078. Took 0.05 sec\n",
            "Epoch 4233, Loss(train/val) 0.24025/0.15069. Took 0.05 sec\n",
            "Epoch 4234, Loss(train/val) 0.22704/0.15081. Took 0.05 sec\n",
            "Epoch 4235, Loss(train/val) 0.22978/0.15074. Took 0.05 sec\n",
            "Epoch 4236, Loss(train/val) 0.21331/0.15093. Took 0.05 sec\n",
            "Epoch 4237, Loss(train/val) 0.21909/0.15107. Took 0.06 sec\n",
            "Epoch 4238, Loss(train/val) 0.21880/0.15108. Took 0.05 sec\n",
            "Epoch 4239, Loss(train/val) 0.22197/0.15076. Took 0.05 sec\n",
            "Epoch 4240, Loss(train/val) 0.24729/0.15081. Took 0.05 sec\n",
            "Epoch 4241, Loss(train/val) 0.25925/0.15099. Took 0.05 sec\n",
            "Epoch 4242, Loss(train/val) 0.23426/0.15092. Took 0.05 sec\n",
            "Epoch 4243, Loss(train/val) 0.22925/0.15059. Took 0.04 sec\n",
            "Epoch 4244, Loss(train/val) 0.21563/0.15050. Took 0.05 sec\n",
            "Epoch 4245, Loss(train/val) 0.21643/0.15109. Took 0.05 sec\n",
            "Epoch 4246, Loss(train/val) 0.21553/0.15098. Took 0.05 sec\n",
            "Epoch 4247, Loss(train/val) 0.21842/0.15063. Took 0.05 sec\n",
            "Epoch 4248, Loss(train/val) 0.22481/0.15055. Took 0.04 sec\n",
            "Epoch 4249, Loss(train/val) 0.22014/0.15042. Took 0.06 sec\n",
            "Epoch 4250, Loss(train/val) 0.21842/0.15045. Took 0.05 sec\n",
            "Epoch 4251, Loss(train/val) 0.22492/0.15045. Took 0.05 sec\n",
            "Epoch 4252, Loss(train/val) 0.22348/0.15048. Took 0.05 sec\n",
            "Epoch 4253, Loss(train/val) 0.22090/0.15041. Took 0.05 sec\n",
            "Epoch 4254, Loss(train/val) 0.23302/0.15058. Took 0.05 sec\n",
            "Epoch 4255, Loss(train/val) 0.22848/0.15132. Took 0.05 sec\n",
            "Epoch 4256, Loss(train/val) 0.23428/0.15146. Took 0.04 sec\n",
            "Epoch 4257, Loss(train/val) 0.23174/0.15079. Took 0.05 sec\n",
            "Epoch 4258, Loss(train/val) 0.20895/0.15091. Took 0.05 sec\n",
            "Epoch 4259, Loss(train/val) 0.21633/0.15164. Took 0.04 sec\n",
            "Epoch 4260, Loss(train/val) 0.22001/0.15133. Took 0.05 sec\n",
            "Epoch 4261, Loss(train/val) 0.22782/0.15074. Took 0.04 sec\n",
            "Epoch 4262, Loss(train/val) 0.22149/0.15057. Took 0.06 sec\n",
            "Epoch 4263, Loss(train/val) 0.21779/0.15082. Took 0.04 sec\n",
            "Epoch 4264, Loss(train/val) 0.22572/0.15110. Took 0.04 sec\n",
            "Epoch 4265, Loss(train/val) 0.21766/0.15085. Took 0.05 sec\n",
            "Epoch 4266, Loss(train/val) 0.23313/0.15075. Took 0.05 sec\n",
            "Epoch 4267, Loss(train/val) 0.22646/0.15084. Took 0.05 sec\n",
            "Epoch 4268, Loss(train/val) 0.21873/0.15075. Took 0.05 sec\n",
            "Epoch 4269, Loss(train/val) 0.22142/0.15071. Took 0.05 sec\n",
            "Epoch 4270, Loss(train/val) 0.22055/0.15060. Took 0.05 sec\n",
            "Epoch 4271, Loss(train/val) 0.21825/0.15084. Took 0.05 sec\n",
            "Epoch 4272, Loss(train/val) 0.21748/0.15073. Took 0.06 sec\n",
            "Epoch 4273, Loss(train/val) 0.22478/0.15057. Took 0.05 sec\n",
            "Epoch 4274, Loss(train/val) 0.22187/0.15058. Took 0.04 sec\n",
            "Epoch 4275, Loss(train/val) 0.21902/0.15072. Took 0.04 sec\n",
            "Epoch 4276, Loss(train/val) 0.22468/0.15080. Took 0.04 sec\n",
            "Epoch 4277, Loss(train/val) 0.21259/0.15081. Took 0.05 sec\n",
            "Epoch 4278, Loss(train/val) 0.22790/0.15051. Took 0.05 sec\n",
            "Epoch 4279, Loss(train/val) 0.22023/0.15041. Took 0.04 sec\n",
            "Epoch 4280, Loss(train/val) 0.24851/0.15049. Took 0.05 sec\n",
            "Epoch 4281, Loss(train/val) 0.22172/0.15107. Took 0.05 sec\n",
            "Epoch 4282, Loss(train/val) 0.22816/0.15235. Took 0.06 sec\n",
            "Epoch 4283, Loss(train/val) 0.21748/0.15222. Took 0.05 sec\n",
            "Epoch 4284, Loss(train/val) 0.22861/0.15169. Took 0.04 sec\n",
            "Epoch 4285, Loss(train/val) 0.22512/0.15186. Took 0.05 sec\n",
            "Epoch 4286, Loss(train/val) 0.21702/0.15077. Took 0.05 sec\n",
            "Epoch 4287, Loss(train/val) 0.21462/0.15045. Took 0.05 sec\n",
            "Epoch 4288, Loss(train/val) 0.22279/0.15045. Took 0.05 sec\n",
            "Epoch 4289, Loss(train/val) 0.23459/0.15042. Took 0.04 sec\n",
            "Epoch 4290, Loss(train/val) 0.23062/0.15056. Took 0.05 sec\n",
            "Epoch 4291, Loss(train/val) 0.21404/0.15058. Took 0.06 sec\n",
            "Epoch 4292, Loss(train/val) 0.22702/0.15068. Took 0.05 sec\n",
            "Epoch 4293, Loss(train/val) 0.22923/0.15099. Took 0.05 sec\n",
            "Epoch 4294, Loss(train/val) 0.22791/0.15065. Took 0.04 sec\n",
            "Epoch 4295, Loss(train/val) 0.21865/0.15050. Took 0.04 sec\n",
            "Epoch 4296, Loss(train/val) 0.21576/0.15045. Took 0.05 sec\n",
            "Epoch 4297, Loss(train/val) 0.21627/0.15105. Took 0.05 sec\n",
            "Epoch 4298, Loss(train/val) 0.22377/0.15100. Took 0.05 sec\n",
            "Epoch 4299, Loss(train/val) 0.21288/0.15075. Took 0.04 sec\n",
            "Epoch 4300, Loss(train/val) 0.21874/0.15124. Took 0.04 sec\n",
            "Epoch 4301, Loss(train/val) 0.21737/0.15106. Took 0.05 sec\n",
            "Epoch 4302, Loss(train/val) 0.21407/0.15099. Took 0.05 sec\n",
            "Epoch 4303, Loss(train/val) 0.21977/0.15088. Took 0.04 sec\n",
            "Epoch 4304, Loss(train/val) 0.22381/0.15066. Took 0.04 sec\n",
            "Epoch 4305, Loss(train/val) 0.20816/0.15083. Took 0.04 sec\n",
            "Epoch 4306, Loss(train/val) 0.22245/0.15185. Took 0.05 sec\n",
            "Epoch 4307, Loss(train/val) 0.21719/0.15117. Took 0.06 sec\n",
            "Epoch 4308, Loss(train/val) 0.22717/0.15101. Took 0.05 sec\n",
            "Epoch 4309, Loss(train/val) 0.22243/0.15087. Took 0.04 sec\n",
            "Epoch 4310, Loss(train/val) 0.24849/0.15082. Took 0.05 sec\n",
            "Epoch 4311, Loss(train/val) 0.25814/0.15135. Took 0.04 sec\n",
            "Epoch 4312, Loss(train/val) 0.22242/0.15107. Took 0.06 sec\n",
            "Epoch 4313, Loss(train/val) 0.22323/0.15132. Took 0.05 sec\n",
            "Epoch 4314, Loss(train/val) 0.22327/0.15197. Took 0.05 sec\n",
            "Epoch 4315, Loss(train/val) 0.24221/0.15431. Took 0.05 sec\n",
            "Epoch 4316, Loss(train/val) 0.21844/0.15316. Took 0.05 sec\n",
            "Epoch 4317, Loss(train/val) 0.22090/0.15308. Took 0.05 sec\n",
            "Epoch 4318, Loss(train/val) 0.22102/0.15122. Took 0.05 sec\n",
            "Epoch 4319, Loss(train/val) 0.21684/0.15069. Took 0.05 sec\n",
            "Epoch 4320, Loss(train/val) 0.22994/0.15149. Took 0.05 sec\n",
            "Epoch 4321, Loss(train/val) 0.21952/0.15208. Took 0.05 sec\n",
            "Epoch 4322, Loss(train/val) 0.23640/0.15239. Took 0.06 sec\n",
            "Epoch 4323, Loss(train/val) 0.21427/0.15149. Took 0.05 sec\n",
            "Epoch 4324, Loss(train/val) 0.23764/0.15095. Took 0.05 sec\n",
            "Epoch 4325, Loss(train/val) 0.23662/0.15097. Took 0.05 sec\n",
            "Epoch 4326, Loss(train/val) 0.21362/0.15108. Took 0.05 sec\n",
            "Epoch 4327, Loss(train/val) 0.23602/0.15093. Took 0.05 sec\n",
            "Epoch 4328, Loss(train/val) 0.22242/0.15060. Took 0.05 sec\n",
            "Epoch 4329, Loss(train/val) 0.21447/0.15040. Took 0.05 sec\n",
            "Epoch 4330, Loss(train/val) 0.21030/0.15071. Took 0.04 sec\n",
            "Epoch 4331, Loss(train/val) 0.22392/0.15061. Took 0.05 sec\n",
            "Epoch 4332, Loss(train/val) 0.22676/0.15054. Took 0.06 sec\n",
            "Epoch 4333, Loss(train/val) 0.21503/0.15054. Took 0.06 sec\n",
            "Epoch 4334, Loss(train/val) 0.21592/0.15056. Took 0.05 sec\n",
            "Epoch 4335, Loss(train/val) 0.22674/0.15080. Took 0.05 sec\n",
            "Epoch 4336, Loss(train/val) 0.22455/0.15169. Took 0.04 sec\n",
            "Epoch 4337, Loss(train/val) 0.21457/0.15238. Took 0.05 sec\n",
            "Epoch 4338, Loss(train/val) 0.21187/0.15093. Took 0.05 sec\n",
            "Epoch 4339, Loss(train/val) 0.21913/0.15059. Took 0.04 sec\n",
            "Epoch 4340, Loss(train/val) 0.21642/0.15120. Took 0.05 sec\n",
            "Epoch 4341, Loss(train/val) 0.22155/0.15158. Took 0.04 sec\n",
            "Epoch 4342, Loss(train/val) 0.24264/0.15155. Took 0.05 sec\n",
            "Epoch 4343, Loss(train/val) 0.23319/0.15198. Took 0.05 sec\n",
            "Epoch 4344, Loss(train/val) 0.21841/0.15103. Took 0.05 sec\n",
            "Epoch 4345, Loss(train/val) 0.22043/0.15069. Took 0.05 sec\n",
            "Epoch 4346, Loss(train/val) 0.23932/0.15056. Took 0.04 sec\n",
            "Epoch 4347, Loss(train/val) 0.21532/0.15100. Took 0.05 sec\n",
            "Epoch 4348, Loss(train/val) 0.21782/0.15098. Took 0.04 sec\n",
            "Epoch 4349, Loss(train/val) 0.22996/0.15114. Took 0.05 sec\n",
            "Epoch 4350, Loss(train/val) 0.21019/0.15163. Took 0.04 sec\n",
            "Epoch 4351, Loss(train/val) 0.21706/0.15213. Took 0.04 sec\n",
            "Epoch 4352, Loss(train/val) 0.20897/0.15122. Took 0.05 sec\n",
            "Epoch 4353, Loss(train/val) 0.21183/0.15116. Took 0.05 sec\n",
            "Epoch 4354, Loss(train/val) 0.24330/0.15128. Took 0.05 sec\n",
            "Epoch 4355, Loss(train/val) 0.22223/0.15141. Took 0.05 sec\n",
            "Epoch 4356, Loss(train/val) 0.23347/0.15150. Took 0.05 sec\n",
            "Epoch 4357, Loss(train/val) 0.23339/0.15212. Took 0.05 sec\n",
            "Epoch 4358, Loss(train/val) 0.22659/0.15211. Took 0.05 sec\n",
            "Epoch 4359, Loss(train/val) 0.23627/0.15219. Took 0.04 sec\n",
            "Epoch 4360, Loss(train/val) 0.22437/0.15205. Took 0.05 sec\n",
            "Epoch 4361, Loss(train/val) 0.23642/0.15233. Took 0.05 sec\n",
            "Epoch 4362, Loss(train/val) 0.21694/0.15109. Took 0.05 sec\n",
            "Epoch 4363, Loss(train/val) 0.23625/0.15078. Took 0.05 sec\n",
            "Epoch 4364, Loss(train/val) 0.23650/0.15086. Took 0.05 sec\n",
            "Epoch 4365, Loss(train/val) 0.21719/0.15122. Took 0.05 sec\n",
            "Epoch 4366, Loss(train/val) 0.22407/0.15093. Took 0.05 sec\n",
            "Epoch 4367, Loss(train/val) 0.21976/0.15094. Took 0.05 sec\n",
            "Epoch 4368, Loss(train/val) 0.22790/0.15094. Took 0.05 sec\n",
            "Epoch 4369, Loss(train/val) 0.22823/0.15061. Took 0.05 sec\n",
            "Epoch 4370, Loss(train/val) 0.21474/0.15050. Took 0.05 sec\n",
            "Epoch 4371, Loss(train/val) 0.22326/0.15043. Took 0.05 sec\n",
            "Epoch 4372, Loss(train/val) 0.22293/0.15085. Took 0.06 sec\n",
            "Epoch 4373, Loss(train/val) 0.22721/0.15056. Took 0.04 sec\n",
            "Epoch 4374, Loss(train/val) 0.22467/0.15041. Took 0.05 sec\n",
            "Epoch 4375, Loss(train/val) 0.21971/0.15056. Took 0.05 sec\n",
            "Epoch 4376, Loss(train/val) 0.21983/0.15073. Took 0.05 sec\n",
            "Epoch 4377, Loss(train/val) 0.21378/0.15084. Took 0.05 sec\n",
            "Epoch 4378, Loss(train/val) 0.22614/0.15079. Took 0.05 sec\n",
            "Epoch 4379, Loss(train/val) 0.21914/0.15052. Took 0.05 sec\n",
            "Epoch 4380, Loss(train/val) 0.21464/0.15032. Took 0.04 sec\n",
            "Epoch 4381, Loss(train/val) 0.22390/0.15026. Took 0.04 sec\n",
            "Epoch 4382, Loss(train/val) 0.23883/0.15055. Took 0.05 sec\n",
            "Epoch 4383, Loss(train/val) 0.21201/0.15067. Took 0.05 sec\n",
            "Epoch 4384, Loss(train/val) 0.21834/0.15070. Took 0.05 sec\n",
            "Epoch 4385, Loss(train/val) 0.21277/0.15087. Took 0.05 sec\n",
            "Epoch 4386, Loss(train/val) 0.21469/0.15066. Took 0.04 sec\n",
            "Epoch 4387, Loss(train/val) 0.21306/0.15033. Took 0.05 sec\n",
            "Epoch 4388, Loss(train/val) 0.23084/0.15079. Took 0.05 sec\n",
            "Epoch 4389, Loss(train/val) 0.20920/0.15150. Took 0.05 sec\n",
            "Epoch 4390, Loss(train/val) 0.22168/0.15117. Took 0.05 sec\n",
            "Epoch 4391, Loss(train/val) 0.22645/0.15073. Took 0.05 sec\n",
            "Epoch 4392, Loss(train/val) 0.21311/0.15058. Took 0.05 sec\n",
            "Epoch 4393, Loss(train/val) 0.22364/0.15069. Took 0.05 sec\n",
            "Epoch 4394, Loss(train/val) 0.21517/0.15068. Took 0.04 sec\n",
            "Epoch 4395, Loss(train/val) 0.23658/0.15061. Took 0.05 sec\n",
            "Epoch 4396, Loss(train/val) 0.22128/0.15054. Took 0.04 sec\n",
            "Epoch 4397, Loss(train/val) 0.21350/0.15052. Took 0.06 sec\n",
            "Epoch 4398, Loss(train/val) 0.22486/0.15056. Took 0.05 sec\n",
            "Epoch 4399, Loss(train/val) 0.22103/0.15056. Took 0.05 sec\n",
            "Epoch 4400, Loss(train/val) 0.21773/0.15046. Took 0.05 sec\n",
            "Epoch 4401, Loss(train/val) 0.21471/0.15057. Took 0.05 sec\n",
            "Epoch 4402, Loss(train/val) 0.22708/0.15069. Took 0.05 sec\n",
            "Epoch 4403, Loss(train/val) 0.22353/0.15080. Took 0.05 sec\n",
            "Epoch 4404, Loss(train/val) 0.23339/0.15056. Took 0.05 sec\n",
            "Epoch 4405, Loss(train/val) 0.22055/0.15073. Took 0.04 sec\n",
            "Epoch 4406, Loss(train/val) 0.21648/0.15077. Took 0.05 sec\n",
            "Epoch 4407, Loss(train/val) 0.22197/0.15042. Took 0.05 sec\n",
            "Epoch 4408, Loss(train/val) 0.21965/0.15087. Took 0.05 sec\n",
            "Epoch 4409, Loss(train/val) 0.21556/0.15075. Took 0.04 sec\n",
            "Epoch 4410, Loss(train/val) 0.22405/0.15086. Took 0.04 sec\n",
            "Epoch 4411, Loss(train/val) 0.21995/0.15060. Took 0.05 sec\n",
            "Epoch 4412, Loss(train/val) 0.22422/0.15025. Took 0.05 sec\n",
            "Epoch 4413, Loss(train/val) 0.21947/0.15023. Took 0.05 sec\n",
            "Epoch 4414, Loss(train/val) 0.21635/0.15020. Took 0.05 sec\n",
            "Epoch 4415, Loss(train/val) 0.22125/0.15029. Took 0.05 sec\n",
            "Epoch 4416, Loss(train/val) 0.24436/0.15076. Took 0.05 sec\n",
            "Epoch 4417, Loss(train/val) 0.21689/0.15142. Took 0.06 sec\n",
            "Epoch 4418, Loss(train/val) 0.21404/0.15083. Took 0.05 sec\n",
            "Epoch 4419, Loss(train/val) 0.22440/0.15007. Took 0.05 sec\n",
            "Epoch 4420, Loss(train/val) 0.21801/0.15019. Took 0.05 sec\n",
            "Epoch 4421, Loss(train/val) 0.23511/0.15026. Took 0.05 sec\n",
            "Epoch 4422, Loss(train/val) 0.21470/0.15037. Took 0.06 sec\n",
            "Epoch 4423, Loss(train/val) 0.23744/0.15067. Took 0.05 sec\n",
            "Epoch 4424, Loss(train/val) 0.22356/0.15041. Took 0.05 sec\n",
            "Epoch 4425, Loss(train/val) 0.21923/0.15045. Took 0.05 sec\n",
            "Epoch 4426, Loss(train/val) 0.21932/0.15037. Took 0.06 sec\n",
            "Epoch 4427, Loss(train/val) 0.21333/0.15106. Took 0.05 sec\n",
            "Epoch 4428, Loss(train/val) 0.22557/0.15045. Took 0.05 sec\n",
            "Epoch 4429, Loss(train/val) 0.22230/0.15053. Took 0.05 sec\n",
            "Epoch 4430, Loss(train/val) 0.21444/0.15110. Took 0.05 sec\n",
            "Epoch 4431, Loss(train/val) 0.21349/0.15109. Took 0.06 sec\n",
            "Epoch 4432, Loss(train/val) 0.23322/0.15086. Took 0.05 sec\n",
            "Epoch 4433, Loss(train/val) 0.24612/0.15092. Took 0.04 sec\n",
            "Epoch 4434, Loss(train/val) 0.21843/0.15151. Took 0.04 sec\n",
            "Epoch 4435, Loss(train/val) 0.23509/0.15145. Took 0.04 sec\n",
            "Epoch 4436, Loss(train/val) 0.21469/0.15112. Took 0.05 sec\n",
            "Epoch 4437, Loss(train/val) 0.21279/0.15076. Took 0.04 sec\n",
            "Epoch 4438, Loss(train/val) 0.23630/0.15087. Took 0.05 sec\n",
            "Epoch 4439, Loss(train/val) 0.21568/0.15073. Took 0.05 sec\n",
            "Epoch 4440, Loss(train/val) 0.21717/0.15055. Took 0.06 sec\n",
            "Epoch 4441, Loss(train/val) 0.24161/0.15057. Took 0.05 sec\n",
            "Epoch 4442, Loss(train/val) 0.22554/0.15052. Took 0.05 sec\n",
            "Epoch 4443, Loss(train/val) 0.21395/0.15042. Took 0.05 sec\n",
            "Epoch 4444, Loss(train/val) 0.22485/0.15050. Took 0.05 sec\n",
            "Epoch 4445, Loss(train/val) 0.23480/0.15042. Took 0.05 sec\n",
            "Epoch 4446, Loss(train/val) 0.22191/0.15050. Took 0.06 sec\n",
            "Epoch 4447, Loss(train/val) 0.21721/0.15116. Took 0.05 sec\n",
            "Epoch 4448, Loss(train/val) 0.21581/0.15048. Took 0.04 sec\n",
            "Epoch 4449, Loss(train/val) 0.22359/0.15049. Took 0.04 sec\n",
            "Epoch 4450, Loss(train/val) 0.23636/0.15057. Took 0.05 sec\n",
            "Epoch 4451, Loss(train/val) 0.23301/0.15091. Took 0.06 sec\n",
            "Epoch 4452, Loss(train/val) 0.22917/0.15055. Took 0.05 sec\n",
            "Epoch 4453, Loss(train/val) 0.24503/0.15031. Took 0.04 sec\n",
            "Epoch 4454, Loss(train/val) 0.21406/0.15039. Took 0.04 sec\n",
            "Epoch 4455, Loss(train/val) 0.21450/0.15057. Took 0.05 sec\n",
            "Epoch 4456, Loss(train/val) 0.21563/0.15049. Took 0.05 sec\n",
            "Epoch 4457, Loss(train/val) 0.21734/0.15051. Took 0.05 sec\n",
            "Epoch 4458, Loss(train/val) 0.22432/0.15120. Took 0.04 sec\n",
            "Epoch 4459, Loss(train/val) 0.22453/0.15151. Took 0.05 sec\n",
            "Epoch 4460, Loss(train/val) 0.21310/0.15096. Took 0.04 sec\n",
            "Epoch 4461, Loss(train/val) 0.22293/0.15046. Took 0.05 sec\n",
            "Epoch 4462, Loss(train/val) 0.22275/0.15034. Took 0.05 sec\n",
            "Epoch 4463, Loss(train/val) 0.21576/0.15039. Took 0.05 sec\n",
            "Epoch 4464, Loss(train/val) 0.21791/0.15049. Took 0.05 sec\n",
            "Epoch 4465, Loss(train/val) 0.22888/0.15042. Took 0.04 sec\n",
            "Epoch 4466, Loss(train/val) 0.23157/0.15046. Took 0.05 sec\n",
            "Epoch 4467, Loss(train/val) 0.22298/0.15051. Took 0.05 sec\n",
            "Epoch 4468, Loss(train/val) 0.21623/0.15047. Took 0.04 sec\n",
            "Epoch 4469, Loss(train/val) 0.22013/0.15059. Took 0.05 sec\n",
            "Epoch 4470, Loss(train/val) 0.21949/0.15104. Took 0.04 sec\n",
            "Epoch 4471, Loss(train/val) 0.22569/0.15139. Took 0.05 sec\n",
            "Epoch 4472, Loss(train/val) 0.22628/0.15162. Took 0.05 sec\n",
            "Epoch 4473, Loss(train/val) 0.22126/0.15087. Took 0.05 sec\n",
            "Epoch 4474, Loss(train/val) 0.22483/0.15059. Took 0.04 sec\n",
            "Epoch 4475, Loss(train/val) 0.24185/0.15057. Took 0.04 sec\n",
            "Epoch 4476, Loss(train/val) 0.22822/0.15143. Took 0.05 sec\n",
            "Epoch 4477, Loss(train/val) 0.21895/0.15182. Took 0.05 sec\n",
            "Epoch 4478, Loss(train/val) 0.21638/0.15073. Took 0.05 sec\n",
            "Epoch 4479, Loss(train/val) 0.22537/0.15045. Took 0.05 sec\n",
            "Epoch 4480, Loss(train/val) 0.21687/0.15069. Took 0.04 sec\n",
            "Epoch 4481, Loss(train/val) 0.21852/0.15069. Took 0.06 sec\n",
            "Epoch 4482, Loss(train/val) 0.21171/0.15088. Took 0.05 sec\n",
            "Epoch 4483, Loss(train/val) 0.21676/0.15107. Took 0.04 sec\n",
            "Epoch 4484, Loss(train/val) 0.22168/0.15108. Took 0.04 sec\n",
            "Epoch 4485, Loss(train/val) 0.22665/0.15219. Took 0.05 sec\n",
            "Epoch 4486, Loss(train/val) 0.21449/0.15128. Took 0.05 sec\n",
            "Epoch 4487, Loss(train/val) 0.21762/0.15034. Took 0.05 sec\n",
            "Epoch 4488, Loss(train/val) 0.22453/0.15079. Took 0.05 sec\n",
            "Epoch 4489, Loss(train/val) 0.22070/0.15119. Took 0.05 sec\n",
            "Epoch 4490, Loss(train/val) 0.21810/0.15115. Took 0.04 sec\n",
            "Epoch 4491, Loss(train/val) 0.22143/0.15089. Took 0.06 sec\n",
            "Epoch 4492, Loss(train/val) 0.23654/0.15066. Took 0.05 sec\n",
            "Epoch 4493, Loss(train/val) 0.23032/0.15066. Took 0.05 sec\n",
            "Epoch 4494, Loss(train/val) 0.21594/0.15034. Took 0.05 sec\n",
            "Epoch 4495, Loss(train/val) 0.21920/0.15039. Took 0.04 sec\n",
            "Epoch 4496, Loss(train/val) 0.22170/0.15045. Took 0.05 sec\n",
            "Epoch 4497, Loss(train/val) 0.22102/0.15040. Took 0.05 sec\n",
            "Epoch 4498, Loss(train/val) 0.22140/0.15087. Took 0.05 sec\n",
            "Epoch 4499, Loss(train/val) 0.22382/0.15125. Took 0.05 sec\n",
            "Epoch 4500, Loss(train/val) 0.21619/0.15135. Took 0.04 sec\n",
            "Epoch 4501, Loss(train/val) 0.21959/0.15096. Took 0.05 sec\n",
            "Epoch 4502, Loss(train/val) 0.21868/0.15054. Took 0.06 sec\n",
            "Epoch 4503, Loss(train/val) 0.21710/0.15037. Took 0.05 sec\n",
            "Epoch 4504, Loss(train/val) 0.21469/0.15029. Took 0.05 sec\n",
            "Epoch 4505, Loss(train/val) 0.21528/0.15067. Took 0.05 sec\n",
            "Epoch 4506, Loss(train/val) 0.24271/0.15055. Took 0.05 sec\n",
            "Epoch 4507, Loss(train/val) 0.22262/0.15049. Took 0.05 sec\n",
            "Epoch 4508, Loss(train/val) 0.21806/0.15093. Took 0.05 sec\n",
            "Epoch 4509, Loss(train/val) 0.21356/0.15090. Took 0.04 sec\n",
            "Epoch 4510, Loss(train/val) 0.22067/0.15121. Took 0.05 sec\n",
            "Epoch 4511, Loss(train/val) 0.21298/0.15074. Took 0.04 sec\n",
            "Epoch 4512, Loss(train/val) 0.21646/0.15054. Took 0.04 sec\n",
            "Epoch 4513, Loss(train/val) 0.22017/0.15048. Took 0.04 sec\n",
            "Epoch 4514, Loss(train/val) 0.21433/0.15113. Took 0.04 sec\n",
            "Epoch 4515, Loss(train/val) 0.23114/0.15152. Took 0.06 sec\n",
            "Epoch 4516, Loss(train/val) 0.22013/0.15149. Took 0.04 sec\n",
            "Epoch 4517, Loss(train/val) 0.23714/0.15165. Took 0.05 sec\n",
            "Epoch 4518, Loss(train/val) 0.21381/0.15169. Took 0.05 sec\n",
            "Epoch 4519, Loss(train/val) 0.21677/0.15112. Took 0.05 sec\n",
            "Epoch 4520, Loss(train/val) 0.21151/0.15080. Took 0.05 sec\n",
            "Epoch 4521, Loss(train/val) 0.22165/0.15047. Took 0.04 sec\n",
            "Epoch 4522, Loss(train/val) 0.21754/0.15088. Took 0.05 sec\n",
            "Epoch 4523, Loss(train/val) 0.22049/0.15035. Took 0.05 sec\n",
            "Epoch 4524, Loss(train/val) 0.23290/0.15029. Took 0.05 sec\n",
            "Epoch 4525, Loss(train/val) 0.22648/0.15035. Took 0.05 sec\n",
            "Epoch 4526, Loss(train/val) 0.22308/0.15048. Took 0.05 sec\n",
            "Epoch 4527, Loss(train/val) 0.21608/0.15011. Took 0.04 sec\n",
            "Epoch 4528, Loss(train/val) 0.23225/0.15014. Took 0.04 sec\n",
            "Epoch 4529, Loss(train/val) 0.21618/0.15022. Took 0.04 sec\n",
            "Epoch 4530, Loss(train/val) 0.21529/0.15120. Took 0.05 sec\n",
            "Epoch 4531, Loss(train/val) 0.22503/0.15077. Took 0.05 sec\n",
            "Epoch 4532, Loss(train/val) 0.22434/0.15015. Took 0.05 sec\n",
            "Epoch 4533, Loss(train/val) 0.23375/0.15006. Took 0.05 sec\n",
            "Epoch 4534, Loss(train/val) 0.25429/0.15034. Took 0.05 sec\n",
            "Epoch 4535, Loss(train/val) 0.21821/0.15026. Took 0.05 sec\n",
            "Epoch 4536, Loss(train/val) 0.23600/0.15040. Took 0.05 sec\n",
            "Epoch 4537, Loss(train/val) 0.21773/0.15058. Took 0.04 sec\n",
            "Epoch 4538, Loss(train/val) 0.21415/0.15041. Took 0.04 sec\n",
            "Epoch 4539, Loss(train/val) 0.24645/0.15044. Took 0.04 sec\n",
            "Epoch 4540, Loss(train/val) 0.21601/0.15047. Took 0.05 sec\n",
            "Epoch 4541, Loss(train/val) 0.22343/0.15073. Took 0.05 sec\n",
            "Epoch 4542, Loss(train/val) 0.21787/0.15124. Took 0.05 sec\n",
            "Epoch 4543, Loss(train/val) 0.22143/0.15125. Took 0.04 sec\n",
            "Epoch 4544, Loss(train/val) 0.21734/0.15036. Took 0.05 sec\n",
            "Epoch 4545, Loss(train/val) 0.21374/0.15051. Took 0.06 sec\n",
            "Epoch 4546, Loss(train/val) 0.24623/0.15068. Took 0.05 sec\n",
            "Epoch 4547, Loss(train/val) 0.22112/0.15048. Took 0.05 sec\n",
            "Epoch 4548, Loss(train/val) 0.22364/0.15084. Took 0.05 sec\n",
            "Epoch 4549, Loss(train/val) 0.21945/0.15059. Took 0.05 sec\n",
            "Epoch 4550, Loss(train/val) 0.22181/0.15074. Took 0.05 sec\n",
            "Epoch 4551, Loss(train/val) 0.21290/0.15036. Took 0.05 sec\n",
            "Epoch 4552, Loss(train/val) 0.22133/0.15033. Took 0.04 sec\n",
            "Epoch 4553, Loss(train/val) 0.21424/0.15057. Took 0.04 sec\n",
            "Epoch 4554, Loss(train/val) 0.21595/0.15064. Took 0.05 sec\n",
            "Epoch 4555, Loss(train/val) 0.22398/0.15054. Took 0.06 sec\n",
            "Epoch 4556, Loss(train/val) 0.21285/0.15024. Took 0.04 sec\n",
            "Epoch 4557, Loss(train/val) 0.22800/0.15024. Took 0.04 sec\n",
            "Epoch 4558, Loss(train/val) 0.22473/0.15029. Took 0.04 sec\n",
            "Epoch 4559, Loss(train/val) 0.22694/0.15031. Took 0.04 sec\n",
            "Epoch 4560, Loss(train/val) 0.22248/0.15021. Took 0.05 sec\n",
            "Epoch 4561, Loss(train/val) 0.23445/0.15010. Took 0.05 sec\n",
            "Epoch 4562, Loss(train/val) 0.21371/0.15021. Took 0.04 sec\n",
            "Epoch 4563, Loss(train/val) 0.21911/0.15033. Took 0.04 sec\n",
            "Epoch 4564, Loss(train/val) 0.22032/0.15026. Took 0.05 sec\n",
            "Epoch 4565, Loss(train/val) 0.22232/0.15037. Took 0.05 sec\n",
            "Epoch 4566, Loss(train/val) 0.22125/0.15040. Took 0.05 sec\n",
            "Epoch 4567, Loss(train/val) 0.21298/0.15031. Took 0.05 sec\n",
            "Epoch 4568, Loss(train/val) 0.21744/0.15027. Took 0.04 sec\n",
            "Epoch 4569, Loss(train/val) 0.21920/0.15031. Took 0.05 sec\n",
            "Epoch 4570, Loss(train/val) 0.22264/0.15040. Took 0.05 sec\n",
            "Epoch 4571, Loss(train/val) 0.22096/0.15083. Took 0.05 sec\n",
            "Epoch 4572, Loss(train/val) 0.22157/0.15050. Took 0.04 sec\n",
            "Epoch 4573, Loss(train/val) 0.21228/0.15050. Took 0.04 sec\n",
            "Epoch 4574, Loss(train/val) 0.22519/0.15077. Took 0.04 sec\n",
            "Epoch 4575, Loss(train/val) 0.22217/0.15065. Took 0.05 sec\n",
            "Epoch 4576, Loss(train/val) 0.21621/0.15063. Took 0.05 sec\n",
            "Epoch 4577, Loss(train/val) 0.21564/0.15040. Took 0.05 sec\n",
            "Epoch 4578, Loss(train/val) 0.21746/0.15067. Took 0.05 sec\n",
            "Epoch 4579, Loss(train/val) 0.22026/0.15033. Took 0.04 sec\n",
            "Epoch 4580, Loss(train/val) 0.21196/0.15041. Took 0.05 sec\n",
            "Epoch 4581, Loss(train/val) 0.21731/0.15055. Took 0.04 sec\n",
            "Epoch 4582, Loss(train/val) 0.21880/0.15074. Took 0.05 sec\n",
            "Epoch 4583, Loss(train/val) 0.21766/0.15070. Took 0.04 sec\n",
            "Epoch 4584, Loss(train/val) 0.21493/0.15089. Took 0.05 sec\n",
            "Epoch 4585, Loss(train/val) 0.22421/0.15088. Took 0.06 sec\n",
            "Epoch 4586, Loss(train/val) 0.22059/0.15034. Took 0.05 sec\n",
            "Epoch 4587, Loss(train/val) 0.23208/0.15049. Took 0.04 sec\n",
            "Epoch 4588, Loss(train/val) 0.22348/0.15063. Took 0.05 sec\n",
            "Epoch 4589, Loss(train/val) 0.21767/0.15050. Took 0.05 sec\n",
            "Epoch 4590, Loss(train/val) 0.21688/0.15039. Took 0.05 sec\n",
            "Epoch 4591, Loss(train/val) 0.23448/0.15038. Took 0.05 sec\n",
            "Epoch 4592, Loss(train/val) 0.21970/0.15059. Took 0.05 sec\n",
            "Epoch 4593, Loss(train/val) 0.21856/0.15068. Took 0.05 sec\n",
            "Epoch 4594, Loss(train/val) 0.23214/0.15038. Took 0.05 sec\n",
            "Epoch 4595, Loss(train/val) 0.22953/0.15046. Took 0.05 sec\n",
            "Epoch 4596, Loss(train/val) 0.24682/0.15121. Took 0.05 sec\n",
            "Epoch 4597, Loss(train/val) 0.21364/0.15133. Took 0.05 sec\n",
            "Epoch 4598, Loss(train/val) 0.22326/0.15136. Took 0.05 sec\n",
            "Epoch 4599, Loss(train/val) 0.21772/0.15124. Took 0.05 sec\n",
            "Epoch 4600, Loss(train/val) 0.22003/0.15048. Took 0.05 sec\n",
            "Epoch 4601, Loss(train/val) 0.22272/0.15127. Took 0.04 sec\n",
            "Epoch 4602, Loss(train/val) 0.22988/0.15333. Took 0.04 sec\n",
            "Epoch 4603, Loss(train/val) 0.21295/0.15365. Took 0.05 sec\n",
            "Epoch 4604, Loss(train/val) 0.23002/0.15269. Took 0.05 sec\n",
            "Epoch 4605, Loss(train/val) 0.21463/0.15125. Took 0.06 sec\n",
            "Epoch 4606, Loss(train/val) 0.21977/0.15031. Took 0.05 sec\n",
            "Epoch 4607, Loss(train/val) 0.22466/0.15051. Took 0.05 sec\n",
            "Epoch 4608, Loss(train/val) 0.22013/0.15097. Took 0.06 sec\n",
            "Epoch 4609, Loss(train/val) 0.21899/0.15029. Took 0.05 sec\n",
            "Epoch 4610, Loss(train/val) 0.21292/0.15039. Took 0.05 sec\n",
            "Epoch 4611, Loss(train/val) 0.22941/0.15040. Took 0.04 sec\n",
            "Epoch 4612, Loss(train/val) 0.22321/0.15038. Took 0.05 sec\n",
            "Epoch 4613, Loss(train/val) 0.22175/0.15035. Took 0.05 sec\n",
            "Epoch 4614, Loss(train/val) 0.23436/0.15033. Took 0.05 sec\n",
            "Epoch 4615, Loss(train/val) 0.21110/0.15010. Took 0.05 sec\n",
            "Epoch 4616, Loss(train/val) 0.22431/0.15000. Took 0.04 sec\n",
            "Epoch 4617, Loss(train/val) 0.23890/0.15033. Took 0.04 sec\n",
            "Epoch 4618, Loss(train/val) 0.21445/0.15022. Took 0.05 sec\n",
            "Epoch 4619, Loss(train/val) 0.21784/0.15075. Took 0.05 sec\n",
            "Epoch 4620, Loss(train/val) 0.21586/0.15135. Took 0.04 sec\n",
            "Epoch 4621, Loss(train/val) 0.23748/0.15067. Took 0.04 sec\n",
            "Epoch 4622, Loss(train/val) 0.21893/0.15043. Took 0.04 sec\n",
            "Epoch 4623, Loss(train/val) 0.23034/0.15030. Took 0.05 sec\n",
            "Epoch 4624, Loss(train/val) 0.22242/0.15042. Took 0.05 sec\n",
            "Epoch 4625, Loss(train/val) 0.21742/0.15053. Took 0.04 sec\n",
            "Epoch 4626, Loss(train/val) 0.21833/0.15101. Took 0.05 sec\n",
            "Epoch 4627, Loss(train/val) 0.22227/0.15069. Took 0.04 sec\n",
            "Epoch 4628, Loss(train/val) 0.21567/0.15037. Took 0.04 sec\n",
            "Epoch 4629, Loss(train/val) 0.23933/0.15049. Took 0.05 sec\n",
            "Epoch 4630, Loss(train/val) 0.24900/0.15069. Took 0.05 sec\n",
            "Epoch 4631, Loss(train/val) 0.21836/0.15080. Took 0.05 sec\n",
            "Epoch 4632, Loss(train/val) 0.22700/0.15024. Took 0.04 sec\n",
            "Epoch 4633, Loss(train/val) 0.20738/0.15037. Took 0.04 sec\n",
            "Epoch 4634, Loss(train/val) 0.21445/0.15044. Took 0.06 sec\n",
            "Epoch 4635, Loss(train/val) 0.21120/0.15035. Took 0.05 sec\n",
            "Epoch 4636, Loss(train/val) 0.23343/0.15035. Took 0.04 sec\n",
            "Epoch 4637, Loss(train/val) 0.22320/0.15060. Took 0.04 sec\n",
            "Epoch 4638, Loss(train/val) 0.22077/0.15033. Took 0.05 sec\n",
            "Epoch 4639, Loss(train/val) 0.20739/0.15025. Took 0.05 sec\n",
            "Epoch 4640, Loss(train/val) 0.21675/0.15042. Took 0.05 sec\n",
            "Epoch 4641, Loss(train/val) 0.21881/0.15059. Took 0.04 sec\n",
            "Epoch 4642, Loss(train/val) 0.22065/0.15058. Took 0.04 sec\n",
            "Epoch 4643, Loss(train/val) 0.22106/0.15036. Took 0.04 sec\n",
            "Epoch 4644, Loss(train/val) 0.22077/0.15034. Took 0.05 sec\n",
            "Epoch 4645, Loss(train/val) 0.24014/0.15089. Took 0.04 sec\n",
            "Epoch 4646, Loss(train/val) 0.21266/0.15143. Took 0.04 sec\n",
            "Epoch 4647, Loss(train/val) 0.22203/0.15102. Took 0.05 sec\n",
            "Epoch 4648, Loss(train/val) 0.23250/0.15120. Took 0.04 sec\n",
            "Epoch 4649, Loss(train/val) 0.21814/0.15033. Took 0.06 sec\n",
            "Epoch 4650, Loss(train/val) 0.22439/0.15049. Took 0.04 sec\n",
            "Epoch 4651, Loss(train/val) 0.21521/0.15036. Took 0.05 sec\n",
            "Epoch 4652, Loss(train/val) 0.22460/0.15026. Took 0.05 sec\n",
            "Epoch 4653, Loss(train/val) 0.22705/0.15033. Took 0.05 sec\n",
            "Epoch 4654, Loss(train/val) 0.21946/0.15046. Took 0.05 sec\n",
            "Epoch 4655, Loss(train/val) 0.22144/0.15065. Took 0.05 sec\n",
            "Epoch 4656, Loss(train/val) 0.24701/0.15046. Took 0.04 sec\n",
            "Epoch 4657, Loss(train/val) 0.21279/0.15015. Took 0.04 sec\n",
            "Epoch 4658, Loss(train/val) 0.21798/0.15010. Took 0.04 sec\n",
            "Epoch 4659, Loss(train/val) 0.22989/0.15065. Took 0.05 sec\n",
            "Epoch 4660, Loss(train/val) 0.21917/0.15154. Took 0.04 sec\n",
            "Epoch 4661, Loss(train/val) 0.21382/0.15119. Took 0.04 sec\n",
            "Epoch 4662, Loss(train/val) 0.22665/0.15179. Took 0.04 sec\n",
            "Epoch 4663, Loss(train/val) 0.21735/0.15126. Took 0.04 sec\n",
            "Epoch 4664, Loss(train/val) 0.21904/0.15051. Took 0.05 sec\n",
            "Epoch 4665, Loss(train/val) 0.25235/0.15058. Took 0.04 sec\n",
            "Epoch 4666, Loss(train/val) 0.21151/0.15033. Took 0.05 sec\n",
            "Epoch 4667, Loss(train/val) 0.22148/0.15037. Took 0.04 sec\n",
            "Epoch 4668, Loss(train/val) 0.21550/0.15017. Took 0.05 sec\n",
            "Epoch 4669, Loss(train/val) 0.22331/0.15053. Took 0.05 sec\n",
            "Epoch 4670, Loss(train/val) 0.21685/0.15065. Took 0.04 sec\n",
            "Epoch 4671, Loss(train/val) 0.21302/0.15038. Took 0.04 sec\n",
            "Epoch 4672, Loss(train/val) 0.21658/0.15021. Took 0.05 sec\n",
            "Epoch 4673, Loss(train/val) 0.23391/0.15039. Took 0.05 sec\n",
            "Epoch 4674, Loss(train/val) 0.22791/0.15060. Took 0.05 sec\n",
            "Epoch 4675, Loss(train/val) 0.21919/0.15046. Took 0.05 sec\n",
            "Epoch 4676, Loss(train/val) 0.21821/0.15050. Took 0.05 sec\n",
            "Epoch 4677, Loss(train/val) 0.23640/0.15042. Took 0.05 sec\n",
            "Epoch 4678, Loss(train/val) 0.21968/0.15033. Took 0.04 sec\n",
            "Epoch 4679, Loss(train/val) 0.21900/0.15054. Took 0.05 sec\n",
            "Epoch 4680, Loss(train/val) 0.23817/0.15033. Took 0.05 sec\n",
            "Epoch 4681, Loss(train/val) 0.21091/0.15041. Took 0.05 sec\n",
            "Epoch 4682, Loss(train/val) 0.22381/0.15041. Took 0.05 sec\n",
            "Epoch 4683, Loss(train/val) 0.21972/0.15018. Took 0.05 sec\n",
            "Epoch 4684, Loss(train/val) 0.22393/0.15017. Took 0.05 sec\n",
            "Epoch 4685, Loss(train/val) 0.23356/0.15027. Took 0.05 sec\n",
            "Epoch 4686, Loss(train/val) 0.21716/0.15027. Took 0.05 sec\n",
            "Epoch 4687, Loss(train/val) 0.23962/0.15044. Took 0.05 sec\n",
            "Epoch 4688, Loss(train/val) 0.21936/0.15071. Took 0.05 sec\n",
            "Epoch 4689, Loss(train/val) 0.22043/0.15187. Took 0.05 sec\n",
            "Epoch 4690, Loss(train/val) 0.21063/0.15252. Took 0.04 sec\n",
            "Epoch 4691, Loss(train/val) 0.22393/0.15091. Took 0.04 sec\n",
            "Epoch 4692, Loss(train/val) 0.21246/0.15021. Took 0.05 sec\n",
            "Epoch 4693, Loss(train/val) 0.22903/0.15088. Took 0.05 sec\n",
            "Epoch 4694, Loss(train/val) 0.21669/0.15093. Took 0.06 sec\n",
            "Epoch 4695, Loss(train/val) 0.21256/0.15168. Took 0.04 sec\n",
            "Epoch 4696, Loss(train/val) 0.22643/0.15095. Took 0.04 sec\n",
            "Epoch 4697, Loss(train/val) 0.21571/0.15110. Took 0.05 sec\n",
            "Epoch 4698, Loss(train/val) 0.21633/0.15037. Took 0.05 sec\n",
            "Epoch 4699, Loss(train/val) 0.22189/0.15041. Took 0.05 sec\n",
            "Epoch 4700, Loss(train/val) 0.21994/0.15047. Took 0.05 sec\n",
            "Epoch 4701, Loss(train/val) 0.21995/0.15060. Took 0.05 sec\n",
            "Epoch 4702, Loss(train/val) 0.21222/0.15062. Took 0.05 sec\n",
            "Epoch 4703, Loss(train/val) 0.23269/0.15059. Took 0.05 sec\n",
            "Epoch 4704, Loss(train/val) 0.21458/0.15045. Took 0.05 sec\n",
            "Epoch 4705, Loss(train/val) 0.22328/0.15044. Took 0.05 sec\n",
            "Epoch 4706, Loss(train/val) 0.23649/0.15038. Took 0.04 sec\n",
            "Epoch 4707, Loss(train/val) 0.21525/0.15094. Took 0.05 sec\n",
            "Epoch 4708, Loss(train/val) 0.21744/0.15069. Took 0.05 sec\n",
            "Epoch 4709, Loss(train/val) 0.21770/0.15122. Took 0.06 sec\n",
            "Epoch 4710, Loss(train/val) 0.21645/0.15062. Took 0.05 sec\n",
            "Epoch 4711, Loss(train/val) 0.24059/0.15033. Took 0.04 sec\n",
            "Epoch 4712, Loss(train/val) 0.22904/0.15046. Took 0.05 sec\n",
            "Epoch 4713, Loss(train/val) 0.23516/0.15045. Took 0.05 sec\n",
            "Epoch 4714, Loss(train/val) 0.22620/0.15148. Took 0.05 sec\n",
            "Epoch 4715, Loss(train/val) 0.21245/0.15151. Took 0.05 sec\n",
            "Epoch 4716, Loss(train/val) 0.21409/0.15216. Took 0.05 sec\n",
            "Epoch 4717, Loss(train/val) 0.21573/0.15172. Took 0.05 sec\n",
            "Epoch 4718, Loss(train/val) 0.21304/0.15087. Took 0.05 sec\n",
            "Epoch 4719, Loss(train/val) 0.22862/0.15052. Took 0.05 sec\n",
            "Epoch 4720, Loss(train/val) 0.21561/0.15019. Took 0.05 sec\n",
            "Epoch 4721, Loss(train/val) 0.21222/0.15006. Took 0.04 sec\n",
            "Epoch 4722, Loss(train/val) 0.21882/0.15010. Took 0.05 sec\n",
            "Epoch 4723, Loss(train/val) 0.21447/0.15028. Took 0.04 sec\n",
            "Epoch 4724, Loss(train/val) 0.22075/0.15091. Took 0.05 sec\n",
            "Epoch 4725, Loss(train/val) 0.22614/0.15035. Took 0.04 sec\n",
            "Epoch 4726, Loss(train/val) 0.21918/0.15014. Took 0.05 sec\n",
            "Epoch 4727, Loss(train/val) 0.21376/0.15041. Took 0.05 sec\n",
            "Epoch 4728, Loss(train/val) 0.22285/0.15098. Took 0.05 sec\n",
            "Epoch 4729, Loss(train/val) 0.21805/0.15047. Took 0.05 sec\n",
            "Epoch 4730, Loss(train/val) 0.21083/0.15029. Took 0.05 sec\n",
            "Epoch 4731, Loss(train/val) 0.22239/0.15016. Took 0.05 sec\n",
            "Epoch 4732, Loss(train/val) 0.25083/0.15027. Took 0.05 sec\n",
            "Epoch 4733, Loss(train/val) 0.22259/0.15049. Took 0.05 sec\n",
            "Epoch 4734, Loss(train/val) 0.22173/0.15015. Took 0.05 sec\n",
            "Epoch 4735, Loss(train/val) 0.21880/0.15013. Took 0.04 sec\n",
            "Epoch 4736, Loss(train/val) 0.22307/0.15002. Took 0.05 sec\n",
            "Epoch 4737, Loss(train/val) 0.23885/0.15019. Took 0.05 sec\n",
            "Epoch 4738, Loss(train/val) 0.21450/0.15000. Took 0.05 sec\n",
            "Epoch 4739, Loss(train/val) 0.22000/0.15009. Took 0.05 sec\n",
            "Epoch 4740, Loss(train/val) 0.21094/0.15025. Took 0.05 sec\n",
            "Epoch 4741, Loss(train/val) 0.22122/0.15022. Took 0.05 sec\n",
            "Epoch 4742, Loss(train/val) 0.22020/0.15021. Took 0.04 sec\n",
            "Epoch 4743, Loss(train/val) 0.22405/0.15020. Took 0.04 sec\n",
            "Epoch 4744, Loss(train/val) 0.21468/0.15036. Took 0.05 sec\n",
            "Epoch 4745, Loss(train/val) 0.21430/0.15042. Took 0.04 sec\n",
            "Epoch 4746, Loss(train/val) 0.21512/0.15035. Took 0.04 sec\n",
            "Epoch 4747, Loss(train/val) 0.22626/0.15057. Took 0.05 sec\n",
            "Epoch 4748, Loss(train/val) 0.21360/0.15059. Took 0.05 sec\n",
            "Epoch 4749, Loss(train/val) 0.22111/0.15030. Took 0.05 sec\n",
            "Epoch 4750, Loss(train/val) 0.21091/0.15029. Took 0.05 sec\n",
            "Epoch 4751, Loss(train/val) 0.21395/0.15021. Took 0.05 sec\n",
            "Epoch 4752, Loss(train/val) 0.21586/0.15023. Took 0.04 sec\n",
            "Epoch 4753, Loss(train/val) 0.20829/0.15027. Took 0.04 sec\n",
            "Epoch 4754, Loss(train/val) 0.22447/0.15009. Took 0.05 sec\n",
            "Epoch 4755, Loss(train/val) 0.21757/0.15040. Took 0.05 sec\n",
            "Epoch 4756, Loss(train/val) 0.22414/0.15027. Took 0.05 sec\n",
            "Epoch 4757, Loss(train/val) 0.21766/0.15010. Took 0.05 sec\n",
            "Epoch 4758, Loss(train/val) 0.22241/0.15032. Took 0.04 sec\n",
            "Epoch 4759, Loss(train/val) 0.21285/0.15027. Took 0.06 sec\n",
            "Epoch 4760, Loss(train/val) 0.22040/0.15038. Took 0.05 sec\n",
            "Epoch 4761, Loss(train/val) 0.23721/0.15107. Took 0.04 sec\n",
            "Epoch 4762, Loss(train/val) 0.21917/0.15162. Took 0.04 sec\n",
            "Epoch 4763, Loss(train/val) 0.21510/0.15087. Took 0.04 sec\n",
            "Epoch 4764, Loss(train/val) 0.21498/0.15007. Took 0.05 sec\n",
            "Epoch 4765, Loss(train/val) 0.22191/0.15011. Took 0.05 sec\n",
            "Epoch 4766, Loss(train/val) 0.22228/0.15108. Took 0.05 sec\n",
            "Epoch 4767, Loss(train/val) 0.21829/0.15087. Took 0.04 sec\n",
            "Epoch 4768, Loss(train/val) 0.21793/0.15028. Took 0.04 sec\n",
            "Epoch 4769, Loss(train/val) 0.21374/0.15026. Took 0.05 sec\n",
            "Epoch 4770, Loss(train/val) 0.21619/0.15003. Took 0.05 sec\n",
            "Epoch 4771, Loss(train/val) 0.21615/0.15016. Took 0.04 sec\n",
            "Epoch 4772, Loss(train/val) 0.21673/0.15041. Took 0.04 sec\n",
            "Epoch 4773, Loss(train/val) 0.22034/0.15037. Took 0.04 sec\n",
            "Epoch 4774, Loss(train/val) 0.22157/0.15035. Took 0.05 sec\n",
            "Epoch 4775, Loss(train/val) 0.22069/0.15021. Took 0.04 sec\n",
            "Epoch 4776, Loss(train/val) 0.21635/0.15021. Took 0.05 sec\n",
            "Epoch 4777, Loss(train/val) 0.21493/0.15017. Took 0.05 sec\n",
            "Epoch 4778, Loss(train/val) 0.21803/0.15017. Took 0.04 sec\n",
            "Epoch 4779, Loss(train/val) 0.21398/0.15046. Took 0.05 sec\n",
            "Epoch 4780, Loss(train/val) 0.21714/0.15071. Took 0.05 sec\n",
            "Epoch 4781, Loss(train/val) 0.21814/0.15081. Took 0.05 sec\n",
            "Epoch 4782, Loss(train/val) 0.22557/0.15019. Took 0.04 sec\n",
            "Epoch 4783, Loss(train/val) 0.22048/0.15060. Took 0.04 sec\n",
            "Epoch 4784, Loss(train/val) 0.21323/0.15044. Took 0.05 sec\n",
            "Epoch 4785, Loss(train/val) 0.22095/0.15027. Took 0.04 sec\n",
            "Epoch 4786, Loss(train/val) 0.21731/0.15033. Took 0.04 sec\n",
            "Epoch 4787, Loss(train/val) 0.22306/0.15028. Took 0.05 sec\n",
            "Epoch 4788, Loss(train/val) 0.21725/0.15017. Took 0.05 sec\n",
            "Epoch 4789, Loss(train/val) 0.22165/0.15016. Took 0.05 sec\n",
            "Epoch 4790, Loss(train/val) 0.21738/0.15020. Took 0.04 sec\n",
            "Epoch 4791, Loss(train/val) 0.22194/0.15059. Took 0.04 sec\n",
            "Epoch 4792, Loss(train/val) 0.21778/0.15048. Took 0.04 sec\n",
            "Epoch 4793, Loss(train/val) 0.22467/0.15163. Took 0.04 sec\n",
            "Epoch 4794, Loss(train/val) 0.21370/0.15162. Took 0.05 sec\n",
            "Epoch 4795, Loss(train/val) 0.22357/0.15079. Took 0.05 sec\n",
            "Epoch 4796, Loss(train/val) 0.21264/0.15044. Took 0.05 sec\n",
            "Epoch 4797, Loss(train/val) 0.22598/0.15043. Took 0.04 sec\n",
            "Epoch 4798, Loss(train/val) 0.21515/0.15036. Took 0.04 sec\n",
            "Epoch 4799, Loss(train/val) 0.21297/0.15025. Took 0.05 sec\n",
            "Epoch 4800, Loss(train/val) 0.26630/0.15048. Took 0.05 sec\n",
            "Epoch 4801, Loss(train/val) 0.24521/0.15067. Took 0.05 sec\n",
            "Epoch 4802, Loss(train/val) 0.21646/0.15004. Took 0.05 sec\n",
            "Epoch 4803, Loss(train/val) 0.22356/0.15007. Took 0.05 sec\n",
            "Epoch 4804, Loss(train/val) 0.25021/0.15010. Took 0.05 sec\n",
            "Epoch 4805, Loss(train/val) 0.21332/0.15009. Took 0.05 sec\n",
            "Epoch 4806, Loss(train/val) 0.22043/0.15018. Took 0.05 sec\n",
            "Epoch 4807, Loss(train/val) 0.22566/0.15004. Took 0.05 sec\n",
            "Epoch 4808, Loss(train/val) 0.21327/0.15034. Took 0.05 sec\n",
            "Epoch 4809, Loss(train/val) 0.21555/0.15153. Took 0.05 sec\n",
            "Epoch 4810, Loss(train/val) 0.21941/0.15244. Took 0.05 sec\n",
            "Epoch 4811, Loss(train/val) 0.21517/0.15174. Took 0.05 sec\n",
            "Epoch 4812, Loss(train/val) 0.21666/0.15049. Took 0.05 sec\n",
            "Epoch 4813, Loss(train/val) 0.21959/0.15047. Took 0.05 sec\n",
            "Epoch 4814, Loss(train/val) 0.21182/0.15089. Took 0.06 sec\n",
            "Epoch 4815, Loss(train/val) 0.22149/0.15092. Took 0.05 sec\n",
            "Epoch 4816, Loss(train/val) 0.22071/0.15055. Took 0.05 sec\n",
            "Epoch 4817, Loss(train/val) 0.22067/0.15019. Took 0.05 sec\n",
            "Epoch 4818, Loss(train/val) 0.21743/0.15021. Took 0.04 sec\n",
            "Epoch 4819, Loss(train/val) 0.23005/0.15064. Took 0.05 sec\n",
            "Epoch 4820, Loss(train/val) 0.21549/0.15054. Took 0.05 sec\n",
            "Epoch 4821, Loss(train/val) 0.22799/0.15028. Took 0.05 sec\n",
            "Epoch 4822, Loss(train/val) 0.22726/0.15061. Took 0.05 sec\n",
            "Epoch 4823, Loss(train/val) 0.21278/0.15115. Took 0.05 sec\n",
            "Epoch 4824, Loss(train/val) 0.21245/0.15135. Took 0.05 sec\n",
            "Epoch 4825, Loss(train/val) 0.22913/0.15082. Took 0.05 sec\n",
            "Epoch 4826, Loss(train/val) 0.21903/0.15037. Took 0.05 sec\n",
            "Epoch 4827, Loss(train/val) 0.21848/0.15016. Took 0.05 sec\n",
            "Epoch 4828, Loss(train/val) 0.21719/0.15020. Took 0.05 sec\n",
            "Epoch 4829, Loss(train/val) 0.22235/0.15016. Took 0.05 sec\n",
            "Epoch 4830, Loss(train/val) 0.22377/0.15010. Took 0.05 sec\n",
            "Epoch 4831, Loss(train/val) 0.21870/0.15017. Took 0.05 sec\n",
            "Epoch 4832, Loss(train/val) 0.21479/0.15020. Took 0.04 sec\n",
            "Epoch 4833, Loss(train/val) 0.21263/0.15053. Took 0.05 sec\n",
            "Epoch 4834, Loss(train/val) 0.21465/0.15027. Took 0.05 sec\n",
            "Epoch 4835, Loss(train/val) 0.21658/0.15019. Took 0.06 sec\n",
            "Epoch 4836, Loss(train/val) 0.21413/0.15007. Took 0.05 sec\n",
            "Epoch 4837, Loss(train/val) 0.21896/0.15026. Took 0.05 sec\n",
            "Epoch 4838, Loss(train/val) 0.21458/0.15026. Took 0.05 sec\n",
            "Epoch 4839, Loss(train/val) 0.21907/0.15034. Took 0.05 sec\n",
            "Epoch 4840, Loss(train/val) 0.21630/0.15032. Took 0.04 sec\n",
            "Epoch 4841, Loss(train/val) 0.21025/0.15034. Took 0.05 sec\n",
            "Epoch 4842, Loss(train/val) 0.22931/0.15024. Took 0.04 sec\n",
            "Epoch 4843, Loss(train/val) 0.23119/0.15023. Took 0.05 sec\n",
            "Epoch 4844, Loss(train/val) 0.22139/0.15021. Took 0.05 sec\n",
            "Epoch 4845, Loss(train/val) 0.22208/0.15096. Took 0.05 sec\n",
            "Epoch 4846, Loss(train/val) 0.21354/0.15080. Took 0.04 sec\n",
            "Epoch 4847, Loss(train/val) 0.21680/0.15022. Took 0.04 sec\n",
            "Epoch 4848, Loss(train/val) 0.25008/0.15088. Took 0.05 sec\n",
            "Epoch 4849, Loss(train/val) 0.21176/0.15058. Took 0.05 sec\n",
            "Epoch 4850, Loss(train/val) 0.22062/0.15115. Took 0.05 sec\n",
            "Epoch 4851, Loss(train/val) 0.21883/0.15172. Took 0.04 sec\n",
            "Epoch 4852, Loss(train/val) 0.21792/0.15086. Took 0.05 sec\n",
            "Epoch 4853, Loss(train/val) 0.21222/0.15012. Took 0.05 sec\n",
            "Epoch 4854, Loss(train/val) 0.21550/0.15095. Took 0.05 sec\n",
            "Epoch 4855, Loss(train/val) 0.21653/0.15104. Took 0.04 sec\n",
            "Epoch 4856, Loss(train/val) 0.22247/0.15071. Took 0.04 sec\n",
            "Epoch 4857, Loss(train/val) 0.22026/0.15081. Took 0.05 sec\n",
            "Epoch 4858, Loss(train/val) 0.22176/0.15065. Took 0.06 sec\n",
            "Epoch 4859, Loss(train/val) 0.21539/0.15042. Took 0.04 sec\n",
            "Epoch 4860, Loss(train/val) 0.22253/0.15055. Took 0.04 sec\n",
            "Epoch 4861, Loss(train/val) 0.21862/0.15169. Took 0.05 sec\n",
            "Epoch 4862, Loss(train/val) 0.21152/0.15257. Took 0.04 sec\n",
            "Epoch 4863, Loss(train/val) 0.21579/0.15173. Took 0.05 sec\n",
            "Epoch 4864, Loss(train/val) 0.21208/0.15053. Took 0.05 sec\n",
            "Epoch 4865, Loss(train/val) 0.21800/0.15017. Took 0.05 sec\n",
            "Epoch 4866, Loss(train/val) 0.22233/0.15028. Took 0.04 sec\n",
            "Epoch 4867, Loss(train/val) 0.21424/0.15008. Took 0.05 sec\n",
            "Epoch 4868, Loss(train/val) 0.21127/0.15037. Took 0.05 sec\n",
            "Epoch 4869, Loss(train/val) 0.22584/0.15177. Took 0.04 sec\n",
            "Epoch 4870, Loss(train/val) 0.22341/0.15148. Took 0.04 sec\n",
            "Epoch 4871, Loss(train/val) 0.22017/0.15143. Took 0.05 sec\n",
            "Epoch 4872, Loss(train/val) 0.23614/0.15191. Took 0.04 sec\n",
            "Epoch 4873, Loss(train/val) 0.21282/0.15108. Took 0.05 sec\n",
            "Epoch 4874, Loss(train/val) 0.22511/0.15034. Took 0.04 sec\n",
            "Epoch 4875, Loss(train/val) 0.21333/0.15013. Took 0.04 sec\n",
            "Epoch 4876, Loss(train/val) 0.21602/0.15017. Took 0.04 sec\n",
            "Epoch 4877, Loss(train/val) 0.21485/0.15086. Took 0.05 sec\n",
            "Epoch 4878, Loss(train/val) 0.24760/0.15047. Took 0.05 sec\n",
            "Epoch 4879, Loss(train/val) 0.21181/0.15089. Took 0.04 sec\n",
            "Epoch 4880, Loss(train/val) 0.21280/0.15015. Took 0.04 sec\n",
            "Epoch 4881, Loss(train/val) 0.23049/0.15075. Took 0.04 sec\n",
            "Epoch 4882, Loss(train/val) 0.21584/0.15084. Took 0.04 sec\n",
            "Epoch 4883, Loss(train/val) 0.21672/0.15157. Took 0.05 sec\n",
            "Epoch 4884, Loss(train/val) 0.21766/0.15111. Took 0.05 sec\n",
            "Epoch 4885, Loss(train/val) 0.22102/0.15092. Took 0.05 sec\n",
            "Epoch 4886, Loss(train/val) 0.21673/0.15116. Took 0.05 sec\n",
            "Epoch 4887, Loss(train/val) 0.22245/0.15034. Took 0.05 sec\n",
            "Epoch 4888, Loss(train/val) 0.22429/0.15065. Took 0.05 sec\n",
            "Epoch 4889, Loss(train/val) 0.22687/0.15032. Took 0.04 sec\n",
            "Epoch 4890, Loss(train/val) 0.22255/0.15043. Took 0.04 sec\n",
            "Epoch 4891, Loss(train/val) 0.23263/0.15231. Took 0.05 sec\n",
            "Epoch 4892, Loss(train/val) 0.23418/0.15289. Took 0.05 sec\n",
            "Epoch 4893, Loss(train/val) 0.22712/0.15117. Took 0.05 sec\n",
            "Epoch 4894, Loss(train/val) 0.21675/0.15001. Took 0.05 sec\n",
            "Epoch 4895, Loss(train/val) 0.21625/0.15034. Took 0.05 sec\n",
            "Epoch 4896, Loss(train/val) 0.21390/0.15082. Took 0.05 sec\n",
            "Epoch 4897, Loss(train/val) 0.23689/0.15133. Took 0.04 sec\n",
            "Epoch 4898, Loss(train/val) 0.21559/0.15126. Took 0.05 sec\n",
            "Epoch 4899, Loss(train/val) 0.22142/0.15075. Took 0.05 sec\n",
            "Epoch 4900, Loss(train/val) 0.23936/0.15061. Took 0.04 sec\n",
            "Epoch 4901, Loss(train/val) 0.23925/0.15046. Took 0.04 sec\n",
            "Epoch 4902, Loss(train/val) 0.23542/0.15078. Took 0.04 sec\n",
            "Epoch 4903, Loss(train/val) 0.21069/0.15157. Took 0.06 sec\n",
            "Epoch 4904, Loss(train/val) 0.21599/0.15230. Took 0.04 sec\n",
            "Epoch 4905, Loss(train/val) 0.22074/0.15120. Took 0.05 sec\n",
            "Epoch 4906, Loss(train/val) 0.21451/0.15097. Took 0.04 sec\n",
            "Epoch 4907, Loss(train/val) 0.22857/0.15041. Took 0.05 sec\n",
            "Epoch 4908, Loss(train/val) 0.21871/0.15050. Took 0.06 sec\n",
            "Epoch 4909, Loss(train/val) 0.22247/0.15061. Took 0.05 sec\n",
            "Epoch 4910, Loss(train/val) 0.22010/0.15066. Took 0.04 sec\n",
            "Epoch 4911, Loss(train/val) 0.22142/0.15048. Took 0.05 sec\n",
            "Epoch 4912, Loss(train/val) 0.21560/0.15030. Took 0.05 sec\n",
            "Epoch 4913, Loss(train/val) 0.22121/0.15049. Took 0.05 sec\n",
            "Epoch 4914, Loss(train/val) 0.21445/0.15026. Took 0.05 sec\n",
            "Epoch 4915, Loss(train/val) 0.21971/0.15013. Took 0.04 sec\n",
            "Epoch 4916, Loss(train/val) 0.23363/0.15073. Took 0.04 sec\n",
            "Epoch 4917, Loss(train/val) 0.21842/0.15029. Took 0.04 sec\n",
            "Epoch 4918, Loss(train/val) 0.23029/0.15024. Took 0.06 sec\n",
            "Epoch 4919, Loss(train/val) 0.21425/0.15042. Took 0.04 sec\n",
            "Epoch 4920, Loss(train/val) 0.21280/0.15069. Took 0.04 sec\n",
            "Epoch 4921, Loss(train/val) 0.21376/0.15074. Took 0.04 sec\n",
            "Epoch 4922, Loss(train/val) 0.22021/0.15100. Took 0.05 sec\n",
            "Epoch 4923, Loss(train/val) 0.21950/0.15034. Took 0.05 sec\n",
            "Epoch 4924, Loss(train/val) 0.21124/0.15019. Took 0.05 sec\n",
            "Epoch 4925, Loss(train/val) 0.22475/0.15024. Took 0.05 sec\n",
            "Epoch 4926, Loss(train/val) 0.21267/0.15012. Took 0.04 sec\n",
            "Epoch 4927, Loss(train/val) 0.21609/0.15017. Took 0.04 sec\n",
            "Epoch 4928, Loss(train/val) 0.23362/0.15107. Took 0.05 sec\n",
            "Epoch 4929, Loss(train/val) 0.21861/0.15082. Took 0.04 sec\n",
            "Epoch 4930, Loss(train/val) 0.21478/0.15102. Took 0.05 sec\n",
            "Epoch 4931, Loss(train/val) 0.22423/0.15085. Took 0.05 sec\n",
            "Epoch 4932, Loss(train/val) 0.21594/0.15003. Took 0.04 sec\n",
            "Epoch 4933, Loss(train/val) 0.21446/0.15026. Took 0.05 sec\n",
            "Epoch 4934, Loss(train/val) 0.22975/0.15097. Took 0.05 sec\n",
            "Epoch 4935, Loss(train/val) 0.22065/0.15163. Took 0.05 sec\n",
            "Epoch 4936, Loss(train/val) 0.21650/0.15079. Took 0.04 sec\n",
            "Epoch 4937, Loss(train/val) 0.22187/0.15065. Took 0.04 sec\n",
            "Epoch 4938, Loss(train/val) 0.21672/0.15032. Took 0.05 sec\n",
            "Epoch 4939, Loss(train/val) 0.23258/0.15047. Took 0.04 sec\n",
            "Epoch 4940, Loss(train/val) 0.21250/0.15049. Took 0.05 sec\n",
            "Epoch 4941, Loss(train/val) 0.22305/0.15058. Took 0.05 sec\n",
            "Epoch 4942, Loss(train/val) 0.22516/0.15060. Took 0.04 sec\n",
            "Epoch 4943, Loss(train/val) 0.21935/0.15053. Took 0.05 sec\n",
            "Epoch 4944, Loss(train/val) 0.21319/0.15107. Took 0.05 sec\n",
            "Epoch 4945, Loss(train/val) 0.23087/0.15157. Took 0.04 sec\n",
            "Epoch 4946, Loss(train/val) 0.21178/0.15043. Took 0.04 sec\n",
            "Epoch 4947, Loss(train/val) 0.21912/0.15028. Took 0.05 sec\n",
            "Epoch 4948, Loss(train/val) 0.21765/0.15033. Took 0.05 sec\n",
            "Epoch 4949, Loss(train/val) 0.23245/0.15074. Took 0.05 sec\n",
            "Epoch 4950, Loss(train/val) 0.21810/0.15048. Took 0.04 sec\n",
            "Epoch 4951, Loss(train/val) 0.21620/0.15029. Took 0.05 sec\n",
            "Epoch 4952, Loss(train/val) 0.21564/0.15032. Took 0.04 sec\n",
            "Epoch 4953, Loss(train/val) 0.21357/0.15018. Took 0.06 sec\n",
            "Epoch 4954, Loss(train/val) 0.21465/0.15063. Took 0.04 sec\n",
            "Epoch 4955, Loss(train/val) 0.21781/0.15125. Took 0.04 sec\n",
            "Epoch 4956, Loss(train/val) 0.22618/0.15050. Took 0.04 sec\n",
            "Epoch 4957, Loss(train/val) 0.22127/0.15048. Took 0.04 sec\n",
            "Epoch 4958, Loss(train/val) 0.21733/0.15089. Took 0.05 sec\n",
            "Epoch 4959, Loss(train/val) 0.21536/0.15131. Took 0.04 sec\n",
            "Epoch 4960, Loss(train/val) 0.23781/0.15118. Took 0.05 sec\n",
            "Epoch 4961, Loss(train/val) 0.22010/0.15038. Took 0.05 sec\n",
            "Epoch 4962, Loss(train/val) 0.22012/0.14998. Took 0.04 sec\n",
            "Epoch 4963, Loss(train/val) 0.22699/0.15020. Took 0.05 sec\n",
            "Epoch 4964, Loss(train/val) 0.21453/0.15036. Took 0.04 sec\n",
            "Epoch 4965, Loss(train/val) 0.21003/0.15068. Took 0.05 sec\n",
            "Epoch 4966, Loss(train/val) 0.21460/0.15044. Took 0.04 sec\n",
            "Epoch 4967, Loss(train/val) 0.21775/0.15102. Took 0.05 sec\n",
            "Epoch 4968, Loss(train/val) 0.21522/0.15061. Took 0.05 sec\n",
            "Epoch 4969, Loss(train/val) 0.21237/0.15053. Took 0.05 sec\n",
            "Epoch 4970, Loss(train/val) 0.24021/0.15025. Took 0.04 sec\n",
            "Epoch 4971, Loss(train/val) 0.22244/0.15029. Took 0.05 sec\n",
            "Epoch 4972, Loss(train/val) 0.21888/0.15021. Took 0.04 sec\n",
            "Epoch 4973, Loss(train/val) 0.22045/0.15012. Took 0.06 sec\n",
            "Epoch 4974, Loss(train/val) 0.22600/0.15040. Took 0.04 sec\n",
            "Epoch 4975, Loss(train/val) 0.22790/0.15020. Took 0.04 sec\n",
            "Epoch 4976, Loss(train/val) 0.22739/0.15039. Took 0.04 sec\n",
            "Epoch 4977, Loss(train/val) 0.22010/0.15026. Took 0.04 sec\n",
            "Epoch 4978, Loss(train/val) 0.21993/0.15075. Took 0.05 sec\n",
            "Epoch 4979, Loss(train/val) 0.21174/0.15019. Took 0.05 sec\n",
            "Epoch 4980, Loss(train/val) 0.22639/0.15063. Took 0.04 sec\n",
            "Epoch 4981, Loss(train/val) 0.20877/0.15023. Took 0.05 sec\n",
            "Epoch 4982, Loss(train/val) 0.22241/0.15021. Took 0.05 sec\n",
            "Epoch 4983, Loss(train/val) 0.21293/0.15032. Took 0.05 sec\n",
            "Epoch 4984, Loss(train/val) 0.21651/0.15030. Took 0.04 sec\n",
            "Epoch 4985, Loss(train/val) 0.22241/0.15012. Took 0.04 sec\n",
            "Epoch 4986, Loss(train/val) 0.22863/0.15039. Took 0.05 sec\n",
            "Epoch 4987, Loss(train/val) 0.20976/0.15071. Took 0.04 sec\n",
            "Epoch 4988, Loss(train/val) 0.22993/0.15083. Took 0.06 sec\n",
            "Epoch 4989, Loss(train/val) 0.22627/0.15021. Took 0.05 sec\n",
            "Epoch 4990, Loss(train/val) 0.20530/0.15042. Took 0.04 sec\n",
            "Epoch 4991, Loss(train/val) 0.22821/0.15065. Took 0.04 sec\n",
            "Epoch 4992, Loss(train/val) 0.21898/0.15066. Took 0.05 sec\n",
            "Epoch 4993, Loss(train/val) 0.22185/0.15061. Took 0.05 sec\n",
            "Epoch 4994, Loss(train/val) 0.22088/0.15017. Took 0.04 sec\n",
            "Epoch 4995, Loss(train/val) 0.24276/0.15024. Took 0.05 sec\n",
            "Epoch 4996, Loss(train/val) 0.21550/0.15014. Took 0.05 sec\n",
            "Epoch 4997, Loss(train/val) 0.21621/0.15013. Took 0.05 sec\n",
            "Epoch 4998, Loss(train/val) 0.22146/0.15006. Took 0.05 sec\n",
            "Epoch 4999, Loss(train/val) 0.21701/0.15015. Took 0.04 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var1 = 'lr'\n",
        "df = load_exp_result('exp3_lr_deep2')\n",
        "\n",
        "plot_loss_variation(var1, df, sharey=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "lMMjXDXCwVDQ",
        "outputId": "a7ce8dd9-93bc-4eec-fc94-9bf5d2d8a3db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 707.375x216 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "### lr = 0.00004"
      ],
      "metadata": {
        "id": "aw21WXHbw54Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. L2 norm(l2)"
      ],
      "metadata": {
        "id": "Wf1ht7fLthts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 666\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp4_l2\"\n",
        "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "args.batch_size = 193\n",
        "args.x_frames = 4\n",
        "args.y_frames = 4\n",
        "trainset = MyDataset(args.x_frames, args.y_frames, mydata, 0, int(0.6*len(mydata)))\n",
        "valset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.6*len(mydata)), int(0.8*len(mydata)))\n",
        "testset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.8*len(mydata)), int(len(mydata)))\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
        "\n",
        "args.input_dim = 1\n",
        "args.hid_dim = 16\n",
        "args.n_layers = 8\n",
        "\n",
        "args.l2 = 0.00001\n",
        "args.dropout = 0.0\n",
        "args.use_bn = True\n",
        "\n",
        "args.optim = 'Adam'\n",
        "args.lr = 0.00004\n",
        "args.epoch = 2800\n",
        "\n",
        "name_var1 = 'l2'\n",
        "list_var1 = [0.000003, 0.00001, 0.00003, 0.0001]\n",
        "\n",
        "for var1 in list_var1:\n",
        "    setattr(args, name_var1, var1)\n",
        "    print(args)\n",
        "                \n",
        "    setting, result = experiment(partition, deepcopy(args))\n",
        "    save_exp_result(setting, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c6Ra2ls1ieO",
        "outputId": "713a511c-5bdf-4730-efab-fbbbb2dafd39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Epoch 601, Loss(train/val) 0.55843/0.27868. Took 0.05 sec\n",
            "Epoch 602, Loss(train/val) 0.55182/0.27826. Took 0.05 sec\n",
            "Epoch 603, Loss(train/val) 0.55322/0.27897. Took 0.05 sec\n",
            "Epoch 604, Loss(train/val) 0.54376/0.28034. Took 0.06 sec\n",
            "Epoch 605, Loss(train/val) 0.55819/0.28074. Took 0.05 sec\n",
            "Epoch 606, Loss(train/val) 0.54330/0.28034. Took 0.05 sec\n",
            "Epoch 607, Loss(train/val) 0.54430/0.27978. Took 0.04 sec\n",
            "Epoch 608, Loss(train/val) 0.55404/0.27783. Took 0.04 sec\n",
            "Epoch 609, Loss(train/val) 0.54497/0.27543. Took 0.05 sec\n",
            "Epoch 610, Loss(train/val) 0.56123/0.27467. Took 0.05 sec\n",
            "Epoch 611, Loss(train/val) 0.54252/0.27298. Took 0.05 sec\n",
            "Epoch 612, Loss(train/val) 0.54891/0.27272. Took 0.04 sec\n",
            "Epoch 613, Loss(train/val) 0.54857/0.27242. Took 0.04 sec\n",
            "Epoch 614, Loss(train/val) 0.54161/0.27237. Took 0.05 sec\n",
            "Epoch 615, Loss(train/val) 0.54345/0.27225. Took 0.04 sec\n",
            "Epoch 616, Loss(train/val) 0.54888/0.27302. Took 0.04 sec\n",
            "Epoch 617, Loss(train/val) 0.54492/0.27287. Took 0.05 sec\n",
            "Epoch 618, Loss(train/val) 0.55050/0.27394. Took 0.04 sec\n",
            "Epoch 619, Loss(train/val) 0.53369/0.27366. Took 0.05 sec\n",
            "Epoch 620, Loss(train/val) 0.53266/0.27342. Took 0.05 sec\n",
            "Epoch 621, Loss(train/val) 0.54603/0.27318. Took 0.05 sec\n",
            "Epoch 622, Loss(train/val) 0.54277/0.27388. Took 0.04 sec\n",
            "Epoch 623, Loss(train/val) 0.54205/0.27380. Took 0.05 sec\n",
            "Epoch 624, Loss(train/val) 0.53422/0.27356. Took 0.05 sec\n",
            "Epoch 625, Loss(train/val) 0.54426/0.27389. Took 0.04 sec\n",
            "Epoch 626, Loss(train/val) 0.54650/0.27356. Took 0.05 sec\n",
            "Epoch 627, Loss(train/val) 0.54632/0.27271. Took 0.05 sec\n",
            "Epoch 628, Loss(train/val) 0.54361/0.27231. Took 0.04 sec\n",
            "Epoch 629, Loss(train/val) 0.54254/0.27262. Took 0.05 sec\n",
            "Epoch 630, Loss(train/val) 0.54247/0.27186. Took 0.04 sec\n",
            "Epoch 631, Loss(train/val) 0.54455/0.27089. Took 0.05 sec\n",
            "Epoch 632, Loss(train/val) 0.54091/0.27108. Took 0.05 sec\n",
            "Epoch 633, Loss(train/val) 0.54385/0.27170. Took 0.05 sec\n",
            "Epoch 634, Loss(train/val) 0.54432/0.27243. Took 0.05 sec\n",
            "Epoch 635, Loss(train/val) 0.54468/0.27217. Took 0.04 sec\n",
            "Epoch 636, Loss(train/val) 0.52279/0.27212. Took 0.05 sec\n",
            "Epoch 637, Loss(train/val) 0.53458/0.27229. Took 0.05 sec\n",
            "Epoch 638, Loss(train/val) 0.53514/0.27205. Took 0.05 sec\n",
            "Epoch 639, Loss(train/val) 0.54183/0.27351. Took 0.05 sec\n",
            "Epoch 640, Loss(train/val) 0.54300/0.27371. Took 0.04 sec\n",
            "Epoch 641, Loss(train/val) 0.52933/0.27277. Took 0.04 sec\n",
            "Epoch 642, Loss(train/val) 0.54274/0.27195. Took 0.05 sec\n",
            "Epoch 643, Loss(train/val) 0.53710/0.27176. Took 0.04 sec\n",
            "Epoch 644, Loss(train/val) 0.52486/0.27090. Took 0.05 sec\n",
            "Epoch 645, Loss(train/val) 0.54009/0.26984. Took 0.04 sec\n",
            "Epoch 646, Loss(train/val) 0.53742/0.26906. Took 0.04 sec\n",
            "Epoch 647, Loss(train/val) 0.52742/0.26869. Took 0.05 sec\n",
            "Epoch 648, Loss(train/val) 0.51911/0.26762. Took 0.05 sec\n",
            "Epoch 649, Loss(train/val) 0.51198/0.26742. Took 0.05 sec\n",
            "Epoch 650, Loss(train/val) 0.53496/0.26726. Took 0.04 sec\n",
            "Epoch 651, Loss(train/val) 0.52863/0.26761. Took 0.04 sec\n",
            "Epoch 652, Loss(train/val) 0.53459/0.26899. Took 0.04 sec\n",
            "Epoch 653, Loss(train/val) 0.51898/0.26849. Took 0.05 sec\n",
            "Epoch 654, Loss(train/val) 0.54132/0.26991. Took 0.05 sec\n",
            "Epoch 655, Loss(train/val) 0.52217/0.26828. Took 0.05 sec\n",
            "Epoch 656, Loss(train/val) 0.53464/0.26846. Took 0.04 sec\n",
            "Epoch 657, Loss(train/val) 0.52339/0.26927. Took 0.04 sec\n",
            "Epoch 658, Loss(train/val) 0.53323/0.26904. Took 0.04 sec\n",
            "Epoch 659, Loss(train/val) 0.51838/0.26834. Took 0.05 sec\n",
            "Epoch 660, Loss(train/val) 0.52962/0.26773. Took 0.05 sec\n",
            "Epoch 661, Loss(train/val) 0.52191/0.26811. Took 0.04 sec\n",
            "Epoch 662, Loss(train/val) 0.52736/0.26706. Took 0.04 sec\n",
            "Epoch 663, Loss(train/val) 0.53288/0.26634. Took 0.04 sec\n",
            "Epoch 664, Loss(train/val) 0.53139/0.26761. Took 0.05 sec\n",
            "Epoch 665, Loss(train/val) 0.50980/0.26669. Took 0.04 sec\n",
            "Epoch 666, Loss(train/val) 0.52867/0.26751. Took 0.05 sec\n",
            "Epoch 667, Loss(train/val) 0.52527/0.26948. Took 0.04 sec\n",
            "Epoch 668, Loss(train/val) 0.52579/0.26813. Took 0.04 sec\n",
            "Epoch 669, Loss(train/val) 0.52689/0.26795. Took 0.05 sec\n",
            "Epoch 670, Loss(train/val) 0.52157/0.26676. Took 0.05 sec\n",
            "Epoch 671, Loss(train/val) 0.53114/0.26713. Took 0.06 sec\n",
            "Epoch 672, Loss(train/val) 0.53436/0.26793. Took 0.04 sec\n",
            "Epoch 673, Loss(train/val) 0.52518/0.26660. Took 0.05 sec\n",
            "Epoch 674, Loss(train/val) 0.52431/0.26773. Took 0.05 sec\n",
            "Epoch 675, Loss(train/val) 0.51694/0.26735. Took 0.05 sec\n",
            "Epoch 676, Loss(train/val) 0.52648/0.26629. Took 0.05 sec\n",
            "Epoch 677, Loss(train/val) 0.52528/0.26677. Took 0.04 sec\n",
            "Epoch 678, Loss(train/val) 0.51765/0.26621. Took 0.06 sec\n",
            "Epoch 679, Loss(train/val) 0.52252/0.26540. Took 0.05 sec\n",
            "Epoch 680, Loss(train/val) 0.51676/0.26546. Took 0.05 sec\n",
            "Epoch 681, Loss(train/val) 0.51150/0.26663. Took 0.04 sec\n",
            "Epoch 682, Loss(train/val) 0.50386/0.26747. Took 0.04 sec\n",
            "Epoch 683, Loss(train/val) 0.51216/0.26832. Took 0.06 sec\n",
            "Epoch 684, Loss(train/val) 0.51805/0.26797. Took 0.05 sec\n",
            "Epoch 685, Loss(train/val) 0.52154/0.26875. Took 0.04 sec\n",
            "Epoch 686, Loss(train/val) 0.51420/0.26715. Took 0.04 sec\n",
            "Epoch 687, Loss(train/val) 0.52239/0.26631. Took 0.05 sec\n",
            "Epoch 688, Loss(train/val) 0.50704/0.26553. Took 0.05 sec\n",
            "Epoch 689, Loss(train/val) 0.51589/0.26541. Took 0.04 sec\n",
            "Epoch 690, Loss(train/val) 0.51938/0.26511. Took 0.05 sec\n",
            "Epoch 691, Loss(train/val) 0.52393/0.26450. Took 0.05 sec\n",
            "Epoch 692, Loss(train/val) 0.51573/0.26345. Took 0.04 sec\n",
            "Epoch 693, Loss(train/val) 0.51568/0.26301. Took 0.05 sec\n",
            "Epoch 694, Loss(train/val) 0.51964/0.26287. Took 0.04 sec\n",
            "Epoch 695, Loss(train/val) 0.52074/0.26375. Took 0.04 sec\n",
            "Epoch 696, Loss(train/val) 0.51322/0.26408. Took 0.05 sec\n",
            "Epoch 697, Loss(train/val) 0.50635/0.26375. Took 0.05 sec\n",
            "Epoch 698, Loss(train/val) 0.52445/0.26334. Took 0.05 sec\n",
            "Epoch 699, Loss(train/val) 0.51131/0.26219. Took 0.04 sec\n",
            "Epoch 700, Loss(train/val) 0.51863/0.26161. Took 0.04 sec\n",
            "Epoch 701, Loss(train/val) 0.51828/0.26193. Took 0.05 sec\n",
            "Epoch 702, Loss(train/val) 0.51768/0.26198. Took 0.05 sec\n",
            "Epoch 703, Loss(train/val) 0.52135/0.26285. Took 0.05 sec\n",
            "Epoch 704, Loss(train/val) 0.51913/0.26222. Took 0.05 sec\n",
            "Epoch 705, Loss(train/val) 0.51394/0.26177. Took 0.05 sec\n",
            "Epoch 706, Loss(train/val) 0.51161/0.26258. Took 0.04 sec\n",
            "Epoch 707, Loss(train/val) 0.51869/0.26235. Took 0.04 sec\n",
            "Epoch 708, Loss(train/val) 0.51918/0.26281. Took 0.05 sec\n",
            "Epoch 709, Loss(train/val) 0.50335/0.26230. Took 0.04 sec\n",
            "Epoch 710, Loss(train/val) 0.51215/0.26171. Took 0.05 sec\n",
            "Epoch 711, Loss(train/val) 0.50042/0.26126. Took 0.04 sec\n",
            "Epoch 712, Loss(train/val) 0.50893/0.26114. Took 0.04 sec\n",
            "Epoch 713, Loss(train/val) 0.51161/0.26318. Took 0.05 sec\n",
            "Epoch 714, Loss(train/val) 0.50484/0.26368. Took 0.06 sec\n",
            "Epoch 715, Loss(train/val) 0.50954/0.26554. Took 0.05 sec\n",
            "Epoch 716, Loss(train/val) 0.50667/0.26552. Took 0.05 sec\n",
            "Epoch 717, Loss(train/val) 0.51050/0.26731. Took 0.05 sec\n",
            "Epoch 718, Loss(train/val) 0.50538/0.26804. Took 0.05 sec\n",
            "Epoch 719, Loss(train/val) 0.48819/0.26465. Took 0.04 sec\n",
            "Epoch 720, Loss(train/val) 0.50359/0.26087. Took 0.04 sec\n",
            "Epoch 721, Loss(train/val) 0.49700/0.26005. Took 0.04 sec\n",
            "Epoch 722, Loss(train/val) 0.51044/0.25890. Took 0.05 sec\n",
            "Epoch 723, Loss(train/val) 0.50344/0.25820. Took 0.05 sec\n",
            "Epoch 724, Loss(train/val) 0.50913/0.25896. Took 0.04 sec\n",
            "Epoch 725, Loss(train/val) 0.51006/0.25887. Took 0.04 sec\n",
            "Epoch 726, Loss(train/val) 0.50682/0.26059. Took 0.05 sec\n",
            "Epoch 727, Loss(train/val) 0.50657/0.26020. Took 0.05 sec\n",
            "Epoch 728, Loss(train/val) 0.49494/0.25971. Took 0.05 sec\n",
            "Epoch 729, Loss(train/val) 0.50485/0.25976. Took 0.05 sec\n",
            "Epoch 730, Loss(train/val) 0.50482/0.26136. Took 0.05 sec\n",
            "Epoch 731, Loss(train/val) 0.50981/0.26159. Took 0.05 sec\n",
            "Epoch 732, Loss(train/val) 0.50224/0.26076. Took 0.04 sec\n",
            "Epoch 733, Loss(train/val) 0.50557/0.26196. Took 0.05 sec\n",
            "Epoch 734, Loss(train/val) 0.50620/0.26063. Took 0.05 sec\n",
            "Epoch 735, Loss(train/val) 0.50836/0.25978. Took 0.05 sec\n",
            "Epoch 736, Loss(train/val) 0.49761/0.25907. Took 0.04 sec\n",
            "Epoch 737, Loss(train/val) 0.50848/0.25821. Took 0.05 sec\n",
            "Epoch 738, Loss(train/val) 0.49766/0.25853. Took 0.05 sec\n",
            "Epoch 739, Loss(train/val) 0.50807/0.25978. Took 0.04 sec\n",
            "Epoch 740, Loss(train/val) 0.49823/0.26043. Took 0.04 sec\n",
            "Epoch 741, Loss(train/val) 0.50258/0.25985. Took 0.04 sec\n",
            "Epoch 742, Loss(train/val) 0.50520/0.25826. Took 0.05 sec\n",
            "Epoch 743, Loss(train/val) 0.49752/0.25707. Took 0.05 sec\n",
            "Epoch 744, Loss(train/val) 0.50515/0.25767. Took 0.05 sec\n",
            "Epoch 745, Loss(train/val) 0.49317/0.25716. Took 0.05 sec\n",
            "Epoch 746, Loss(train/val) 0.48388/0.25623. Took 0.05 sec\n",
            "Epoch 747, Loss(train/val) 0.48742/0.25551. Took 0.05 sec\n",
            "Epoch 748, Loss(train/val) 0.50161/0.25530. Took 0.05 sec\n",
            "Epoch 749, Loss(train/val) 0.50672/0.25507. Took 0.05 sec\n",
            "Epoch 750, Loss(train/val) 0.49810/0.25501. Took 0.04 sec\n",
            "Epoch 751, Loss(train/val) 0.49958/0.25502. Took 0.04 sec\n",
            "Epoch 752, Loss(train/val) 0.49041/0.25574. Took 0.04 sec\n",
            "Epoch 753, Loss(train/val) 0.49930/0.25702. Took 0.06 sec\n",
            "Epoch 754, Loss(train/val) 0.49566/0.25842. Took 0.05 sec\n",
            "Epoch 755, Loss(train/val) 0.50037/0.25937. Took 0.05 sec\n",
            "Epoch 756, Loss(train/val) 0.50240/0.25850. Took 0.04 sec\n",
            "Epoch 757, Loss(train/val) 0.48882/0.25745. Took 0.05 sec\n",
            "Epoch 758, Loss(train/val) 0.49510/0.25651. Took 0.05 sec\n",
            "Epoch 759, Loss(train/val) 0.47664/0.25622. Took 0.05 sec\n",
            "Epoch 760, Loss(train/val) 0.49905/0.25667. Took 0.05 sec\n",
            "Epoch 761, Loss(train/val) 0.49079/0.25788. Took 0.05 sec\n",
            "Epoch 762, Loss(train/val) 0.49118/0.25657. Took 0.05 sec\n",
            "Epoch 763, Loss(train/val) 0.49549/0.25684. Took 0.05 sec\n",
            "Epoch 764, Loss(train/val) 0.47866/0.25672. Took 0.04 sec\n",
            "Epoch 765, Loss(train/val) 0.49236/0.25703. Took 0.04 sec\n",
            "Epoch 766, Loss(train/val) 0.49459/0.25602. Took 0.05 sec\n",
            "Epoch 767, Loss(train/val) 0.48714/0.25556. Took 0.05 sec\n",
            "Epoch 768, Loss(train/val) 0.49409/0.25559. Took 0.05 sec\n",
            "Epoch 769, Loss(train/val) 0.48586/0.25544. Took 0.05 sec\n",
            "Epoch 770, Loss(train/val) 0.49006/0.25679. Took 0.05 sec\n",
            "Epoch 771, Loss(train/val) 0.49441/0.25764. Took 0.04 sec\n",
            "Epoch 772, Loss(train/val) 0.48687/0.25773. Took 0.05 sec\n",
            "Epoch 773, Loss(train/val) 0.49860/0.25595. Took 0.05 sec\n",
            "Epoch 774, Loss(train/val) 0.48864/0.25412. Took 0.04 sec\n",
            "Epoch 775, Loss(train/val) 0.48430/0.25318. Took 0.04 sec\n",
            "Epoch 776, Loss(train/val) 0.48653/0.25266. Took 0.04 sec\n",
            "Epoch 777, Loss(train/val) 0.49441/0.25244. Took 0.05 sec\n",
            "Epoch 778, Loss(train/val) 0.49169/0.25233. Took 0.05 sec\n",
            "Epoch 779, Loss(train/val) 0.49302/0.25402. Took 0.05 sec\n",
            "Epoch 780, Loss(train/val) 0.48853/0.25393. Took 0.05 sec\n",
            "Epoch 781, Loss(train/val) 0.48478/0.25535. Took 0.05 sec\n",
            "Epoch 782, Loss(train/val) 0.48799/0.25676. Took 0.04 sec\n",
            "Epoch 783, Loss(train/val) 0.48922/0.25858. Took 0.05 sec\n",
            "Epoch 784, Loss(train/val) 0.49593/0.25819. Took 0.04 sec\n",
            "Epoch 785, Loss(train/val) 0.48386/0.25512. Took 0.04 sec\n",
            "Epoch 786, Loss(train/val) 0.48873/0.25310. Took 0.04 sec\n",
            "Epoch 787, Loss(train/val) 0.49102/0.25256. Took 0.04 sec\n",
            "Epoch 788, Loss(train/val) 0.47271/0.25257. Took 0.05 sec\n",
            "Epoch 789, Loss(train/val) 0.47349/0.25244. Took 0.05 sec\n",
            "Epoch 790, Loss(train/val) 0.48831/0.25256. Took 0.05 sec\n",
            "Epoch 791, Loss(train/val) 0.48492/0.25230. Took 0.05 sec\n",
            "Epoch 792, Loss(train/val) 0.49075/0.25211. Took 0.05 sec\n",
            "Epoch 793, Loss(train/val) 0.48173/0.25299. Took 0.05 sec\n",
            "Epoch 794, Loss(train/val) 0.46251/0.25251. Took 0.05 sec\n",
            "Epoch 795, Loss(train/val) 0.48545/0.25173. Took 0.05 sec\n",
            "Epoch 796, Loss(train/val) 0.48598/0.25237. Took 0.04 sec\n",
            "Epoch 797, Loss(train/val) 0.46561/0.25219. Took 0.04 sec\n",
            "Epoch 798, Loss(train/val) 0.48503/0.25205. Took 0.06 sec\n",
            "Epoch 799, Loss(train/val) 0.48613/0.25292. Took 0.05 sec\n",
            "Epoch 800, Loss(train/val) 0.48966/0.25433. Took 0.05 sec\n",
            "Epoch 801, Loss(train/val) 0.47766/0.25338. Took 0.05 sec\n",
            "Epoch 802, Loss(train/val) 0.47688/0.25320. Took 0.05 sec\n",
            "Epoch 803, Loss(train/val) 0.48676/0.25382. Took 0.05 sec\n",
            "Epoch 804, Loss(train/val) 0.48033/0.25196. Took 0.05 sec\n",
            "Epoch 805, Loss(train/val) 0.48436/0.25118. Took 0.05 sec\n",
            "Epoch 806, Loss(train/val) 0.47951/0.25221. Took 0.05 sec\n",
            "Epoch 807, Loss(train/val) 0.47096/0.25370. Took 0.04 sec\n",
            "Epoch 808, Loss(train/val) 0.48436/0.25282. Took 0.05 sec\n",
            "Epoch 809, Loss(train/val) 0.48026/0.25370. Took 0.05 sec\n",
            "Epoch 810, Loss(train/val) 0.46242/0.25220. Took 0.04 sec\n",
            "Epoch 811, Loss(train/val) 0.48141/0.25090. Took 0.05 sec\n",
            "Epoch 812, Loss(train/val) 0.48326/0.24978. Took 0.05 sec\n",
            "Epoch 813, Loss(train/val) 0.46412/0.24867. Took 0.05 sec\n",
            "Epoch 814, Loss(train/val) 0.48028/0.24828. Took 0.05 sec\n",
            "Epoch 815, Loss(train/val) 0.48197/0.24810. Took 0.04 sec\n",
            "Epoch 816, Loss(train/val) 0.47811/0.24797. Took 0.04 sec\n",
            "Epoch 817, Loss(train/val) 0.47676/0.24816. Took 0.04 sec\n",
            "Epoch 818, Loss(train/val) 0.47574/0.24831. Took 0.05 sec\n",
            "Epoch 819, Loss(train/val) 0.47963/0.24826. Took 0.04 sec\n",
            "Epoch 820, Loss(train/val) 0.47715/0.24841. Took 0.05 sec\n",
            "Epoch 821, Loss(train/val) 0.47705/0.24917. Took 0.05 sec\n",
            "Epoch 822, Loss(train/val) 0.48162/0.24948. Took 0.04 sec\n",
            "Epoch 823, Loss(train/val) 0.46866/0.24963. Took 0.05 sec\n",
            "Epoch 824, Loss(train/val) 0.47538/0.24961. Took 0.05 sec\n",
            "Epoch 825, Loss(train/val) 0.47483/0.24976. Took 0.04 sec\n",
            "Epoch 826, Loss(train/val) 0.47606/0.24944. Took 0.05 sec\n",
            "Epoch 827, Loss(train/val) 0.47213/0.24851. Took 0.05 sec\n",
            "Epoch 828, Loss(train/val) 0.47453/0.24748. Took 0.06 sec\n",
            "Epoch 829, Loss(train/val) 0.45985/0.24658. Took 0.05 sec\n",
            "Epoch 830, Loss(train/val) 0.47268/0.24627. Took 0.05 sec\n",
            "Epoch 831, Loss(train/val) 0.47137/0.24615. Took 0.04 sec\n",
            "Epoch 832, Loss(train/val) 0.47220/0.24593. Took 0.05 sec\n",
            "Epoch 833, Loss(train/val) 0.47248/0.24682. Took 0.05 sec\n",
            "Epoch 834, Loss(train/val) 0.47270/0.24702. Took 0.05 sec\n",
            "Epoch 835, Loss(train/val) 0.46812/0.24698. Took 0.04 sec\n",
            "Epoch 836, Loss(train/val) 0.47087/0.24618. Took 0.04 sec\n",
            "Epoch 837, Loss(train/val) 0.46694/0.24579. Took 0.05 sec\n",
            "Epoch 838, Loss(train/val) 0.46877/0.24534. Took 0.05 sec\n",
            "Epoch 839, Loss(train/val) 0.47184/0.24481. Took 0.04 sec\n",
            "Epoch 840, Loss(train/val) 0.45993/0.24516. Took 0.05 sec\n",
            "Epoch 841, Loss(train/val) 0.46985/0.24506. Took 0.05 sec\n",
            "Epoch 842, Loss(train/val) 0.48001/0.24567. Took 0.05 sec\n",
            "Epoch 843, Loss(train/val) 0.47668/0.24688. Took 0.06 sec\n",
            "Epoch 844, Loss(train/val) 0.45914/0.24780. Took 0.04 sec\n",
            "Epoch 845, Loss(train/val) 0.46305/0.24684. Took 0.05 sec\n",
            "Epoch 846, Loss(train/val) 0.46893/0.24650. Took 0.04 sec\n",
            "Epoch 847, Loss(train/val) 0.46903/0.24513. Took 0.04 sec\n",
            "Epoch 848, Loss(train/val) 0.46096/0.24418. Took 0.05 sec\n",
            "Epoch 849, Loss(train/val) 0.45662/0.24358. Took 0.05 sec\n",
            "Epoch 850, Loss(train/val) 0.46619/0.24320. Took 0.04 sec\n",
            "Epoch 851, Loss(train/val) 0.46930/0.24292. Took 0.05 sec\n",
            "Epoch 852, Loss(train/val) 0.45589/0.24273. Took 0.05 sec\n",
            "Epoch 853, Loss(train/val) 0.46851/0.24273. Took 0.05 sec\n",
            "Epoch 854, Loss(train/val) 0.46176/0.24312. Took 0.04 sec\n",
            "Epoch 855, Loss(train/val) 0.46077/0.24406. Took 0.04 sec\n",
            "Epoch 856, Loss(train/val) 0.45616/0.24453. Took 0.04 sec\n",
            "Epoch 857, Loss(train/val) 0.46204/0.24505. Took 0.05 sec\n",
            "Epoch 858, Loss(train/val) 0.46311/0.25019. Took 0.05 sec\n",
            "Epoch 859, Loss(train/val) 0.45664/0.25063. Took 0.05 sec\n",
            "Epoch 860, Loss(train/val) 0.46201/0.25346. Took 0.05 sec\n",
            "Epoch 861, Loss(train/val) 0.44854/0.25099. Took 0.04 sec\n",
            "Epoch 862, Loss(train/val) 0.47968/0.24845. Took 0.05 sec\n",
            "Epoch 863, Loss(train/val) 0.46916/0.24659. Took 0.06 sec\n",
            "Epoch 864, Loss(train/val) 0.46151/0.24407. Took 0.05 sec\n",
            "Epoch 865, Loss(train/val) 0.45730/0.24357. Took 0.04 sec\n",
            "Epoch 866, Loss(train/val) 0.45144/0.24135. Took 0.04 sec\n",
            "Epoch 867, Loss(train/val) 0.44882/0.24032. Took 0.04 sec\n",
            "Epoch 868, Loss(train/val) 0.46490/0.23979. Took 0.05 sec\n",
            "Epoch 869, Loss(train/val) 0.45978/0.23950. Took 0.05 sec\n",
            "Epoch 870, Loss(train/val) 0.45398/0.23953. Took 0.04 sec\n",
            "Epoch 871, Loss(train/val) 0.44824/0.24017. Took 0.05 sec\n",
            "Epoch 872, Loss(train/val) 0.46251/0.24127. Took 0.04 sec\n",
            "Epoch 873, Loss(train/val) 0.46398/0.24268. Took 0.05 sec\n",
            "Epoch 874, Loss(train/val) 0.45365/0.24257. Took 0.05 sec\n",
            "Epoch 875, Loss(train/val) 0.45708/0.24272. Took 0.04 sec\n",
            "Epoch 876, Loss(train/val) 0.46075/0.24307. Took 0.05 sec\n",
            "Epoch 877, Loss(train/val) 0.45483/0.24318. Took 0.04 sec\n",
            "Epoch 878, Loss(train/val) 0.45644/0.24476. Took 0.05 sec\n",
            "Epoch 879, Loss(train/val) 0.44162/0.24442. Took 0.05 sec\n",
            "Epoch 880, Loss(train/val) 0.45786/0.24532. Took 0.05 sec\n",
            "Epoch 881, Loss(train/val) 0.44811/0.24340. Took 0.04 sec\n",
            "Epoch 882, Loss(train/val) 0.45149/0.24088. Took 0.04 sec\n",
            "Epoch 883, Loss(train/val) 0.45617/0.24548. Took 0.05 sec\n",
            "Epoch 884, Loss(train/val) 0.45290/0.24381. Took 0.05 sec\n",
            "Epoch 885, Loss(train/val) 0.45204/0.24379. Took 0.05 sec\n",
            "Epoch 886, Loss(train/val) 0.45356/0.24168. Took 0.04 sec\n",
            "Epoch 887, Loss(train/val) 0.45169/0.24100. Took 0.05 sec\n",
            "Epoch 888, Loss(train/val) 0.45480/0.24048. Took 0.05 sec\n",
            "Epoch 889, Loss(train/val) 0.45683/0.24035. Took 0.05 sec\n",
            "Epoch 890, Loss(train/val) 0.45271/0.23917. Took 0.04 sec\n",
            "Epoch 891, Loss(train/val) 0.45092/0.23962. Took 0.04 sec\n",
            "Epoch 892, Loss(train/val) 0.44725/0.23923. Took 0.04 sec\n",
            "Epoch 893, Loss(train/val) 0.44999/0.24048. Took 0.05 sec\n",
            "Epoch 894, Loss(train/val) 0.44556/0.24096. Took 0.05 sec\n",
            "Epoch 895, Loss(train/val) 0.44884/0.24121. Took 0.05 sec\n",
            "Epoch 896, Loss(train/val) 0.44858/0.24071. Took 0.04 sec\n",
            "Epoch 897, Loss(train/val) 0.44524/0.24170. Took 0.05 sec\n",
            "Epoch 898, Loss(train/val) 0.44341/0.24100. Took 0.05 sec\n",
            "Epoch 899, Loss(train/val) 0.44325/0.23980. Took 0.05 sec\n",
            "Epoch 900, Loss(train/val) 0.45165/0.23999. Took 0.04 sec\n",
            "Epoch 901, Loss(train/val) 0.43083/0.23997. Took 0.04 sec\n",
            "Epoch 902, Loss(train/val) 0.44422/0.24295. Took 0.06 sec\n",
            "Epoch 903, Loss(train/val) 0.43937/0.24424. Took 0.05 sec\n",
            "Epoch 904, Loss(train/val) 0.42919/0.24390. Took 0.04 sec\n",
            "Epoch 905, Loss(train/val) 0.44391/0.24590. Took 0.05 sec\n",
            "Epoch 906, Loss(train/val) 0.43533/0.25655. Took 0.05 sec\n",
            "Epoch 907, Loss(train/val) 0.44128/0.26430. Took 0.05 sec\n",
            "Epoch 908, Loss(train/val) 0.44366/0.27191. Took 0.05 sec\n",
            "Epoch 909, Loss(train/val) 0.43857/0.27656. Took 0.04 sec\n",
            "Epoch 910, Loss(train/val) 0.43704/0.28207. Took 0.04 sec\n",
            "Epoch 911, Loss(train/val) 0.43175/0.28942. Took 0.04 sec\n",
            "Epoch 912, Loss(train/val) 0.42913/0.28086. Took 0.05 sec\n",
            "Epoch 913, Loss(train/val) 0.43491/0.26692. Took 0.05 sec\n",
            "Epoch 914, Loss(train/val) 0.42545/0.25156. Took 0.05 sec\n",
            "Epoch 915, Loss(train/val) 0.43522/0.24540. Took 0.04 sec\n",
            "Epoch 916, Loss(train/val) 0.42927/0.24381. Took 0.05 sec\n",
            "Epoch 917, Loss(train/val) 0.42575/0.24015. Took 0.04 sec\n",
            "Epoch 918, Loss(train/val) 0.42666/0.24002. Took 0.05 sec\n",
            "Epoch 919, Loss(train/val) 0.41273/0.23758. Took 0.05 sec\n",
            "Epoch 920, Loss(train/val) 0.41966/0.23726. Took 0.05 sec\n",
            "Epoch 921, Loss(train/val) 0.41483/0.23723. Took 0.04 sec\n",
            "Epoch 922, Loss(train/val) 0.42296/0.23728. Took 0.05 sec\n",
            "Epoch 923, Loss(train/val) 0.42245/0.23610. Took 0.06 sec\n",
            "Epoch 924, Loss(train/val) 0.40645/0.23456. Took 0.04 sec\n",
            "Epoch 925, Loss(train/val) 0.42657/0.23537. Took 0.04 sec\n",
            "Epoch 926, Loss(train/val) 0.41578/0.23645. Took 0.04 sec\n",
            "Epoch 927, Loss(train/val) 0.42122/0.23726. Took 0.05 sec\n",
            "Epoch 928, Loss(train/val) 0.40853/0.23540. Took 0.05 sec\n",
            "Epoch 929, Loss(train/val) 0.41733/0.23398. Took 0.04 sec\n",
            "Epoch 930, Loss(train/val) 0.41620/0.23390. Took 0.04 sec\n",
            "Epoch 931, Loss(train/val) 0.41256/0.23544. Took 0.05 sec\n",
            "Epoch 932, Loss(train/val) 0.41278/0.23741. Took 0.04 sec\n",
            "Epoch 933, Loss(train/val) 0.41773/0.23746. Took 0.05 sec\n",
            "Epoch 934, Loss(train/val) 0.41315/0.23713. Took 0.05 sec\n",
            "Epoch 935, Loss(train/val) 0.40553/0.24028. Took 0.05 sec\n",
            "Epoch 936, Loss(train/val) 0.40982/0.23765. Took 0.04 sec\n",
            "Epoch 937, Loss(train/val) 0.40499/0.23740. Took 0.05 sec\n",
            "Epoch 938, Loss(train/val) 0.41213/0.23586. Took 0.05 sec\n",
            "Epoch 939, Loss(train/val) 0.40606/0.23481. Took 0.05 sec\n",
            "Epoch 940, Loss(train/val) 0.40502/0.23578. Took 0.04 sec\n",
            "Epoch 941, Loss(train/val) 0.40325/0.23498. Took 0.04 sec\n",
            "Epoch 942, Loss(train/val) 0.40551/0.23369. Took 0.05 sec\n",
            "Epoch 943, Loss(train/val) 0.40522/0.23206. Took 0.05 sec\n",
            "Epoch 944, Loss(train/val) 0.39981/0.23123. Took 0.05 sec\n",
            "Epoch 945, Loss(train/val) 0.40295/0.23127. Took 0.05 sec\n",
            "Epoch 946, Loss(train/val) 0.40280/0.23105. Took 0.05 sec\n",
            "Epoch 947, Loss(train/val) 0.40621/0.23075. Took 0.04 sec\n",
            "Epoch 948, Loss(train/val) 0.40925/0.23091. Took 0.06 sec\n",
            "Epoch 949, Loss(train/val) 0.40073/0.23074. Took 0.05 sec\n",
            "Epoch 950, Loss(train/val) 0.40060/0.23089. Took 0.04 sec\n",
            "Epoch 951, Loss(train/val) 0.39779/0.23067. Took 0.05 sec\n",
            "Epoch 952, Loss(train/val) 0.39351/0.23061. Took 0.05 sec\n",
            "Epoch 953, Loss(train/val) 0.39575/0.23070. Took 0.05 sec\n",
            "Epoch 954, Loss(train/val) 0.39950/0.23081. Took 0.05 sec\n",
            "Epoch 955, Loss(train/val) 0.40537/0.23093. Took 0.04 sec\n",
            "Epoch 956, Loss(train/val) 0.40537/0.23079. Took 0.04 sec\n",
            "Epoch 957, Loss(train/val) 0.39851/0.23084. Took 0.04 sec\n",
            "Epoch 958, Loss(train/val) 0.39700/0.23115. Took 0.05 sec\n",
            "Epoch 959, Loss(train/val) 0.40500/0.23098. Took 0.05 sec\n",
            "Epoch 960, Loss(train/val) 0.39834/0.23051. Took 0.04 sec\n",
            "Epoch 961, Loss(train/val) 0.40113/0.23029. Took 0.05 sec\n",
            "Epoch 962, Loss(train/val) 0.38837/0.23040. Took 0.04 sec\n",
            "Epoch 963, Loss(train/val) 0.39804/0.23057. Took 0.05 sec\n",
            "Epoch 964, Loss(train/val) 0.40549/0.23117. Took 0.05 sec\n",
            "Epoch 965, Loss(train/val) 0.39763/0.23068. Took 0.05 sec\n",
            "Epoch 966, Loss(train/val) 0.38494/0.23038. Took 0.05 sec\n",
            "Epoch 967, Loss(train/val) 0.38948/0.23018. Took 0.05 sec\n",
            "Epoch 968, Loss(train/val) 0.39213/0.23066. Took 0.05 sec\n",
            "Epoch 969, Loss(train/val) 0.40127/0.23030. Took 0.05 sec\n",
            "Epoch 970, Loss(train/val) 0.39741/0.23014. Took 0.06 sec\n",
            "Epoch 971, Loss(train/val) 0.39374/0.22982. Took 0.05 sec\n",
            "Epoch 972, Loss(train/val) 0.39747/0.23011. Took 0.05 sec\n",
            "Epoch 973, Loss(train/val) 0.39939/0.22999. Took 0.05 sec\n",
            "Epoch 974, Loss(train/val) 0.37956/0.22998. Took 0.04 sec\n",
            "Epoch 975, Loss(train/val) 0.38194/0.22952. Took 0.04 sec\n",
            "Epoch 976, Loss(train/val) 0.38366/0.22959. Took 0.04 sec\n",
            "Epoch 977, Loss(train/val) 0.39388/0.22939. Took 0.05 sec\n",
            "Epoch 978, Loss(train/val) 0.39014/0.22930. Took 0.06 sec\n",
            "Epoch 979, Loss(train/val) 0.39794/0.22952. Took 0.05 sec\n",
            "Epoch 980, Loss(train/val) 0.39125/0.22919. Took 0.05 sec\n",
            "Epoch 981, Loss(train/val) 0.39111/0.22894. Took 0.05 sec\n",
            "Epoch 982, Loss(train/val) 0.38300/0.22890. Took 0.05 sec\n",
            "Epoch 983, Loss(train/val) 0.40035/0.22949. Took 0.05 sec\n",
            "Epoch 984, Loss(train/val) 0.38424/0.22953. Took 0.05 sec\n",
            "Epoch 985, Loss(train/val) 0.39121/0.22889. Took 0.05 sec\n",
            "Epoch 986, Loss(train/val) 0.38280/0.22896. Took 0.04 sec\n",
            "Epoch 987, Loss(train/val) 0.39223/0.22856. Took 0.05 sec\n",
            "Epoch 988, Loss(train/val) 0.39078/0.22791. Took 0.05 sec\n",
            "Epoch 989, Loss(train/val) 0.39367/0.22772. Took 0.05 sec\n",
            "Epoch 990, Loss(train/val) 0.37677/0.22792. Took 0.05 sec\n",
            "Epoch 991, Loss(train/val) 0.38704/0.22780. Took 0.06 sec\n",
            "Epoch 992, Loss(train/val) 0.38704/0.22790. Took 0.05 sec\n",
            "Epoch 993, Loss(train/val) 0.39084/0.22775. Took 0.05 sec\n",
            "Epoch 994, Loss(train/val) 0.38583/0.22796. Took 0.04 sec\n",
            "Epoch 995, Loss(train/val) 0.38528/0.22751. Took 0.04 sec\n",
            "Epoch 996, Loss(train/val) 0.38145/0.22766. Took 0.04 sec\n",
            "Epoch 997, Loss(train/val) 0.38458/0.22758. Took 0.05 sec\n",
            "Epoch 998, Loss(train/val) 0.38088/0.22757. Took 0.04 sec\n",
            "Epoch 999, Loss(train/val) 0.38597/0.22763. Took 0.04 sec\n",
            "Epoch 1000, Loss(train/val) 0.38165/0.22731. Took 0.05 sec\n",
            "Epoch 1001, Loss(train/val) 0.37004/0.22729. Took 0.04 sec\n",
            "Epoch 1002, Loss(train/val) 0.38360/0.22713. Took 0.05 sec\n",
            "Epoch 1003, Loss(train/val) 0.38037/0.22734. Took 0.04 sec\n",
            "Epoch 1004, Loss(train/val) 0.38496/0.22704. Took 0.04 sec\n",
            "Epoch 1005, Loss(train/val) 0.37937/0.22702. Took 0.05 sec\n",
            "Epoch 1006, Loss(train/val) 0.38945/0.22694. Took 0.04 sec\n",
            "Epoch 1007, Loss(train/val) 0.37748/0.22778. Took 0.05 sec\n",
            "Epoch 1008, Loss(train/val) 0.37614/0.22772. Took 0.04 sec\n",
            "Epoch 1009, Loss(train/val) 0.38206/0.22809. Took 0.04 sec\n",
            "Epoch 1010, Loss(train/val) 0.38971/0.22860. Took 0.04 sec\n",
            "Epoch 1011, Loss(train/val) 0.38605/0.22852. Took 0.05 sec\n",
            "Epoch 1012, Loss(train/val) 0.37918/0.22852. Took 0.05 sec\n",
            "Epoch 1013, Loss(train/val) 0.37835/0.22677. Took 0.06 sec\n",
            "Epoch 1014, Loss(train/val) 0.37318/0.22679. Took 0.05 sec\n",
            "Epoch 1015, Loss(train/val) 0.37894/0.22649. Took 0.05 sec\n",
            "Epoch 1016, Loss(train/val) 0.37881/0.22673. Took 0.05 sec\n",
            "Epoch 1017, Loss(train/val) 0.37371/0.22713. Took 0.05 sec\n",
            "Epoch 1018, Loss(train/val) 0.38015/0.22694. Took 0.04 sec\n",
            "Epoch 1019, Loss(train/val) 0.36885/0.22691. Took 0.04 sec\n",
            "Epoch 1020, Loss(train/val) 0.37245/0.22714. Took 0.05 sec\n",
            "Epoch 1021, Loss(train/val) 0.37074/0.22731. Took 0.05 sec\n",
            "Epoch 1022, Loss(train/val) 0.38589/0.22713. Took 0.04 sec\n",
            "Epoch 1023, Loss(train/val) 0.38167/0.22749. Took 0.04 sec\n",
            "Epoch 1024, Loss(train/val) 0.37864/0.22689. Took 0.05 sec\n",
            "Epoch 1025, Loss(train/val) 0.37718/0.22645. Took 0.05 sec\n",
            "Epoch 1026, Loss(train/val) 0.37126/0.22657. Took 0.05 sec\n",
            "Epoch 1027, Loss(train/val) 0.38112/0.22641. Took 0.04 sec\n",
            "Epoch 1028, Loss(train/val) 0.38209/0.22628. Took 0.04 sec\n",
            "Epoch 1029, Loss(train/val) 0.37455/0.22617. Took 0.04 sec\n",
            "Epoch 1030, Loss(train/val) 0.37366/0.22644. Took 0.04 sec\n",
            "Epoch 1031, Loss(train/val) 0.37063/0.22606. Took 0.05 sec\n",
            "Epoch 1032, Loss(train/val) 0.37856/0.22618. Took 0.05 sec\n",
            "Epoch 1033, Loss(train/val) 0.36963/0.22605. Took 0.05 sec\n",
            "Epoch 1034, Loss(train/val) 0.38444/0.22582. Took 0.05 sec\n",
            "Epoch 1035, Loss(train/val) 0.37293/0.22584. Took 0.05 sec\n",
            "Epoch 1036, Loss(train/val) 0.37536/0.22577. Took 0.05 sec\n",
            "Epoch 1037, Loss(train/val) 0.37326/0.22585. Took 0.04 sec\n",
            "Epoch 1038, Loss(train/val) 0.38119/0.22624. Took 0.05 sec\n",
            "Epoch 1039, Loss(train/val) 0.36941/0.22631. Took 0.05 sec\n",
            "Epoch 1040, Loss(train/val) 0.38024/0.22588. Took 0.05 sec\n",
            "Epoch 1041, Loss(train/val) 0.36954/0.22594. Took 0.05 sec\n",
            "Epoch 1042, Loss(train/val) 0.38005/0.22579. Took 0.05 sec\n",
            "Epoch 1043, Loss(train/val) 0.37262/0.22554. Took 0.04 sec\n",
            "Epoch 1044, Loss(train/val) 0.37075/0.22585. Took 0.04 sec\n",
            "Epoch 1045, Loss(train/val) 0.37625/0.22552. Took 0.04 sec\n",
            "Epoch 1046, Loss(train/val) 0.38070/0.22526. Took 0.05 sec\n",
            "Epoch 1047, Loss(train/val) 0.36973/0.22532. Took 0.05 sec\n",
            "Epoch 1048, Loss(train/val) 0.39292/0.22558. Took 0.04 sec\n",
            "Epoch 1049, Loss(train/val) 0.37610/0.22570. Took 0.04 sec\n",
            "Epoch 1050, Loss(train/val) 0.36950/0.22577. Took 0.04 sec\n",
            "Epoch 1051, Loss(train/val) 0.36881/0.22557. Took 0.05 sec\n",
            "Epoch 1052, Loss(train/val) 0.37384/0.22530. Took 0.04 sec\n",
            "Epoch 1053, Loss(train/val) 0.36172/0.22503. Took 0.05 sec\n",
            "Epoch 1054, Loss(train/val) 0.37540/0.22505. Took 0.05 sec\n",
            "Epoch 1055, Loss(train/val) 0.36508/0.22509. Took 0.04 sec\n",
            "Epoch 1056, Loss(train/val) 0.37316/0.22538. Took 0.06 sec\n",
            "Epoch 1057, Loss(train/val) 0.36981/0.22669. Took 0.04 sec\n",
            "Epoch 1058, Loss(train/val) 0.35672/0.22701. Took 0.04 sec\n",
            "Epoch 1059, Loss(train/val) 0.37423/0.22742. Took 0.04 sec\n",
            "Epoch 1060, Loss(train/val) 0.37313/0.22776. Took 0.04 sec\n",
            "Epoch 1061, Loss(train/val) 0.36733/0.22562. Took 0.06 sec\n",
            "Epoch 1062, Loss(train/val) 0.37351/0.22489. Took 0.05 sec\n",
            "Epoch 1063, Loss(train/val) 0.37061/0.22438. Took 0.05 sec\n",
            "Epoch 1064, Loss(train/val) 0.36818/0.22437. Took 0.05 sec\n",
            "Epoch 1065, Loss(train/val) 0.36130/0.22433. Took 0.05 sec\n",
            "Epoch 1066, Loss(train/val) 0.37587/0.22437. Took 0.05 sec\n",
            "Epoch 1067, Loss(train/val) 0.36398/0.22432. Took 0.04 sec\n",
            "Epoch 1068, Loss(train/val) 0.36448/0.22440. Took 0.04 sec\n",
            "Epoch 1069, Loss(train/val) 0.36529/0.22425. Took 0.04 sec\n",
            "Epoch 1070, Loss(train/val) 0.36378/0.22432. Took 0.05 sec\n",
            "Epoch 1071, Loss(train/val) 0.37101/0.22435. Took 0.05 sec\n",
            "Epoch 1072, Loss(train/val) 0.35481/0.22451. Took 0.04 sec\n",
            "Epoch 1073, Loss(train/val) 0.36883/0.22452. Took 0.04 sec\n",
            "Epoch 1074, Loss(train/val) 0.36794/0.22475. Took 0.05 sec\n",
            "Epoch 1075, Loss(train/val) 0.37143/0.22469. Took 0.05 sec\n",
            "Epoch 1076, Loss(train/val) 0.35455/0.22544. Took 0.05 sec\n",
            "Epoch 1077, Loss(train/val) 0.36519/0.22536. Took 0.05 sec\n",
            "Epoch 1078, Loss(train/val) 0.36896/0.22491. Took 0.05 sec\n",
            "Epoch 1079, Loss(train/val) 0.36321/0.22506. Took 0.05 sec\n",
            "Epoch 1080, Loss(train/val) 0.36941/0.22453. Took 0.05 sec\n",
            "Epoch 1081, Loss(train/val) 0.36377/0.22385. Took 0.05 sec\n",
            "Epoch 1082, Loss(train/val) 0.36440/0.22373. Took 0.05 sec\n",
            "Epoch 1083, Loss(train/val) 0.36318/0.22391. Took 0.04 sec\n",
            "Epoch 1084, Loss(train/val) 0.38477/0.22405. Took 0.05 sec\n",
            "Epoch 1085, Loss(train/val) 0.36885/0.22396. Took 0.04 sec\n",
            "Epoch 1086, Loss(train/val) 0.35926/0.22369. Took 0.05 sec\n",
            "Epoch 1087, Loss(train/val) 0.36384/0.22336. Took 0.04 sec\n",
            "Epoch 1088, Loss(train/val) 0.36167/0.22407. Took 0.04 sec\n",
            "Epoch 1089, Loss(train/val) 0.36147/0.22360. Took 0.05 sec\n",
            "Epoch 1090, Loss(train/val) 0.35744/0.22343. Took 0.04 sec\n",
            "Epoch 1091, Loss(train/val) 0.36167/0.22318. Took 0.05 sec\n",
            "Epoch 1092, Loss(train/val) 0.35896/0.22301. Took 0.04 sec\n",
            "Epoch 1093, Loss(train/val) 0.36172/0.22291. Took 0.05 sec\n",
            "Epoch 1094, Loss(train/val) 0.36408/0.22293. Took 0.04 sec\n",
            "Epoch 1095, Loss(train/val) 0.35195/0.22318. Took 0.05 sec\n",
            "Epoch 1096, Loss(train/val) 0.37328/0.22322. Took 0.05 sec\n",
            "Epoch 1097, Loss(train/val) 0.36728/0.22317. Took 0.05 sec\n",
            "Epoch 1098, Loss(train/val) 0.36850/0.22286. Took 0.05 sec\n",
            "Epoch 1099, Loss(train/val) 0.37326/0.22345. Took 0.06 sec\n",
            "Epoch 1100, Loss(train/val) 0.35544/0.22513. Took 0.04 sec\n",
            "Epoch 1101, Loss(train/val) 0.35075/0.22731. Took 0.05 sec\n",
            "Epoch 1102, Loss(train/val) 0.36494/0.22567. Took 0.04 sec\n",
            "Epoch 1103, Loss(train/val) 0.36318/0.22404. Took 0.05 sec\n",
            "Epoch 1104, Loss(train/val) 0.34868/0.22303. Took 0.04 sec\n",
            "Epoch 1105, Loss(train/val) 0.35357/0.22292. Took 0.04 sec\n",
            "Epoch 1106, Loss(train/val) 0.35553/0.22308. Took 0.06 sec\n",
            "Epoch 1107, Loss(train/val) 0.37164/0.22308. Took 0.04 sec\n",
            "Epoch 1108, Loss(train/val) 0.35197/0.22303. Took 0.05 sec\n",
            "Epoch 1109, Loss(train/val) 0.36740/0.22299. Took 0.04 sec\n",
            "Epoch 1110, Loss(train/val) 0.35566/0.22302. Took 0.04 sec\n",
            "Epoch 1111, Loss(train/val) 0.35479/0.22340. Took 0.05 sec\n",
            "Epoch 1112, Loss(train/val) 0.35799/0.22242. Took 0.05 sec\n",
            "Epoch 1113, Loss(train/val) 0.36966/0.22213. Took 0.04 sec\n",
            "Epoch 1114, Loss(train/val) 0.36687/0.22204. Took 0.05 sec\n",
            "Epoch 1115, Loss(train/val) 0.35697/0.22230. Took 0.05 sec\n",
            "Epoch 1116, Loss(train/val) 0.35126/0.22237. Took 0.05 sec\n",
            "Epoch 1117, Loss(train/val) 0.36006/0.22277. Took 0.05 sec\n",
            "Epoch 1118, Loss(train/val) 0.35652/0.22364. Took 0.05 sec\n",
            "Epoch 1119, Loss(train/val) 0.35814/0.22229. Took 0.04 sec\n",
            "Epoch 1120, Loss(train/val) 0.35806/0.22195. Took 0.05 sec\n",
            "Epoch 1121, Loss(train/val) 0.33767/0.22198. Took 0.06 sec\n",
            "Epoch 1122, Loss(train/val) 0.36895/0.22210. Took 0.05 sec\n",
            "Epoch 1123, Loss(train/val) 0.35867/0.22177. Took 0.05 sec\n",
            "Epoch 1124, Loss(train/val) 0.35158/0.22161. Took 0.06 sec\n",
            "Epoch 1125, Loss(train/val) 0.36014/0.22166. Took 0.05 sec\n",
            "Epoch 1126, Loss(train/val) 0.35660/0.22197. Took 0.05 sec\n",
            "Epoch 1127, Loss(train/val) 0.35541/0.22187. Took 0.05 sec\n",
            "Epoch 1128, Loss(train/val) 0.34500/0.22180. Took 0.04 sec\n",
            "Epoch 1129, Loss(train/val) 0.36386/0.22178. Took 0.04 sec\n",
            "Epoch 1130, Loss(train/val) 0.36079/0.22143. Took 0.04 sec\n",
            "Epoch 1131, Loss(train/val) 0.34823/0.22136. Took 0.05 sec\n",
            "Epoch 1132, Loss(train/val) 0.36218/0.22162. Took 0.04 sec\n",
            "Epoch 1133, Loss(train/val) 0.36732/0.22135. Took 0.04 sec\n",
            "Epoch 1134, Loss(train/val) 0.37635/0.22107. Took 0.05 sec\n",
            "Epoch 1135, Loss(train/val) 0.35691/0.22118. Took 0.05 sec\n",
            "Epoch 1136, Loss(train/val) 0.36122/0.22121. Took 0.05 sec\n",
            "Epoch 1137, Loss(train/val) 0.35965/0.22134. Took 0.05 sec\n",
            "Epoch 1138, Loss(train/val) 0.35722/0.22135. Took 0.05 sec\n",
            "Epoch 1139, Loss(train/val) 0.36262/0.22107. Took 0.05 sec\n",
            "Epoch 1140, Loss(train/val) 0.35667/0.22171. Took 0.05 sec\n",
            "Epoch 1141, Loss(train/val) 0.36282/0.22189. Took 0.05 sec\n",
            "Epoch 1142, Loss(train/val) 0.36021/0.22165. Took 0.05 sec\n",
            "Epoch 1143, Loss(train/val) 0.36010/0.22147. Took 0.05 sec\n",
            "Epoch 1144, Loss(train/val) 0.35182/0.22116. Took 0.05 sec\n",
            "Epoch 1145, Loss(train/val) 0.35305/0.22084. Took 0.05 sec\n",
            "Epoch 1146, Loss(train/val) 0.35343/0.22065. Took 0.06 sec\n",
            "Epoch 1147, Loss(train/val) 0.35875/0.22064. Took 0.05 sec\n",
            "Epoch 1148, Loss(train/val) 0.35109/0.22097. Took 0.04 sec\n",
            "Epoch 1149, Loss(train/val) 0.35677/0.22104. Took 0.04 sec\n",
            "Epoch 1150, Loss(train/val) 0.35917/0.22083. Took 0.04 sec\n",
            "Epoch 1151, Loss(train/val) 0.34961/0.22048. Took 0.05 sec\n",
            "Epoch 1152, Loss(train/val) 0.35142/0.22062. Took 0.04 sec\n",
            "Epoch 1153, Loss(train/val) 0.35793/0.22049. Took 0.05 sec\n",
            "Epoch 1154, Loss(train/val) 0.36049/0.22091. Took 0.05 sec\n",
            "Epoch 1155, Loss(train/val) 0.35953/0.22073. Took 0.05 sec\n",
            "Epoch 1156, Loss(train/val) 0.35485/0.22074. Took 0.05 sec\n",
            "Epoch 1157, Loss(train/val) 0.34974/0.22054. Took 0.04 sec\n",
            "Epoch 1158, Loss(train/val) 0.35146/0.22061. Took 0.05 sec\n",
            "Epoch 1159, Loss(train/val) 0.35780/0.22051. Took 0.04 sec\n",
            "Epoch 1160, Loss(train/val) 0.35254/0.22032. Took 0.05 sec\n",
            "Epoch 1161, Loss(train/val) 0.34334/0.22026. Took 0.05 sec\n",
            "Epoch 1162, Loss(train/val) 0.35159/0.22067. Took 0.05 sec\n",
            "Epoch 1163, Loss(train/val) 0.34648/0.22060. Took 0.05 sec\n",
            "Epoch 1164, Loss(train/val) 0.34460/0.22094. Took 0.05 sec\n",
            "Epoch 1165, Loss(train/val) 0.35859/0.22095. Took 0.04 sec\n",
            "Epoch 1166, Loss(train/val) 0.36459/0.22087. Took 0.05 sec\n",
            "Epoch 1167, Loss(train/val) 0.35579/0.22081. Took 0.04 sec\n",
            "Epoch 1168, Loss(train/val) 0.36493/0.22073. Took 0.04 sec\n",
            "Epoch 1169, Loss(train/val) 0.34864/0.22071. Took 0.05 sec\n",
            "Epoch 1170, Loss(train/val) 0.36089/0.22018. Took 0.04 sec\n",
            "Epoch 1171, Loss(train/val) 0.35432/0.22008. Took 0.07 sec\n",
            "Epoch 1172, Loss(train/val) 0.33689/0.21990. Took 0.05 sec\n",
            "Epoch 1173, Loss(train/val) 0.33839/0.21997. Took 0.05 sec\n",
            "Epoch 1174, Loss(train/val) 0.35402/0.22006. Took 0.05 sec\n",
            "Epoch 1175, Loss(train/val) 0.35115/0.21982. Took 0.04 sec\n",
            "Epoch 1176, Loss(train/val) 0.35213/0.22000. Took 0.05 sec\n",
            "Epoch 1177, Loss(train/val) 0.34848/0.22032. Took 0.04 sec\n",
            "Epoch 1178, Loss(train/val) 0.34714/0.22042. Took 0.05 sec\n",
            "Epoch 1179, Loss(train/val) 0.34513/0.22061. Took 0.05 sec\n",
            "Epoch 1180, Loss(train/val) 0.34634/0.21959. Took 0.04 sec\n",
            "Epoch 1181, Loss(train/val) 0.34799/0.21907. Took 0.05 sec\n",
            "Epoch 1182, Loss(train/val) 0.34196/0.21909. Took 0.05 sec\n",
            "Epoch 1183, Loss(train/val) 0.35231/0.21908. Took 0.05 sec\n",
            "Epoch 1184, Loss(train/val) 0.35952/0.21904. Took 0.05 sec\n",
            "Epoch 1185, Loss(train/val) 0.34719/0.21957. Took 0.05 sec\n",
            "Epoch 1186, Loss(train/val) 0.34243/0.21960. Took 0.05 sec\n",
            "Epoch 1187, Loss(train/val) 0.35418/0.21917. Took 0.05 sec\n",
            "Epoch 1188, Loss(train/val) 0.34740/0.21896. Took 0.05 sec\n",
            "Epoch 1189, Loss(train/val) 0.35060/0.21917. Took 0.05 sec\n",
            "Epoch 1190, Loss(train/val) 0.35145/0.21907. Took 0.05 sec\n",
            "Epoch 1191, Loss(train/val) 0.34214/0.21919. Took 0.05 sec\n",
            "Epoch 1192, Loss(train/val) 0.34774/0.21933. Took 0.04 sec\n",
            "Epoch 1193, Loss(train/val) 0.35256/0.21944. Took 0.04 sec\n",
            "Epoch 1194, Loss(train/val) 0.35432/0.21906. Took 0.04 sec\n",
            "Epoch 1195, Loss(train/val) 0.34863/0.21891. Took 0.04 sec\n",
            "Epoch 1196, Loss(train/val) 0.36791/0.21889. Took 0.05 sec\n",
            "Epoch 1197, Loss(train/val) 0.35660/0.21914. Took 0.04 sec\n",
            "Epoch 1198, Loss(train/val) 0.34688/0.21868. Took 0.04 sec\n",
            "Epoch 1199, Loss(train/val) 0.34258/0.21854. Took 0.05 sec\n",
            "Epoch 1200, Loss(train/val) 0.35090/0.21911. Took 0.05 sec\n",
            "Epoch 1201, Loss(train/val) 0.34497/0.21860. Took 0.05 sec\n",
            "Epoch 1202, Loss(train/val) 0.35374/0.21834. Took 0.05 sec\n",
            "Epoch 1203, Loss(train/val) 0.34504/0.21829. Took 0.04 sec\n",
            "Epoch 1204, Loss(train/val) 0.36732/0.21816. Took 0.04 sec\n",
            "Epoch 1205, Loss(train/val) 0.35089/0.21823. Took 0.04 sec\n",
            "Epoch 1206, Loss(train/val) 0.33226/0.21827. Took 0.06 sec\n",
            "Epoch 1207, Loss(train/val) 0.34763/0.21844. Took 0.04 sec\n",
            "Epoch 1208, Loss(train/val) 0.34264/0.21927. Took 0.04 sec\n",
            "Epoch 1209, Loss(train/val) 0.34087/0.21993. Took 0.05 sec\n",
            "Epoch 1210, Loss(train/val) 0.35740/0.22073. Took 0.04 sec\n",
            "Epoch 1211, Loss(train/val) 0.35043/0.21994. Took 0.05 sec\n",
            "Epoch 1212, Loss(train/val) 0.35051/0.21924. Took 0.04 sec\n",
            "Epoch 1213, Loss(train/val) 0.34125/0.21834. Took 0.04 sec\n",
            "Epoch 1214, Loss(train/val) 0.34911/0.21811. Took 0.04 sec\n",
            "Epoch 1215, Loss(train/val) 0.34741/0.21783. Took 0.04 sec\n",
            "Epoch 1216, Loss(train/val) 0.34791/0.21827. Took 0.05 sec\n",
            "Epoch 1217, Loss(train/val) 0.34312/0.21791. Took 0.05 sec\n",
            "Epoch 1218, Loss(train/val) 0.33920/0.21779. Took 0.04 sec\n",
            "Epoch 1219, Loss(train/val) 0.34404/0.21765. Took 0.04 sec\n",
            "Epoch 1220, Loss(train/val) 0.34597/0.21753. Took 0.04 sec\n",
            "Epoch 1221, Loss(train/val) 0.34571/0.21760. Took 0.05 sec\n",
            "Epoch 1222, Loss(train/val) 0.34594/0.21778. Took 0.05 sec\n",
            "Epoch 1223, Loss(train/val) 0.33866/0.21798. Took 0.04 sec\n",
            "Epoch 1224, Loss(train/val) 0.34319/0.21819. Took 0.04 sec\n",
            "Epoch 1225, Loss(train/val) 0.34642/0.21797. Took 0.05 sec\n",
            "Epoch 1226, Loss(train/val) 0.34112/0.21799. Took 0.05 sec\n",
            "Epoch 1227, Loss(train/val) 0.33738/0.21764. Took 0.04 sec\n",
            "Epoch 1228, Loss(train/val) 0.33838/0.21765. Took 0.05 sec\n",
            "Epoch 1229, Loss(train/val) 0.34865/0.21860. Took 0.05 sec\n",
            "Epoch 1230, Loss(train/val) 0.35513/0.21952. Took 0.05 sec\n",
            "Epoch 1231, Loss(train/val) 0.35160/0.21962. Took 0.05 sec\n",
            "Epoch 1232, Loss(train/val) 0.34861/0.21894. Took 0.05 sec\n",
            "Epoch 1233, Loss(train/val) 0.34442/0.21803. Took 0.04 sec\n",
            "Epoch 1234, Loss(train/val) 0.34285/0.21785. Took 0.04 sec\n",
            "Epoch 1235, Loss(train/val) 0.33482/0.21751. Took 0.05 sec\n",
            "Epoch 1236, Loss(train/val) 0.35653/0.21759. Took 0.05 sec\n",
            "Epoch 1237, Loss(train/val) 0.34880/0.21766. Took 0.05 sec\n",
            "Epoch 1238, Loss(train/val) 0.34648/0.21722. Took 0.05 sec\n",
            "Epoch 1239, Loss(train/val) 0.34320/0.21694. Took 0.05 sec\n",
            "Epoch 1240, Loss(train/val) 0.34185/0.21713. Took 0.05 sec\n",
            "Epoch 1241, Loss(train/val) 0.33870/0.21721. Took 0.05 sec\n",
            "Epoch 1242, Loss(train/val) 0.34350/0.21771. Took 0.04 sec\n",
            "Epoch 1243, Loss(train/val) 0.34425/0.21822. Took 0.05 sec\n",
            "Epoch 1244, Loss(train/val) 0.35642/0.21711. Took 0.04 sec\n",
            "Epoch 1245, Loss(train/val) 0.33506/0.21717. Took 0.05 sec\n",
            "Epoch 1246, Loss(train/val) 0.34353/0.21713. Took 0.05 sec\n",
            "Epoch 1247, Loss(train/val) 0.34713/0.21686. Took 0.04 sec\n",
            "Epoch 1248, Loss(train/val) 0.35899/0.21674. Took 0.04 sec\n",
            "Epoch 1249, Loss(train/val) 0.33852/0.21665. Took 0.05 sec\n",
            "Epoch 1250, Loss(train/val) 0.34073/0.21650. Took 0.06 sec\n",
            "Epoch 1251, Loss(train/val) 0.34093/0.21643. Took 0.05 sec\n",
            "Epoch 1252, Loss(train/val) 0.34050/0.21628. Took 0.05 sec\n",
            "Epoch 1253, Loss(train/val) 0.34875/0.21638. Took 0.05 sec\n",
            "Epoch 1254, Loss(train/val) 0.34301/0.21647. Took 0.05 sec\n",
            "Epoch 1255, Loss(train/val) 0.34625/0.21647. Took 0.05 sec\n",
            "Epoch 1256, Loss(train/val) 0.34019/0.21677. Took 0.05 sec\n",
            "Epoch 1257, Loss(train/val) 0.34399/0.21710. Took 0.04 sec\n",
            "Epoch 1258, Loss(train/val) 0.33534/0.21679. Took 0.05 sec\n",
            "Epoch 1259, Loss(train/val) 0.33757/0.21668. Took 0.04 sec\n",
            "Epoch 1260, Loss(train/val) 0.34215/0.21656. Took 0.04 sec\n",
            "Epoch 1261, Loss(train/val) 0.33471/0.21624. Took 0.05 sec\n",
            "Epoch 1262, Loss(train/val) 0.34436/0.21611. Took 0.04 sec\n",
            "Epoch 1263, Loss(train/val) 0.34654/0.21623. Took 0.04 sec\n",
            "Epoch 1264, Loss(train/val) 0.34172/0.21635. Took 0.05 sec\n",
            "Epoch 1265, Loss(train/val) 0.34084/0.21646. Took 0.05 sec\n",
            "Epoch 1266, Loss(train/val) 0.34326/0.21620. Took 0.05 sec\n",
            "Epoch 1267, Loss(train/val) 0.34460/0.21607. Took 0.05 sec\n",
            "Epoch 1268, Loss(train/val) 0.33922/0.21577. Took 0.04 sec\n",
            "Epoch 1269, Loss(train/val) 0.34230/0.21575. Took 0.04 sec\n",
            "Epoch 1270, Loss(train/val) 0.33639/0.21602. Took 0.04 sec\n",
            "Epoch 1271, Loss(train/val) 0.34071/0.21619. Took 0.06 sec\n",
            "Epoch 1272, Loss(train/val) 0.33345/0.21617. Took 0.04 sec\n",
            "Epoch 1273, Loss(train/val) 0.34438/0.21615. Took 0.05 sec\n",
            "Epoch 1274, Loss(train/val) 0.33953/0.21626. Took 0.05 sec\n",
            "Epoch 1275, Loss(train/val) 0.33151/0.21608. Took 0.05 sec\n",
            "Epoch 1276, Loss(train/val) 0.34349/0.21587. Took 0.05 sec\n",
            "Epoch 1277, Loss(train/val) 0.33714/0.21614. Took 0.05 sec\n",
            "Epoch 1278, Loss(train/val) 0.33874/0.21592. Took 0.05 sec\n",
            "Epoch 1279, Loss(train/val) 0.33272/0.21586. Took 0.04 sec\n",
            "Epoch 1280, Loss(train/val) 0.34049/0.21568. Took 0.05 sec\n",
            "Epoch 1281, Loss(train/val) 0.34436/0.21595. Took 0.05 sec\n",
            "Epoch 1282, Loss(train/val) 0.33544/0.21570. Took 0.05 sec\n",
            "Epoch 1283, Loss(train/val) 0.33639/0.21527. Took 0.04 sec\n",
            "Epoch 1284, Loss(train/val) 0.33975/0.21564. Took 0.04 sec\n",
            "Epoch 1285, Loss(train/val) 0.34471/0.21565. Took 0.05 sec\n",
            "Epoch 1286, Loss(train/val) 0.34457/0.21556. Took 0.05 sec\n",
            "Epoch 1287, Loss(train/val) 0.34770/0.21509. Took 0.05 sec\n",
            "Epoch 1288, Loss(train/val) 0.34436/0.21488. Took 0.04 sec\n",
            "Epoch 1289, Loss(train/val) 0.33277/0.21489. Took 0.05 sec\n",
            "Epoch 1290, Loss(train/val) 0.32997/0.21486. Took 0.05 sec\n",
            "Epoch 1291, Loss(train/val) 0.34495/0.21488. Took 0.05 sec\n",
            "Epoch 1292, Loss(train/val) 0.32889/0.21504. Took 0.05 sec\n",
            "Epoch 1293, Loss(train/val) 0.33588/0.21598. Took 0.05 sec\n",
            "Epoch 1294, Loss(train/val) 0.33927/0.21573. Took 0.05 sec\n",
            "Epoch 1295, Loss(train/val) 0.33644/0.21560. Took 0.05 sec\n",
            "Epoch 1296, Loss(train/val) 0.34143/0.21534. Took 0.05 sec\n",
            "Epoch 1297, Loss(train/val) 0.33615/0.21492. Took 0.04 sec\n",
            "Epoch 1298, Loss(train/val) 0.33659/0.21495. Took 0.05 sec\n",
            "Epoch 1299, Loss(train/val) 0.33453/0.21455. Took 0.04 sec\n",
            "Epoch 1300, Loss(train/val) 0.33658/0.21440. Took 0.05 sec\n",
            "Epoch 1301, Loss(train/val) 0.33050/0.21443. Took 0.06 sec\n",
            "Epoch 1302, Loss(train/val) 0.33623/0.21451. Took 0.05 sec\n",
            "Epoch 1303, Loss(train/val) 0.33854/0.21446. Took 0.04 sec\n",
            "Epoch 1304, Loss(train/val) 0.33828/0.21454. Took 0.04 sec\n",
            "Epoch 1305, Loss(train/val) 0.33208/0.21436. Took 0.05 sec\n",
            "Epoch 1306, Loss(train/val) 0.32844/0.21459. Took 0.05 sec\n",
            "Epoch 1307, Loss(train/val) 0.33801/0.21493. Took 0.05 sec\n",
            "Epoch 1308, Loss(train/val) 0.33396/0.21490. Took 0.05 sec\n",
            "Epoch 1309, Loss(train/val) 0.32635/0.21527. Took 0.05 sec\n",
            "Epoch 1310, Loss(train/val) 0.33972/0.21500. Took 0.05 sec\n",
            "Epoch 1311, Loss(train/val) 0.32278/0.21529. Took 0.05 sec\n",
            "Epoch 1312, Loss(train/val) 0.33747/0.21478. Took 0.04 sec\n",
            "Epoch 1313, Loss(train/val) 0.34697/0.21460. Took 0.04 sec\n",
            "Epoch 1314, Loss(train/val) 0.32534/0.21447. Took 0.05 sec\n",
            "Epoch 1315, Loss(train/val) 0.35041/0.21458. Took 0.05 sec\n",
            "Epoch 1316, Loss(train/val) 0.34013/0.21423. Took 0.05 sec\n",
            "Epoch 1317, Loss(train/val) 0.32013/0.21384. Took 0.05 sec\n",
            "Epoch 1318, Loss(train/val) 0.33171/0.21355. Took 0.04 sec\n",
            "Epoch 1319, Loss(train/val) 0.33339/0.21347. Took 0.04 sec\n",
            "Epoch 1320, Loss(train/val) 0.33698/0.21360. Took 0.05 sec\n",
            "Epoch 1321, Loss(train/val) 0.32782/0.21364. Took 0.05 sec\n",
            "Epoch 1322, Loss(train/val) 0.32989/0.21388. Took 0.05 sec\n",
            "Epoch 1323, Loss(train/val) 0.33167/0.21341. Took 0.05 sec\n",
            "Epoch 1324, Loss(train/val) 0.32974/0.21329. Took 0.05 sec\n",
            "Epoch 1325, Loss(train/val) 0.32945/0.21343. Took 0.05 sec\n",
            "Epoch 1326, Loss(train/val) 0.33065/0.21366. Took 0.05 sec\n",
            "Epoch 1327, Loss(train/val) 0.33153/0.21382. Took 0.05 sec\n",
            "Epoch 1328, Loss(train/val) 0.33122/0.21357. Took 0.05 sec\n",
            "Epoch 1329, Loss(train/val) 0.34757/0.21406. Took 0.05 sec\n",
            "Epoch 1330, Loss(train/val) 0.32920/0.21414. Took 0.05 sec\n",
            "Epoch 1331, Loss(train/val) 0.33268/0.21456. Took 0.05 sec\n",
            "Epoch 1332, Loss(train/val) 0.33423/0.21459. Took 0.05 sec\n",
            "Epoch 1333, Loss(train/val) 0.32522/0.21367. Took 0.04 sec\n",
            "Epoch 1334, Loss(train/val) 0.32988/0.21361. Took 0.05 sec\n",
            "Epoch 1335, Loss(train/val) 0.34204/0.21300. Took 0.05 sec\n",
            "Epoch 1336, Loss(train/val) 0.32757/0.21309. Took 0.05 sec\n",
            "Epoch 1337, Loss(train/val) 0.33998/0.21261. Took 0.05 sec\n",
            "Epoch 1338, Loss(train/val) 0.33598/0.21267. Took 0.05 sec\n",
            "Epoch 1339, Loss(train/val) 0.32545/0.21270. Took 0.04 sec\n",
            "Epoch 1340, Loss(train/val) 0.32703/0.21276. Took 0.04 sec\n",
            "Epoch 1341, Loss(train/val) 0.32678/0.21282. Took 0.05 sec\n",
            "Epoch 1342, Loss(train/val) 0.32828/0.21281. Took 0.06 sec\n",
            "Epoch 1343, Loss(train/val) 0.32787/0.21295. Took 0.04 sec\n",
            "Epoch 1344, Loss(train/val) 0.33897/0.21331. Took 0.04 sec\n",
            "Epoch 1345, Loss(train/val) 0.33113/0.21310. Took 0.04 sec\n",
            "Epoch 1346, Loss(train/val) 0.33683/0.21262. Took 0.05 sec\n",
            "Epoch 1347, Loss(train/val) 0.33580/0.21277. Took 0.05 sec\n",
            "Epoch 1348, Loss(train/val) 0.33973/0.21217. Took 0.04 sec\n",
            "Epoch 1349, Loss(train/val) 0.32289/0.21225. Took 0.05 sec\n",
            "Epoch 1350, Loss(train/val) 0.32495/0.21222. Took 0.05 sec\n",
            "Epoch 1351, Loss(train/val) 0.33377/0.21222. Took 0.05 sec\n",
            "Epoch 1352, Loss(train/val) 0.32619/0.21214. Took 0.05 sec\n",
            "Epoch 1353, Loss(train/val) 0.32237/0.21192. Took 0.05 sec\n",
            "Epoch 1354, Loss(train/val) 0.33071/0.21186. Took 0.05 sec\n",
            "Epoch 1355, Loss(train/val) 0.32986/0.21165. Took 0.04 sec\n",
            "Epoch 1356, Loss(train/val) 0.33025/0.21164. Took 0.06 sec\n",
            "Epoch 1357, Loss(train/val) 0.33028/0.21170. Took 0.05 sec\n",
            "Epoch 1358, Loss(train/val) 0.32769/0.21167. Took 0.05 sec\n",
            "Epoch 1359, Loss(train/val) 0.33891/0.21167. Took 0.04 sec\n",
            "Epoch 1360, Loss(train/val) 0.32966/0.21155. Took 0.04 sec\n",
            "Epoch 1361, Loss(train/val) 0.32478/0.21147. Took 0.05 sec\n",
            "Epoch 1362, Loss(train/val) 0.33459/0.21151. Took 0.04 sec\n",
            "Epoch 1363, Loss(train/val) 0.33790/0.21156. Took 0.05 sec\n",
            "Epoch 1364, Loss(train/val) 0.33997/0.21173. Took 0.04 sec\n",
            "Epoch 1365, Loss(train/val) 0.32978/0.21191. Took 0.04 sec\n",
            "Epoch 1366, Loss(train/val) 0.33971/0.21194. Took 0.05 sec\n",
            "Epoch 1367, Loss(train/val) 0.33241/0.21218. Took 0.04 sec\n",
            "Epoch 1368, Loss(train/val) 0.33057/0.21184. Took 0.04 sec\n",
            "Epoch 1369, Loss(train/val) 0.32399/0.21156. Took 0.04 sec\n",
            "Epoch 1370, Loss(train/val) 0.32446/0.21159. Took 0.04 sec\n",
            "Epoch 1371, Loss(train/val) 0.33689/0.21130. Took 0.05 sec\n",
            "Epoch 1372, Loss(train/val) 0.33399/0.21150. Took 0.05 sec\n",
            "Epoch 1373, Loss(train/val) 0.33111/0.21129. Took 0.05 sec\n",
            "Epoch 1374, Loss(train/val) 0.32936/0.21135. Took 0.04 sec\n",
            "Epoch 1375, Loss(train/val) 0.33040/0.21149. Took 0.04 sec\n",
            "Epoch 1376, Loss(train/val) 0.32954/0.21189. Took 0.05 sec\n",
            "Epoch 1377, Loss(train/val) 0.32270/0.21155. Took 0.05 sec\n",
            "Epoch 1378, Loss(train/val) 0.32599/0.21132. Took 0.05 sec\n",
            "Epoch 1379, Loss(train/val) 0.32555/0.21092. Took 0.05 sec\n",
            "Epoch 1380, Loss(train/val) 0.32357/0.21056. Took 0.05 sec\n",
            "Epoch 1381, Loss(train/val) 0.33178/0.21053. Took 0.05 sec\n",
            "Epoch 1382, Loss(train/val) 0.32973/0.21061. Took 0.04 sec\n",
            "Epoch 1383, Loss(train/val) 0.32687/0.21067. Took 0.04 sec\n",
            "Epoch 1384, Loss(train/val) 0.31896/0.21089. Took 0.04 sec\n",
            "Epoch 1385, Loss(train/val) 0.32717/0.21076. Took 0.05 sec\n",
            "Epoch 1386, Loss(train/val) 0.33036/0.21063. Took 0.05 sec\n",
            "Epoch 1387, Loss(train/val) 0.32558/0.21096. Took 0.05 sec\n",
            "Epoch 1388, Loss(train/val) 0.32084/0.21225. Took 0.05 sec\n",
            "Epoch 1389, Loss(train/val) 0.32832/0.21242. Took 0.05 sec\n",
            "Epoch 1390, Loss(train/val) 0.34201/0.21159. Took 0.05 sec\n",
            "Epoch 1391, Loss(train/val) 0.34631/0.21119. Took 0.06 sec\n",
            "Epoch 1392, Loss(train/val) 0.32784/0.21181. Took 0.05 sec\n",
            "Epoch 1393, Loss(train/val) 0.32424/0.21095. Took 0.04 sec\n",
            "Epoch 1394, Loss(train/val) 0.33282/0.21062. Took 0.05 sec\n",
            "Epoch 1395, Loss(train/val) 0.32569/0.21023. Took 0.05 sec\n",
            "Epoch 1396, Loss(train/val) 0.32919/0.21022. Took 0.05 sec\n",
            "Epoch 1397, Loss(train/val) 0.32826/0.21025. Took 0.05 sec\n",
            "Epoch 1398, Loss(train/val) 0.32484/0.21024. Took 0.05 sec\n",
            "Epoch 1399, Loss(train/val) 0.32892/0.21031. Took 0.06 sec\n",
            "Epoch 1400, Loss(train/val) 0.33042/0.21000. Took 0.05 sec\n",
            "Epoch 1401, Loss(train/val) 0.33641/0.21030. Took 0.05 sec\n",
            "Epoch 1402, Loss(train/val) 0.32315/0.21016. Took 0.05 sec\n",
            "Epoch 1403, Loss(train/val) 0.32722/0.21004. Took 0.04 sec\n",
            "Epoch 1404, Loss(train/val) 0.33583/0.20985. Took 0.04 sec\n",
            "Epoch 1405, Loss(train/val) 0.31958/0.20976. Took 0.05 sec\n",
            "Epoch 1406, Loss(train/val) 0.32823/0.20954. Took 0.05 sec\n",
            "Epoch 1407, Loss(train/val) 0.32421/0.20963. Took 0.04 sec\n",
            "Epoch 1408, Loss(train/val) 0.33174/0.20997. Took 0.05 sec\n",
            "Epoch 1409, Loss(train/val) 0.32654/0.21002. Took 0.04 sec\n",
            "Epoch 1410, Loss(train/val) 0.32225/0.21004. Took 0.04 sec\n",
            "Epoch 1411, Loss(train/val) 0.32046/0.20976. Took 0.05 sec\n",
            "Epoch 1412, Loss(train/val) 0.33234/0.20930. Took 0.05 sec\n",
            "Epoch 1413, Loss(train/val) 0.33024/0.20905. Took 0.04 sec\n",
            "Epoch 1414, Loss(train/val) 0.33036/0.20896. Took 0.04 sec\n",
            "Epoch 1415, Loss(train/val) 0.33193/0.20907. Took 0.04 sec\n",
            "Epoch 1416, Loss(train/val) 0.33434/0.20898. Took 0.06 sec\n",
            "Epoch 1417, Loss(train/val) 0.33058/0.20879. Took 0.05 sec\n",
            "Epoch 1418, Loss(train/val) 0.32499/0.20874. Took 0.05 sec\n",
            "Epoch 1419, Loss(train/val) 0.34279/0.20877. Took 0.04 sec\n",
            "Epoch 1420, Loss(train/val) 0.32920/0.20870. Took 0.04 sec\n",
            "Epoch 1421, Loss(train/val) 0.31740/0.20873. Took 0.06 sec\n",
            "Epoch 1422, Loss(train/val) 0.33107/0.20874. Took 0.04 sec\n",
            "Epoch 1423, Loss(train/val) 0.33041/0.20972. Took 0.04 sec\n",
            "Epoch 1424, Loss(train/val) 0.32296/0.21037. Took 0.05 sec\n",
            "Epoch 1425, Loss(train/val) 0.32958/0.21025. Took 0.04 sec\n",
            "Epoch 1426, Loss(train/val) 0.32435/0.20966. Took 0.05 sec\n",
            "Epoch 1427, Loss(train/val) 0.33702/0.20997. Took 0.05 sec\n",
            "Epoch 1428, Loss(train/val) 0.33227/0.20875. Took 0.05 sec\n",
            "Epoch 1429, Loss(train/val) 0.32551/0.20807. Took 0.05 sec\n",
            "Epoch 1430, Loss(train/val) 0.31406/0.20781. Took 0.04 sec\n",
            "Epoch 1431, Loss(train/val) 0.32373/0.20816. Took 0.05 sec\n",
            "Epoch 1432, Loss(train/val) 0.32129/0.20855. Took 0.05 sec\n",
            "Epoch 1433, Loss(train/val) 0.31498/0.20842. Took 0.05 sec\n",
            "Epoch 1434, Loss(train/val) 0.32610/0.20798. Took 0.05 sec\n",
            "Epoch 1435, Loss(train/val) 0.32164/0.20781. Took 0.05 sec\n",
            "Epoch 1436, Loss(train/val) 0.32685/0.20776. Took 0.06 sec\n",
            "Epoch 1437, Loss(train/val) 0.32809/0.20773. Took 0.04 sec\n",
            "Epoch 1438, Loss(train/val) 0.31854/0.20749. Took 0.04 sec\n",
            "Epoch 1439, Loss(train/val) 0.31435/0.20752. Took 0.04 sec\n",
            "Epoch 1440, Loss(train/val) 0.32159/0.20738. Took 0.04 sec\n",
            "Epoch 1441, Loss(train/val) 0.32907/0.20730. Took 0.05 sec\n",
            "Epoch 1442, Loss(train/val) 0.32153/0.20722. Took 0.05 sec\n",
            "Epoch 1443, Loss(train/val) 0.32018/0.20736. Took 0.05 sec\n",
            "Epoch 1444, Loss(train/val) 0.32357/0.20713. Took 0.05 sec\n",
            "Epoch 1445, Loss(train/val) 0.32474/0.20713. Took 0.05 sec\n",
            "Epoch 1446, Loss(train/val) 0.32366/0.20708. Took 0.06 sec\n",
            "Epoch 1447, Loss(train/val) 0.32015/0.20679. Took 0.04 sec\n",
            "Epoch 1448, Loss(train/val) 0.31997/0.20673. Took 0.05 sec\n",
            "Epoch 1449, Loss(train/val) 0.31538/0.20670. Took 0.05 sec\n",
            "Epoch 1450, Loss(train/val) 0.31775/0.20668. Took 0.05 sec\n",
            "Epoch 1451, Loss(train/val) 0.31857/0.20697. Took 0.05 sec\n",
            "Epoch 1452, Loss(train/val) 0.32121/0.20713. Took 0.05 sec\n",
            "Epoch 1453, Loss(train/val) 0.32061/0.20699. Took 0.05 sec\n",
            "Epoch 1454, Loss(train/val) 0.32649/0.20667. Took 0.05 sec\n",
            "Epoch 1455, Loss(train/val) 0.32382/0.20661. Took 0.05 sec\n",
            "Epoch 1456, Loss(train/val) 0.32458/0.20647. Took 0.05 sec\n",
            "Epoch 1457, Loss(train/val) 0.31949/0.20638. Took 0.05 sec\n",
            "Epoch 1458, Loss(train/val) 0.32546/0.20649. Took 0.05 sec\n",
            "Epoch 1459, Loss(train/val) 0.31873/0.20645. Took 0.05 sec\n",
            "Epoch 1460, Loss(train/val) 0.32025/0.20615. Took 0.05 sec\n",
            "Epoch 1461, Loss(train/val) 0.31326/0.20619. Took 0.06 sec\n",
            "Epoch 1462, Loss(train/val) 0.32572/0.20639. Took 0.04 sec\n",
            "Epoch 1463, Loss(train/val) 0.31752/0.20626. Took 0.05 sec\n",
            "Epoch 1464, Loss(train/val) 0.32361/0.20611. Took 0.05 sec\n",
            "Epoch 1465, Loss(train/val) 0.31880/0.20615. Took 0.04 sec\n",
            "Epoch 1466, Loss(train/val) 0.32262/0.20683. Took 0.05 sec\n",
            "Epoch 1467, Loss(train/val) 0.31912/0.20683. Took 0.05 sec\n",
            "Epoch 1468, Loss(train/val) 0.32441/0.20598. Took 0.04 sec\n",
            "Epoch 1469, Loss(train/val) 0.32422/0.20617. Took 0.04 sec\n",
            "Epoch 1470, Loss(train/val) 0.32403/0.20575. Took 0.04 sec\n",
            "Epoch 1471, Loss(train/val) 0.33126/0.20595. Took 0.05 sec\n",
            "Epoch 1472, Loss(train/val) 0.32032/0.20589. Took 0.05 sec\n",
            "Epoch 1473, Loss(train/val) 0.31716/0.20585. Took 0.06 sec\n",
            "Epoch 1474, Loss(train/val) 0.31915/0.20548. Took 0.05 sec\n",
            "Epoch 1475, Loss(train/val) 0.32792/0.20565. Took 0.04 sec\n",
            "Epoch 1476, Loss(train/val) 0.31434/0.20543. Took 0.05 sec\n",
            "Epoch 1477, Loss(train/val) 0.32200/0.20531. Took 0.05 sec\n",
            "Epoch 1478, Loss(train/val) 0.32490/0.20593. Took 0.05 sec\n",
            "Epoch 1479, Loss(train/val) 0.31609/0.20578. Took 0.04 sec\n",
            "Epoch 1480, Loss(train/val) 0.32789/0.20553. Took 0.04 sec\n",
            "Epoch 1481, Loss(train/val) 0.31511/0.20520. Took 0.05 sec\n",
            "Epoch 1482, Loss(train/val) 0.31950/0.20489. Took 0.04 sec\n",
            "Epoch 1483, Loss(train/val) 0.32106/0.20472. Took 0.04 sec\n",
            "Epoch 1484, Loss(train/val) 0.30956/0.20459. Took 0.05 sec\n",
            "Epoch 1485, Loss(train/val) 0.31409/0.20465. Took 0.05 sec\n",
            "Epoch 1486, Loss(train/val) 0.30465/0.20490. Took 0.05 sec\n",
            "Epoch 1487, Loss(train/val) 0.31282/0.20477. Took 0.04 sec\n",
            "Epoch 1488, Loss(train/val) 0.32381/0.20499. Took 0.04 sec\n",
            "Epoch 1489, Loss(train/val) 0.32405/0.20478. Took 0.04 sec\n",
            "Epoch 1490, Loss(train/val) 0.31099/0.20476. Took 0.04 sec\n",
            "Epoch 1491, Loss(train/val) 0.31331/0.20454. Took 0.05 sec\n",
            "Epoch 1492, Loss(train/val) 0.31691/0.20441. Took 0.04 sec\n",
            "Epoch 1493, Loss(train/val) 0.31741/0.20425. Took 0.04 sec\n",
            "Epoch 1494, Loss(train/val) 0.31978/0.20403. Took 0.05 sec\n",
            "Epoch 1495, Loss(train/val) 0.31539/0.20405. Took 0.05 sec\n",
            "Epoch 1496, Loss(train/val) 0.35719/0.20403. Took 0.05 sec\n",
            "Epoch 1497, Loss(train/val) 0.32587/0.20386. Took 0.05 sec\n",
            "Epoch 1498, Loss(train/val) 0.31199/0.20357. Took 0.05 sec\n",
            "Epoch 1499, Loss(train/val) 0.31733/0.20319. Took 0.05 sec\n",
            "Epoch 1500, Loss(train/val) 0.31108/0.20320. Took 0.04 sec\n",
            "Epoch 1501, Loss(train/val) 0.31676/0.20307. Took 0.05 sec\n",
            "Epoch 1502, Loss(train/val) 0.31561/0.20306. Took 0.05 sec\n",
            "Epoch 1503, Loss(train/val) 0.30434/0.20302. Took 0.05 sec\n",
            "Epoch 1504, Loss(train/val) 0.31351/0.20304. Took 0.04 sec\n",
            "Epoch 1505, Loss(train/val) 0.31888/0.20292. Took 0.05 sec\n",
            "Epoch 1506, Loss(train/val) 0.31843/0.20305. Took 0.07 sec\n",
            "Epoch 1507, Loss(train/val) 0.32124/0.20286. Took 0.05 sec\n",
            "Epoch 1508, Loss(train/val) 0.32317/0.20337. Took 0.05 sec\n",
            "Epoch 1509, Loss(train/val) 0.31899/0.20350. Took 0.04 sec\n",
            "Epoch 1510, Loss(train/val) 0.31669/0.20367. Took 0.04 sec\n",
            "Epoch 1511, Loss(train/val) 0.30757/0.20336. Took 0.05 sec\n",
            "Epoch 1512, Loss(train/val) 0.31348/0.20290. Took 0.04 sec\n",
            "Epoch 1513, Loss(train/val) 0.31615/0.20322. Took 0.04 sec\n",
            "Epoch 1514, Loss(train/val) 0.31429/0.20286. Took 0.05 sec\n",
            "Epoch 1515, Loss(train/val) 0.31302/0.20293. Took 0.05 sec\n",
            "Epoch 1516, Loss(train/val) 0.32400/0.20276. Took 0.05 sec\n",
            "Epoch 1517, Loss(train/val) 0.30601/0.20257. Took 0.05 sec\n",
            "Epoch 1518, Loss(train/val) 0.31580/0.20248. Took 0.05 sec\n",
            "Epoch 1519, Loss(train/val) 0.32348/0.20242. Took 0.05 sec\n",
            "Epoch 1520, Loss(train/val) 0.32775/0.20231. Took 0.05 sec\n",
            "Epoch 1521, Loss(train/val) 0.30889/0.20234. Took 0.05 sec\n",
            "Epoch 1522, Loss(train/val) 0.30508/0.20218. Took 0.05 sec\n",
            "Epoch 1523, Loss(train/val) 0.32016/0.20218. Took 0.04 sec\n",
            "Epoch 1524, Loss(train/val) 0.31004/0.20172. Took 0.05 sec\n",
            "Epoch 1525, Loss(train/val) 0.31676/0.20177. Took 0.05 sec\n",
            "Epoch 1526, Loss(train/val) 0.32757/0.20179. Took 0.05 sec\n",
            "Epoch 1527, Loss(train/val) 0.34289/0.20170. Took 0.04 sec\n",
            "Epoch 1528, Loss(train/val) 0.32529/0.20159. Took 0.06 sec\n",
            "Epoch 1529, Loss(train/val) 0.31740/0.20202. Took 0.04 sec\n",
            "Epoch 1530, Loss(train/val) 0.31147/0.20159. Took 0.04 sec\n",
            "Epoch 1531, Loss(train/val) 0.32213/0.20100. Took 0.05 sec\n",
            "Epoch 1532, Loss(train/val) 0.31341/0.20091. Took 0.04 sec\n",
            "Epoch 1533, Loss(train/val) 0.30834/0.20092. Took 0.05 sec\n",
            "Epoch 1534, Loss(train/val) 0.31772/0.20096. Took 0.04 sec\n",
            "Epoch 1535, Loss(train/val) 0.31008/0.20093. Took 0.04 sec\n",
            "Epoch 1536, Loss(train/val) 0.31826/0.20078. Took 0.05 sec\n",
            "Epoch 1537, Loss(train/val) 0.30678/0.20059. Took 0.05 sec\n",
            "Epoch 1538, Loss(train/val) 0.31246/0.20048. Took 0.04 sec\n",
            "Epoch 1539, Loss(train/val) 0.30141/0.20050. Took 0.05 sec\n",
            "Epoch 1540, Loss(train/val) 0.31214/0.20050. Took 0.05 sec\n",
            "Epoch 1541, Loss(train/val) 0.31434/0.20045. Took 0.05 sec\n",
            "Epoch 1542, Loss(train/val) 0.30617/0.20044. Took 0.05 sec\n",
            "Epoch 1543, Loss(train/val) 0.31347/0.20053. Took 0.05 sec\n",
            "Epoch 1544, Loss(train/val) 0.31107/0.20040. Took 0.04 sec\n",
            "Epoch 1545, Loss(train/val) 0.32127/0.20004. Took 0.05 sec\n",
            "Epoch 1546, Loss(train/val) 0.32499/0.19945. Took 0.05 sec\n",
            "Epoch 1547, Loss(train/val) 0.30280/0.19962. Took 0.05 sec\n",
            "Epoch 1548, Loss(train/val) 0.31118/0.19944. Took 0.04 sec\n",
            "Epoch 1549, Loss(train/val) 0.30636/0.19967. Took 0.05 sec\n",
            "Epoch 1550, Loss(train/val) 0.32443/0.19978. Took 0.04 sec\n",
            "Epoch 1551, Loss(train/val) 0.30933/0.19961. Took 0.05 sec\n",
            "Epoch 1552, Loss(train/val) 0.31052/0.19954. Took 0.04 sec\n",
            "Epoch 1553, Loss(train/val) 0.31975/0.19935. Took 0.05 sec\n",
            "Epoch 1554, Loss(train/val) 0.30567/0.19953. Took 0.04 sec\n",
            "Epoch 1555, Loss(train/val) 0.32140/0.19945. Took 0.04 sec\n",
            "Epoch 1556, Loss(train/val) 0.30711/0.19958. Took 0.05 sec\n",
            "Epoch 1557, Loss(train/val) 0.30830/0.19942. Took 0.05 sec\n",
            "Epoch 1558, Loss(train/val) 0.31581/0.19911. Took 0.04 sec\n",
            "Epoch 1559, Loss(train/val) 0.31284/0.19898. Took 0.04 sec\n",
            "Epoch 1560, Loss(train/val) 0.30601/0.19876. Took 0.05 sec\n",
            "Epoch 1561, Loss(train/val) 0.31149/0.19883. Took 0.05 sec\n",
            "Epoch 1562, Loss(train/val) 0.30357/0.19867. Took 0.05 sec\n",
            "Epoch 1563, Loss(train/val) 0.31052/0.19873. Took 0.05 sec\n",
            "Epoch 1564, Loss(train/val) 0.30722/0.19849. Took 0.05 sec\n",
            "Epoch 1565, Loss(train/val) 0.30752/0.19828. Took 0.05 sec\n",
            "Epoch 1566, Loss(train/val) 0.31189/0.19865. Took 0.05 sec\n",
            "Epoch 1567, Loss(train/val) 0.30497/0.19855. Took 0.05 sec\n",
            "Epoch 1568, Loss(train/val) 0.30204/0.19837. Took 0.06 sec\n",
            "Epoch 1569, Loss(train/val) 0.30274/0.19823. Took 0.04 sec\n",
            "Epoch 1570, Loss(train/val) 0.30414/0.19812. Took 0.05 sec\n",
            "Epoch 1571, Loss(train/val) 0.30315/0.19820. Took 0.05 sec\n",
            "Epoch 1572, Loss(train/val) 0.31552/0.19793. Took 0.04 sec\n",
            "Epoch 1573, Loss(train/val) 0.30052/0.19765. Took 0.04 sec\n",
            "Epoch 1574, Loss(train/val) 0.30492/0.19761. Took 0.05 sec\n",
            "Epoch 1575, Loss(train/val) 0.30353/0.19754. Took 0.05 sec\n",
            "Epoch 1576, Loss(train/val) 0.29837/0.19757. Took 0.05 sec\n",
            "Epoch 1577, Loss(train/val) 0.29613/0.19739. Took 0.05 sec\n",
            "Epoch 1578, Loss(train/val) 0.30421/0.19729. Took 0.05 sec\n",
            "Epoch 1579, Loss(train/val) 0.30726/0.19714. Took 0.05 sec\n",
            "Epoch 1580, Loss(train/val) 0.30364/0.19722. Took 0.04 sec\n",
            "Epoch 1581, Loss(train/val) 0.29901/0.19734. Took 0.05 sec\n",
            "Epoch 1582, Loss(train/val) 0.31026/0.19739. Took 0.05 sec\n",
            "Epoch 1583, Loss(train/val) 0.29937/0.19718. Took 0.04 sec\n",
            "Epoch 1584, Loss(train/val) 0.29797/0.19731. Took 0.04 sec\n",
            "Epoch 1585, Loss(train/val) 0.29190/0.19736. Took 0.04 sec\n",
            "Epoch 1586, Loss(train/val) 0.31156/0.19662. Took 0.05 sec\n",
            "Epoch 1587, Loss(train/val) 0.30278/0.19607. Took 0.04 sec\n",
            "Epoch 1588, Loss(train/val) 0.30611/0.19604. Took 0.04 sec\n",
            "Epoch 1589, Loss(train/val) 0.30612/0.19632. Took 0.05 sec\n",
            "Epoch 1590, Loss(train/val) 0.31508/0.19610. Took 0.04 sec\n",
            "Epoch 1591, Loss(train/val) 0.30063/0.19620. Took 0.05 sec\n",
            "Epoch 1592, Loss(train/val) 0.32134/0.19605. Took 0.06 sec\n",
            "Epoch 1593, Loss(train/val) 0.30034/0.19590. Took 0.04 sec\n",
            "Epoch 1594, Loss(train/val) 0.29946/0.19542. Took 0.04 sec\n",
            "Epoch 1595, Loss(train/val) 0.30174/0.19508. Took 0.05 sec\n",
            "Epoch 1596, Loss(train/val) 0.29138/0.19491. Took 0.05 sec\n",
            "Epoch 1597, Loss(train/val) 0.31170/0.19509. Took 0.05 sec\n",
            "Epoch 1598, Loss(train/val) 0.30637/0.19496. Took 0.04 sec\n",
            "Epoch 1599, Loss(train/val) 0.29159/0.19502. Took 0.04 sec\n",
            "Epoch 1600, Loss(train/val) 0.30490/0.19474. Took 0.05 sec\n",
            "Epoch 1601, Loss(train/val) 0.30533/0.19479. Took 0.05 sec\n",
            "Epoch 1602, Loss(train/val) 0.30432/0.19465. Took 0.05 sec\n",
            "Epoch 1603, Loss(train/val) 0.29975/0.19443. Took 0.05 sec\n",
            "Epoch 1604, Loss(train/val) 0.30068/0.19426. Took 0.05 sec\n",
            "Epoch 1605, Loss(train/val) 0.29959/0.19413. Took 0.04 sec\n",
            "Epoch 1606, Loss(train/val) 0.29549/0.19412. Took 0.05 sec\n",
            "Epoch 1607, Loss(train/val) 0.30132/0.19421. Took 0.04 sec\n",
            "Epoch 1608, Loss(train/val) 0.29274/0.19435. Took 0.05 sec\n",
            "Epoch 1609, Loss(train/val) 0.30293/0.19423. Took 0.04 sec\n",
            "Epoch 1610, Loss(train/val) 0.29920/0.19393. Took 0.04 sec\n",
            "Epoch 1611, Loss(train/val) 0.29730/0.19396. Took 0.05 sec\n",
            "Epoch 1612, Loss(train/val) 0.30267/0.19385. Took 0.05 sec\n",
            "Epoch 1613, Loss(train/val) 0.31001/0.19392. Took 0.04 sec\n",
            "Epoch 1614, Loss(train/val) 0.29974/0.19361. Took 0.05 sec\n",
            "Epoch 1615, Loss(train/val) 0.30626/0.19346. Took 0.04 sec\n",
            "Epoch 1616, Loss(train/val) 0.29184/0.19322. Took 0.05 sec\n",
            "Epoch 1617, Loss(train/val) 0.30952/0.19297. Took 0.05 sec\n",
            "Epoch 1618, Loss(train/val) 0.31590/0.19263. Took 0.05 sec\n",
            "Epoch 1619, Loss(train/val) 0.30646/0.19260. Took 0.04 sec\n",
            "Epoch 1620, Loss(train/val) 0.29035/0.19264. Took 0.05 sec\n",
            "Epoch 1621, Loss(train/val) 0.29631/0.19298. Took 0.06 sec\n",
            "Epoch 1622, Loss(train/val) 0.30395/0.19309. Took 0.05 sec\n",
            "Epoch 1623, Loss(train/val) 0.30091/0.19357. Took 0.05 sec\n",
            "Epoch 1624, Loss(train/val) 0.30212/0.19302. Took 0.05 sec\n",
            "Epoch 1625, Loss(train/val) 0.29152/0.19258. Took 0.05 sec\n",
            "Epoch 1626, Loss(train/val) 0.32148/0.19211. Took 0.06 sec\n",
            "Epoch 1627, Loss(train/val) 0.29979/0.19187. Took 0.05 sec\n",
            "Epoch 1628, Loss(train/val) 0.30995/0.19175. Took 0.05 sec\n",
            "Epoch 1629, Loss(train/val) 0.30296/0.19151. Took 0.04 sec\n",
            "Epoch 1630, Loss(train/val) 0.29049/0.19158. Took 0.04 sec\n",
            "Epoch 1631, Loss(train/val) 0.30954/0.19093. Took 0.05 sec\n",
            "Epoch 1632, Loss(train/val) 0.29510/0.19068. Took 0.04 sec\n",
            "Epoch 1633, Loss(train/val) 0.31006/0.19087. Took 0.05 sec\n",
            "Epoch 1634, Loss(train/val) 0.28612/0.19087. Took 0.04 sec\n",
            "Epoch 1635, Loss(train/val) 0.29070/0.19100. Took 0.05 sec\n",
            "Epoch 1636, Loss(train/val) 0.28968/0.19108. Took 0.05 sec\n",
            "Epoch 1637, Loss(train/val) 0.29372/0.19086. Took 0.05 sec\n",
            "Epoch 1638, Loss(train/val) 0.30326/0.19070. Took 0.05 sec\n",
            "Epoch 1639, Loss(train/val) 0.29451/0.19053. Took 0.05 sec\n",
            "Epoch 1640, Loss(train/val) 0.29297/0.19026. Took 0.04 sec\n",
            "Epoch 1641, Loss(train/val) 0.31133/0.19012. Took 0.06 sec\n",
            "Epoch 1642, Loss(train/val) 0.29126/0.19014. Took 0.05 sec\n",
            "Epoch 1643, Loss(train/val) 0.29215/0.19001. Took 0.05 sec\n",
            "Epoch 1644, Loss(train/val) 0.30407/0.18953. Took 0.05 sec\n",
            "Epoch 1645, Loss(train/val) 0.30224/0.18936. Took 0.05 sec\n",
            "Epoch 1646, Loss(train/val) 0.29905/0.18903. Took 0.05 sec\n",
            "Epoch 1647, Loss(train/val) 0.29334/0.18897. Took 0.05 sec\n",
            "Epoch 1648, Loss(train/val) 0.30014/0.18880. Took 0.04 sec\n",
            "Epoch 1649, Loss(train/val) 0.29642/0.18879. Took 0.04 sec\n",
            "Epoch 1650, Loss(train/val) 0.29088/0.18907. Took 0.04 sec\n",
            "Epoch 1651, Loss(train/val) 0.29327/0.18904. Took 0.06 sec\n",
            "Epoch 1652, Loss(train/val) 0.30425/0.18916. Took 0.05 sec\n",
            "Epoch 1653, Loss(train/val) 0.28581/0.18860. Took 0.04 sec\n",
            "Epoch 1654, Loss(train/val) 0.29392/0.18871. Took 0.04 sec\n",
            "Epoch 1655, Loss(train/val) 0.30182/0.18853. Took 0.05 sec\n",
            "Epoch 1656, Loss(train/val) 0.28472/0.18823. Took 0.06 sec\n",
            "Epoch 1657, Loss(train/val) 0.29490/0.18798. Took 0.04 sec\n",
            "Epoch 1658, Loss(train/val) 0.30097/0.18787. Took 0.04 sec\n",
            "Epoch 1659, Loss(train/val) 0.29210/0.18758. Took 0.05 sec\n",
            "Epoch 1660, Loss(train/val) 0.29118/0.18768. Took 0.05 sec\n",
            "Epoch 1661, Loss(train/val) 0.28470/0.18757. Took 0.06 sec\n",
            "Epoch 1662, Loss(train/val) 0.29201/0.18754. Took 0.05 sec\n",
            "Epoch 1663, Loss(train/val) 0.30464/0.18749. Took 0.04 sec\n",
            "Epoch 1664, Loss(train/val) 0.30084/0.18742. Took 0.05 sec\n",
            "Epoch 1665, Loss(train/val) 0.28477/0.18745. Took 0.05 sec\n",
            "Epoch 1666, Loss(train/val) 0.28929/0.18717. Took 0.05 sec\n",
            "Epoch 1667, Loss(train/val) 0.29365/0.18707. Took 0.04 sec\n",
            "Epoch 1668, Loss(train/val) 0.28894/0.18694. Took 0.04 sec\n",
            "Epoch 1669, Loss(train/val) 0.29358/0.18692. Took 0.05 sec\n",
            "Epoch 1670, Loss(train/val) 0.30512/0.18679. Took 0.05 sec\n",
            "Epoch 1671, Loss(train/val) 0.29041/0.18649. Took 0.06 sec\n",
            "Epoch 1672, Loss(train/val) 0.28123/0.18643. Took 0.05 sec\n",
            "Epoch 1673, Loss(train/val) 0.29593/0.18630. Took 0.05 sec\n",
            "Epoch 1674, Loss(train/val) 0.30157/0.18650. Took 0.04 sec\n",
            "Epoch 1675, Loss(train/val) 0.28414/0.18585. Took 0.04 sec\n",
            "Epoch 1676, Loss(train/val) 0.29934/0.18568. Took 0.05 sec\n",
            "Epoch 1677, Loss(train/val) 0.28578/0.18536. Took 0.05 sec\n",
            "Epoch 1678, Loss(train/val) 0.30271/0.18539. Took 0.04 sec\n",
            "Epoch 1679, Loss(train/val) 0.29040/0.18543. Took 0.05 sec\n",
            "Epoch 1680, Loss(train/val) 0.28253/0.18578. Took 0.04 sec\n",
            "Epoch 1681, Loss(train/val) 0.27796/0.18533. Took 0.05 sec\n",
            "Epoch 1682, Loss(train/val) 0.28006/0.18574. Took 0.04 sec\n",
            "Epoch 1683, Loss(train/val) 0.28517/0.18522. Took 0.05 sec\n",
            "Epoch 1684, Loss(train/val) 0.28370/0.18460. Took 0.05 sec\n",
            "Epoch 1685, Loss(train/val) 0.28560/0.18498. Took 0.05 sec\n",
            "Epoch 1686, Loss(train/val) 0.28428/0.18452. Took 0.06 sec\n",
            "Epoch 1687, Loss(train/val) 0.28470/0.18448. Took 0.05 sec\n",
            "Epoch 1688, Loss(train/val) 0.28437/0.18432. Took 0.05 sec\n",
            "Epoch 1689, Loss(train/val) 0.29143/0.18394. Took 0.05 sec\n",
            "Epoch 1690, Loss(train/val) 0.29048/0.18432. Took 0.04 sec\n",
            "Epoch 1691, Loss(train/val) 0.29966/0.18379. Took 0.05 sec\n",
            "Epoch 1692, Loss(train/val) 0.28974/0.18353. Took 0.05 sec\n",
            "Epoch 1693, Loss(train/val) 0.30046/0.18343. Took 0.05 sec\n",
            "Epoch 1694, Loss(train/val) 0.27536/0.18327. Took 0.05 sec\n",
            "Epoch 1695, Loss(train/val) 0.28124/0.18311. Took 0.05 sec\n",
            "Epoch 1696, Loss(train/val) 0.28033/0.18298. Took 0.05 sec\n",
            "Epoch 1697, Loss(train/val) 0.27930/0.18326. Took 0.05 sec\n",
            "Epoch 1698, Loss(train/val) 0.28991/0.18403. Took 0.05 sec\n",
            "Epoch 1699, Loss(train/val) 0.28157/0.18410. Took 0.05 sec\n",
            "Epoch 1700, Loss(train/val) 0.29217/0.18356. Took 0.05 sec\n",
            "Epoch 1701, Loss(train/val) 0.28844/0.18313. Took 0.06 sec\n",
            "Epoch 1702, Loss(train/val) 0.28597/0.18274. Took 0.05 sec\n",
            "Epoch 1703, Loss(train/val) 0.28194/0.18275. Took 0.05 sec\n",
            "Epoch 1704, Loss(train/val) 0.29529/0.18226. Took 0.05 sec\n",
            "Epoch 1705, Loss(train/val) 0.28341/0.18216. Took 0.05 sec\n",
            "Epoch 1706, Loss(train/val) 0.28009/0.18219. Took 0.06 sec\n",
            "Epoch 1707, Loss(train/val) 0.28926/0.18252. Took 0.05 sec\n",
            "Epoch 1708, Loss(train/val) 0.27899/0.18188. Took 0.05 sec\n",
            "Epoch 1709, Loss(train/val) 0.28082/0.18156. Took 0.06 sec\n",
            "Epoch 1710, Loss(train/val) 0.27818/0.18119. Took 0.06 sec\n",
            "Epoch 1711, Loss(train/val) 0.28732/0.18108. Took 0.05 sec\n",
            "Epoch 1712, Loss(train/val) 0.28357/0.18097. Took 0.05 sec\n",
            "Epoch 1713, Loss(train/val) 0.27676/0.18090. Took 0.05 sec\n",
            "Epoch 1714, Loss(train/val) 0.28274/0.18068. Took 0.05 sec\n",
            "Epoch 1715, Loss(train/val) 0.26974/0.18092. Took 0.06 sec\n",
            "Epoch 1716, Loss(train/val) 0.27859/0.18125. Took 0.05 sec\n",
            "Epoch 1717, Loss(train/val) 0.30252/0.18067. Took 0.05 sec\n",
            "Epoch 1718, Loss(train/val) 0.27646/0.18031. Took 0.07 sec\n",
            "Epoch 1719, Loss(train/val) 0.29516/0.18062. Took 0.05 sec\n",
            "Epoch 1720, Loss(train/val) 0.27763/0.18099. Took 0.05 sec\n",
            "Epoch 1721, Loss(train/val) 0.27605/0.18081. Took 0.05 sec\n",
            "Epoch 1722, Loss(train/val) 0.27171/0.18054. Took 0.05 sec\n",
            "Epoch 1723, Loss(train/val) 0.28460/0.18007. Took 0.05 sec\n",
            "Epoch 1724, Loss(train/val) 0.27421/0.17965. Took 0.05 sec\n",
            "Epoch 1725, Loss(train/val) 0.28215/0.17975. Took 0.05 sec\n",
            "Epoch 1726, Loss(train/val) 0.29505/0.18062. Took 0.05 sec\n",
            "Epoch 1727, Loss(train/val) 0.27881/0.17965. Took 0.05 sec\n",
            "Epoch 1728, Loss(train/val) 0.29414/0.17940. Took 0.06 sec\n",
            "Epoch 1729, Loss(train/val) 0.27610/0.17906. Took 0.05 sec\n",
            "Epoch 1730, Loss(train/val) 0.27448/0.17887. Took 0.04 sec\n",
            "Epoch 1731, Loss(train/val) 0.27086/0.17905. Took 0.05 sec\n",
            "Epoch 1732, Loss(train/val) 0.28260/0.17931. Took 0.05 sec\n",
            "Epoch 1733, Loss(train/val) 0.27403/0.17861. Took 0.05 sec\n",
            "Epoch 1734, Loss(train/val) 0.27424/0.17841. Took 0.05 sec\n",
            "Epoch 1735, Loss(train/val) 0.27671/0.17840. Took 0.04 sec\n",
            "Epoch 1736, Loss(train/val) 0.27961/0.17799. Took 0.04 sec\n",
            "Epoch 1737, Loss(train/val) 0.29112/0.17803. Took 0.05 sec\n",
            "Epoch 1738, Loss(train/val) 0.27255/0.17776. Took 0.05 sec\n",
            "Epoch 1739, Loss(train/val) 0.28341/0.17748. Took 0.05 sec\n",
            "Epoch 1740, Loss(train/val) 0.27786/0.17748. Took 0.05 sec\n",
            "Epoch 1741, Loss(train/val) 0.28485/0.17736. Took 0.04 sec\n",
            "Epoch 1742, Loss(train/val) 0.28533/0.17742. Took 0.04 sec\n",
            "Epoch 1743, Loss(train/val) 0.27007/0.17713. Took 0.05 sec\n",
            "Epoch 1744, Loss(train/val) 0.27515/0.17679. Took 0.05 sec\n",
            "Epoch 1745, Loss(train/val) 0.27786/0.17669. Took 0.05 sec\n",
            "Epoch 1746, Loss(train/val) 0.26617/0.17683. Took 0.04 sec\n",
            "Epoch 1747, Loss(train/val) 0.27293/0.17752. Took 0.04 sec\n",
            "Epoch 1748, Loss(train/val) 0.27383/0.17728. Took 0.05 sec\n",
            "Epoch 1749, Loss(train/val) 0.27888/0.17790. Took 0.04 sec\n",
            "Epoch 1750, Loss(train/val) 0.27252/0.17796. Took 0.05 sec\n",
            "Epoch 1751, Loss(train/val) 0.27155/0.17695. Took 0.05 sec\n",
            "Epoch 1752, Loss(train/val) 0.26508/0.17783. Took 0.05 sec\n",
            "Epoch 1753, Loss(train/val) 0.27981/0.17734. Took 0.05 sec\n",
            "Epoch 1754, Loss(train/val) 0.27209/0.17744. Took 0.05 sec\n",
            "Epoch 1755, Loss(train/val) 0.27923/0.17711. Took 0.04 sec\n",
            "Epoch 1756, Loss(train/val) 0.26007/0.17640. Took 0.04 sec\n",
            "Epoch 1757, Loss(train/val) 0.27405/0.17574. Took 0.04 sec\n",
            "Epoch 1758, Loss(train/val) 0.26817/0.17568. Took 0.05 sec\n",
            "Epoch 1759, Loss(train/val) 0.27826/0.17538. Took 0.05 sec\n",
            "Epoch 1760, Loss(train/val) 0.27481/0.17511. Took 0.05 sec\n",
            "Epoch 1761, Loss(train/val) 0.27990/0.17496. Took 0.04 sec\n",
            "Epoch 1762, Loss(train/val) 0.27257/0.17554. Took 0.04 sec\n",
            "Epoch 1763, Loss(train/val) 0.27276/0.17622. Took 0.05 sec\n",
            "Epoch 1764, Loss(train/val) 0.28346/0.17589. Took 0.05 sec\n",
            "Epoch 1765, Loss(train/val) 0.27432/0.17546. Took 0.04 sec\n",
            "Epoch 1766, Loss(train/val) 0.26993/0.17679. Took 0.04 sec\n",
            "Epoch 1767, Loss(train/val) 0.28402/0.17731. Took 0.05 sec\n",
            "Epoch 1768, Loss(train/val) 0.28021/0.17700. Took 0.05 sec\n",
            "Epoch 1769, Loss(train/val) 0.27038/0.17637. Took 0.05 sec\n",
            "Epoch 1770, Loss(train/val) 0.26785/0.17553. Took 0.05 sec\n",
            "Epoch 1771, Loss(train/val) 0.28002/0.17525. Took 0.05 sec\n",
            "Epoch 1772, Loss(train/val) 0.26995/0.17436. Took 0.05 sec\n",
            "Epoch 1773, Loss(train/val) 0.26277/0.17439. Took 0.05 sec\n",
            "Epoch 1774, Loss(train/val) 0.28860/0.17431. Took 0.05 sec\n",
            "Epoch 1775, Loss(train/val) 0.26593/0.17354. Took 0.05 sec\n",
            "Epoch 1776, Loss(train/val) 0.26724/0.17354. Took 0.05 sec\n",
            "Epoch 1777, Loss(train/val) 0.26531/0.17359. Took 0.04 sec\n",
            "Epoch 1778, Loss(train/val) 0.26837/0.17357. Took 0.05 sec\n",
            "Epoch 1779, Loss(train/val) 0.27720/0.17343. Took 0.04 sec\n",
            "Epoch 1780, Loss(train/val) 0.25720/0.17299. Took 0.04 sec\n",
            "Epoch 1781, Loss(train/val) 0.26699/0.17270. Took 0.05 sec\n",
            "Epoch 1782, Loss(train/val) 0.27186/0.17255. Took 0.05 sec\n",
            "Epoch 1783, Loss(train/val) 0.26946/0.17257. Took 0.05 sec\n",
            "Epoch 1784, Loss(train/val) 0.27192/0.17271. Took 0.05 sec\n",
            "Epoch 1785, Loss(train/val) 0.27372/0.17273. Took 0.05 sec\n",
            "Epoch 1786, Loss(train/val) 0.26612/0.17265. Took 0.05 sec\n",
            "Epoch 1787, Loss(train/val) 0.26418/0.17327. Took 0.06 sec\n",
            "Epoch 1788, Loss(train/val) 0.26986/0.17372. Took 0.06 sec\n",
            "Epoch 1789, Loss(train/val) 0.26606/0.17361. Took 0.04 sec\n",
            "Epoch 1790, Loss(train/val) 0.26490/0.17391. Took 0.05 sec\n",
            "Epoch 1791, Loss(train/val) 0.26786/0.17349. Took 0.04 sec\n",
            "Epoch 1792, Loss(train/val) 0.26191/0.17260. Took 0.04 sec\n",
            "Epoch 1793, Loss(train/val) 0.27194/0.17244. Took 0.05 sec\n",
            "Epoch 1794, Loss(train/val) 0.26248/0.17278. Took 0.05 sec\n",
            "Epoch 1795, Loss(train/val) 0.26645/0.17246. Took 0.05 sec\n",
            "Epoch 1796, Loss(train/val) 0.26577/0.17185. Took 0.04 sec\n",
            "Epoch 1797, Loss(train/val) 0.26929/0.17080. Took 0.05 sec\n",
            "Epoch 1798, Loss(train/val) 0.28063/0.17066. Took 0.05 sec\n",
            "Epoch 1799, Loss(train/val) 0.26539/0.17076. Took 0.05 sec\n",
            "Epoch 1800, Loss(train/val) 0.26202/0.17139. Took 0.05 sec\n",
            "Epoch 1801, Loss(train/val) 0.26062/0.17190. Took 0.05 sec\n",
            "Epoch 1802, Loss(train/val) 0.26189/0.17244. Took 0.05 sec\n",
            "Epoch 1803, Loss(train/val) 0.26081/0.17237. Took 0.06 sec\n",
            "Epoch 1804, Loss(train/val) 0.26359/0.17160. Took 0.05 sec\n",
            "Epoch 1805, Loss(train/val) 0.26085/0.17141. Took 0.05 sec\n",
            "Epoch 1806, Loss(train/val) 0.27006/0.17096. Took 0.05 sec\n",
            "Epoch 1807, Loss(train/val) 0.26828/0.17100. Took 0.05 sec\n",
            "Epoch 1808, Loss(train/val) 0.26770/0.17132. Took 0.06 sec\n",
            "Epoch 1809, Loss(train/val) 0.27302/0.17188. Took 0.05 sec\n",
            "Epoch 1810, Loss(train/val) 0.25895/0.17233. Took 0.04 sec\n",
            "Epoch 1811, Loss(train/val) 0.26922/0.17351. Took 0.04 sec\n",
            "Epoch 1812, Loss(train/val) 0.27902/0.17326. Took 0.05 sec\n",
            "Epoch 1813, Loss(train/val) 0.27046/0.17209. Took 0.05 sec\n",
            "Epoch 1814, Loss(train/val) 0.26102/0.17121. Took 0.05 sec\n",
            "Epoch 1815, Loss(train/val) 0.26308/0.17155. Took 0.04 sec\n",
            "Epoch 1816, Loss(train/val) 0.26159/0.17037. Took 0.05 sec\n",
            "Epoch 1817, Loss(train/val) 0.26483/0.16990. Took 0.05 sec\n",
            "Epoch 1818, Loss(train/val) 0.25589/0.16997. Took 0.05 sec\n",
            "Epoch 1819, Loss(train/val) 0.26372/0.17019. Took 0.04 sec\n",
            "Epoch 1820, Loss(train/val) 0.26369/0.16988. Took 0.04 sec\n",
            "Epoch 1821, Loss(train/val) 0.26122/0.17007. Took 0.04 sec\n",
            "Epoch 1822, Loss(train/val) 0.26095/0.16956. Took 0.05 sec\n",
            "Epoch 1823, Loss(train/val) 0.26225/0.16972. Took 0.05 sec\n",
            "Epoch 1824, Loss(train/val) 0.27242/0.16936. Took 0.05 sec\n",
            "Epoch 1825, Loss(train/val) 0.26406/0.16981. Took 0.05 sec\n",
            "Epoch 1826, Loss(train/val) 0.26249/0.16904. Took 0.05 sec\n",
            "Epoch 1827, Loss(train/val) 0.27018/0.16887. Took 0.05 sec\n",
            "Epoch 1828, Loss(train/val) 0.25649/0.16905. Took 0.05 sec\n",
            "Epoch 1829, Loss(train/val) 0.25730/0.17038. Took 0.05 sec\n",
            "Epoch 1830, Loss(train/val) 0.27323/0.17081. Took 0.04 sec\n",
            "Epoch 1831, Loss(train/val) 0.26672/0.16958. Took 0.05 sec\n",
            "Epoch 1832, Loss(train/val) 0.25882/0.16959. Took 0.05 sec\n",
            "Epoch 1833, Loss(train/val) 0.27030/0.16949. Took 0.06 sec\n",
            "Epoch 1834, Loss(train/val) 0.25837/0.16986. Took 0.04 sec\n",
            "Epoch 1835, Loss(train/val) 0.25356/0.16886. Took 0.05 sec\n",
            "Epoch 1836, Loss(train/val) 0.25946/0.16930. Took 0.05 sec\n",
            "Epoch 1837, Loss(train/val) 0.25557/0.16952. Took 0.05 sec\n",
            "Epoch 1838, Loss(train/val) 0.27571/0.17053. Took 0.05 sec\n",
            "Epoch 1839, Loss(train/val) 0.25326/0.16984. Took 0.05 sec\n",
            "Epoch 1840, Loss(train/val) 0.25216/0.17044. Took 0.05 sec\n",
            "Epoch 1841, Loss(train/val) 0.25426/0.17066. Took 0.05 sec\n",
            "Epoch 1842, Loss(train/val) 0.25773/0.17084. Took 0.04 sec\n",
            "Epoch 1843, Loss(train/val) 0.25720/0.17051. Took 0.05 sec\n",
            "Epoch 1844, Loss(train/val) 0.27021/0.16885. Took 0.04 sec\n",
            "Epoch 1845, Loss(train/val) 0.26983/0.16851. Took 0.05 sec\n",
            "Epoch 1846, Loss(train/val) 0.26078/0.16964. Took 0.04 sec\n",
            "Epoch 1847, Loss(train/val) 0.27856/0.17089. Took 0.05 sec\n",
            "Epoch 1848, Loss(train/val) 0.25367/0.17201. Took 0.05 sec\n",
            "Epoch 1849, Loss(train/val) 0.25600/0.17143. Took 0.05 sec\n",
            "Epoch 1850, Loss(train/val) 0.26011/0.17142. Took 0.05 sec\n",
            "Epoch 1851, Loss(train/val) 0.25943/0.17054. Took 0.05 sec\n",
            "Epoch 1852, Loss(train/val) 0.26516/0.16849. Took 0.05 sec\n",
            "Epoch 1853, Loss(train/val) 0.25816/0.16745. Took 0.05 sec\n",
            "Epoch 1854, Loss(train/val) 0.26516/0.16747. Took 0.05 sec\n",
            "Epoch 1855, Loss(train/val) 0.25242/0.16728. Took 0.05 sec\n",
            "Epoch 1856, Loss(train/val) 0.25509/0.16748. Took 0.05 sec\n",
            "Epoch 1857, Loss(train/val) 0.25601/0.16779. Took 0.05 sec\n",
            "Epoch 1858, Loss(train/val) 0.25667/0.16866. Took 0.06 sec\n",
            "Epoch 1859, Loss(train/val) 0.25192/0.17082. Took 0.04 sec\n",
            "Epoch 1860, Loss(train/val) 0.25700/0.17215. Took 0.05 sec\n",
            "Epoch 1861, Loss(train/val) 0.25654/0.17250. Took 0.04 sec\n",
            "Epoch 1862, Loss(train/val) 0.25645/0.17162. Took 0.05 sec\n",
            "Epoch 1863, Loss(train/val) 0.25966/0.17142. Took 0.06 sec\n",
            "Epoch 1864, Loss(train/val) 0.25422/0.17180. Took 0.05 sec\n",
            "Epoch 1865, Loss(train/val) 0.25931/0.17006. Took 0.05 sec\n",
            "Epoch 1866, Loss(train/val) 0.26513/0.16904. Took 0.05 sec\n",
            "Epoch 1867, Loss(train/val) 0.25174/0.16933. Took 0.05 sec\n",
            "Epoch 1868, Loss(train/val) 0.25292/0.16936. Took 0.05 sec\n",
            "Epoch 1869, Loss(train/val) 0.27975/0.16764. Took 0.05 sec\n",
            "Epoch 1870, Loss(train/val) 0.26526/0.16732. Took 0.05 sec\n",
            "Epoch 1871, Loss(train/val) 0.25703/0.16738. Took 0.05 sec\n",
            "Epoch 1872, Loss(train/val) 0.25773/0.16698. Took 0.04 sec\n",
            "Epoch 1873, Loss(train/val) 0.25416/0.16696. Took 0.05 sec\n",
            "Epoch 1874, Loss(train/val) 0.24932/0.16693. Took 0.05 sec\n",
            "Epoch 1875, Loss(train/val) 0.25593/0.16677. Took 0.04 sec\n",
            "Epoch 1876, Loss(train/val) 0.25022/0.16739. Took 0.05 sec\n",
            "Epoch 1877, Loss(train/val) 0.25557/0.16896. Took 0.05 sec\n",
            "Epoch 1878, Loss(train/val) 0.25650/0.17076. Took 0.06 sec\n",
            "Epoch 1879, Loss(train/val) 0.25187/0.17140. Took 0.05 sec\n",
            "Epoch 1880, Loss(train/val) 0.25720/0.17184. Took 0.05 sec\n",
            "Epoch 1881, Loss(train/val) 0.26414/0.17181. Took 0.04 sec\n",
            "Epoch 1882, Loss(train/val) 0.24877/0.16884. Took 0.05 sec\n",
            "Epoch 1883, Loss(train/val) 0.25073/0.16812. Took 0.05 sec\n",
            "Epoch 1884, Loss(train/val) 0.25437/0.16729. Took 0.05 sec\n",
            "Epoch 1885, Loss(train/val) 0.26575/0.16722. Took 0.04 sec\n",
            "Epoch 1886, Loss(train/val) 0.25665/0.16753. Took 0.04 sec\n",
            "Epoch 1887, Loss(train/val) 0.26006/0.16755. Took 0.05 sec\n",
            "Epoch 1888, Loss(train/val) 0.25038/0.16853. Took 0.05 sec\n",
            "Epoch 1889, Loss(train/val) 0.25298/0.17033. Took 0.05 sec\n",
            "Epoch 1890, Loss(train/val) 0.25992/0.17064. Took 0.05 sec\n",
            "Epoch 1891, Loss(train/val) 0.25302/0.16997. Took 0.05 sec\n",
            "Epoch 1892, Loss(train/val) 0.26070/0.16844. Took 0.05 sec\n",
            "Epoch 1893, Loss(train/val) 0.28090/0.16741. Took 0.05 sec\n",
            "Epoch 1894, Loss(train/val) 0.25272/0.16700. Took 0.05 sec\n",
            "Epoch 1895, Loss(train/val) 0.25844/0.16690. Took 0.05 sec\n",
            "Epoch 1896, Loss(train/val) 0.26722/0.16757. Took 0.04 sec\n",
            "Epoch 1897, Loss(train/val) 0.24812/0.16708. Took 0.04 sec\n",
            "Epoch 1898, Loss(train/val) 0.24854/0.16814. Took 0.05 sec\n",
            "Epoch 1899, Loss(train/val) 0.29877/0.16732. Took 0.05 sec\n",
            "Epoch 1900, Loss(train/val) 0.25086/0.16737. Took 0.04 sec\n",
            "Epoch 1901, Loss(train/val) 0.26038/0.16712. Took 0.04 sec\n",
            "Epoch 1902, Loss(train/val) 0.25115/0.16744. Took 0.05 sec\n",
            "Epoch 1903, Loss(train/val) 0.25239/0.16744. Took 0.05 sec\n",
            "Epoch 1904, Loss(train/val) 0.25096/0.16730. Took 0.05 sec\n",
            "Epoch 1905, Loss(train/val) 0.27181/0.16731. Took 0.05 sec\n",
            "Epoch 1906, Loss(train/val) 0.26301/0.16696. Took 0.04 sec\n",
            "Epoch 1907, Loss(train/val) 0.25546/0.16727. Took 0.04 sec\n",
            "Epoch 1908, Loss(train/val) 0.24628/0.16778. Took 0.06 sec\n",
            "Epoch 1909, Loss(train/val) 0.26297/0.16828. Took 0.05 sec\n",
            "Epoch 1910, Loss(train/val) 0.24615/0.17001. Took 0.04 sec\n",
            "Epoch 1911, Loss(train/val) 0.28395/0.16923. Took 0.05 sec\n",
            "Epoch 1912, Loss(train/val) 0.25853/0.16767. Took 0.05 sec\n",
            "Epoch 1913, Loss(train/val) 0.24804/0.16739. Took 0.05 sec\n",
            "Epoch 1914, Loss(train/val) 0.26420/0.16699. Took 0.05 sec\n",
            "Epoch 1915, Loss(train/val) 0.26044/0.16716. Took 0.05 sec\n",
            "Epoch 1916, Loss(train/val) 0.25590/0.16705. Took 0.04 sec\n",
            "Epoch 1917, Loss(train/val) 0.25207/0.16763. Took 0.04 sec\n",
            "Epoch 1918, Loss(train/val) 0.24946/0.16762. Took 0.05 sec\n",
            "Epoch 1919, Loss(train/val) 0.25817/0.16754. Took 0.04 sec\n",
            "Epoch 1920, Loss(train/val) 0.25278/0.16709. Took 0.05 sec\n",
            "Epoch 1921, Loss(train/val) 0.24949/0.16711. Took 0.05 sec\n",
            "Epoch 1922, Loss(train/val) 0.24788/0.16776. Took 0.05 sec\n",
            "Epoch 1923, Loss(train/val) 0.24676/0.16766. Took 0.05 sec\n",
            "Epoch 1924, Loss(train/val) 0.25599/0.16781. Took 0.05 sec\n",
            "Epoch 1925, Loss(train/val) 0.25100/0.16875. Took 0.04 sec\n",
            "Epoch 1926, Loss(train/val) 0.26956/0.16835. Took 0.04 sec\n",
            "Epoch 1927, Loss(train/val) 0.25983/0.16799. Took 0.04 sec\n",
            "Epoch 1928, Loss(train/val) 0.25303/0.16746. Took 0.05 sec\n",
            "Epoch 1929, Loss(train/val) 0.24904/0.16725. Took 0.04 sec\n",
            "Epoch 1930, Loss(train/val) 0.24635/0.16811. Took 0.05 sec\n",
            "Epoch 1931, Loss(train/val) 0.25175/0.16775. Took 0.04 sec\n",
            "Epoch 1932, Loss(train/val) 0.25382/0.16762. Took 0.04 sec\n",
            "Epoch 1933, Loss(train/val) 0.25352/0.16738. Took 0.05 sec\n",
            "Epoch 1934, Loss(train/val) 0.25520/0.16820. Took 0.05 sec\n",
            "Epoch 1935, Loss(train/val) 0.26380/0.16814. Took 0.04 sec\n",
            "Epoch 1936, Loss(train/val) 0.26322/0.16807. Took 0.04 sec\n",
            "Epoch 1937, Loss(train/val) 0.24635/0.16953. Took 0.05 sec\n",
            "Epoch 1938, Loss(train/val) 0.26085/0.16850. Took 0.05 sec\n",
            "Epoch 1939, Loss(train/val) 0.24815/0.16845. Took 0.05 sec\n",
            "Epoch 1940, Loss(train/val) 0.24655/0.16837. Took 0.04 sec\n",
            "Epoch 1941, Loss(train/val) 0.24396/0.16741. Took 0.05 sec\n",
            "Epoch 1942, Loss(train/val) 0.24135/0.16755. Took 0.04 sec\n",
            "Epoch 1943, Loss(train/val) 0.24697/0.16732. Took 0.05 sec\n",
            "Epoch 1944, Loss(train/val) 0.24955/0.16755. Took 0.04 sec\n",
            "Epoch 1945, Loss(train/val) 0.25043/0.16740. Took 0.04 sec\n",
            "Epoch 1946, Loss(train/val) 0.24470/0.16760. Took 0.05 sec\n",
            "Epoch 1947, Loss(train/val) 0.24284/0.16750. Took 0.04 sec\n",
            "Epoch 1948, Loss(train/val) 0.24797/0.16773. Took 0.05 sec\n",
            "Epoch 1949, Loss(train/val) 0.25478/0.16802. Took 0.04 sec\n",
            "Epoch 1950, Loss(train/val) 0.24712/0.16934. Took 0.05 sec\n",
            "Epoch 1951, Loss(train/val) 0.25364/0.16952. Took 0.05 sec\n",
            "Epoch 1952, Loss(train/val) 0.25956/0.16956. Took 0.05 sec\n",
            "Epoch 1953, Loss(train/val) 0.23672/0.16874. Took 0.05 sec\n",
            "Epoch 1954, Loss(train/val) 0.25945/0.16776. Took 0.04 sec\n",
            "Epoch 1955, Loss(train/val) 0.26157/0.16735. Took 0.04 sec\n",
            "Epoch 1956, Loss(train/val) 0.26087/0.16758. Took 0.04 sec\n",
            "Epoch 1957, Loss(train/val) 0.25167/0.16853. Took 0.05 sec\n",
            "Epoch 1958, Loss(train/val) 0.24316/0.16868. Took 0.05 sec\n",
            "Epoch 1959, Loss(train/val) 0.24317/0.16818. Took 0.04 sec\n",
            "Epoch 1960, Loss(train/val) 0.25231/0.16881. Took 0.05 sec\n",
            "Epoch 1961, Loss(train/val) 0.25086/0.16847. Took 0.05 sec\n",
            "Epoch 1962, Loss(train/val) 0.24346/0.16787. Took 0.05 sec\n",
            "Epoch 1963, Loss(train/val) 0.24881/0.16764. Took 0.05 sec\n",
            "Epoch 1964, Loss(train/val) 0.25485/0.16773. Took 0.04 sec\n",
            "Epoch 1965, Loss(train/val) 0.25277/0.16776. Took 0.04 sec\n",
            "Epoch 1966, Loss(train/val) 0.24426/0.16738. Took 0.04 sec\n",
            "Epoch 1967, Loss(train/val) 0.24839/0.16729. Took 0.04 sec\n",
            "Epoch 1968, Loss(train/val) 0.25599/0.16742. Took 0.05 sec\n",
            "Epoch 1969, Loss(train/val) 0.25705/0.16736. Took 0.04 sec\n",
            "Epoch 1970, Loss(train/val) 0.24910/0.16752. Took 0.04 sec\n",
            "Epoch 1971, Loss(train/val) 0.25779/0.16754. Took 0.04 sec\n",
            "Epoch 1972, Loss(train/val) 0.25350/0.16733. Took 0.06 sec\n",
            "Epoch 1973, Loss(train/val) 0.24469/0.16754. Took 0.06 sec\n",
            "Epoch 1974, Loss(train/val) 0.26337/0.16734. Took 0.04 sec\n",
            "Epoch 1975, Loss(train/val) 0.25452/0.16733. Took 0.04 sec\n",
            "Epoch 1976, Loss(train/val) 0.25670/0.16810. Took 0.05 sec\n",
            "Epoch 1977, Loss(train/val) 0.24623/0.17015. Took 0.04 sec\n",
            "Epoch 1978, Loss(train/val) 0.25215/0.16980. Took 0.05 sec\n",
            "Epoch 1979, Loss(train/val) 0.25505/0.16872. Took 0.04 sec\n",
            "Epoch 1980, Loss(train/val) 0.25461/0.16872. Took 0.04 sec\n",
            "Epoch 1981, Loss(train/val) 0.24473/0.16793. Took 0.04 sec\n",
            "Epoch 1982, Loss(train/val) 0.25477/0.16786. Took 0.04 sec\n",
            "Epoch 1983, Loss(train/val) 0.24547/0.16800. Took 0.05 sec\n",
            "Epoch 1984, Loss(train/val) 0.26457/0.16879. Took 0.04 sec\n",
            "Epoch 1985, Loss(train/val) 0.24528/0.16789. Took 0.04 sec\n",
            "Epoch 1986, Loss(train/val) 0.24509/0.16724. Took 0.04 sec\n",
            "Epoch 1987, Loss(train/val) 0.24568/0.16726. Took 0.04 sec\n",
            "Epoch 1988, Loss(train/val) 0.24656/0.16789. Took 0.05 sec\n",
            "Epoch 1989, Loss(train/val) 0.25512/0.16817. Took 0.05 sec\n",
            "Epoch 1990, Loss(train/val) 0.26851/0.16748. Took 0.04 sec\n",
            "Epoch 1991, Loss(train/val) 0.25285/0.16752. Took 0.04 sec\n",
            "Epoch 1992, Loss(train/val) 0.25435/0.16726. Took 0.05 sec\n",
            "Epoch 1993, Loss(train/val) 0.25507/0.16721. Took 0.05 sec\n",
            "Epoch 1994, Loss(train/val) 0.24395/0.16730. Took 0.05 sec\n",
            "Epoch 1995, Loss(train/val) 0.25190/0.16736. Took 0.05 sec\n",
            "Epoch 1996, Loss(train/val) 0.24726/0.16832. Took 0.05 sec\n",
            "Epoch 1997, Loss(train/val) 0.24320/0.16847. Took 0.05 sec\n",
            "Epoch 1998, Loss(train/val) 0.25283/0.16841. Took 0.05 sec\n",
            "Epoch 1999, Loss(train/val) 0.25405/0.16810. Took 0.04 sec\n",
            "Epoch 2000, Loss(train/val) 0.24963/0.16917. Took 0.04 sec\n",
            "Epoch 2001, Loss(train/val) 0.25107/0.16853. Took 0.04 sec\n",
            "Epoch 2002, Loss(train/val) 0.24425/0.16820. Took 0.04 sec\n",
            "Epoch 2003, Loss(train/val) 0.24520/0.16743. Took 0.05 sec\n",
            "Epoch 2004, Loss(train/val) 0.24593/0.16768. Took 0.05 sec\n",
            "Epoch 2005, Loss(train/val) 0.24707/0.16798. Took 0.05 sec\n",
            "Epoch 2006, Loss(train/val) 0.24655/0.16775. Took 0.05 sec\n",
            "Epoch 2007, Loss(train/val) 0.25901/0.16748. Took 0.04 sec\n",
            "Epoch 2008, Loss(train/val) 0.24201/0.16776. Took 0.06 sec\n",
            "Epoch 2009, Loss(train/val) 0.24774/0.16695. Took 0.05 sec\n",
            "Epoch 2010, Loss(train/val) 0.24993/0.16733. Took 0.04 sec\n",
            "Epoch 2011, Loss(train/val) 0.24589/0.16765. Took 0.04 sec\n",
            "Epoch 2012, Loss(train/val) 0.26346/0.16723. Took 0.04 sec\n",
            "Epoch 2013, Loss(train/val) 0.26490/0.16707. Took 0.05 sec\n",
            "Epoch 2014, Loss(train/val) 0.27483/0.16748. Took 0.04 sec\n",
            "Epoch 2015, Loss(train/val) 0.24470/0.16722. Took 0.05 sec\n",
            "Epoch 2016, Loss(train/val) 0.24750/0.16819. Took 0.06 sec\n",
            "Epoch 2017, Loss(train/val) 0.24089/0.16962. Took 0.05 sec\n",
            "Epoch 2018, Loss(train/val) 0.25957/0.16837. Took 0.05 sec\n",
            "Epoch 2019, Loss(train/val) 0.24792/0.16822. Took 0.05 sec\n",
            "Epoch 2020, Loss(train/val) 0.27812/0.16785. Took 0.05 sec\n",
            "Epoch 2021, Loss(train/val) 0.24376/0.16728. Took 0.05 sec\n",
            "Epoch 2022, Loss(train/val) 0.24996/0.16727. Took 0.05 sec\n",
            "Epoch 2023, Loss(train/val) 0.27303/0.16756. Took 0.05 sec\n",
            "Epoch 2024, Loss(train/val) 0.24962/0.16697. Took 0.05 sec\n",
            "Epoch 2025, Loss(train/val) 0.24817/0.16714. Took 0.05 sec\n",
            "Epoch 2026, Loss(train/val) 0.24656/0.16729. Took 0.05 sec\n",
            "Epoch 2027, Loss(train/val) 0.26102/0.16964. Took 0.06 sec\n",
            "Epoch 2028, Loss(train/val) 0.25348/0.16922. Took 0.05 sec\n",
            "Epoch 2029, Loss(train/val) 0.24908/0.17086. Took 0.05 sec\n",
            "Epoch 2030, Loss(train/val) 0.24714/0.16837. Took 0.05 sec\n",
            "Epoch 2031, Loss(train/val) 0.26817/0.16909. Took 0.05 sec\n",
            "Epoch 2032, Loss(train/val) 0.25382/0.16847. Took 0.05 sec\n",
            "Epoch 2033, Loss(train/val) 0.24823/0.16735. Took 0.04 sec\n",
            "Epoch 2034, Loss(train/val) 0.25979/0.16728. Took 0.05 sec\n",
            "Epoch 2035, Loss(train/val) 0.24225/0.16797. Took 0.05 sec\n",
            "Epoch 2036, Loss(train/val) 0.24252/0.16830. Took 0.05 sec\n",
            "Epoch 2037, Loss(train/val) 0.24286/0.16727. Took 0.06 sec\n",
            "Epoch 2038, Loss(train/val) 0.24991/0.16731. Took 0.05 sec\n",
            "Epoch 2039, Loss(train/val) 0.26072/0.16717. Took 0.05 sec\n",
            "Epoch 2040, Loss(train/val) 0.24487/0.16749. Took 0.05 sec\n",
            "Epoch 2041, Loss(train/val) 0.25281/0.16939. Took 0.05 sec\n",
            "Epoch 2042, Loss(train/val) 0.24776/0.17148. Took 0.05 sec\n",
            "Epoch 2043, Loss(train/val) 0.25060/0.17137. Took 0.05 sec\n",
            "Epoch 2044, Loss(train/val) 0.24576/0.17064. Took 0.05 sec\n",
            "Epoch 2045, Loss(train/val) 0.24331/0.16839. Took 0.04 sec\n",
            "Epoch 2046, Loss(train/val) 0.23922/0.16731. Took 0.05 sec\n",
            "Epoch 2047, Loss(train/val) 0.24127/0.16703. Took 0.05 sec\n",
            "Epoch 2048, Loss(train/val) 0.24593/0.16706. Took 0.04 sec\n",
            "Epoch 2049, Loss(train/val) 0.24248/0.16708. Took 0.04 sec\n",
            "Epoch 2050, Loss(train/val) 0.24138/0.16761. Took 0.04 sec\n",
            "Epoch 2051, Loss(train/val) 0.25054/0.16737. Took 0.05 sec\n",
            "Epoch 2052, Loss(train/val) 0.23450/0.16768. Took 0.05 sec\n",
            "Epoch 2053, Loss(train/val) 0.24201/0.16800. Took 0.04 sec\n",
            "Epoch 2054, Loss(train/val) 0.24916/0.16677. Took 0.04 sec\n",
            "Epoch 2055, Loss(train/val) 0.24956/0.16747. Took 0.04 sec\n",
            "Epoch 2056, Loss(train/val) 0.24060/0.16803. Took 0.04 sec\n",
            "Epoch 2057, Loss(train/val) 0.24340/0.16745. Took 0.05 sec\n",
            "Epoch 2058, Loss(train/val) 0.24677/0.16646. Took 0.05 sec\n",
            "Epoch 2059, Loss(train/val) 0.24017/0.16649. Took 0.06 sec\n",
            "Epoch 2060, Loss(train/val) 0.24303/0.16655. Took 0.05 sec\n",
            "Epoch 2061, Loss(train/val) 0.25455/0.16667. Took 0.05 sec\n",
            "Epoch 2062, Loss(train/val) 0.24883/0.16667. Took 0.05 sec\n",
            "Epoch 2063, Loss(train/val) 0.26955/0.16680. Took 0.04 sec\n",
            "Epoch 2064, Loss(train/val) 0.24698/0.16644. Took 0.04 sec\n",
            "Epoch 2065, Loss(train/val) 0.25699/0.16637. Took 0.04 sec\n",
            "Epoch 2066, Loss(train/val) 0.24273/0.16623. Took 0.05 sec\n",
            "Epoch 2067, Loss(train/val) 0.24211/0.16648. Took 0.05 sec\n",
            "Epoch 2068, Loss(train/val) 0.25341/0.16615. Took 0.04 sec\n",
            "Epoch 2069, Loss(train/val) 0.24214/0.16620. Took 0.04 sec\n",
            "Epoch 2070, Loss(train/val) 0.25650/0.16635. Took 0.05 sec\n",
            "Epoch 2071, Loss(train/val) 0.26279/0.16646. Took 0.05 sec\n",
            "Epoch 2072, Loss(train/val) 0.24666/0.16640. Took 0.04 sec\n",
            "Epoch 2073, Loss(train/val) 0.24354/0.16633. Took 0.04 sec\n",
            "Epoch 2074, Loss(train/val) 0.25746/0.16641. Took 0.04 sec\n",
            "Epoch 2075, Loss(train/val) 0.24197/0.16760. Took 0.04 sec\n",
            "Epoch 2076, Loss(train/val) 0.25279/0.16768. Took 0.05 sec\n",
            "Epoch 2077, Loss(train/val) 0.24604/0.16884. Took 0.05 sec\n",
            "Epoch 2078, Loss(train/val) 0.23872/0.16787. Took 0.05 sec\n",
            "Epoch 2079, Loss(train/val) 0.24277/0.16815. Took 0.05 sec\n",
            "Epoch 2080, Loss(train/val) 0.24797/0.16734. Took 0.05 sec\n",
            "Epoch 2081, Loss(train/val) 0.26442/0.16710. Took 0.05 sec\n",
            "Epoch 2082, Loss(train/val) 0.24746/0.16687. Took 0.05 sec\n",
            "Epoch 2083, Loss(train/val) 0.25591/0.16667. Took 0.05 sec\n",
            "Epoch 2084, Loss(train/val) 0.24365/0.16644. Took 0.04 sec\n",
            "Epoch 2085, Loss(train/val) 0.24688/0.16672. Took 0.05 sec\n",
            "Epoch 2086, Loss(train/val) 0.25453/0.16760. Took 0.05 sec\n",
            "Epoch 2087, Loss(train/val) 0.25923/0.16939. Took 0.05 sec\n",
            "Epoch 2088, Loss(train/val) 0.24951/0.16882. Took 0.05 sec\n",
            "Epoch 2089, Loss(train/val) 0.25133/0.16658. Took 0.05 sec\n",
            "Epoch 2090, Loss(train/val) 0.25022/0.16640. Took 0.04 sec\n",
            "Epoch 2091, Loss(train/val) 0.24606/0.16652. Took 0.05 sec\n",
            "Epoch 2092, Loss(train/val) 0.24759/0.16670. Took 0.05 sec\n",
            "Epoch 2093, Loss(train/val) 0.25076/0.16645. Took 0.04 sec\n",
            "Epoch 2094, Loss(train/val) 0.25601/0.16668. Took 0.04 sec\n",
            "Epoch 2095, Loss(train/val) 0.26123/0.16617. Took 0.04 sec\n",
            "Epoch 2096, Loss(train/val) 0.25624/0.16614. Took 0.05 sec\n",
            "Epoch 2097, Loss(train/val) 0.24427/0.16607. Took 0.04 sec\n",
            "Epoch 2098, Loss(train/val) 0.24282/0.16601. Took 0.05 sec\n",
            "Epoch 2099, Loss(train/val) 0.24626/0.16581. Took 0.05 sec\n",
            "Epoch 2100, Loss(train/val) 0.24418/0.16568. Took 0.05 sec\n",
            "Epoch 2101, Loss(train/val) 0.25333/0.16591. Took 0.05 sec\n",
            "Epoch 2102, Loss(train/val) 0.24551/0.16610. Took 0.05 sec\n",
            "Epoch 2103, Loss(train/val) 0.24964/0.16640. Took 0.05 sec\n",
            "Epoch 2104, Loss(train/val) 0.25823/0.16626. Took 0.04 sec\n",
            "Epoch 2105, Loss(train/val) 0.26418/0.16614. Took 0.05 sec\n",
            "Epoch 2106, Loss(train/val) 0.24726/0.16605. Took 0.05 sec\n",
            "Epoch 2107, Loss(train/val) 0.24047/0.16600. Took 0.04 sec\n",
            "Epoch 2108, Loss(train/val) 0.24641/0.16593. Took 0.04 sec\n",
            "Epoch 2109, Loss(train/val) 0.24148/0.16604. Took 0.04 sec\n",
            "Epoch 2110, Loss(train/val) 0.24402/0.16599. Took 0.05 sec\n",
            "Epoch 2111, Loss(train/val) 0.23701/0.16663. Took 0.05 sec\n",
            "Epoch 2112, Loss(train/val) 0.25967/0.16776. Took 0.04 sec\n",
            "Epoch 2113, Loss(train/val) 0.23524/0.16727. Took 0.05 sec\n",
            "Epoch 2114, Loss(train/val) 0.24571/0.16654. Took 0.04 sec\n",
            "Epoch 2115, Loss(train/val) 0.23387/0.16645. Took 0.04 sec\n",
            "Epoch 2116, Loss(train/val) 0.24958/0.16663. Took 0.05 sec\n",
            "Epoch 2117, Loss(train/val) 0.25382/0.16636. Took 0.04 sec\n",
            "Epoch 2118, Loss(train/val) 0.24115/0.16603. Took 0.04 sec\n",
            "Epoch 2119, Loss(train/val) 0.24843/0.16590. Took 0.04 sec\n",
            "Epoch 2120, Loss(train/val) 0.24392/0.16593. Took 0.05 sec\n",
            "Epoch 2121, Loss(train/val) 0.27333/0.16621. Took 0.06 sec\n",
            "Epoch 2122, Loss(train/val) 0.24336/0.16624. Took 0.04 sec\n",
            "Epoch 2123, Loss(train/val) 0.23981/0.16636. Took 0.05 sec\n",
            "Epoch 2124, Loss(train/val) 0.27803/0.16629. Took 0.05 sec\n",
            "Epoch 2125, Loss(train/val) 0.24438/0.16633. Took 0.04 sec\n",
            "Epoch 2126, Loss(train/val) 0.26785/0.16626. Took 0.05 sec\n",
            "Epoch 2127, Loss(train/val) 0.23897/0.16613. Took 0.05 sec\n",
            "Epoch 2128, Loss(train/val) 0.24758/0.16598. Took 0.04 sec\n",
            "Epoch 2129, Loss(train/val) 0.23571/0.16629. Took 0.05 sec\n",
            "Epoch 2130, Loss(train/val) 0.24391/0.16575. Took 0.05 sec\n",
            "Epoch 2131, Loss(train/val) 0.24305/0.16613. Took 0.06 sec\n",
            "Epoch 2132, Loss(train/val) 0.23747/0.16622. Took 0.05 sec\n",
            "Epoch 2133, Loss(train/val) 0.26520/0.16634. Took 0.05 sec\n",
            "Epoch 2134, Loss(train/val) 0.24003/0.16647. Took 0.05 sec\n",
            "Epoch 2135, Loss(train/val) 0.23850/0.16634. Took 0.05 sec\n",
            "Epoch 2136, Loss(train/val) 0.24253/0.16637. Took 0.05 sec\n",
            "Epoch 2137, Loss(train/val) 0.25197/0.16571. Took 0.05 sec\n",
            "Epoch 2138, Loss(train/val) 0.25907/0.16563. Took 0.04 sec\n",
            "Epoch 2139, Loss(train/val) 0.24649/0.16555. Took 0.04 sec\n",
            "Epoch 2140, Loss(train/val) 0.25159/0.16546. Took 0.04 sec\n",
            "Epoch 2141, Loss(train/val) 0.23930/0.16587. Took 0.05 sec\n",
            "Epoch 2142, Loss(train/val) 0.24648/0.16581. Took 0.04 sec\n",
            "Epoch 2143, Loss(train/val) 0.24600/0.16617. Took 0.05 sec\n",
            "Epoch 2144, Loss(train/val) 0.25043/0.16656. Took 0.05 sec\n",
            "Epoch 2145, Loss(train/val) 0.23745/0.16645. Took 0.05 sec\n",
            "Epoch 2146, Loss(train/val) 0.24178/0.16567. Took 0.05 sec\n",
            "Epoch 2147, Loss(train/val) 0.24047/0.16544. Took 0.04 sec\n",
            "Epoch 2148, Loss(train/val) 0.24122/0.16572. Took 0.04 sec\n",
            "Epoch 2149, Loss(train/val) 0.24709/0.16520. Took 0.04 sec\n",
            "Epoch 2150, Loss(train/val) 0.27907/0.16525. Took 0.05 sec\n",
            "Epoch 2151, Loss(train/val) 0.24706/0.16524. Took 0.05 sec\n",
            "Epoch 2152, Loss(train/val) 0.25199/0.16524. Took 0.05 sec\n",
            "Epoch 2153, Loss(train/val) 0.24583/0.16520. Took 0.04 sec\n",
            "Epoch 2154, Loss(train/val) 0.25179/0.16514. Took 0.05 sec\n",
            "Epoch 2155, Loss(train/val) 0.24583/0.16554. Took 0.05 sec\n",
            "Epoch 2156, Loss(train/val) 0.25397/0.16554. Took 0.05 sec\n",
            "Epoch 2157, Loss(train/val) 0.25145/0.16583. Took 0.05 sec\n",
            "Epoch 2158, Loss(train/val) 0.24260/0.16556. Took 0.04 sec\n",
            "Epoch 2159, Loss(train/val) 0.23355/0.16546. Took 0.04 sec\n",
            "Epoch 2160, Loss(train/val) 0.24980/0.16569. Took 0.05 sec\n",
            "Epoch 2161, Loss(train/val) 0.25914/0.16618. Took 0.05 sec\n",
            "Epoch 2162, Loss(train/val) 0.24187/0.16584. Took 0.05 sec\n",
            "Epoch 2163, Loss(train/val) 0.24800/0.16610. Took 0.04 sec\n",
            "Epoch 2164, Loss(train/val) 0.24389/0.16572. Took 0.05 sec\n",
            "Epoch 2165, Loss(train/val) 0.24759/0.16552. Took 0.04 sec\n",
            "Epoch 2166, Loss(train/val) 0.24535/0.16512. Took 0.06 sec\n",
            "Epoch 2167, Loss(train/val) 0.26345/0.16522. Took 0.05 sec\n",
            "Epoch 2168, Loss(train/val) 0.23835/0.16558. Took 0.05 sec\n",
            "Epoch 2169, Loss(train/val) 0.25211/0.16551. Took 0.05 sec\n",
            "Epoch 2170, Loss(train/val) 0.24105/0.16585. Took 0.05 sec\n",
            "Epoch 2171, Loss(train/val) 0.24584/0.16595. Took 0.05 sec\n",
            "Epoch 2172, Loss(train/val) 0.24613/0.16529. Took 0.05 sec\n",
            "Epoch 2173, Loss(train/val) 0.24231/0.16543. Took 0.05 sec\n",
            "Epoch 2174, Loss(train/val) 0.24474/0.16597. Took 0.04 sec\n",
            "Epoch 2175, Loss(train/val) 0.25050/0.16701. Took 0.05 sec\n",
            "Epoch 2176, Loss(train/val) 0.24636/0.16650. Took 0.05 sec\n",
            "Epoch 2177, Loss(train/val) 0.25415/0.16644. Took 0.04 sec\n",
            "Epoch 2178, Loss(train/val) 0.23450/0.16581. Took 0.05 sec\n",
            "Epoch 2179, Loss(train/val) 0.24692/0.16541. Took 0.05 sec\n",
            "Epoch 2180, Loss(train/val) 0.24207/0.16499. Took 0.05 sec\n",
            "Epoch 2181, Loss(train/val) 0.24621/0.16572. Took 0.05 sec\n",
            "Epoch 2182, Loss(train/val) 0.25309/0.16688. Took 0.04 sec\n",
            "Epoch 2183, Loss(train/val) 0.26092/0.16557. Took 0.05 sec\n",
            "Epoch 2184, Loss(train/val) 0.25346/0.16526. Took 0.04 sec\n",
            "Epoch 2185, Loss(train/val) 0.24516/0.16522. Took 0.04 sec\n",
            "Epoch 2186, Loss(train/val) 0.24520/0.16492. Took 0.05 sec\n",
            "Epoch 2187, Loss(train/val) 0.24320/0.16521. Took 0.05 sec\n",
            "Epoch 2188, Loss(train/val) 0.25470/0.16547. Took 0.05 sec\n",
            "Epoch 2189, Loss(train/val) 0.24291/0.16533. Took 0.04 sec\n",
            "Epoch 2190, Loss(train/val) 0.23672/0.16540. Took 0.05 sec\n",
            "Epoch 2191, Loss(train/val) 0.25834/0.16532. Took 0.05 sec\n",
            "Epoch 2192, Loss(train/val) 0.25583/0.16503. Took 0.05 sec\n",
            "Epoch 2193, Loss(train/val) 0.24498/0.16514. Took 0.04 sec\n",
            "Epoch 2194, Loss(train/val) 0.24408/0.16549. Took 0.05 sec\n",
            "Epoch 2195, Loss(train/val) 0.24513/0.16631. Took 0.05 sec\n",
            "Epoch 2196, Loss(train/val) 0.25028/0.16675. Took 0.05 sec\n",
            "Epoch 2197, Loss(train/val) 0.25683/0.16529. Took 0.05 sec\n",
            "Epoch 2198, Loss(train/val) 0.26146/0.16484. Took 0.04 sec\n",
            "Epoch 2199, Loss(train/val) 0.24387/0.16538. Took 0.04 sec\n",
            "Epoch 2200, Loss(train/val) 0.24886/0.16514. Took 0.05 sec\n",
            "Epoch 2201, Loss(train/val) 0.24898/0.16474. Took 0.05 sec\n",
            "Epoch 2202, Loss(train/val) 0.25351/0.16506. Took 0.04 sec\n",
            "Epoch 2203, Loss(train/val) 0.24531/0.16464. Took 0.05 sec\n",
            "Epoch 2204, Loss(train/val) 0.24819/0.16465. Took 0.05 sec\n",
            "Epoch 2205, Loss(train/val) 0.24426/0.16472. Took 0.04 sec\n",
            "Epoch 2206, Loss(train/val) 0.25124/0.16506. Took 0.05 sec\n",
            "Epoch 2207, Loss(train/val) 0.24117/0.16488. Took 0.05 sec\n",
            "Epoch 2208, Loss(train/val) 0.25296/0.16473. Took 0.04 sec\n",
            "Epoch 2209, Loss(train/val) 0.23456/0.16465. Took 0.05 sec\n",
            "Epoch 2210, Loss(train/val) 0.24687/0.16472. Took 0.04 sec\n",
            "Epoch 2211, Loss(train/val) 0.24026/0.16513. Took 0.05 sec\n",
            "Epoch 2212, Loss(train/val) 0.24366/0.16508. Took 0.04 sec\n",
            "Epoch 2213, Loss(train/val) 0.24130/0.16629. Took 0.04 sec\n",
            "Epoch 2214, Loss(train/val) 0.25661/0.16616. Took 0.05 sec\n",
            "Epoch 2215, Loss(train/val) 0.24662/0.16560. Took 0.05 sec\n",
            "Epoch 2216, Loss(train/val) 0.23494/0.16562. Took 0.05 sec\n",
            "Epoch 2217, Loss(train/val) 0.25420/0.16516. Took 0.05 sec\n",
            "Epoch 2218, Loss(train/val) 0.23829/0.16484. Took 0.05 sec\n",
            "Epoch 2219, Loss(train/val) 0.24194/0.16472. Took 0.04 sec\n",
            "Epoch 2220, Loss(train/val) 0.23832/0.16465. Took 0.05 sec\n",
            "Epoch 2221, Loss(train/val) 0.24243/0.16492. Took 0.05 sec\n",
            "Epoch 2222, Loss(train/val) 0.24272/0.16458. Took 0.05 sec\n",
            "Epoch 2223, Loss(train/val) 0.23772/0.16499. Took 0.05 sec\n",
            "Epoch 2224, Loss(train/val) 0.23478/0.16505. Took 0.04 sec\n",
            "Epoch 2225, Loss(train/val) 0.25251/0.16482. Took 0.04 sec\n",
            "Epoch 2226, Loss(train/val) 0.23765/0.16493. Took 0.06 sec\n",
            "Epoch 2227, Loss(train/val) 0.23886/0.16500. Took 0.05 sec\n",
            "Epoch 2228, Loss(train/val) 0.23615/0.16520. Took 0.04 sec\n",
            "Epoch 2229, Loss(train/val) 0.24361/0.16528. Took 0.04 sec\n",
            "Epoch 2230, Loss(train/val) 0.23357/0.16519. Took 0.04 sec\n",
            "Epoch 2231, Loss(train/val) 0.24129/0.16473. Took 0.06 sec\n",
            "Epoch 2232, Loss(train/val) 0.25584/0.16518. Took 0.05 sec\n",
            "Epoch 2233, Loss(train/val) 0.25010/0.16506. Took 0.04 sec\n",
            "Epoch 2234, Loss(train/val) 0.24559/0.16451. Took 0.05 sec\n",
            "Epoch 2235, Loss(train/val) 0.25339/0.16456. Took 0.05 sec\n",
            "Epoch 2236, Loss(train/val) 0.24094/0.16472. Took 0.06 sec\n",
            "Epoch 2237, Loss(train/val) 0.23554/0.16505. Took 0.04 sec\n",
            "Epoch 2238, Loss(train/val) 0.24393/0.16437. Took 0.04 sec\n",
            "Epoch 2239, Loss(train/val) 0.24371/0.16444. Took 0.05 sec\n",
            "Epoch 2240, Loss(train/val) 0.23838/0.16435. Took 0.04 sec\n",
            "Epoch 2241, Loss(train/val) 0.24709/0.16534. Took 0.05 sec\n",
            "Epoch 2242, Loss(train/val) 0.24100/0.16488. Took 0.05 sec\n",
            "Epoch 2243, Loss(train/val) 0.24771/0.16443. Took 0.04 sec\n",
            "Epoch 2244, Loss(train/val) 0.24647/0.16439. Took 0.04 sec\n",
            "Epoch 2245, Loss(train/val) 0.24380/0.16447. Took 0.04 sec\n",
            "Epoch 2246, Loss(train/val) 0.23787/0.16474. Took 0.05 sec\n",
            "Epoch 2247, Loss(train/val) 0.25044/0.16437. Took 0.05 sec\n",
            "Epoch 2248, Loss(train/val) 0.25650/0.16414. Took 0.05 sec\n",
            "Epoch 2249, Loss(train/val) 0.25150/0.16444. Took 0.05 sec\n",
            "Epoch 2250, Loss(train/val) 0.24743/0.16427. Took 0.05 sec\n",
            "Epoch 2251, Loss(train/val) 0.24124/0.16412. Took 0.06 sec\n",
            "Epoch 2252, Loss(train/val) 0.23403/0.16430. Took 0.05 sec\n",
            "Epoch 2253, Loss(train/val) 0.25585/0.16393. Took 0.05 sec\n",
            "Epoch 2254, Loss(train/val) 0.24493/0.16397. Took 0.05 sec\n",
            "Epoch 2255, Loss(train/val) 0.23728/0.16409. Took 0.05 sec\n",
            "Epoch 2256, Loss(train/val) 0.24599/0.16398. Took 0.06 sec\n",
            "Epoch 2257, Loss(train/val) 0.24872/0.16404. Took 0.05 sec\n",
            "Epoch 2258, Loss(train/val) 0.25367/0.16481. Took 0.05 sec\n",
            "Epoch 2259, Loss(train/val) 0.23993/0.16451. Took 0.05 sec\n",
            "Epoch 2260, Loss(train/val) 0.24949/0.16428. Took 0.05 sec\n",
            "Epoch 2261, Loss(train/val) 0.25723/0.16396. Took 0.05 sec\n",
            "Epoch 2262, Loss(train/val) 0.23477/0.16433. Took 0.05 sec\n",
            "Epoch 2263, Loss(train/val) 0.24139/0.16416. Took 0.05 sec\n",
            "Epoch 2264, Loss(train/val) 0.24430/0.16498. Took 0.04 sec\n",
            "Epoch 2265, Loss(train/val) 0.24360/0.16409. Took 0.05 sec\n",
            "Epoch 2266, Loss(train/val) 0.24390/0.16448. Took 0.05 sec\n",
            "Epoch 2267, Loss(train/val) 0.24132/0.16389. Took 0.05 sec\n",
            "Epoch 2268, Loss(train/val) 0.23736/0.16393. Took 0.05 sec\n",
            "Epoch 2269, Loss(train/val) 0.23277/0.16416. Took 0.05 sec\n",
            "Epoch 2270, Loss(train/val) 0.23629/0.16403. Took 0.05 sec\n",
            "Epoch 2271, Loss(train/val) 0.24323/0.16392. Took 0.05 sec\n",
            "Epoch 2272, Loss(train/val) 0.25771/0.16374. Took 0.04 sec\n",
            "Epoch 2273, Loss(train/val) 0.23639/0.16362. Took 0.05 sec\n",
            "Epoch 2274, Loss(train/val) 0.26417/0.16360. Took 0.05 sec\n",
            "Epoch 2275, Loss(train/val) 0.23863/0.16350. Took 0.04 sec\n",
            "Epoch 2276, Loss(train/val) 0.24932/0.16410. Took 0.05 sec\n",
            "Epoch 2277, Loss(train/val) 0.23897/0.16388. Took 0.05 sec\n",
            "Epoch 2278, Loss(train/val) 0.23651/0.16413. Took 0.05 sec\n",
            "Epoch 2279, Loss(train/val) 0.23776/0.16456. Took 0.04 sec\n",
            "Epoch 2280, Loss(train/val) 0.24695/0.16481. Took 0.05 sec\n",
            "Epoch 2281, Loss(train/val) 0.23988/0.16460. Took 0.05 sec\n",
            "Epoch 2282, Loss(train/val) 0.23768/0.16554. Took 0.04 sec\n",
            "Epoch 2283, Loss(train/val) 0.24266/0.16634. Took 0.04 sec\n",
            "Epoch 2284, Loss(train/val) 0.24297/0.16550. Took 0.05 sec\n",
            "Epoch 2285, Loss(train/val) 0.24798/0.16466. Took 0.05 sec\n",
            "Epoch 2286, Loss(train/val) 0.25104/0.16404. Took 0.05 sec\n",
            "Epoch 2287, Loss(train/val) 0.23991/0.16437. Took 0.04 sec\n",
            "Epoch 2288, Loss(train/val) 0.23608/0.16421. Took 0.04 sec\n",
            "Epoch 2289, Loss(train/val) 0.24734/0.16386. Took 0.05 sec\n",
            "Epoch 2290, Loss(train/val) 0.23136/0.16398. Took 0.04 sec\n",
            "Epoch 2291, Loss(train/val) 0.24304/0.16374. Took 0.06 sec\n",
            "Epoch 2292, Loss(train/val) 0.24624/0.16353. Took 0.05 sec\n",
            "Epoch 2293, Loss(train/val) 0.25013/0.16335. Took 0.04 sec\n",
            "Epoch 2294, Loss(train/val) 0.25270/0.16351. Took 0.05 sec\n",
            "Epoch 2295, Loss(train/val) 0.23550/0.16326. Took 0.05 sec\n",
            "Epoch 2296, Loss(train/val) 0.24721/0.16309. Took 0.05 sec\n",
            "Epoch 2297, Loss(train/val) 0.24030/0.16362. Took 0.04 sec\n",
            "Epoch 2298, Loss(train/val) 0.27125/0.16366. Took 0.05 sec\n",
            "Epoch 2299, Loss(train/val) 0.24447/0.16379. Took 0.04 sec\n",
            "Epoch 2300, Loss(train/val) 0.25137/0.16375. Took 0.04 sec\n",
            "Epoch 2301, Loss(train/val) 0.25511/0.16360. Took 0.05 sec\n",
            "Epoch 2302, Loss(train/val) 0.23386/0.16379. Took 0.05 sec\n",
            "Epoch 2303, Loss(train/val) 0.24171/0.16369. Took 0.04 sec\n",
            "Epoch 2304, Loss(train/val) 0.24520/0.16369. Took 0.04 sec\n",
            "Epoch 2305, Loss(train/val) 0.25894/0.16410. Took 0.05 sec\n",
            "Epoch 2306, Loss(train/val) 0.23802/0.16331. Took 0.05 sec\n",
            "Epoch 2307, Loss(train/val) 0.26231/0.16319. Took 0.04 sec\n",
            "Epoch 2308, Loss(train/val) 0.25289/0.16324. Took 0.04 sec\n",
            "Epoch 2309, Loss(train/val) 0.23745/0.16315. Took 0.05 sec\n",
            "Epoch 2310, Loss(train/val) 0.24626/0.16346. Took 0.04 sec\n",
            "Epoch 2311, Loss(train/val) 0.25029/0.16308. Took 0.05 sec\n",
            "Epoch 2312, Loss(train/val) 0.24682/0.16387. Took 0.04 sec\n",
            "Epoch 2313, Loss(train/val) 0.23925/0.16389. Took 0.05 sec\n",
            "Epoch 2314, Loss(train/val) 0.24202/0.16414. Took 0.05 sec\n",
            "Epoch 2315, Loss(train/val) 0.25254/0.16301. Took 0.05 sec\n",
            "Epoch 2316, Loss(train/val) 0.24003/0.16285. Took 0.06 sec\n",
            "Epoch 2317, Loss(train/val) 0.23735/0.16300. Took 0.04 sec\n",
            "Epoch 2318, Loss(train/val) 0.23474/0.16325. Took 0.05 sec\n",
            "Epoch 2319, Loss(train/val) 0.23750/0.16437. Took 0.05 sec\n",
            "Epoch 2320, Loss(train/val) 0.25000/0.16529. Took 0.05 sec\n",
            "Epoch 2321, Loss(train/val) 0.25797/0.16576. Took 0.05 sec\n",
            "Epoch 2322, Loss(train/val) 0.23667/0.16457. Took 0.05 sec\n",
            "Epoch 2323, Loss(train/val) 0.24954/0.16333. Took 0.05 sec\n",
            "Epoch 2324, Loss(train/val) 0.23914/0.16298. Took 0.04 sec\n",
            "Epoch 2325, Loss(train/val) 0.23693/0.16282. Took 0.04 sec\n",
            "Epoch 2326, Loss(train/val) 0.23670/0.16283. Took 0.05 sec\n",
            "Epoch 2327, Loss(train/val) 0.25458/0.16314. Took 0.05 sec\n",
            "Epoch 2328, Loss(train/val) 0.24383/0.16320. Took 0.04 sec\n",
            "Epoch 2329, Loss(train/val) 0.24335/0.16336. Took 0.04 sec\n",
            "Epoch 2330, Loss(train/val) 0.23694/0.16321. Took 0.04 sec\n",
            "Epoch 2331, Loss(train/val) 0.24962/0.16267. Took 0.05 sec\n",
            "Epoch 2332, Loss(train/val) 0.24631/0.16258. Took 0.05 sec\n",
            "Epoch 2333, Loss(train/val) 0.23185/0.16278. Took 0.05 sec\n",
            "Epoch 2334, Loss(train/val) 0.25353/0.16342. Took 0.05 sec\n",
            "Epoch 2335, Loss(train/val) 0.24375/0.16332. Took 0.04 sec\n",
            "Epoch 2336, Loss(train/val) 0.23132/0.16321. Took 0.05 sec\n",
            "Epoch 2337, Loss(train/val) 0.24304/0.16318. Took 0.05 sec\n",
            "Epoch 2338, Loss(train/val) 0.24916/0.16348. Took 0.05 sec\n",
            "Epoch 2339, Loss(train/val) 0.23716/0.16338. Took 0.04 sec\n",
            "Epoch 2340, Loss(train/val) 0.24563/0.16306. Took 0.04 sec\n",
            "Epoch 2341, Loss(train/val) 0.24961/0.16306. Took 0.05 sec\n",
            "Epoch 2342, Loss(train/val) 0.23313/0.16287. Took 0.05 sec\n",
            "Epoch 2343, Loss(train/val) 0.26573/0.16309. Took 0.04 sec\n",
            "Epoch 2344, Loss(train/val) 0.25201/0.16288. Took 0.04 sec\n",
            "Epoch 2345, Loss(train/val) 0.25068/0.16262. Took 0.05 sec\n",
            "Epoch 2346, Loss(train/val) 0.22907/0.16330. Took 0.05 sec\n",
            "Epoch 2347, Loss(train/val) 0.26828/0.16376. Took 0.04 sec\n",
            "Epoch 2348, Loss(train/val) 0.23619/0.16315. Took 0.04 sec\n",
            "Epoch 2349, Loss(train/val) 0.24957/0.16312. Took 0.05 sec\n",
            "Epoch 2350, Loss(train/val) 0.24317/0.16267. Took 0.04 sec\n",
            "Epoch 2351, Loss(train/val) 0.24319/0.16247. Took 0.05 sec\n",
            "Epoch 2352, Loss(train/val) 0.25514/0.16257. Took 0.05 sec\n",
            "Epoch 2353, Loss(train/val) 0.24833/0.16269. Took 0.05 sec\n",
            "Epoch 2354, Loss(train/val) 0.25724/0.16258. Took 0.04 sec\n",
            "Epoch 2355, Loss(train/val) 0.25219/0.16224. Took 0.05 sec\n",
            "Epoch 2356, Loss(train/val) 0.24696/0.16211. Took 0.05 sec\n",
            "Epoch 2357, Loss(train/val) 0.23431/0.16225. Took 0.05 sec\n",
            "Epoch 2358, Loss(train/val) 0.24418/0.16233. Took 0.04 sec\n",
            "Epoch 2359, Loss(train/val) 0.24067/0.16262. Took 0.05 sec\n",
            "Epoch 2360, Loss(train/val) 0.25401/0.16249. Took 0.05 sec\n",
            "Epoch 2361, Loss(train/val) 0.23476/0.16302. Took 0.05 sec\n",
            "Epoch 2362, Loss(train/val) 0.24168/0.16267. Took 0.05 sec\n",
            "Epoch 2363, Loss(train/val) 0.23929/0.16248. Took 0.04 sec\n",
            "Epoch 2364, Loss(train/val) 0.25164/0.16248. Took 0.05 sec\n",
            "Epoch 2365, Loss(train/val) 0.24472/0.16254. Took 0.05 sec\n",
            "Epoch 2366, Loss(train/val) 0.23846/0.16248. Took 0.05 sec\n",
            "Epoch 2367, Loss(train/val) 0.24479/0.16269. Took 0.04 sec\n",
            "Epoch 2368, Loss(train/val) 0.24157/0.16253. Took 0.04 sec\n",
            "Epoch 2369, Loss(train/val) 0.24014/0.16236. Took 0.04 sec\n",
            "Epoch 2370, Loss(train/val) 0.23531/0.16264. Took 0.04 sec\n",
            "Epoch 2371, Loss(train/val) 0.24934/0.16255. Took 0.05 sec\n",
            "Epoch 2372, Loss(train/val) 0.23277/0.16251. Took 0.04 sec\n",
            "Epoch 2373, Loss(train/val) 0.23725/0.16252. Took 0.05 sec\n",
            "Epoch 2374, Loss(train/val) 0.24502/0.16250. Took 0.04 sec\n",
            "Epoch 2375, Loss(train/val) 0.23903/0.16224. Took 0.04 sec\n",
            "Epoch 2376, Loss(train/val) 0.25209/0.16212. Took 0.05 sec\n",
            "Epoch 2377, Loss(train/val) 0.24749/0.16233. Took 0.05 sec\n",
            "Epoch 2378, Loss(train/val) 0.24416/0.16282. Took 0.04 sec\n",
            "Epoch 2379, Loss(train/val) 0.24207/0.16259. Took 0.05 sec\n",
            "Epoch 2380, Loss(train/val) 0.23982/0.16307. Took 0.04 sec\n",
            "Epoch 2381, Loss(train/val) 0.23858/0.16254. Took 0.06 sec\n",
            "Epoch 2382, Loss(train/val) 0.23406/0.16248. Took 0.04 sec\n",
            "Epoch 2383, Loss(train/val) 0.24499/0.16223. Took 0.05 sec\n",
            "Epoch 2384, Loss(train/val) 0.23435/0.16203. Took 0.05 sec\n",
            "Epoch 2385, Loss(train/val) 0.24853/0.16192. Took 0.05 sec\n",
            "Epoch 2386, Loss(train/val) 0.25048/0.16275. Took 0.05 sec\n",
            "Epoch 2387, Loss(train/val) 0.26668/0.16192. Took 0.04 sec\n",
            "Epoch 2388, Loss(train/val) 0.22961/0.16146. Took 0.05 sec\n",
            "Epoch 2389, Loss(train/val) 0.26985/0.16126. Took 0.04 sec\n",
            "Epoch 2390, Loss(train/val) 0.24560/0.16131. Took 0.05 sec\n",
            "Epoch 2391, Loss(train/val) 0.25139/0.16135. Took 0.05 sec\n",
            "Epoch 2392, Loss(train/val) 0.23642/0.16147. Took 0.04 sec\n",
            "Epoch 2393, Loss(train/val) 0.23123/0.16260. Took 0.05 sec\n",
            "Epoch 2394, Loss(train/val) 0.23883/0.16267. Took 0.05 sec\n",
            "Epoch 2395, Loss(train/val) 0.24933/0.16283. Took 0.05 sec\n",
            "Epoch 2396, Loss(train/val) 0.23435/0.16308. Took 0.05 sec\n",
            "Epoch 2397, Loss(train/val) 0.23333/0.16255. Took 0.05 sec\n",
            "Epoch 2398, Loss(train/val) 0.25171/0.16216. Took 0.05 sec\n",
            "Epoch 2399, Loss(train/val) 0.23818/0.16282. Took 0.05 sec\n",
            "Epoch 2400, Loss(train/val) 0.24625/0.16245. Took 0.04 sec\n",
            "Epoch 2401, Loss(train/val) 0.23904/0.16207. Took 0.05 sec\n",
            "Epoch 2402, Loss(train/val) 0.24164/0.16217. Took 0.05 sec\n",
            "Epoch 2403, Loss(train/val) 0.25198/0.16225. Took 0.05 sec\n",
            "Epoch 2404, Loss(train/val) 0.26085/0.16214. Took 0.05 sec\n",
            "Epoch 2405, Loss(train/val) 0.23249/0.16192. Took 0.05 sec\n",
            "Epoch 2406, Loss(train/val) 0.24064/0.16162. Took 0.05 sec\n",
            "Epoch 2407, Loss(train/val) 0.23886/0.16149. Took 0.05 sec\n",
            "Epoch 2408, Loss(train/val) 0.23494/0.16161. Took 0.04 sec\n",
            "Epoch 2409, Loss(train/val) 0.23633/0.16201. Took 0.04 sec\n",
            "Epoch 2410, Loss(train/val) 0.24254/0.16218. Took 0.04 sec\n",
            "Epoch 2411, Loss(train/val) 0.24180/0.16196. Took 0.05 sec\n",
            "Epoch 2412, Loss(train/val) 0.23922/0.16241. Took 0.05 sec\n",
            "Epoch 2413, Loss(train/val) 0.25009/0.16197. Took 0.04 sec\n",
            "Epoch 2414, Loss(train/val) 0.24103/0.16260. Took 0.04 sec\n",
            "Epoch 2415, Loss(train/val) 0.23763/0.16330. Took 0.05 sec\n",
            "Epoch 2416, Loss(train/val) 0.23904/0.16238. Took 0.05 sec\n",
            "Epoch 2417, Loss(train/val) 0.22991/0.16190. Took 0.05 sec\n",
            "Epoch 2418, Loss(train/val) 0.24864/0.16178. Took 0.05 sec\n",
            "Epoch 2419, Loss(train/val) 0.24324/0.16217. Took 0.05 sec\n",
            "Epoch 2420, Loss(train/val) 0.24886/0.16236. Took 0.05 sec\n",
            "Epoch 2421, Loss(train/val) 0.23187/0.16246. Took 0.05 sec\n",
            "Epoch 2422, Loss(train/val) 0.23382/0.16250. Took 0.05 sec\n",
            "Epoch 2423, Loss(train/val) 0.23304/0.16230. Took 0.05 sec\n",
            "Epoch 2424, Loss(train/val) 0.23950/0.16220. Took 0.05 sec\n",
            "Epoch 2425, Loss(train/val) 0.24485/0.16312. Took 0.05 sec\n",
            "Epoch 2426, Loss(train/val) 0.24046/0.16356. Took 0.06 sec\n",
            "Epoch 2427, Loss(train/val) 0.25255/0.16422. Took 0.04 sec\n",
            "Epoch 2428, Loss(train/val) 0.23465/0.16301. Took 0.04 sec\n",
            "Epoch 2429, Loss(train/val) 0.24379/0.16254. Took 0.05 sec\n",
            "Epoch 2430, Loss(train/val) 0.30234/0.16250. Took 0.05 sec\n",
            "Epoch 2431, Loss(train/val) 0.24020/0.16152. Took 0.05 sec\n",
            "Epoch 2432, Loss(train/val) 0.24679/0.16155. Took 0.04 sec\n",
            "Epoch 2433, Loss(train/val) 0.22927/0.16128. Took 0.04 sec\n",
            "Epoch 2434, Loss(train/val) 0.24648/0.16141. Took 0.04 sec\n",
            "Epoch 2435, Loss(train/val) 0.23352/0.16158. Took 0.04 sec\n",
            "Epoch 2436, Loss(train/val) 0.23694/0.16159. Took 0.05 sec\n",
            "Epoch 2437, Loss(train/val) 0.25041/0.16161. Took 0.04 sec\n",
            "Epoch 2438, Loss(train/val) 0.24157/0.16186. Took 0.05 sec\n",
            "Epoch 2439, Loss(train/val) 0.24319/0.16186. Took 0.04 sec\n",
            "Epoch 2440, Loss(train/val) 0.23720/0.16186. Took 0.04 sec\n",
            "Epoch 2441, Loss(train/val) 0.23591/0.16183. Took 0.06 sec\n",
            "Epoch 2442, Loss(train/val) 0.23709/0.16189. Took 0.05 sec\n",
            "Epoch 2443, Loss(train/val) 0.24793/0.16129. Took 0.04 sec\n",
            "Epoch 2444, Loss(train/val) 0.24175/0.16129. Took 0.05 sec\n",
            "Epoch 2445, Loss(train/val) 0.23558/0.16129. Took 0.05 sec\n",
            "Epoch 2446, Loss(train/val) 0.24260/0.16183. Took 0.07 sec\n",
            "Epoch 2447, Loss(train/val) 0.27842/0.16158. Took 0.04 sec\n",
            "Epoch 2448, Loss(train/val) 0.24316/0.16124. Took 0.04 sec\n",
            "Epoch 2449, Loss(train/val) 0.25362/0.16126. Took 0.04 sec\n",
            "Epoch 2450, Loss(train/val) 0.27502/0.16142. Took 0.05 sec\n",
            "Epoch 2451, Loss(train/val) 0.24210/0.16111. Took 0.05 sec\n",
            "Epoch 2452, Loss(train/val) 0.23866/0.16099. Took 0.04 sec\n",
            "Epoch 2453, Loss(train/val) 0.23681/0.16133. Took 0.04 sec\n",
            "Epoch 2454, Loss(train/val) 0.24648/0.16159. Took 0.05 sec\n",
            "Epoch 2455, Loss(train/val) 0.23165/0.16128. Took 0.04 sec\n",
            "Epoch 2456, Loss(train/val) 0.23969/0.16102. Took 0.05 sec\n",
            "Epoch 2457, Loss(train/val) 0.24823/0.16120. Took 0.05 sec\n",
            "Epoch 2458, Loss(train/val) 0.23384/0.16119. Took 0.04 sec\n",
            "Epoch 2459, Loss(train/val) 0.23483/0.16142. Took 0.04 sec\n",
            "Epoch 2460, Loss(train/val) 0.24211/0.16198. Took 0.04 sec\n",
            "Epoch 2461, Loss(train/val) 0.25083/0.16136. Took 0.06 sec\n",
            "Epoch 2462, Loss(train/val) 0.23328/0.16110. Took 0.04 sec\n",
            "Epoch 2463, Loss(train/val) 0.23206/0.16141. Took 0.04 sec\n",
            "Epoch 2464, Loss(train/val) 0.24807/0.16147. Took 0.05 sec\n",
            "Epoch 2465, Loss(train/val) 0.23934/0.16111. Took 0.04 sec\n",
            "Epoch 2466, Loss(train/val) 0.23902/0.16144. Took 0.06 sec\n",
            "Epoch 2467, Loss(train/val) 0.23507/0.16100. Took 0.05 sec\n",
            "Epoch 2468, Loss(train/val) 0.24539/0.16068. Took 0.04 sec\n",
            "Epoch 2469, Loss(train/val) 0.23722/0.16052. Took 0.04 sec\n",
            "Epoch 2470, Loss(train/val) 0.23918/0.16068. Took 0.04 sec\n",
            "Epoch 2471, Loss(train/val) 0.23436/0.16095. Took 0.05 sec\n",
            "Epoch 2472, Loss(train/val) 0.23547/0.16084. Took 0.04 sec\n",
            "Epoch 2473, Loss(train/val) 0.23400/0.16098. Took 0.05 sec\n",
            "Epoch 2474, Loss(train/val) 0.23570/0.16140. Took 0.04 sec\n",
            "Epoch 2475, Loss(train/val) 0.23557/0.16196. Took 0.04 sec\n",
            "Epoch 2476, Loss(train/val) 0.23540/0.16111. Took 0.05 sec\n",
            "Epoch 2477, Loss(train/val) 0.24001/0.16072. Took 0.04 sec\n",
            "Epoch 2478, Loss(train/val) 0.23602/0.16098. Took 0.05 sec\n",
            "Epoch 2479, Loss(train/val) 0.24619/0.16096. Took 0.04 sec\n",
            "Epoch 2480, Loss(train/val) 0.25283/0.16088. Took 0.05 sec\n",
            "Epoch 2481, Loss(train/val) 0.24205/0.16080. Took 0.05 sec\n",
            "Epoch 2482, Loss(train/val) 0.23332/0.16082. Took 0.05 sec\n",
            "Epoch 2483, Loss(train/val) 0.23461/0.16055. Took 0.04 sec\n",
            "Epoch 2484, Loss(train/val) 0.24000/0.16071. Took 0.05 sec\n",
            "Epoch 2485, Loss(train/val) 0.25125/0.16078. Took 0.05 sec\n",
            "Epoch 2486, Loss(train/val) 0.23572/0.16062. Took 0.05 sec\n",
            "Epoch 2487, Loss(train/val) 0.23147/0.16076. Took 0.05 sec\n",
            "Epoch 2488, Loss(train/val) 0.26206/0.16081. Took 0.05 sec\n",
            "Epoch 2489, Loss(train/val) 0.24305/0.16103. Took 0.04 sec\n",
            "Epoch 2490, Loss(train/val) 0.23667/0.16090. Took 0.04 sec\n",
            "Epoch 2491, Loss(train/val) 0.23382/0.16126. Took 0.05 sec\n",
            "Epoch 2492, Loss(train/val) 0.23411/0.16079. Took 0.05 sec\n",
            "Epoch 2493, Loss(train/val) 0.23974/0.16083. Took 0.05 sec\n",
            "Epoch 2494, Loss(train/val) 0.24141/0.16084. Took 0.05 sec\n",
            "Epoch 2495, Loss(train/val) 0.23619/0.16152. Took 0.05 sec\n",
            "Epoch 2496, Loss(train/val) 0.24895/0.16242. Took 0.05 sec\n",
            "Epoch 2497, Loss(train/val) 0.23896/0.16198. Took 0.05 sec\n",
            "Epoch 2498, Loss(train/val) 0.24090/0.16289. Took 0.04 sec\n",
            "Epoch 2499, Loss(train/val) 0.23246/0.16209. Took 0.05 sec\n",
            "Epoch 2500, Loss(train/val) 0.24482/0.16194. Took 0.04 sec\n",
            "Epoch 2501, Loss(train/val) 0.22893/0.16180. Took 0.05 sec\n",
            "Epoch 2502, Loss(train/val) 0.23155/0.16083. Took 0.05 sec\n",
            "Epoch 2503, Loss(train/val) 0.24065/0.16058. Took 0.05 sec\n",
            "Epoch 2504, Loss(train/val) 0.24842/0.16063. Took 0.05 sec\n",
            "Epoch 2505, Loss(train/val) 0.23290/0.16087. Took 0.05 sec\n",
            "Epoch 2506, Loss(train/val) 0.24435/0.16149. Took 0.06 sec\n",
            "Epoch 2507, Loss(train/val) 0.26596/0.16181. Took 0.05 sec\n",
            "Epoch 2508, Loss(train/val) 0.23876/0.16225. Took 0.05 sec\n",
            "Epoch 2509, Loss(train/val) 0.23564/0.16201. Took 0.06 sec\n",
            "Epoch 2510, Loss(train/val) 0.23717/0.16061. Took 0.04 sec\n",
            "Epoch 2511, Loss(train/val) 0.23682/0.16065. Took 0.05 sec\n",
            "Epoch 2512, Loss(train/val) 0.24460/0.16006. Took 0.05 sec\n",
            "Epoch 2513, Loss(train/val) 0.26792/0.16059. Took 0.04 sec\n",
            "Epoch 2514, Loss(train/val) 0.24483/0.16014. Took 0.05 sec\n",
            "Epoch 2515, Loss(train/val) 0.24159/0.16076. Took 0.04 sec\n",
            "Epoch 2516, Loss(train/val) 0.23845/0.16032. Took 0.05 sec\n",
            "Epoch 2517, Loss(train/val) 0.23737/0.16087. Took 0.04 sec\n",
            "Epoch 2518, Loss(train/val) 0.25144/0.16108. Took 0.05 sec\n",
            "Epoch 2519, Loss(train/val) 0.24805/0.16148. Took 0.05 sec\n",
            "Epoch 2520, Loss(train/val) 0.24444/0.16175. Took 0.05 sec\n",
            "Epoch 2521, Loss(train/val) 0.24371/0.16167. Took 0.05 sec\n",
            "Epoch 2522, Loss(train/val) 0.23884/0.16067. Took 0.05 sec\n",
            "Epoch 2523, Loss(train/val) 0.23361/0.16009. Took 0.05 sec\n",
            "Epoch 2524, Loss(train/val) 0.23280/0.16016. Took 0.06 sec\n",
            "Epoch 2525, Loss(train/val) 0.23979/0.16054. Took 0.05 sec\n",
            "Epoch 2526, Loss(train/val) 0.26701/0.16055. Took 0.05 sec\n",
            "Epoch 2527, Loss(train/val) 0.25039/0.16047. Took 0.06 sec\n",
            "Epoch 2528, Loss(train/val) 0.24103/0.16015. Took 0.07 sec\n",
            "Epoch 2529, Loss(train/val) 0.23306/0.16002. Took 0.08 sec\n",
            "Epoch 2530, Loss(train/val) 0.27089/0.16011. Took 0.07 sec\n",
            "Epoch 2531, Loss(train/val) 0.23523/0.16003. Took 0.07 sec\n",
            "Epoch 2532, Loss(train/val) 0.23317/0.15994. Took 0.07 sec\n",
            "Epoch 2533, Loss(train/val) 0.23959/0.15993. Took 0.07 sec\n",
            "Epoch 2534, Loss(train/val) 0.24051/0.15985. Took 0.07 sec\n",
            "Epoch 2535, Loss(train/val) 0.25548/0.15976. Took 0.07 sec\n",
            "Epoch 2536, Loss(train/val) 0.24357/0.15993. Took 0.07 sec\n",
            "Epoch 2537, Loss(train/val) 0.23560/0.16049. Took 0.07 sec\n",
            "Epoch 2538, Loss(train/val) 0.23569/0.16064. Took 0.08 sec\n",
            "Epoch 2539, Loss(train/val) 0.23860/0.16074. Took 0.07 sec\n",
            "Epoch 2540, Loss(train/val) 0.25654/0.16070. Took 0.07 sec\n",
            "Epoch 2541, Loss(train/val) 0.24441/0.16047. Took 0.07 sec\n",
            "Epoch 2542, Loss(train/val) 0.23330/0.16040. Took 0.07 sec\n",
            "Epoch 2543, Loss(train/val) 0.24672/0.16034. Took 0.08 sec\n",
            "Epoch 2544, Loss(train/val) 0.23161/0.16053. Took 0.07 sec\n",
            "Epoch 2545, Loss(train/val) 0.23636/0.16022. Took 0.07 sec\n",
            "Epoch 2546, Loss(train/val) 0.25240/0.16030. Took 0.07 sec\n",
            "Epoch 2547, Loss(train/val) 0.24365/0.15996. Took 0.07 sec\n",
            "Epoch 2548, Loss(train/val) 0.23748/0.16011. Took 0.07 sec\n",
            "Epoch 2549, Loss(train/val) 0.23960/0.16065. Took 0.07 sec\n",
            "Epoch 2550, Loss(train/val) 0.24058/0.16094. Took 0.08 sec\n",
            "Epoch 2551, Loss(train/val) 0.23687/0.16180. Took 0.07 sec\n",
            "Epoch 2552, Loss(train/val) 0.23974/0.16171. Took 0.07 sec\n",
            "Epoch 2553, Loss(train/val) 0.22813/0.16257. Took 0.08 sec\n",
            "Epoch 2554, Loss(train/val) 0.25827/0.16175. Took 0.07 sec\n",
            "Epoch 2555, Loss(train/val) 0.24512/0.16106. Took 0.07 sec\n",
            "Epoch 2556, Loss(train/val) 0.23111/0.16011. Took 0.07 sec\n",
            "Epoch 2557, Loss(train/val) 0.24127/0.16007. Took 0.08 sec\n",
            "Epoch 2558, Loss(train/val) 0.23652/0.16181. Took 0.07 sec\n",
            "Epoch 2559, Loss(train/val) 0.23113/0.16220. Took 0.04 sec\n",
            "Epoch 2560, Loss(train/val) 0.24413/0.16302. Took 0.05 sec\n",
            "Epoch 2561, Loss(train/val) 0.24133/0.16276. Took 0.05 sec\n",
            "Epoch 2562, Loss(train/val) 0.23186/0.16059. Took 0.04 sec\n",
            "Epoch 2563, Loss(train/val) 0.23571/0.16018. Took 0.05 sec\n",
            "Epoch 2564, Loss(train/val) 0.24605/0.15978. Took 0.04 sec\n",
            "Epoch 2565, Loss(train/val) 0.23971/0.15962. Took 0.05 sec\n",
            "Epoch 2566, Loss(train/val) 0.22885/0.15974. Took 0.05 sec\n",
            "Epoch 2567, Loss(train/val) 0.24496/0.15980. Took 0.04 sec\n",
            "Epoch 2568, Loss(train/val) 0.23886/0.15990. Took 0.05 sec\n",
            "Epoch 2569, Loss(train/val) 0.25333/0.15991. Took 0.05 sec\n",
            "Epoch 2570, Loss(train/val) 0.23610/0.15975. Took 0.05 sec\n",
            "Epoch 2571, Loss(train/val) 0.23714/0.15964. Took 0.05 sec\n",
            "Epoch 2572, Loss(train/val) 0.23570/0.15986. Took 0.05 sec\n",
            "Epoch 2573, Loss(train/val) 0.23443/0.15989. Took 0.05 sec\n",
            "Epoch 2574, Loss(train/val) 0.23436/0.16103. Took 0.05 sec\n",
            "Epoch 2575, Loss(train/val) 0.23475/0.16160. Took 0.05 sec\n",
            "Epoch 2576, Loss(train/val) 0.23053/0.16125. Took 0.05 sec\n",
            "Epoch 2577, Loss(train/val) 0.23660/0.16080. Took 0.05 sec\n",
            "Epoch 2578, Loss(train/val) 0.23464/0.16132. Took 0.05 sec\n",
            "Epoch 2579, Loss(train/val) 0.24132/0.16048. Took 0.05 sec\n",
            "Epoch 2580, Loss(train/val) 0.23426/0.15933. Took 0.05 sec\n",
            "Epoch 2581, Loss(train/val) 0.23492/0.15950. Took 0.05 sec\n",
            "Epoch 2582, Loss(train/val) 0.23382/0.16028. Took 0.04 sec\n",
            "Epoch 2583, Loss(train/val) 0.23111/0.15997. Took 0.05 sec\n",
            "Epoch 2584, Loss(train/val) 0.23062/0.15977. Took 0.04 sec\n",
            "Epoch 2585, Loss(train/val) 0.23873/0.15978. Took 0.05 sec\n",
            "Epoch 2586, Loss(train/val) 0.24150/0.15994. Took 0.04 sec\n",
            "Epoch 2587, Loss(train/val) 0.23543/0.16000. Took 0.04 sec\n",
            "Epoch 2588, Loss(train/val) 0.23187/0.15983. Took 0.04 sec\n",
            "Epoch 2589, Loss(train/val) 0.24922/0.15959. Took 0.05 sec\n",
            "Epoch 2590, Loss(train/val) 0.24168/0.15984. Took 0.06 sec\n",
            "Epoch 2591, Loss(train/val) 0.24361/0.16012. Took 0.05 sec\n",
            "Epoch 2592, Loss(train/val) 0.23032/0.16002. Took 0.04 sec\n",
            "Epoch 2593, Loss(train/val) 0.24013/0.15934. Took 0.04 sec\n",
            "Epoch 2594, Loss(train/val) 0.23759/0.15942. Took 0.05 sec\n",
            "Epoch 2595, Loss(train/val) 0.23652/0.16002. Took 0.05 sec\n",
            "Epoch 2596, Loss(train/val) 0.23603/0.15951. Took 0.05 sec\n",
            "Epoch 2597, Loss(train/val) 0.23462/0.15969. Took 0.05 sec\n",
            "Epoch 2598, Loss(train/val) 0.25293/0.15953. Took 0.05 sec\n",
            "Epoch 2599, Loss(train/val) 0.22945/0.15975. Took 0.04 sec\n",
            "Epoch 2600, Loss(train/val) 0.23473/0.15944. Took 0.06 sec\n",
            "Epoch 2601, Loss(train/val) 0.25964/0.15946. Took 0.04 sec\n",
            "Epoch 2602, Loss(train/val) 0.23898/0.15923. Took 0.05 sec\n",
            "Epoch 2603, Loss(train/val) 0.24502/0.15886. Took 0.04 sec\n",
            "Epoch 2604, Loss(train/val) 0.23615/0.15891. Took 0.04 sec\n",
            "Epoch 2605, Loss(train/val) 0.23965/0.15887. Took 0.05 sec\n",
            "Epoch 2606, Loss(train/val) 0.23327/0.15874. Took 0.05 sec\n",
            "Epoch 2607, Loss(train/val) 0.22842/0.15886. Took 0.05 sec\n",
            "Epoch 2608, Loss(train/val) 0.22872/0.15897. Took 0.05 sec\n",
            "Epoch 2609, Loss(train/val) 0.23919/0.15912. Took 0.05 sec\n",
            "Epoch 2610, Loss(train/val) 0.23094/0.15898. Took 0.06 sec\n",
            "Epoch 2611, Loss(train/val) 0.23662/0.15892. Took 0.05 sec\n",
            "Epoch 2612, Loss(train/val) 0.23455/0.15890. Took 0.05 sec\n",
            "Epoch 2613, Loss(train/val) 0.25201/0.15903. Took 0.04 sec\n",
            "Epoch 2614, Loss(train/val) 0.23901/0.15970. Took 0.04 sec\n",
            "Epoch 2615, Loss(train/val) 0.23831/0.15986. Took 0.05 sec\n",
            "Epoch 2616, Loss(train/val) 0.27985/0.16003. Took 0.05 sec\n",
            "Epoch 2617, Loss(train/val) 0.24371/0.15938. Took 0.04 sec\n",
            "Epoch 2618, Loss(train/val) 0.24524/0.15958. Took 0.04 sec\n",
            "Epoch 2619, Loss(train/val) 0.23230/0.16071. Took 0.05 sec\n",
            "Epoch 2620, Loss(train/val) 0.22663/0.16107. Took 0.05 sec\n",
            "Epoch 2621, Loss(train/val) 0.23777/0.15969. Took 0.06 sec\n",
            "Epoch 2622, Loss(train/val) 0.23507/0.15911. Took 0.04 sec\n",
            "Epoch 2623, Loss(train/val) 0.24518/0.15882. Took 0.04 sec\n",
            "Epoch 2624, Loss(train/val) 0.23310/0.15909. Took 0.04 sec\n",
            "Epoch 2625, Loss(train/val) 0.23047/0.15975. Took 0.05 sec\n",
            "Epoch 2626, Loss(train/val) 0.23431/0.16055. Took 0.05 sec\n",
            "Epoch 2627, Loss(train/val) 0.25123/0.16061. Took 0.05 sec\n",
            "Epoch 2628, Loss(train/val) 0.23516/0.15918. Took 0.05 sec\n",
            "Epoch 2629, Loss(train/val) 0.23267/0.15959. Took 0.05 sec\n",
            "Epoch 2630, Loss(train/val) 0.22884/0.15951. Took 0.05 sec\n",
            "Epoch 2631, Loss(train/val) 0.24403/0.15981. Took 0.05 sec\n",
            "Epoch 2632, Loss(train/val) 0.23555/0.15972. Took 0.04 sec\n",
            "Epoch 2633, Loss(train/val) 0.23628/0.15925. Took 0.05 sec\n",
            "Epoch 2634, Loss(train/val) 0.23332/0.15897. Took 0.04 sec\n",
            "Epoch 2635, Loss(train/val) 0.23287/0.15937. Took 0.05 sec\n",
            "Epoch 2636, Loss(train/val) 0.23502/0.15916. Took 0.05 sec\n",
            "Epoch 2637, Loss(train/val) 0.24645/0.15915. Took 0.04 sec\n",
            "Epoch 2638, Loss(train/val) 0.25059/0.15930. Took 0.04 sec\n",
            "Epoch 2639, Loss(train/val) 0.23778/0.15973. Took 0.04 sec\n",
            "Epoch 2640, Loss(train/val) 0.24555/0.15905. Took 0.05 sec\n",
            "Epoch 2641, Loss(train/val) 0.23838/0.15872. Took 0.04 sec\n",
            "Epoch 2642, Loss(train/val) 0.23753/0.15853. Took 0.04 sec\n",
            "Epoch 2643, Loss(train/val) 0.23326/0.15940. Took 0.05 sec\n",
            "Epoch 2644, Loss(train/val) 0.23160/0.15994. Took 0.04 sec\n",
            "Epoch 2645, Loss(train/val) 0.23440/0.16026. Took 0.05 sec\n",
            "Epoch 2646, Loss(train/val) 0.26074/0.15921. Took 0.05 sec\n",
            "Epoch 2647, Loss(train/val) 0.24355/0.15857. Took 0.05 sec\n",
            "Epoch 2648, Loss(train/val) 0.24364/0.15858. Took 0.05 sec\n",
            "Epoch 2649, Loss(train/val) 0.23085/0.15898. Took 0.05 sec\n",
            "Epoch 2650, Loss(train/val) 0.23262/0.15958. Took 0.07 sec\n",
            "Epoch 2651, Loss(train/val) 0.23985/0.15998. Took 0.05 sec\n",
            "Epoch 2652, Loss(train/val) 0.23572/0.15935. Took 0.05 sec\n",
            "Epoch 2653, Loss(train/val) 0.22905/0.15913. Took 0.05 sec\n",
            "Epoch 2654, Loss(train/val) 0.25072/0.15931. Took 0.05 sec\n",
            "Epoch 2655, Loss(train/val) 0.24828/0.15890. Took 0.05 sec\n",
            "Epoch 2656, Loss(train/val) 0.23214/0.15890. Took 0.05 sec\n",
            "Epoch 2657, Loss(train/val) 0.23899/0.15881. Took 0.05 sec\n",
            "Epoch 2658, Loss(train/val) 0.23650/0.15853. Took 0.05 sec\n",
            "Epoch 2659, Loss(train/val) 0.24879/0.15842. Took 0.05 sec\n",
            "Epoch 2660, Loss(train/val) 0.25083/0.15833. Took 0.06 sec\n",
            "Epoch 2661, Loss(train/val) 0.23892/0.15812. Took 0.05 sec\n",
            "Epoch 2662, Loss(train/val) 0.23149/0.15810. Took 0.04 sec\n",
            "Epoch 2663, Loss(train/val) 0.24446/0.15822. Took 0.05 sec\n",
            "Epoch 2664, Loss(train/val) 0.23714/0.15810. Took 0.05 sec\n",
            "Epoch 2665, Loss(train/val) 0.24133/0.15821. Took 0.05 sec\n",
            "Epoch 2666, Loss(train/val) 0.24560/0.15846. Took 0.05 sec\n",
            "Epoch 2667, Loss(train/val) 0.24458/0.15919. Took 0.05 sec\n",
            "Epoch 2668, Loss(train/val) 0.23647/0.16058. Took 0.04 sec\n",
            "Epoch 2669, Loss(train/val) 0.23770/0.15913. Took 0.04 sec\n",
            "Epoch 2670, Loss(train/val) 0.23408/0.15898. Took 0.05 sec\n",
            "Epoch 2671, Loss(train/val) 0.23296/0.15873. Took 0.05 sec\n",
            "Epoch 2672, Loss(train/val) 0.24232/0.15882. Took 0.05 sec\n",
            "Epoch 2673, Loss(train/val) 0.23826/0.15862. Took 0.05 sec\n",
            "Epoch 2674, Loss(train/val) 0.23965/0.15871. Took 0.05 sec\n",
            "Epoch 2675, Loss(train/val) 0.23006/0.15871. Took 0.05 sec\n",
            "Epoch 2676, Loss(train/val) 0.24844/0.15831. Took 0.04 sec\n",
            "Epoch 2677, Loss(train/val) 0.24621/0.15845. Took 0.04 sec\n",
            "Epoch 2678, Loss(train/val) 0.25437/0.15851. Took 0.04 sec\n",
            "Epoch 2679, Loss(train/val) 0.23777/0.15826. Took 0.04 sec\n",
            "Epoch 2680, Loss(train/val) 0.23303/0.15820. Took 0.05 sec\n",
            "Epoch 2681, Loss(train/val) 0.23383/0.15834. Took 0.05 sec\n",
            "Epoch 2682, Loss(train/val) 0.23026/0.15871. Took 0.05 sec\n",
            "Epoch 2683, Loss(train/val) 0.24116/0.15872. Took 0.04 sec\n",
            "Epoch 2684, Loss(train/val) 0.23739/0.15886. Took 0.04 sec\n",
            "Epoch 2685, Loss(train/val) 0.22946/0.15899. Took 0.06 sec\n",
            "Epoch 2686, Loss(train/val) 0.23762/0.15924. Took 0.04 sec\n",
            "Epoch 2687, Loss(train/val) 0.23834/0.15872. Took 0.04 sec\n",
            "Epoch 2688, Loss(train/val) 0.23568/0.15851. Took 0.04 sec\n",
            "Epoch 2689, Loss(train/val) 0.23520/0.15924. Took 0.04 sec\n",
            "Epoch 2690, Loss(train/val) 0.22792/0.15982. Took 0.05 sec\n",
            "Epoch 2691, Loss(train/val) 0.24359/0.15840. Took 0.04 sec\n",
            "Epoch 2692, Loss(train/val) 0.23210/0.15805. Took 0.05 sec\n",
            "Epoch 2693, Loss(train/val) 0.23310/0.15798. Took 0.05 sec\n",
            "Epoch 2694, Loss(train/val) 0.23845/0.15807. Took 0.04 sec\n",
            "Epoch 2695, Loss(train/val) 0.24047/0.15828. Took 0.05 sec\n",
            "Epoch 2696, Loss(train/val) 0.23357/0.15844. Took 0.05 sec\n",
            "Epoch 2697, Loss(train/val) 0.23649/0.15864. Took 0.04 sec\n",
            "Epoch 2698, Loss(train/val) 0.23454/0.15873. Took 0.05 sec\n",
            "Epoch 2699, Loss(train/val) 0.23492/0.15862. Took 0.05 sec\n",
            "Epoch 2700, Loss(train/val) 0.24275/0.15926. Took 0.05 sec\n",
            "Epoch 2701, Loss(train/val) 0.23509/0.15869. Took 0.04 sec\n",
            "Epoch 2702, Loss(train/val) 0.23360/0.15833. Took 0.04 sec\n",
            "Epoch 2703, Loss(train/val) 0.24296/0.15782. Took 0.05 sec\n",
            "Epoch 2704, Loss(train/val) 0.23718/0.15787. Took 0.04 sec\n",
            "Epoch 2705, Loss(train/val) 0.25589/0.15784. Took 0.05 sec\n",
            "Epoch 2706, Loss(train/val) 0.23905/0.15777. Took 0.05 sec\n",
            "Epoch 2707, Loss(train/val) 0.24361/0.15800. Took 0.05 sec\n",
            "Epoch 2708, Loss(train/val) 0.23109/0.15786. Took 0.04 sec\n",
            "Epoch 2709, Loss(train/val) 0.24545/0.15796. Took 0.05 sec\n",
            "Epoch 2710, Loss(train/val) 0.22971/0.15829. Took 0.05 sec\n",
            "Epoch 2711, Loss(train/val) 0.24504/0.15823. Took 0.04 sec\n",
            "Epoch 2712, Loss(train/val) 0.23358/0.15828. Took 0.04 sec\n",
            "Epoch 2713, Loss(train/val) 0.23585/0.15849. Took 0.05 sec\n",
            "Epoch 2714, Loss(train/val) 0.23418/0.15834. Took 0.04 sec\n",
            "Epoch 2715, Loss(train/val) 0.22841/0.15834. Took 0.06 sec\n",
            "Epoch 2716, Loss(train/val) 0.22990/0.15880. Took 0.05 sec\n",
            "Epoch 2717, Loss(train/val) 0.23594/0.15855. Took 0.05 sec\n",
            "Epoch 2718, Loss(train/val) 0.24106/0.15897. Took 0.04 sec\n",
            "Epoch 2719, Loss(train/val) 0.24399/0.15909. Took 0.04 sec\n",
            "Epoch 2720, Loss(train/val) 0.23531/0.15878. Took 0.05 sec\n",
            "Epoch 2721, Loss(train/val) 0.25731/0.15853. Took 0.04 sec\n",
            "Epoch 2722, Loss(train/val) 0.23880/0.15858. Took 0.04 sec\n",
            "Epoch 2723, Loss(train/val) 0.23402/0.15808. Took 0.04 sec\n",
            "Epoch 2724, Loss(train/val) 0.24990/0.15794. Took 0.05 sec\n",
            "Epoch 2725, Loss(train/val) 0.23367/0.15813. Took 0.05 sec\n",
            "Epoch 2726, Loss(train/val) 0.24059/0.15806. Took 0.05 sec\n",
            "Epoch 2727, Loss(train/val) 0.22657/0.15856. Took 0.05 sec\n",
            "Epoch 2728, Loss(train/val) 0.23142/0.15820. Took 0.05 sec\n",
            "Epoch 2729, Loss(train/val) 0.23828/0.15841. Took 0.05 sec\n",
            "Epoch 2730, Loss(train/val) 0.23412/0.15850. Took 0.05 sec\n",
            "Epoch 2731, Loss(train/val) 0.23734/0.15839. Took 0.04 sec\n",
            "Epoch 2732, Loss(train/val) 0.23047/0.15823. Took 0.05 sec\n",
            "Epoch 2733, Loss(train/val) 0.23350/0.15782. Took 0.04 sec\n",
            "Epoch 2734, Loss(train/val) 0.23506/0.15773. Took 0.05 sec\n",
            "Epoch 2735, Loss(train/val) 0.23779/0.15760. Took 0.05 sec\n",
            "Epoch 2736, Loss(train/val) 0.23600/0.15755. Took 0.05 sec\n",
            "Epoch 2737, Loss(train/val) 0.23119/0.15741. Took 0.05 sec\n",
            "Epoch 2738, Loss(train/val) 0.22879/0.15724. Took 0.05 sec\n",
            "Epoch 2739, Loss(train/val) 0.23374/0.15746. Took 0.05 sec\n",
            "Epoch 2740, Loss(train/val) 0.24352/0.15742. Took 0.05 sec\n",
            "Epoch 2741, Loss(train/val) 0.23781/0.15713. Took 0.04 sec\n",
            "Epoch 2742, Loss(train/val) 0.24638/0.15696. Took 0.04 sec\n",
            "Epoch 2743, Loss(train/val) 0.22966/0.15700. Took 0.04 sec\n",
            "Epoch 2744, Loss(train/val) 0.23555/0.15744. Took 0.05 sec\n",
            "Epoch 2745, Loss(train/val) 0.24127/0.15787. Took 0.05 sec\n",
            "Epoch 2746, Loss(train/val) 0.22892/0.15871. Took 0.04 sec\n",
            "Epoch 2747, Loss(train/val) 0.22625/0.16024. Took 0.04 sec\n",
            "Epoch 2748, Loss(train/val) 0.23693/0.16067. Took 0.04 sec\n",
            "Epoch 2749, Loss(train/val) 0.22859/0.15850. Took 0.04 sec\n",
            "Epoch 2750, Loss(train/val) 0.23647/0.15769. Took 0.07 sec\n",
            "Epoch 2751, Loss(train/val) 0.24395/0.15774. Took 0.05 sec\n",
            "Epoch 2752, Loss(train/val) 0.23072/0.15766. Took 0.04 sec\n",
            "Epoch 2753, Loss(train/val) 0.24830/0.15792. Took 0.04 sec\n",
            "Epoch 2754, Loss(train/val) 0.23081/0.15802. Took 0.05 sec\n",
            "Epoch 2755, Loss(train/val) 0.23275/0.15815. Took 0.05 sec\n",
            "Epoch 2756, Loss(train/val) 0.22823/0.15869. Took 0.05 sec\n",
            "Epoch 2757, Loss(train/val) 0.23831/0.15843. Took 0.04 sec\n",
            "Epoch 2758, Loss(train/val) 0.23312/0.15757. Took 0.04 sec\n",
            "Epoch 2759, Loss(train/val) 0.24223/0.15781. Took 0.04 sec\n",
            "Epoch 2760, Loss(train/val) 0.24180/0.15768. Took 0.05 sec\n",
            "Epoch 2761, Loss(train/val) 0.24661/0.15802. Took 0.05 sec\n",
            "Epoch 2762, Loss(train/val) 0.23673/0.15832. Took 0.04 sec\n",
            "Epoch 2763, Loss(train/val) 0.23240/0.15764. Took 0.04 sec\n",
            "Epoch 2764, Loss(train/val) 0.22845/0.15742. Took 0.05 sec\n",
            "Epoch 2765, Loss(train/val) 0.24697/0.15733. Took 0.05 sec\n",
            "Epoch 2766, Loss(train/val) 0.22942/0.15729. Took 0.05 sec\n",
            "Epoch 2767, Loss(train/val) 0.27202/0.15750. Took 0.05 sec\n",
            "Epoch 2768, Loss(train/val) 0.22862/0.15737. Took 0.05 sec\n",
            "Epoch 2769, Loss(train/val) 0.22668/0.15717. Took 0.04 sec\n",
            "Epoch 2770, Loss(train/val) 0.24122/0.15756. Took 0.05 sec\n",
            "Epoch 2771, Loss(train/val) 0.25200/0.15837. Took 0.05 sec\n",
            "Epoch 2772, Loss(train/val) 0.24328/0.15812. Took 0.05 sec\n",
            "Epoch 2773, Loss(train/val) 0.23668/0.15757. Took 0.04 sec\n",
            "Epoch 2774, Loss(train/val) 0.23782/0.15756. Took 0.05 sec\n",
            "Epoch 2775, Loss(train/val) 0.22920/0.15750. Took 0.05 sec\n",
            "Epoch 2776, Loss(train/val) 0.23243/0.15773. Took 0.04 sec\n",
            "Epoch 2777, Loss(train/val) 0.22914/0.15808. Took 0.05 sec\n",
            "Epoch 2778, Loss(train/val) 0.23332/0.15821. Took 0.04 sec\n",
            "Epoch 2779, Loss(train/val) 0.23429/0.15813. Took 0.05 sec\n",
            "Epoch 2780, Loss(train/val) 0.23131/0.15786. Took 0.05 sec\n",
            "Epoch 2781, Loss(train/val) 0.26582/0.15886. Took 0.05 sec\n",
            "Epoch 2782, Loss(train/val) 0.25032/0.15717. Took 0.05 sec\n",
            "Epoch 2783, Loss(train/val) 0.24006/0.15662. Took 0.05 sec\n",
            "Epoch 2784, Loss(train/val) 0.23493/0.15668. Took 0.05 sec\n",
            "Epoch 2785, Loss(train/val) 0.23487/0.15661. Took 0.05 sec\n",
            "Epoch 2786, Loss(train/val) 0.23257/0.15671. Took 0.05 sec\n",
            "Epoch 2787, Loss(train/val) 0.23621/0.15683. Took 0.05 sec\n",
            "Epoch 2788, Loss(train/val) 0.23779/0.15696. Took 0.05 sec\n",
            "Epoch 2789, Loss(train/val) 0.23781/0.15744. Took 0.04 sec\n",
            "Epoch 2790, Loss(train/val) 0.23142/0.15712. Took 0.05 sec\n",
            "Epoch 2791, Loss(train/val) 0.23051/0.15698. Took 0.04 sec\n",
            "Epoch 2792, Loss(train/val) 0.24647/0.15709. Took 0.04 sec\n",
            "Epoch 2793, Loss(train/val) 0.23618/0.15755. Took 0.06 sec\n",
            "Epoch 2794, Loss(train/val) 0.23014/0.15719. Took 0.05 sec\n",
            "Epoch 2795, Loss(train/val) 0.24157/0.15672. Took 0.05 sec\n",
            "Epoch 2796, Loss(train/val) 0.23701/0.15712. Took 0.05 sec\n",
            "Epoch 2797, Loss(train/val) 0.24602/0.15731. Took 0.05 sec\n",
            "Epoch 2798, Loss(train/val) 0.23585/0.15750. Took 0.04 sec\n",
            "Epoch 2799, Loss(train/val) 0.24137/0.15714. Took 0.05 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.0, epoch=2800, exp_name='exp4_l2', hid_dim=16, input_dim=1, l2=0.0001, lr=4e-05, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.07485/0.38801. Took 0.05 sec\n",
            "Epoch 1, Loss(train/val) 1.08042/0.38872. Took 0.05 sec\n",
            "Epoch 2, Loss(train/val) 1.08141/0.38931. Took 0.04 sec\n",
            "Epoch 3, Loss(train/val) 1.08600/0.38981. Took 0.04 sec\n",
            "Epoch 4, Loss(train/val) 1.08558/0.39022. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.08490/0.39056. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.08628/0.39084. Took 0.04 sec\n",
            "Epoch 7, Loss(train/val) 1.07538/0.39107. Took 0.04 sec\n",
            "Epoch 8, Loss(train/val) 1.08285/0.39125. Took 0.05 sec\n",
            "Epoch 9, Loss(train/val) 1.08011/0.39140. Took 0.04 sec\n",
            "Epoch 10, Loss(train/val) 1.06597/0.39151. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.07021/0.39160. Took 0.05 sec\n",
            "Epoch 12, Loss(train/val) 1.08226/0.39167. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 1.08131/0.39173. Took 0.04 sec\n",
            "Epoch 14, Loss(train/val) 1.08724/0.39177. Took 0.05 sec\n",
            "Epoch 15, Loss(train/val) 1.08695/0.39180. Took 0.05 sec\n",
            "Epoch 16, Loss(train/val) 1.08567/0.39182. Took 0.05 sec\n",
            "Epoch 17, Loss(train/val) 1.07357/0.39184. Took 0.05 sec\n",
            "Epoch 18, Loss(train/val) 1.07691/0.39186. Took 0.05 sec\n",
            "Epoch 19, Loss(train/val) 1.08093/0.39189. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 1.08080/0.39192. Took 0.05 sec\n",
            "Epoch 21, Loss(train/val) 1.08381/0.39196. Took 0.05 sec\n",
            "Epoch 22, Loss(train/val) 1.08311/0.39202. Took 0.04 sec\n",
            "Epoch 23, Loss(train/val) 1.07291/0.39209. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 1.08323/0.39219. Took 0.04 sec\n",
            "Epoch 25, Loss(train/val) 1.07874/0.39230. Took 0.05 sec\n",
            "Epoch 26, Loss(train/val) 1.08313/0.39245. Took 0.05 sec\n",
            "Epoch 27, Loss(train/val) 1.08525/0.39263. Took 0.04 sec\n",
            "Epoch 28, Loss(train/val) 1.07821/0.39285. Took 0.05 sec\n",
            "Epoch 29, Loss(train/val) 1.08382/0.39312. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 1.07568/0.39342. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 1.06696/0.39377. Took 0.04 sec\n",
            "Epoch 32, Loss(train/val) 1.08478/0.39415. Took 0.05 sec\n",
            "Epoch 33, Loss(train/val) 1.08462/0.39455. Took 0.05 sec\n",
            "Epoch 34, Loss(train/val) 1.07923/0.39494. Took 0.04 sec\n",
            "Epoch 35, Loss(train/val) 1.08555/0.39533. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 1.08088/0.39570. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.07988/0.39602. Took 0.04 sec\n",
            "Epoch 38, Loss(train/val) 1.08168/0.39629. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 1.06991/0.39652. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 1.07961/0.39669. Took 0.05 sec\n",
            "Epoch 41, Loss(train/val) 1.07641/0.39681. Took 0.05 sec\n",
            "Epoch 42, Loss(train/val) 1.07604/0.39688. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 1.08127/0.39691. Took 0.04 sec\n",
            "Epoch 44, Loss(train/val) 1.08399/0.39689. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 1.07366/0.39686. Took 0.05 sec\n",
            "Epoch 46, Loss(train/val) 1.08488/0.39683. Took 0.04 sec\n",
            "Epoch 47, Loss(train/val) 1.07118/0.39677. Took 0.04 sec\n",
            "Epoch 48, Loss(train/val) 1.07472/0.39671. Took 0.04 sec\n",
            "Epoch 49, Loss(train/val) 1.06897/0.39664. Took 0.04 sec\n",
            "Epoch 50, Loss(train/val) 1.06786/0.39655. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 1.07837/0.39645. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 1.07862/0.39636. Took 0.05 sec\n",
            "Epoch 53, Loss(train/val) 1.07938/0.39626. Took 0.04 sec\n",
            "Epoch 54, Loss(train/val) 1.07940/0.39617. Took 0.04 sec\n",
            "Epoch 55, Loss(train/val) 1.07324/0.39608. Took 0.05 sec\n",
            "Epoch 56, Loss(train/val) 1.06933/0.39600. Took 0.05 sec\n",
            "Epoch 57, Loss(train/val) 1.06856/0.39590. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 1.08057/0.39580. Took 0.05 sec\n",
            "Epoch 59, Loss(train/val) 1.08805/0.39569. Took 0.04 sec\n",
            "Epoch 60, Loss(train/val) 1.08485/0.39559. Took 0.06 sec\n",
            "Epoch 61, Loss(train/val) 1.07354/0.39547. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 1.07466/0.39534. Took 0.05 sec\n",
            "Epoch 63, Loss(train/val) 1.08038/0.39523. Took 0.05 sec\n",
            "Epoch 64, Loss(train/val) 1.08150/0.39512. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 1.08064/0.39501. Took 0.05 sec\n",
            "Epoch 66, Loss(train/val) 1.06929/0.39490. Took 0.04 sec\n",
            "Epoch 67, Loss(train/val) 1.05418/0.39479. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 1.07187/0.39469. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 1.07528/0.39459. Took 0.04 sec\n",
            "Epoch 70, Loss(train/val) 1.06992/0.39449. Took 0.05 sec\n",
            "Epoch 71, Loss(train/val) 1.07724/0.39439. Took 0.06 sec\n",
            "Epoch 72, Loss(train/val) 1.07196/0.39428. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 1.07798/0.39417. Took 0.05 sec\n",
            "Epoch 74, Loss(train/val) 1.07563/0.39406. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 1.07317/0.39394. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 1.08344/0.39383. Took 0.05 sec\n",
            "Epoch 77, Loss(train/val) 1.08564/0.39371. Took 0.05 sec\n",
            "Epoch 78, Loss(train/val) 1.08286/0.39359. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 1.06566/0.39348. Took 0.06 sec\n",
            "Epoch 80, Loss(train/val) 1.07226/0.39337. Took 0.05 sec\n",
            "Epoch 81, Loss(train/val) 1.08096/0.39326. Took 0.06 sec\n",
            "Epoch 82, Loss(train/val) 1.07059/0.39314. Took 0.04 sec\n",
            "Epoch 83, Loss(train/val) 1.08323/0.39302. Took 0.05 sec\n",
            "Epoch 84, Loss(train/val) 1.06606/0.39291. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 1.08466/0.39279. Took 0.05 sec\n",
            "Epoch 86, Loss(train/val) 1.07606/0.39266. Took 0.04 sec\n",
            "Epoch 87, Loss(train/val) 1.06013/0.39251. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 1.07518/0.39238. Took 0.05 sec\n",
            "Epoch 89, Loss(train/val) 1.07355/0.39225. Took 0.06 sec\n",
            "Epoch 90, Loss(train/val) 1.06668/0.39212. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 1.07274/0.39200. Took 0.05 sec\n",
            "Epoch 92, Loss(train/val) 1.06296/0.39187. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 1.07142/0.39174. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 1.07639/0.39162. Took 0.06 sec\n",
            "Epoch 95, Loss(train/val) 1.07522/0.39150. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 1.07996/0.39138. Took 0.04 sec\n",
            "Epoch 97, Loss(train/val) 1.07582/0.39126. Took 0.04 sec\n",
            "Epoch 98, Loss(train/val) 1.06202/0.39113. Took 0.04 sec\n",
            "Epoch 99, Loss(train/val) 1.07288/0.39102. Took 0.06 sec\n",
            "Epoch 100, Loss(train/val) 1.07826/0.39089. Took 0.05 sec\n",
            "Epoch 101, Loss(train/val) 1.07723/0.39075. Took 0.05 sec\n",
            "Epoch 102, Loss(train/val) 1.07218/0.39062. Took 0.04 sec\n",
            "Epoch 103, Loss(train/val) 1.07309/0.39049. Took 0.04 sec\n",
            "Epoch 104, Loss(train/val) 1.08031/0.39035. Took 0.05 sec\n",
            "Epoch 105, Loss(train/val) 1.07675/0.39022. Took 0.04 sec\n",
            "Epoch 106, Loss(train/val) 1.07504/0.39009. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 1.07457/0.38996. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 1.07844/0.38984. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 1.08042/0.38971. Took 0.05 sec\n",
            "Epoch 110, Loss(train/val) 1.07643/0.38958. Took 0.05 sec\n",
            "Epoch 111, Loss(train/val) 1.07382/0.38945. Took 0.05 sec\n",
            "Epoch 112, Loss(train/val) 1.06990/0.38932. Took 0.05 sec\n",
            "Epoch 113, Loss(train/val) 1.07922/0.38920. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 1.07311/0.38907. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 1.06580/0.38896. Took 0.04 sec\n",
            "Epoch 116, Loss(train/val) 1.07311/0.38883. Took 0.05 sec\n",
            "Epoch 117, Loss(train/val) 1.06513/0.38871. Took 0.05 sec\n",
            "Epoch 118, Loss(train/val) 1.07496/0.38859. Took 0.05 sec\n",
            "Epoch 119, Loss(train/val) 1.06073/0.38847. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 1.07480/0.38836. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 1.06874/0.38824. Took 0.05 sec\n",
            "Epoch 122, Loss(train/val) 1.07535/0.38812. Took 0.05 sec\n",
            "Epoch 123, Loss(train/val) 1.07116/0.38800. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 1.06832/0.38788. Took 0.06 sec\n",
            "Epoch 125, Loss(train/val) 1.06985/0.38776. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 1.07515/0.38765. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 1.07457/0.38753. Took 0.06 sec\n",
            "Epoch 128, Loss(train/val) 1.07148/0.38741. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 1.07394/0.38728. Took 0.05 sec\n",
            "Epoch 130, Loss(train/val) 1.07087/0.38717. Took 0.05 sec\n",
            "Epoch 131, Loss(train/val) 1.07092/0.38705. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 1.07393/0.38694. Took 0.06 sec\n",
            "Epoch 133, Loss(train/val) 1.07748/0.38683. Took 0.05 sec\n",
            "Epoch 134, Loss(train/val) 1.07298/0.38673. Took 0.05 sec\n",
            "Epoch 135, Loss(train/val) 1.07216/0.38662. Took 0.05 sec\n",
            "Epoch 136, Loss(train/val) 1.07157/0.38652. Took 0.05 sec\n",
            "Epoch 137, Loss(train/val) 1.06928/0.38641. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 1.06330/0.38631. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 1.07328/0.38621. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 1.07154/0.38609. Took 0.07 sec\n",
            "Epoch 141, Loss(train/val) 1.07224/0.38598. Took 0.05 sec\n",
            "Epoch 142, Loss(train/val) 1.07527/0.38586. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 1.06209/0.38575. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 1.07594/0.38563. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 1.06439/0.38551. Took 0.05 sec\n",
            "Epoch 146, Loss(train/val) 1.07247/0.38537. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 1.07227/0.38523. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 1.06927/0.38508. Took 0.05 sec\n",
            "Epoch 149, Loss(train/val) 1.07745/0.38492. Took 0.06 sec\n",
            "Epoch 150, Loss(train/val) 1.06652/0.38474. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 1.06889/0.38455. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 1.07452/0.38433. Took 0.05 sec\n",
            "Epoch 153, Loss(train/val) 1.06961/0.38410. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 1.06275/0.38386. Took 0.06 sec\n",
            "Epoch 155, Loss(train/val) 1.07186/0.38362. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 1.07028/0.38335. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 1.07266/0.38308. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 1.06815/0.38277. Took 0.05 sec\n",
            "Epoch 159, Loss(train/val) 1.06678/0.38245. Took 0.06 sec\n",
            "Epoch 160, Loss(train/val) 1.07023/0.38210. Took 0.06 sec\n",
            "Epoch 161, Loss(train/val) 1.06711/0.38173. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 1.07122/0.38134. Took 0.05 sec\n",
            "Epoch 163, Loss(train/val) 1.06829/0.38093. Took 0.05 sec\n",
            "Epoch 164, Loss(train/val) 1.06834/0.38050. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 1.06739/0.38005. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 1.06187/0.37958. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 1.05394/0.37910. Took 0.05 sec\n",
            "Epoch 168, Loss(train/val) 1.06705/0.37860. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 1.06907/0.37810. Took 0.05 sec\n",
            "Epoch 170, Loss(train/val) 1.07218/0.37759. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 1.07215/0.37706. Took 0.05 sec\n",
            "Epoch 172, Loss(train/val) 1.06736/0.37654. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 1.06928/0.37603. Took 0.04 sec\n",
            "Epoch 174, Loss(train/val) 1.07130/0.37551. Took 0.05 sec\n",
            "Epoch 175, Loss(train/val) 1.06572/0.37502. Took 0.05 sec\n",
            "Epoch 176, Loss(train/val) 1.06592/0.37453. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 1.06640/0.37407. Took 0.04 sec\n",
            "Epoch 178, Loss(train/val) 1.06180/0.37363. Took 0.05 sec\n",
            "Epoch 179, Loss(train/val) 1.06260/0.37328. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 1.06690/0.37295. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 1.06454/0.37267. Took 0.05 sec\n",
            "Epoch 182, Loss(train/val) 1.06366/0.37243. Took 0.05 sec\n",
            "Epoch 183, Loss(train/val) 1.06847/0.37222. Took 0.04 sec\n",
            "Epoch 184, Loss(train/val) 1.06583/0.37202. Took 0.05 sec\n",
            "Epoch 185, Loss(train/val) 1.06048/0.37188. Took 0.05 sec\n",
            "Epoch 186, Loss(train/val) 1.06855/0.37177. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 1.06801/0.37169. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 1.06264/0.37163. Took 0.04 sec\n",
            "Epoch 189, Loss(train/val) 1.06117/0.37163. Took 0.05 sec\n",
            "Epoch 190, Loss(train/val) 1.06774/0.37164. Took 0.04 sec\n",
            "Epoch 191, Loss(train/val) 1.07192/0.37165. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 1.06568/0.37167. Took 0.05 sec\n",
            "Epoch 193, Loss(train/val) 1.06582/0.37166. Took 0.05 sec\n",
            "Epoch 194, Loss(train/val) 1.06936/0.37164. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 1.05284/0.37162. Took 0.04 sec\n",
            "Epoch 196, Loss(train/val) 1.06302/0.37155. Took 0.05 sec\n",
            "Epoch 197, Loss(train/val) 1.06002/0.37152. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 1.05989/0.37141. Took 0.05 sec\n",
            "Epoch 199, Loss(train/val) 1.06227/0.37126. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 1.06104/0.37116. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 1.05269/0.37102. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 1.05835/0.37088. Took 0.06 sec\n",
            "Epoch 203, Loss(train/val) 1.06179/0.37074. Took 0.05 sec\n",
            "Epoch 204, Loss(train/val) 1.05405/0.37068. Took 0.06 sec\n",
            "Epoch 205, Loss(train/val) 1.06191/0.37064. Took 0.04 sec\n",
            "Epoch 206, Loss(train/val) 1.06149/0.37068. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 1.06087/0.37074. Took 0.05 sec\n",
            "Epoch 208, Loss(train/val) 1.05004/0.37086. Took 0.05 sec\n",
            "Epoch 209, Loss(train/val) 1.05076/0.37110. Took 0.05 sec\n",
            "Epoch 210, Loss(train/val) 1.06109/0.37143. Took 0.05 sec\n",
            "Epoch 211, Loss(train/val) 1.05892/0.37186. Took 0.05 sec\n",
            "Epoch 212, Loss(train/val) 1.04991/0.37250. Took 0.05 sec\n",
            "Epoch 213, Loss(train/val) 1.05035/0.37328. Took 0.05 sec\n",
            "Epoch 214, Loss(train/val) 1.05142/0.37426. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 1.05302/0.37541. Took 0.04 sec\n",
            "Epoch 216, Loss(train/val) 1.05216/0.37658. Took 0.04 sec\n",
            "Epoch 217, Loss(train/val) 1.04767/0.37766. Took 0.05 sec\n",
            "Epoch 218, Loss(train/val) 1.05050/0.37870. Took 0.05 sec\n",
            "Epoch 219, Loss(train/val) 1.04497/0.37979. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 1.04893/0.38068. Took 0.05 sec\n",
            "Epoch 221, Loss(train/val) 1.04966/0.38138. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 1.04505/0.38202. Took 0.05 sec\n",
            "Epoch 223, Loss(train/val) 1.03971/0.38277. Took 0.05 sec\n",
            "Epoch 224, Loss(train/val) 1.02421/0.38384. Took 0.05 sec\n",
            "Epoch 225, Loss(train/val) 1.03826/0.38543. Took 0.05 sec\n",
            "Epoch 226, Loss(train/val) 1.04040/0.38740. Took 0.05 sec\n",
            "Epoch 227, Loss(train/val) 1.03523/0.39018. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 1.03608/0.39395. Took 0.05 sec\n",
            "Epoch 229, Loss(train/val) 1.03456/0.39845. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 1.03106/0.40395. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 1.02532/0.41072. Took 0.05 sec\n",
            "Epoch 232, Loss(train/val) 1.02897/0.41872. Took 0.05 sec\n",
            "Epoch 233, Loss(train/val) 1.02501/0.42751. Took 0.05 sec\n",
            "Epoch 234, Loss(train/val) 1.02384/0.43720. Took 0.05 sec\n",
            "Epoch 235, Loss(train/val) 1.02454/0.44948. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 1.02138/0.46222. Took 0.05 sec\n",
            "Epoch 237, Loss(train/val) 1.01248/0.47470. Took 0.04 sec\n",
            "Epoch 238, Loss(train/val) 1.01229/0.48629. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 1.01422/0.49552. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 1.00687/0.50253. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 0.99726/0.50845. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 1.00448/0.51464. Took 0.05 sec\n",
            "Epoch 243, Loss(train/val) 0.98219/0.52034. Took 0.05 sec\n",
            "Epoch 244, Loss(train/val) 0.99527/0.52844. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.98811/0.53929. Took 0.05 sec\n",
            "Epoch 246, Loss(train/val) 0.98192/0.55253. Took 0.05 sec\n",
            "Epoch 247, Loss(train/val) 0.97592/0.56570. Took 0.04 sec\n",
            "Epoch 248, Loss(train/val) 0.97179/0.57939. Took 0.05 sec\n",
            "Epoch 249, Loss(train/val) 0.96966/0.59529. Took 0.05 sec\n",
            "Epoch 250, Loss(train/val) 0.95663/0.60953. Took 0.05 sec\n",
            "Epoch 251, Loss(train/val) 0.96456/0.61997. Took 0.04 sec\n",
            "Epoch 252, Loss(train/val) 0.95616/0.62994. Took 0.05 sec\n",
            "Epoch 253, Loss(train/val) 0.95044/0.63681. Took 0.04 sec\n",
            "Epoch 254, Loss(train/val) 0.94650/0.64185. Took 0.06 sec\n",
            "Epoch 255, Loss(train/val) 0.93529/0.64925. Took 0.05 sec\n",
            "Epoch 256, Loss(train/val) 0.93121/0.65036. Took 0.05 sec\n",
            "Epoch 257, Loss(train/val) 0.92436/0.65196. Took 0.04 sec\n",
            "Epoch 258, Loss(train/val) 0.92557/0.64932. Took 0.05 sec\n",
            "Epoch 259, Loss(train/val) 0.91669/0.64523. Took 0.05 sec\n",
            "Epoch 260, Loss(train/val) 0.91237/0.63976. Took 0.05 sec\n",
            "Epoch 261, Loss(train/val) 0.89700/0.63146. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.89336/0.61823. Took 0.04 sec\n",
            "Epoch 263, Loss(train/val) 0.89206/0.60554. Took 0.04 sec\n",
            "Epoch 264, Loss(train/val) 0.88666/0.59044. Took 0.05 sec\n",
            "Epoch 265, Loss(train/val) 0.88621/0.57718. Took 0.05 sec\n",
            "Epoch 266, Loss(train/val) 0.88615/0.56244. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 0.87677/0.54688. Took 0.05 sec\n",
            "Epoch 268, Loss(train/val) 0.87789/0.53217. Took 0.04 sec\n",
            "Epoch 269, Loss(train/val) 0.86731/0.51807. Took 0.05 sec\n",
            "Epoch 270, Loss(train/val) 0.86600/0.50345. Took 0.05 sec\n",
            "Epoch 271, Loss(train/val) 0.86079/0.48859. Took 0.05 sec\n",
            "Epoch 272, Loss(train/val) 0.85949/0.47159. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 0.85349/0.45605. Took 0.05 sec\n",
            "Epoch 274, Loss(train/val) 0.85623/0.44096. Took 0.06 sec\n",
            "Epoch 275, Loss(train/val) 0.84159/0.42531. Took 0.05 sec\n",
            "Epoch 276, Loss(train/val) 0.83775/0.41426. Took 0.05 sec\n",
            "Epoch 277, Loss(train/val) 0.85215/0.40310. Took 0.04 sec\n",
            "Epoch 278, Loss(train/val) 0.85033/0.39239. Took 0.04 sec\n",
            "Epoch 279, Loss(train/val) 0.84407/0.38334. Took 0.05 sec\n",
            "Epoch 280, Loss(train/val) 0.84697/0.37577. Took 0.05 sec\n",
            "Epoch 281, Loss(train/val) 0.83310/0.37005. Took 0.04 sec\n",
            "Epoch 282, Loss(train/val) 0.82478/0.36473. Took 0.04 sec\n",
            "Epoch 283, Loss(train/val) 0.83470/0.35979. Took 0.04 sec\n",
            "Epoch 284, Loss(train/val) 0.82385/0.35734. Took 0.05 sec\n",
            "Epoch 285, Loss(train/val) 0.82417/0.35512. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.82480/0.35247. Took 0.06 sec\n",
            "Epoch 287, Loss(train/val) 0.81703/0.35005. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.82178/0.34773. Took 0.05 sec\n",
            "Epoch 289, Loss(train/val) 0.81751/0.34601. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.81957/0.34512. Took 0.04 sec\n",
            "Epoch 291, Loss(train/val) 0.81098/0.34437. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.81123/0.34385. Took 0.04 sec\n",
            "Epoch 293, Loss(train/val) 0.81701/0.34315. Took 0.05 sec\n",
            "Epoch 294, Loss(train/val) 0.80877/0.34209. Took 0.04 sec\n",
            "Epoch 295, Loss(train/val) 0.81242/0.34150. Took 0.04 sec\n",
            "Epoch 296, Loss(train/val) 0.81094/0.34088. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.80038/0.34046. Took 0.04 sec\n",
            "Epoch 298, Loss(train/val) 0.80612/0.34018. Took 0.05 sec\n",
            "Epoch 299, Loss(train/val) 0.80568/0.33990. Took 0.05 sec\n",
            "Epoch 300, Loss(train/val) 0.80042/0.33966. Took 0.04 sec\n",
            "Epoch 301, Loss(train/val) 0.80223/0.33951. Took 0.04 sec\n",
            "Epoch 302, Loss(train/val) 0.79624/0.33910. Took 0.05 sec\n",
            "Epoch 303, Loss(train/val) 0.78979/0.33888. Took 0.05 sec\n",
            "Epoch 304, Loss(train/val) 0.77902/0.33848. Took 0.05 sec\n",
            "Epoch 305, Loss(train/val) 0.79282/0.33812. Took 0.04 sec\n",
            "Epoch 306, Loss(train/val) 0.78526/0.33770. Took 0.04 sec\n",
            "Epoch 307, Loss(train/val) 0.78883/0.33742. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.78193/0.33714. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.77764/0.33670. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.78689/0.33631. Took 0.04 sec\n",
            "Epoch 311, Loss(train/val) 0.78606/0.33615. Took 0.04 sec\n",
            "Epoch 312, Loss(train/val) 0.77851/0.33591. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 0.78245/0.33549. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.77762/0.33514. Took 0.05 sec\n",
            "Epoch 315, Loss(train/val) 0.77488/0.33469. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.77250/0.33416. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.77480/0.33380. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.75673/0.33370. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.77007/0.33348. Took 0.04 sec\n",
            "Epoch 320, Loss(train/val) 0.77126/0.33344. Took 0.04 sec\n",
            "Epoch 321, Loss(train/val) 0.76945/0.33310. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.76480/0.33279. Took 0.05 sec\n",
            "Epoch 323, Loss(train/val) 0.76558/0.33230. Took 0.05 sec\n",
            "Epoch 324, Loss(train/val) 0.76313/0.33203. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.75247/0.33145. Took 0.05 sec\n",
            "Epoch 326, Loss(train/val) 0.76173/0.33082. Took 0.05 sec\n",
            "Epoch 327, Loss(train/val) 0.75682/0.33045. Took 0.05 sec\n",
            "Epoch 328, Loss(train/val) 0.75739/0.33015. Took 0.05 sec\n",
            "Epoch 329, Loss(train/val) 0.75521/0.32969. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.75391/0.32927. Took 0.06 sec\n",
            "Epoch 331, Loss(train/val) 0.74885/0.32910. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 0.75260/0.32895. Took 0.05 sec\n",
            "Epoch 333, Loss(train/val) 0.74487/0.32838. Took 0.05 sec\n",
            "Epoch 334, Loss(train/val) 0.74910/0.32806. Took 0.04 sec\n",
            "Epoch 335, Loss(train/val) 0.73730/0.32758. Took 0.04 sec\n",
            "Epoch 336, Loss(train/val) 0.74384/0.32714. Took 0.04 sec\n",
            "Epoch 337, Loss(train/val) 0.74435/0.32680. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.74201/0.32652. Took 0.05 sec\n",
            "Epoch 339, Loss(train/val) 0.74981/0.32660. Took 0.04 sec\n",
            "Epoch 340, Loss(train/val) 0.74110/0.32626. Took 0.04 sec\n",
            "Epoch 341, Loss(train/val) 0.73954/0.32619. Took 0.04 sec\n",
            "Epoch 342, Loss(train/val) 0.73189/0.32612. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.73271/0.32600. Took 0.05 sec\n",
            "Epoch 344, Loss(train/val) 0.73224/0.32609. Took 0.05 sec\n",
            "Epoch 345, Loss(train/val) 0.72148/0.32600. Took 0.04 sec\n",
            "Epoch 346, Loss(train/val) 0.72861/0.32583. Took 0.05 sec\n",
            "Epoch 347, Loss(train/val) 0.72863/0.32562. Took 0.06 sec\n",
            "Epoch 348, Loss(train/val) 0.71866/0.32537. Took 0.05 sec\n",
            "Epoch 349, Loss(train/val) 0.71981/0.32547. Took 0.05 sec\n",
            "Epoch 350, Loss(train/val) 0.71791/0.32532. Took 0.04 sec\n",
            "Epoch 351, Loss(train/val) 0.71718/0.32497. Took 0.05 sec\n",
            "Epoch 352, Loss(train/val) 0.71563/0.32497. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 0.71963/0.32490. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 0.71771/0.32488. Took 0.04 sec\n",
            "Epoch 355, Loss(train/val) 0.71558/0.32476. Took 0.05 sec\n",
            "Epoch 356, Loss(train/val) 0.71410/0.32436. Took 0.04 sec\n",
            "Epoch 357, Loss(train/val) 0.70261/0.32406. Took 0.05 sec\n",
            "Epoch 358, Loss(train/val) 0.71319/0.32394. Took 0.04 sec\n",
            "Epoch 359, Loss(train/val) 0.70232/0.32343. Took 0.05 sec\n",
            "Epoch 360, Loss(train/val) 0.69771/0.32347. Took 0.04 sec\n",
            "Epoch 361, Loss(train/val) 0.71242/0.32334. Took 0.04 sec\n",
            "Epoch 362, Loss(train/val) 0.70575/0.32331. Took 0.05 sec\n",
            "Epoch 363, Loss(train/val) 0.70018/0.32304. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 0.70542/0.32265. Took 0.05 sec\n",
            "Epoch 365, Loss(train/val) 0.70278/0.32235. Took 0.04 sec\n",
            "Epoch 366, Loss(train/val) 0.69681/0.32205. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 0.69290/0.32176. Took 0.05 sec\n",
            "Epoch 368, Loss(train/val) 0.69636/0.32152. Took 0.04 sec\n",
            "Epoch 369, Loss(train/val) 0.69547/0.32130. Took 0.05 sec\n",
            "Epoch 370, Loss(train/val) 0.69178/0.32105. Took 0.04 sec\n",
            "Epoch 371, Loss(train/val) 0.69745/0.32081. Took 0.05 sec\n",
            "Epoch 372, Loss(train/val) 0.68141/0.32058. Took 0.06 sec\n",
            "Epoch 373, Loss(train/val) 0.68993/0.32038. Took 0.05 sec\n",
            "Epoch 374, Loss(train/val) 0.67842/0.32033. Took 0.05 sec\n",
            "Epoch 375, Loss(train/val) 0.69147/0.32018. Took 0.05 sec\n",
            "Epoch 376, Loss(train/val) 0.68785/0.32008. Took 0.05 sec\n",
            "Epoch 377, Loss(train/val) 0.67584/0.31991. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.68488/0.31953. Took 0.04 sec\n",
            "Epoch 379, Loss(train/val) 0.68243/0.31915. Took 0.04 sec\n",
            "Epoch 380, Loss(train/val) 0.68214/0.31861. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 0.68045/0.31851. Took 0.05 sec\n",
            "Epoch 382, Loss(train/val) 0.66645/0.31817. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.66780/0.31787. Took 0.05 sec\n",
            "Epoch 384, Loss(train/val) 0.67639/0.31742. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 0.67494/0.31718. Took 0.04 sec\n",
            "Epoch 386, Loss(train/val) 0.67133/0.31675. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.67133/0.31639. Took 0.05 sec\n",
            "Epoch 388, Loss(train/val) 0.66896/0.31607. Took 0.05 sec\n",
            "Epoch 389, Loss(train/val) 0.66842/0.31572. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.66196/0.31559. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.66438/0.31533. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.65592/0.31496. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.65884/0.31460. Took 0.05 sec\n",
            "Epoch 394, Loss(train/val) 0.65885/0.31418. Took 0.06 sec\n",
            "Epoch 395, Loss(train/val) 0.65925/0.31387. Took 0.05 sec\n",
            "Epoch 396, Loss(train/val) 0.65949/0.31338. Took 0.05 sec\n",
            "Epoch 397, Loss(train/val) 0.65036/0.31294. Took 0.06 sec\n",
            "Epoch 398, Loss(train/val) 0.66215/0.31254. Took 0.05 sec\n",
            "Epoch 399, Loss(train/val) 0.65468/0.31217. Took 0.04 sec\n",
            "Epoch 400, Loss(train/val) 0.64397/0.31203. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 0.65491/0.31172. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.64885/0.31128. Took 0.05 sec\n",
            "Epoch 403, Loss(train/val) 0.64332/0.31095. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 0.65094/0.31081. Took 0.04 sec\n",
            "Epoch 405, Loss(train/val) 0.65345/0.31061. Took 0.05 sec\n",
            "Epoch 406, Loss(train/val) 0.64056/0.31043. Took 0.05 sec\n",
            "Epoch 407, Loss(train/val) 0.64847/0.31000. Took 0.06 sec\n",
            "Epoch 408, Loss(train/val) 0.64423/0.30969. Took 0.04 sec\n",
            "Epoch 409, Loss(train/val) 0.64147/0.30924. Took 0.04 sec\n",
            "Epoch 410, Loss(train/val) 0.63983/0.30878. Took 0.05 sec\n",
            "Epoch 411, Loss(train/val) 0.64294/0.30848. Took 0.06 sec\n",
            "Epoch 412, Loss(train/val) 0.63473/0.30839. Took 0.06 sec\n",
            "Epoch 413, Loss(train/val) 0.64110/0.30830. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.63418/0.30831. Took 0.04 sec\n",
            "Epoch 415, Loss(train/val) 0.63980/0.30791. Took 0.06 sec\n",
            "Epoch 416, Loss(train/val) 0.62875/0.30761. Took 0.05 sec\n",
            "Epoch 417, Loss(train/val) 0.64108/0.30724. Took 0.05 sec\n",
            "Epoch 418, Loss(train/val) 0.63427/0.30690. Took 0.05 sec\n",
            "Epoch 419, Loss(train/val) 0.63324/0.30657. Took 0.05 sec\n",
            "Epoch 420, Loss(train/val) 0.63401/0.30629. Took 0.05 sec\n",
            "Epoch 421, Loss(train/val) 0.63406/0.30588. Took 0.05 sec\n",
            "Epoch 422, Loss(train/val) 0.63143/0.30560. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.63179/0.30534. Took 0.05 sec\n",
            "Epoch 424, Loss(train/val) 0.62639/0.30518. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.63007/0.30489. Took 0.05 sec\n",
            "Epoch 426, Loss(train/val) 0.61809/0.30482. Took 0.04 sec\n",
            "Epoch 427, Loss(train/val) 0.62292/0.30473. Took 0.05 sec\n",
            "Epoch 428, Loss(train/val) 0.62236/0.30444. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.61697/0.30419. Took 0.04 sec\n",
            "Epoch 430, Loss(train/val) 0.62492/0.30382. Took 0.04 sec\n",
            "Epoch 431, Loss(train/val) 0.62409/0.30374. Took 0.05 sec\n",
            "Epoch 432, Loss(train/val) 0.61340/0.30368. Took 0.06 sec\n",
            "Epoch 433, Loss(train/val) 0.60863/0.30335. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.61767/0.30296. Took 0.05 sec\n",
            "Epoch 435, Loss(train/val) 0.60542/0.30278. Took 0.04 sec\n",
            "Epoch 436, Loss(train/val) 0.61833/0.30238. Took 0.05 sec\n",
            "Epoch 437, Loss(train/val) 0.60695/0.30247. Took 0.06 sec\n",
            "Epoch 438, Loss(train/val) 0.61424/0.30222. Took 0.05 sec\n",
            "Epoch 439, Loss(train/val) 0.61427/0.30169. Took 0.04 sec\n",
            "Epoch 440, Loss(train/val) 0.60808/0.30152. Took 0.04 sec\n",
            "Epoch 441, Loss(train/val) 0.60754/0.30127. Took 0.04 sec\n",
            "Epoch 442, Loss(train/val) 0.60899/0.30116. Took 0.05 sec\n",
            "Epoch 443, Loss(train/val) 0.60188/0.30098. Took 0.04 sec\n",
            "Epoch 444, Loss(train/val) 0.60903/0.30063. Took 0.04 sec\n",
            "Epoch 445, Loss(train/val) 0.59693/0.30074. Took 0.04 sec\n",
            "Epoch 446, Loss(train/val) 0.60394/0.30070. Took 0.04 sec\n",
            "Epoch 447, Loss(train/val) 0.60944/0.30029. Took 0.05 sec\n",
            "Epoch 448, Loss(train/val) 0.60527/0.30012. Took 0.04 sec\n",
            "Epoch 449, Loss(train/val) 0.60395/0.29986. Took 0.04 sec\n",
            "Epoch 450, Loss(train/val) 0.60783/0.29976. Took 0.04 sec\n",
            "Epoch 451, Loss(train/val) 0.59747/0.29968. Took 0.04 sec\n",
            "Epoch 452, Loss(train/val) 0.59516/0.29964. Took 0.05 sec\n",
            "Epoch 453, Loss(train/val) 0.59553/0.29937. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.60481/0.29920. Took 0.04 sec\n",
            "Epoch 455, Loss(train/val) 0.59935/0.29867. Took 0.04 sec\n",
            "Epoch 456, Loss(train/val) 0.59136/0.29856. Took 0.05 sec\n",
            "Epoch 457, Loss(train/val) 0.59721/0.29840. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.60169/0.29795. Took 0.06 sec\n",
            "Epoch 459, Loss(train/val) 0.58328/0.29779. Took 0.04 sec\n",
            "Epoch 460, Loss(train/val) 0.58464/0.29766. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.59954/0.29748. Took 0.05 sec\n",
            "Epoch 462, Loss(train/val) 0.59324/0.29709. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.58558/0.29697. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.59626/0.29686. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.58374/0.29676. Took 0.04 sec\n",
            "Epoch 466, Loss(train/val) 0.59199/0.29656. Took 0.05 sec\n",
            "Epoch 467, Loss(train/val) 0.58643/0.29634. Took 0.05 sec\n",
            "Epoch 468, Loss(train/val) 0.58979/0.29618. Took 0.04 sec\n",
            "Epoch 469, Loss(train/val) 0.58302/0.29600. Took 0.05 sec\n",
            "Epoch 470, Loss(train/val) 0.57648/0.29595. Took 0.04 sec\n",
            "Epoch 471, Loss(train/val) 0.58850/0.29575. Took 0.04 sec\n",
            "Epoch 472, Loss(train/val) 0.58919/0.29543. Took 0.05 sec\n",
            "Epoch 473, Loss(train/val) 0.58983/0.29528. Took 0.05 sec\n",
            "Epoch 474, Loss(train/val) 0.57242/0.29524. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 0.58694/0.29511. Took 0.05 sec\n",
            "Epoch 476, Loss(train/val) 0.56530/0.29494. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.58184/0.29473. Took 0.05 sec\n",
            "Epoch 478, Loss(train/val) 0.57650/0.29465. Took 0.04 sec\n",
            "Epoch 479, Loss(train/val) 0.57676/0.29456. Took 0.06 sec\n",
            "Epoch 480, Loss(train/val) 0.58122/0.29424. Took 0.04 sec\n",
            "Epoch 481, Loss(train/val) 0.58281/0.29419. Took 0.04 sec\n",
            "Epoch 482, Loss(train/val) 0.58134/0.29399. Took 0.05 sec\n",
            "Epoch 483, Loss(train/val) 0.58078/0.29379. Took 0.05 sec\n",
            "Epoch 484, Loss(train/val) 0.57600/0.29364. Took 0.05 sec\n",
            "Epoch 485, Loss(train/val) 0.57185/0.29349. Took 0.04 sec\n",
            "Epoch 486, Loss(train/val) 0.57715/0.29332. Took 0.04 sec\n",
            "Epoch 487, Loss(train/val) 0.57035/0.29325. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.58015/0.29316. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.57982/0.29330. Took 0.04 sec\n",
            "Epoch 490, Loss(train/val) 0.56350/0.29339. Took 0.04 sec\n",
            "Epoch 491, Loss(train/val) 0.57159/0.29296. Took 0.05 sec\n",
            "Epoch 492, Loss(train/val) 0.57718/0.29264. Took 0.05 sec\n",
            "Epoch 493, Loss(train/val) 0.57634/0.29256. Took 0.04 sec\n",
            "Epoch 494, Loss(train/val) 0.57305/0.29242. Took 0.04 sec\n",
            "Epoch 495, Loss(train/val) 0.57808/0.29215. Took 0.05 sec\n",
            "Epoch 496, Loss(train/val) 0.56394/0.29186. Took 0.05 sec\n",
            "Epoch 497, Loss(train/val) 0.57541/0.29163. Took 0.05 sec\n",
            "Epoch 498, Loss(train/val) 0.57605/0.29135. Took 0.04 sec\n",
            "Epoch 499, Loss(train/val) 0.57383/0.29115. Took 0.05 sec\n",
            "Epoch 500, Loss(train/val) 0.56271/0.29108. Took 0.05 sec\n",
            "Epoch 501, Loss(train/val) 0.57737/0.29096. Took 0.05 sec\n",
            "Epoch 502, Loss(train/val) 0.56315/0.29110. Took 0.05 sec\n",
            "Epoch 503, Loss(train/val) 0.56676/0.29086. Took 0.04 sec\n",
            "Epoch 504, Loss(train/val) 0.57686/0.29053. Took 0.04 sec\n",
            "Epoch 505, Loss(train/val) 0.55615/0.29034. Took 0.05 sec\n",
            "Epoch 506, Loss(train/val) 0.56789/0.29022. Took 0.06 sec\n",
            "Epoch 507, Loss(train/val) 0.56983/0.29012. Took 0.05 sec\n",
            "Epoch 508, Loss(train/val) 0.56574/0.28990. Took 0.04 sec\n",
            "Epoch 509, Loss(train/val) 0.55413/0.28976. Took 0.05 sec\n",
            "Epoch 510, Loss(train/val) 0.56500/0.28978. Took 0.04 sec\n",
            "Epoch 511, Loss(train/val) 0.56736/0.28981. Took 0.04 sec\n",
            "Epoch 512, Loss(train/val) 0.56612/0.28994. Took 0.05 sec\n",
            "Epoch 513, Loss(train/val) 0.55919/0.28991. Took 0.05 sec\n",
            "Epoch 514, Loss(train/val) 0.57066/0.29024. Took 0.04 sec\n",
            "Epoch 515, Loss(train/val) 0.56568/0.29008. Took 0.04 sec\n",
            "Epoch 516, Loss(train/val) 0.55949/0.28984. Took 0.04 sec\n",
            "Epoch 517, Loss(train/val) 0.55777/0.28958. Took 0.05 sec\n",
            "Epoch 518, Loss(train/val) 0.56260/0.28944. Took 0.04 sec\n",
            "Epoch 519, Loss(train/val) 0.55064/0.28934. Took 0.04 sec\n",
            "Epoch 520, Loss(train/val) 0.55523/0.28913. Took 0.04 sec\n",
            "Epoch 521, Loss(train/val) 0.54790/0.28875. Took 0.05 sec\n",
            "Epoch 522, Loss(train/val) 0.55604/0.28831. Took 0.06 sec\n",
            "Epoch 523, Loss(train/val) 0.55390/0.28824. Took 0.05 sec\n",
            "Epoch 524, Loss(train/val) 0.56224/0.28809. Took 0.04 sec\n",
            "Epoch 525, Loss(train/val) 0.55056/0.28787. Took 0.04 sec\n",
            "Epoch 526, Loss(train/val) 0.55824/0.28769. Took 0.05 sec\n",
            "Epoch 527, Loss(train/val) 0.55710/0.28759. Took 0.05 sec\n",
            "Epoch 528, Loss(train/val) 0.55137/0.28758. Took 0.04 sec\n",
            "Epoch 529, Loss(train/val) 0.55756/0.28750. Took 0.05 sec\n",
            "Epoch 530, Loss(train/val) 0.53507/0.28750. Took 0.04 sec\n",
            "Epoch 531, Loss(train/val) 0.54320/0.28731. Took 0.04 sec\n",
            "Epoch 532, Loss(train/val) 0.55315/0.28714. Took 0.05 sec\n",
            "Epoch 533, Loss(train/val) 0.56049/0.28731. Took 0.05 sec\n",
            "Epoch 534, Loss(train/val) 0.55526/0.28736. Took 0.04 sec\n",
            "Epoch 535, Loss(train/val) 0.55916/0.28764. Took 0.04 sec\n",
            "Epoch 536, Loss(train/val) 0.55092/0.28758. Took 0.04 sec\n",
            "Epoch 537, Loss(train/val) 0.55620/0.28770. Took 0.05 sec\n",
            "Epoch 538, Loss(train/val) 0.54991/0.28871. Took 0.04 sec\n",
            "Epoch 539, Loss(train/val) 0.55359/0.28852. Took 0.05 sec\n",
            "Epoch 540, Loss(train/val) 0.55427/0.28791. Took 0.05 sec\n",
            "Epoch 541, Loss(train/val) 0.54472/0.28744. Took 0.05 sec\n",
            "Epoch 542, Loss(train/val) 0.55194/0.28699. Took 0.06 sec\n",
            "Epoch 543, Loss(train/val) 0.54441/0.28664. Took 0.05 sec\n",
            "Epoch 544, Loss(train/val) 0.55403/0.28672. Took 0.05 sec\n",
            "Epoch 545, Loss(train/val) 0.54652/0.28713. Took 0.04 sec\n",
            "Epoch 546, Loss(train/val) 0.55293/0.28679. Took 0.04 sec\n",
            "Epoch 547, Loss(train/val) 0.54456/0.28666. Took 0.05 sec\n",
            "Epoch 548, Loss(train/val) 0.54712/0.28634. Took 0.05 sec\n",
            "Epoch 549, Loss(train/val) 0.55194/0.28645. Took 0.04 sec\n",
            "Epoch 550, Loss(train/val) 0.54273/0.28637. Took 0.04 sec\n",
            "Epoch 551, Loss(train/val) 0.54507/0.28639. Took 0.04 sec\n",
            "Epoch 552, Loss(train/val) 0.54689/0.28623. Took 0.06 sec\n",
            "Epoch 553, Loss(train/val) 0.54723/0.28590. Took 0.04 sec\n",
            "Epoch 554, Loss(train/val) 0.54259/0.28595. Took 0.04 sec\n",
            "Epoch 555, Loss(train/val) 0.54759/0.28556. Took 0.04 sec\n",
            "Epoch 556, Loss(train/val) 0.52805/0.28529. Took 0.04 sec\n",
            "Epoch 557, Loss(train/val) 0.54047/0.28525. Took 0.05 sec\n",
            "Epoch 558, Loss(train/val) 0.53543/0.28496. Took 0.05 sec\n",
            "Epoch 559, Loss(train/val) 0.54877/0.28478. Took 0.05 sec\n",
            "Epoch 560, Loss(train/val) 0.54449/0.28446. Took 0.04 sec\n",
            "Epoch 561, Loss(train/val) 0.54808/0.28441. Took 0.05 sec\n",
            "Epoch 562, Loss(train/val) 0.54232/0.28432. Took 0.05 sec\n",
            "Epoch 563, Loss(train/val) 0.53578/0.28451. Took 0.05 sec\n",
            "Epoch 564, Loss(train/val) 0.54579/0.28440. Took 0.05 sec\n",
            "Epoch 565, Loss(train/val) 0.54668/0.28481. Took 0.05 sec\n",
            "Epoch 566, Loss(train/val) 0.53500/0.28514. Took 0.05 sec\n",
            "Epoch 567, Loss(train/val) 0.52693/0.28545. Took 0.05 sec\n",
            "Epoch 568, Loss(train/val) 0.54783/0.28530. Took 0.05 sec\n",
            "Epoch 569, Loss(train/val) 0.53940/0.28522. Took 0.05 sec\n",
            "Epoch 570, Loss(train/val) 0.54318/0.28511. Took 0.04 sec\n",
            "Epoch 571, Loss(train/val) 0.54034/0.28501. Took 0.04 sec\n",
            "Epoch 572, Loss(train/val) 0.54591/0.28465. Took 0.06 sec\n",
            "Epoch 573, Loss(train/val) 0.54221/0.28454. Took 0.05 sec\n",
            "Epoch 574, Loss(train/val) 0.54007/0.28453. Took 0.05 sec\n",
            "Epoch 575, Loss(train/val) 0.53086/0.28431. Took 0.04 sec\n",
            "Epoch 576, Loss(train/val) 0.52794/0.28366. Took 0.04 sec\n",
            "Epoch 577, Loss(train/val) 0.53910/0.28346. Took 0.05 sec\n",
            "Epoch 578, Loss(train/val) 0.52973/0.28326. Took 0.05 sec\n",
            "Epoch 579, Loss(train/val) 0.53417/0.28312. Took 0.05 sec\n",
            "Epoch 580, Loss(train/val) 0.53419/0.28278. Took 0.04 sec\n",
            "Epoch 581, Loss(train/val) 0.54479/0.28263. Took 0.04 sec\n",
            "Epoch 582, Loss(train/val) 0.53342/0.28274. Took 0.05 sec\n",
            "Epoch 583, Loss(train/val) 0.53871/0.28274. Took 0.05 sec\n",
            "Epoch 584, Loss(train/val) 0.53640/0.28262. Took 0.05 sec\n",
            "Epoch 585, Loss(train/val) 0.53893/0.28261. Took 0.04 sec\n",
            "Epoch 586, Loss(train/val) 0.52602/0.28254. Took 0.05 sec\n",
            "Epoch 587, Loss(train/val) 0.53229/0.28225. Took 0.05 sec\n",
            "Epoch 588, Loss(train/val) 0.53090/0.28176. Took 0.05 sec\n",
            "Epoch 589, Loss(train/val) 0.53788/0.28162. Took 0.05 sec\n",
            "Epoch 590, Loss(train/val) 0.53510/0.28156. Took 0.05 sec\n",
            "Epoch 591, Loss(train/val) 0.53342/0.28156. Took 0.05 sec\n",
            "Epoch 592, Loss(train/val) 0.53077/0.28202. Took 0.06 sec\n",
            "Epoch 593, Loss(train/val) 0.52419/0.28258. Took 0.05 sec\n",
            "Epoch 594, Loss(train/val) 0.52843/0.28280. Took 0.05 sec\n",
            "Epoch 595, Loss(train/val) 0.52224/0.28244. Took 0.05 sec\n",
            "Epoch 596, Loss(train/val) 0.53762/0.28265. Took 0.05 sec\n",
            "Epoch 597, Loss(train/val) 0.53392/0.28205. Took 0.05 sec\n",
            "Epoch 598, Loss(train/val) 0.53139/0.28192. Took 0.05 sec\n",
            "Epoch 599, Loss(train/val) 0.53789/0.28177. Took 0.05 sec\n",
            "Epoch 600, Loss(train/val) 0.53253/0.28149. Took 0.05 sec\n",
            "Epoch 601, Loss(train/val) 0.52800/0.28085. Took 0.05 sec\n",
            "Epoch 602, Loss(train/val) 0.52967/0.28083. Took 0.05 sec\n",
            "Epoch 603, Loss(train/val) 0.53176/0.28093. Took 0.05 sec\n",
            "Epoch 604, Loss(train/val) 0.53142/0.28113. Took 0.05 sec\n",
            "Epoch 605, Loss(train/val) 0.53153/0.28115. Took 0.05 sec\n",
            "Epoch 606, Loss(train/val) 0.53201/0.28065. Took 0.05 sec\n",
            "Epoch 607, Loss(train/val) 0.53150/0.28081. Took 0.06 sec\n",
            "Epoch 608, Loss(train/val) 0.54087/0.28085. Took 0.05 sec\n",
            "Epoch 609, Loss(train/val) 0.52771/0.28152. Took 0.05 sec\n",
            "Epoch 610, Loss(train/val) 0.50841/0.28143. Took 0.05 sec\n",
            "Epoch 611, Loss(train/val) 0.52396/0.28173. Took 0.04 sec\n",
            "Epoch 612, Loss(train/val) 0.53164/0.28125. Took 0.05 sec\n",
            "Epoch 613, Loss(train/val) 0.52373/0.28095. Took 0.05 sec\n",
            "Epoch 614, Loss(train/val) 0.52914/0.28072. Took 0.04 sec\n",
            "Epoch 615, Loss(train/val) 0.52529/0.28065. Took 0.04 sec\n",
            "Epoch 616, Loss(train/val) 0.51951/0.28044. Took 0.04 sec\n",
            "Epoch 617, Loss(train/val) 0.52328/0.28019. Took 0.05 sec\n",
            "Epoch 618, Loss(train/val) 0.53058/0.28005. Took 0.05 sec\n",
            "Epoch 619, Loss(train/val) 0.52718/0.28025. Took 0.04 sec\n",
            "Epoch 620, Loss(train/val) 0.51336/0.27998. Took 0.05 sec\n",
            "Epoch 621, Loss(train/val) 0.52733/0.27931. Took 0.05 sec\n",
            "Epoch 622, Loss(train/val) 0.53056/0.27912. Took 0.06 sec\n",
            "Epoch 623, Loss(train/val) 0.51440/0.27915. Took 0.04 sec\n",
            "Epoch 624, Loss(train/val) 0.51559/0.27900. Took 0.05 sec\n",
            "Epoch 625, Loss(train/val) 0.52716/0.27871. Took 0.05 sec\n",
            "Epoch 626, Loss(train/val) 0.52563/0.27853. Took 0.05 sec\n",
            "Epoch 627, Loss(train/val) 0.52915/0.27859. Took 0.05 sec\n",
            "Epoch 628, Loss(train/val) 0.51910/0.27852. Took 0.05 sec\n",
            "Epoch 629, Loss(train/val) 0.52500/0.27876. Took 0.05 sec\n",
            "Epoch 630, Loss(train/val) 0.52516/0.27886. Took 0.05 sec\n",
            "Epoch 631, Loss(train/val) 0.51384/0.27865. Took 0.05 sec\n",
            "Epoch 632, Loss(train/val) 0.53010/0.27863. Took 0.06 sec\n",
            "Epoch 633, Loss(train/val) 0.52706/0.27848. Took 0.04 sec\n",
            "Epoch 634, Loss(train/val) 0.52283/0.27865. Took 0.04 sec\n",
            "Epoch 635, Loss(train/val) 0.52142/0.27796. Took 0.05 sec\n",
            "Epoch 636, Loss(train/val) 0.52338/0.27766. Took 0.04 sec\n",
            "Epoch 637, Loss(train/val) 0.50207/0.27756. Took 0.05 sec\n",
            "Epoch 638, Loss(train/val) 0.51896/0.27748. Took 0.04 sec\n",
            "Epoch 639, Loss(train/val) 0.52473/0.27727. Took 0.04 sec\n",
            "Epoch 640, Loss(train/val) 0.52694/0.27744. Took 0.04 sec\n",
            "Epoch 641, Loss(train/val) 0.52253/0.27749. Took 0.05 sec\n",
            "Epoch 642, Loss(train/val) 0.51937/0.27809. Took 0.05 sec\n",
            "Epoch 643, Loss(train/val) 0.52868/0.27842. Took 0.05 sec\n",
            "Epoch 644, Loss(train/val) 0.52067/0.27780. Took 0.05 sec\n",
            "Epoch 645, Loss(train/val) 0.52076/0.27743. Took 0.04 sec\n",
            "Epoch 646, Loss(train/val) 0.51600/0.27744. Took 0.05 sec\n",
            "Epoch 647, Loss(train/val) 0.52276/0.27724. Took 0.06 sec\n",
            "Epoch 648, Loss(train/val) 0.52258/0.27759. Took 0.05 sec\n",
            "Epoch 649, Loss(train/val) 0.52073/0.27783. Took 0.04 sec\n",
            "Epoch 650, Loss(train/val) 0.51893/0.27739. Took 0.05 sec\n",
            "Epoch 651, Loss(train/val) 0.52169/0.27780. Took 0.05 sec\n",
            "Epoch 652, Loss(train/val) 0.51963/0.27758. Took 0.05 sec\n",
            "Epoch 653, Loss(train/val) 0.51041/0.27730. Took 0.05 sec\n",
            "Epoch 654, Loss(train/val) 0.51899/0.27735. Took 0.04 sec\n",
            "Epoch 655, Loss(train/val) 0.52201/0.27760. Took 0.04 sec\n",
            "Epoch 656, Loss(train/val) 0.51710/0.27673. Took 0.05 sec\n",
            "Epoch 657, Loss(train/val) 0.51663/0.27652. Took 0.05 sec\n",
            "Epoch 658, Loss(train/val) 0.52044/0.27621. Took 0.05 sec\n",
            "Epoch 659, Loss(train/val) 0.51845/0.27602. Took 0.04 sec\n",
            "Epoch 660, Loss(train/val) 0.51958/0.27580. Took 0.05 sec\n",
            "Epoch 661, Loss(train/val) 0.50808/0.27569. Took 0.05 sec\n",
            "Epoch 662, Loss(train/val) 0.52095/0.27550. Took 0.05 sec\n",
            "Epoch 663, Loss(train/val) 0.51589/0.27571. Took 0.05 sec\n",
            "Epoch 664, Loss(train/val) 0.50297/0.27571. Took 0.05 sec\n",
            "Epoch 665, Loss(train/val) 0.52067/0.27545. Took 0.05 sec\n",
            "Epoch 666, Loss(train/val) 0.52181/0.27558. Took 0.05 sec\n",
            "Epoch 667, Loss(train/val) 0.50799/0.27534. Took 0.05 sec\n",
            "Epoch 668, Loss(train/val) 0.51345/0.27492. Took 0.05 sec\n",
            "Epoch 669, Loss(train/val) 0.51442/0.27512. Took 0.05 sec\n",
            "Epoch 670, Loss(train/val) 0.51639/0.27494. Took 0.05 sec\n",
            "Epoch 671, Loss(train/val) 0.51744/0.27519. Took 0.05 sec\n",
            "Epoch 672, Loss(train/val) 0.51641/0.27455. Took 0.05 sec\n",
            "Epoch 673, Loss(train/val) 0.51412/0.27443. Took 0.05 sec\n",
            "Epoch 674, Loss(train/val) 0.51454/0.27445. Took 0.04 sec\n",
            "Epoch 675, Loss(train/val) 0.50378/0.27429. Took 0.04 sec\n",
            "Epoch 676, Loss(train/val) 0.52200/0.27409. Took 0.05 sec\n",
            "Epoch 677, Loss(train/val) 0.50684/0.27389. Took 0.05 sec\n",
            "Epoch 678, Loss(train/val) 0.51834/0.27404. Took 0.04 sec\n",
            "Epoch 679, Loss(train/val) 0.50929/0.27397. Took 0.04 sec\n",
            "Epoch 680, Loss(train/val) 0.51178/0.27375. Took 0.04 sec\n",
            "Epoch 681, Loss(train/val) 0.50012/0.27345. Took 0.04 sec\n",
            "Epoch 682, Loss(train/val) 0.51235/0.27310. Took 0.05 sec\n",
            "Epoch 683, Loss(train/val) 0.50442/0.27321. Took 0.05 sec\n",
            "Epoch 684, Loss(train/val) 0.51728/0.27324. Took 0.05 sec\n",
            "Epoch 685, Loss(train/val) 0.51141/0.27306. Took 0.05 sec\n",
            "Epoch 686, Loss(train/val) 0.50718/0.27270. Took 0.05 sec\n",
            "Epoch 687, Loss(train/val) 0.51281/0.27242. Took 0.05 sec\n",
            "Epoch 688, Loss(train/val) 0.51210/0.27239. Took 0.05 sec\n",
            "Epoch 689, Loss(train/val) 0.51553/0.27231. Took 0.05 sec\n",
            "Epoch 690, Loss(train/val) 0.50563/0.27263. Took 0.05 sec\n",
            "Epoch 691, Loss(train/val) 0.51509/0.27270. Took 0.04 sec\n",
            "Epoch 692, Loss(train/val) 0.51200/0.27273. Took 0.05 sec\n",
            "Epoch 693, Loss(train/val) 0.51176/0.27272. Took 0.05 sec\n",
            "Epoch 694, Loss(train/val) 0.50179/0.27230. Took 0.05 sec\n",
            "Epoch 695, Loss(train/val) 0.51252/0.27237. Took 0.04 sec\n",
            "Epoch 696, Loss(train/val) 0.51141/0.27222. Took 0.04 sec\n",
            "Epoch 697, Loss(train/val) 0.50040/0.27166. Took 0.06 sec\n",
            "Epoch 698, Loss(train/val) 0.50751/0.27133. Took 0.05 sec\n",
            "Epoch 699, Loss(train/val) 0.50720/0.27138. Took 0.04 sec\n",
            "Epoch 700, Loss(train/val) 0.51136/0.27141. Took 0.04 sec\n",
            "Epoch 701, Loss(train/val) 0.51106/0.27120. Took 0.04 sec\n",
            "Epoch 702, Loss(train/val) 0.51027/0.27103. Took 0.05 sec\n",
            "Epoch 703, Loss(train/val) 0.50786/0.27099. Took 0.04 sec\n",
            "Epoch 704, Loss(train/val) 0.50605/0.27141. Took 0.05 sec\n",
            "Epoch 705, Loss(train/val) 0.51020/0.27234. Took 0.05 sec\n",
            "Epoch 706, Loss(train/val) 0.50989/0.27195. Took 0.04 sec\n",
            "Epoch 707, Loss(train/val) 0.50418/0.27174. Took 0.05 sec\n",
            "Epoch 708, Loss(train/val) 0.50442/0.27136. Took 0.04 sec\n",
            "Epoch 709, Loss(train/val) 0.50782/0.27097. Took 0.05 sec\n",
            "Epoch 710, Loss(train/val) 0.49128/0.27074. Took 0.05 sec\n",
            "Epoch 711, Loss(train/val) 0.50472/0.27041. Took 0.04 sec\n",
            "Epoch 712, Loss(train/val) 0.51237/0.27016. Took 0.05 sec\n",
            "Epoch 713, Loss(train/val) 0.50309/0.27038. Took 0.05 sec\n",
            "Epoch 714, Loss(train/val) 0.50920/0.27076. Took 0.06 sec\n",
            "Epoch 715, Loss(train/val) 0.50696/0.27070. Took 0.05 sec\n",
            "Epoch 716, Loss(train/val) 0.50744/0.27019. Took 0.05 sec\n",
            "Epoch 717, Loss(train/val) 0.50034/0.27034. Took 0.05 sec\n",
            "Epoch 718, Loss(train/val) 0.51531/0.27025. Took 0.04 sec\n",
            "Epoch 719, Loss(train/val) 0.49095/0.27059. Took 0.05 sec\n",
            "Epoch 720, Loss(train/val) 0.50999/0.27046. Took 0.05 sec\n",
            "Epoch 721, Loss(train/val) 0.50069/0.27074. Took 0.05 sec\n",
            "Epoch 722, Loss(train/val) 0.50490/0.27094. Took 0.05 sec\n",
            "Epoch 723, Loss(train/val) 0.50369/0.27028. Took 0.04 sec\n",
            "Epoch 724, Loss(train/val) 0.50829/0.26966. Took 0.04 sec\n",
            "Epoch 725, Loss(train/val) 0.50370/0.26892. Took 0.05 sec\n",
            "Epoch 726, Loss(train/val) 0.49716/0.26872. Took 0.05 sec\n",
            "Epoch 727, Loss(train/val) 0.49836/0.26857. Took 0.04 sec\n",
            "Epoch 728, Loss(train/val) 0.49590/0.26855. Took 0.04 sec\n",
            "Epoch 729, Loss(train/val) 0.50835/0.26837. Took 0.06 sec\n",
            "Epoch 730, Loss(train/val) 0.49263/0.26839. Took 0.05 sec\n",
            "Epoch 731, Loss(train/val) 0.50779/0.26826. Took 0.05 sec\n",
            "Epoch 732, Loss(train/val) 0.50141/0.26852. Took 0.05 sec\n",
            "Epoch 733, Loss(train/val) 0.48680/0.26862. Took 0.04 sec\n",
            "Epoch 734, Loss(train/val) 0.50474/0.26842. Took 0.04 sec\n",
            "Epoch 735, Loss(train/val) 0.49862/0.26807. Took 0.06 sec\n",
            "Epoch 736, Loss(train/val) 0.49356/0.26784. Took 0.05 sec\n",
            "Epoch 737, Loss(train/val) 0.50465/0.26794. Took 0.04 sec\n",
            "Epoch 738, Loss(train/val) 0.50633/0.26766. Took 0.04 sec\n",
            "Epoch 739, Loss(train/val) 0.49265/0.26771. Took 0.04 sec\n",
            "Epoch 740, Loss(train/val) 0.49166/0.26717. Took 0.05 sec\n",
            "Epoch 741, Loss(train/val) 0.50199/0.26713. Took 0.05 sec\n",
            "Epoch 742, Loss(train/val) 0.50118/0.26734. Took 0.05 sec\n",
            "Epoch 743, Loss(train/val) 0.50166/0.26704. Took 0.04 sec\n",
            "Epoch 744, Loss(train/val) 0.49627/0.26662. Took 0.04 sec\n",
            "Epoch 745, Loss(train/val) 0.50389/0.26680. Took 0.04 sec\n",
            "Epoch 746, Loss(train/val) 0.48700/0.26676. Took 0.05 sec\n",
            "Epoch 747, Loss(train/val) 0.50020/0.26694. Took 0.05 sec\n",
            "Epoch 748, Loss(train/val) 0.49600/0.26702. Took 0.05 sec\n",
            "Epoch 749, Loss(train/val) 0.50740/0.26744. Took 0.05 sec\n",
            "Epoch 750, Loss(train/val) 0.50340/0.26755. Took 0.05 sec\n",
            "Epoch 751, Loss(train/val) 0.49958/0.26711. Took 0.06 sec\n",
            "Epoch 752, Loss(train/val) 0.50796/0.26680. Took 0.05 sec\n",
            "Epoch 753, Loss(train/val) 0.49939/0.26646. Took 0.04 sec\n",
            "Epoch 754, Loss(train/val) 0.50002/0.26618. Took 0.04 sec\n",
            "Epoch 755, Loss(train/val) 0.50166/0.26597. Took 0.04 sec\n",
            "Epoch 756, Loss(train/val) 0.49985/0.26567. Took 0.07 sec\n",
            "Epoch 757, Loss(train/val) 0.49927/0.26552. Took 0.05 sec\n",
            "Epoch 758, Loss(train/val) 0.48474/0.26532. Took 0.04 sec\n",
            "Epoch 759, Loss(train/val) 0.48862/0.26503. Took 0.04 sec\n",
            "Epoch 760, Loss(train/val) 0.49607/0.26504. Took 0.05 sec\n",
            "Epoch 761, Loss(train/val) 0.49217/0.26495. Took 0.05 sec\n",
            "Epoch 762, Loss(train/val) 0.50112/0.26499. Took 0.05 sec\n",
            "Epoch 763, Loss(train/val) 0.49726/0.26522. Took 0.05 sec\n",
            "Epoch 764, Loss(train/val) 0.49943/0.26582. Took 0.05 sec\n",
            "Epoch 765, Loss(train/val) 0.49365/0.26543. Took 0.04 sec\n",
            "Epoch 766, Loss(train/val) 0.49687/0.26475. Took 0.05 sec\n",
            "Epoch 767, Loss(train/val) 0.50366/0.26476. Took 0.04 sec\n",
            "Epoch 768, Loss(train/val) 0.49891/0.26486. Took 0.05 sec\n",
            "Epoch 769, Loss(train/val) 0.49860/0.26496. Took 0.04 sec\n",
            "Epoch 770, Loss(train/val) 0.49466/0.26458. Took 0.04 sec\n",
            "Epoch 771, Loss(train/val) 0.49262/0.26424. Took 0.05 sec\n",
            "Epoch 772, Loss(train/val) 0.49774/0.26408. Took 0.04 sec\n",
            "Epoch 773, Loss(train/val) 0.49891/0.26360. Took 0.05 sec\n",
            "Epoch 774, Loss(train/val) 0.49348/0.26337. Took 0.04 sec\n",
            "Epoch 775, Loss(train/val) 0.49610/0.26339. Took 0.05 sec\n",
            "Epoch 776, Loss(train/val) 0.48893/0.26330. Took 0.05 sec\n",
            "Epoch 777, Loss(train/val) 0.49619/0.26301. Took 0.05 sec\n",
            "Epoch 778, Loss(train/val) 0.49601/0.26320. Took 0.05 sec\n",
            "Epoch 779, Loss(train/val) 0.49280/0.26301. Took 0.04 sec\n",
            "Epoch 780, Loss(train/val) 0.49715/0.26265. Took 0.04 sec\n",
            "Epoch 781, Loss(train/val) 0.49662/0.26244. Took 0.06 sec\n",
            "Epoch 782, Loss(train/val) 0.49777/0.26233. Took 0.04 sec\n",
            "Epoch 783, Loss(train/val) 0.49650/0.26232. Took 0.04 sec\n",
            "Epoch 784, Loss(train/val) 0.48458/0.26269. Took 0.05 sec\n",
            "Epoch 785, Loss(train/val) 0.49311/0.26298. Took 0.04 sec\n",
            "Epoch 786, Loss(train/val) 0.49574/0.26338. Took 0.05 sec\n",
            "Epoch 787, Loss(train/val) 0.49512/0.26388. Took 0.05 sec\n",
            "Epoch 788, Loss(train/val) 0.49546/0.26384. Took 0.05 sec\n",
            "Epoch 789, Loss(train/val) 0.49508/0.26344. Took 0.04 sec\n",
            "Epoch 790, Loss(train/val) 0.50789/0.26320. Took 0.04 sec\n",
            "Epoch 791, Loss(train/val) 0.48738/0.26239. Took 0.05 sec\n",
            "Epoch 792, Loss(train/val) 0.49886/0.26219. Took 0.05 sec\n",
            "Epoch 793, Loss(train/val) 0.48802/0.26213. Took 0.05 sec\n",
            "Epoch 794, Loss(train/val) 0.49577/0.26180. Took 0.05 sec\n",
            "Epoch 795, Loss(train/val) 0.49118/0.26202. Took 0.04 sec\n",
            "Epoch 796, Loss(train/val) 0.49050/0.26179. Took 0.05 sec\n",
            "Epoch 797, Loss(train/val) 0.49030/0.26123. Took 0.04 sec\n",
            "Epoch 798, Loss(train/val) 0.48705/0.26126. Took 0.04 sec\n",
            "Epoch 799, Loss(train/val) 0.49163/0.26122. Took 0.05 sec\n",
            "Epoch 800, Loss(train/val) 0.48962/0.26114. Took 0.05 sec\n",
            "Epoch 801, Loss(train/val) 0.48644/0.26120. Took 0.05 sec\n",
            "Epoch 802, Loss(train/val) 0.49314/0.26126. Took 0.04 sec\n",
            "Epoch 803, Loss(train/val) 0.49466/0.26134. Took 0.04 sec\n",
            "Epoch 804, Loss(train/val) 0.48127/0.26200. Took 0.04 sec\n",
            "Epoch 805, Loss(train/val) 0.48267/0.26199. Took 0.05 sec\n",
            "Epoch 806, Loss(train/val) 0.48747/0.26120. Took 0.05 sec\n",
            "Epoch 807, Loss(train/val) 0.48260/0.26118. Took 0.05 sec\n",
            "Epoch 808, Loss(train/val) 0.49360/0.26055. Took 0.04 sec\n",
            "Epoch 809, Loss(train/val) 0.49764/0.26039. Took 0.05 sec\n",
            "Epoch 810, Loss(train/val) 0.49141/0.26025. Took 0.04 sec\n",
            "Epoch 811, Loss(train/val) 0.48667/0.26014. Took 0.06 sec\n",
            "Epoch 812, Loss(train/val) 0.49065/0.26007. Took 0.05 sec\n",
            "Epoch 813, Loss(train/val) 0.48470/0.25993. Took 0.05 sec\n",
            "Epoch 814, Loss(train/val) 0.48354/0.25995. Took 0.05 sec\n",
            "Epoch 815, Loss(train/val) 0.48923/0.25962. Took 0.05 sec\n",
            "Epoch 816, Loss(train/val) 0.48994/0.25955. Took 0.05 sec\n",
            "Epoch 817, Loss(train/val) 0.49417/0.25917. Took 0.05 sec\n",
            "Epoch 818, Loss(train/val) 0.48711/0.25890. Took 0.04 sec\n",
            "Epoch 819, Loss(train/val) 0.48523/0.25877. Took 0.04 sec\n",
            "Epoch 820, Loss(train/val) 0.47587/0.25891. Took 0.05 sec\n",
            "Epoch 821, Loss(train/val) 0.48114/0.25889. Took 0.06 sec\n",
            "Epoch 822, Loss(train/val) 0.48573/0.25951. Took 0.04 sec\n",
            "Epoch 823, Loss(train/val) 0.47909/0.26104. Took 0.04 sec\n",
            "Epoch 824, Loss(train/val) 0.48662/0.26050. Took 0.04 sec\n",
            "Epoch 825, Loss(train/val) 0.49285/0.26117. Took 0.04 sec\n",
            "Epoch 826, Loss(train/val) 0.48854/0.26077. Took 0.05 sec\n",
            "Epoch 827, Loss(train/val) 0.48125/0.25978. Took 0.04 sec\n",
            "Epoch 828, Loss(train/val) 0.48883/0.25947. Took 0.04 sec\n",
            "Epoch 829, Loss(train/val) 0.48636/0.25868. Took 0.04 sec\n",
            "Epoch 830, Loss(train/val) 0.48763/0.25926. Took 0.04 sec\n",
            "Epoch 831, Loss(train/val) 0.48992/0.25933. Took 0.05 sec\n",
            "Epoch 832, Loss(train/val) 0.47890/0.25883. Took 0.04 sec\n",
            "Epoch 833, Loss(train/val) 0.48840/0.25850. Took 0.05 sec\n",
            "Epoch 834, Loss(train/val) 0.48348/0.25817. Took 0.04 sec\n",
            "Epoch 835, Loss(train/val) 0.48449/0.25785. Took 0.04 sec\n",
            "Epoch 836, Loss(train/val) 0.48731/0.25754. Took 0.05 sec\n",
            "Epoch 837, Loss(train/val) 0.48395/0.25754. Took 0.05 sec\n",
            "Epoch 838, Loss(train/val) 0.49288/0.25739. Took 0.05 sec\n",
            "Epoch 839, Loss(train/val) 0.48430/0.25738. Took 0.04 sec\n",
            "Epoch 840, Loss(train/val) 0.48179/0.25741. Took 0.04 sec\n",
            "Epoch 841, Loss(train/val) 0.49049/0.25751. Took 0.05 sec\n",
            "Epoch 842, Loss(train/val) 0.49195/0.25752. Took 0.05 sec\n",
            "Epoch 843, Loss(train/val) 0.48404/0.25731. Took 0.05 sec\n",
            "Epoch 844, Loss(train/val) 0.48675/0.25711. Took 0.04 sec\n",
            "Epoch 845, Loss(train/val) 0.48752/0.25701. Took 0.05 sec\n",
            "Epoch 846, Loss(train/val) 0.48239/0.25691. Took 0.05 sec\n",
            "Epoch 847, Loss(train/val) 0.48925/0.25708. Took 0.05 sec\n",
            "Epoch 848, Loss(train/val) 0.48678/0.25685. Took 0.05 sec\n",
            "Epoch 849, Loss(train/val) 0.46792/0.25681. Took 0.05 sec\n",
            "Epoch 850, Loss(train/val) 0.47543/0.25649. Took 0.04 sec\n",
            "Epoch 851, Loss(train/val) 0.48810/0.25627. Took 0.06 sec\n",
            "Epoch 852, Loss(train/val) 0.48127/0.25628. Took 0.05 sec\n",
            "Epoch 853, Loss(train/val) 0.48941/0.25629. Took 0.04 sec\n",
            "Epoch 854, Loss(train/val) 0.48461/0.25602. Took 0.04 sec\n",
            "Epoch 855, Loss(train/val) 0.48158/0.25580. Took 0.04 sec\n",
            "Epoch 856, Loss(train/val) 0.48532/0.25587. Took 0.06 sec\n",
            "Epoch 857, Loss(train/val) 0.45881/0.25594. Took 0.05 sec\n",
            "Epoch 858, Loss(train/val) 0.48804/0.25562. Took 0.05 sec\n",
            "Epoch 859, Loss(train/val) 0.47369/0.25532. Took 0.05 sec\n",
            "Epoch 860, Loss(train/val) 0.48232/0.25546. Took 0.04 sec\n",
            "Epoch 861, Loss(train/val) 0.48234/0.25553. Took 0.05 sec\n",
            "Epoch 862, Loss(train/val) 0.48627/0.25571. Took 0.05 sec\n",
            "Epoch 863, Loss(train/val) 0.47696/0.25631. Took 0.05 sec\n",
            "Epoch 864, Loss(train/val) 0.47971/0.25614. Took 0.05 sec\n",
            "Epoch 865, Loss(train/val) 0.48809/0.25620. Took 0.04 sec\n",
            "Epoch 866, Loss(train/val) 0.47249/0.25620. Took 0.07 sec\n",
            "Epoch 867, Loss(train/val) 0.48333/0.25564. Took 0.04 sec\n",
            "Epoch 868, Loss(train/val) 0.47832/0.25506. Took 0.04 sec\n",
            "Epoch 869, Loss(train/val) 0.48538/0.25479. Took 0.04 sec\n",
            "Epoch 870, Loss(train/val) 0.47858/0.25477. Took 0.04 sec\n",
            "Epoch 871, Loss(train/val) 0.47823/0.25446. Took 0.05 sec\n",
            "Epoch 872, Loss(train/val) 0.46308/0.25433. Took 0.05 sec\n",
            "Epoch 873, Loss(train/val) 0.47621/0.25398. Took 0.05 sec\n",
            "Epoch 874, Loss(train/val) 0.48102/0.25391. Took 0.04 sec\n",
            "Epoch 875, Loss(train/val) 0.48435/0.25384. Took 0.05 sec\n",
            "Epoch 876, Loss(train/val) 0.47500/0.25414. Took 0.05 sec\n",
            "Epoch 877, Loss(train/val) 0.48057/0.25374. Took 0.05 sec\n",
            "Epoch 878, Loss(train/val) 0.47318/0.25365. Took 0.05 sec\n",
            "Epoch 879, Loss(train/val) 0.48334/0.25358. Took 0.04 sec\n",
            "Epoch 880, Loss(train/val) 0.47485/0.25337. Took 0.05 sec\n",
            "Epoch 881, Loss(train/val) 0.46209/0.25320. Took 0.05 sec\n",
            "Epoch 882, Loss(train/val) 0.47890/0.25304. Took 0.05 sec\n",
            "Epoch 883, Loss(train/val) 0.47991/0.25279. Took 0.05 sec\n",
            "Epoch 884, Loss(train/val) 0.47226/0.25286. Took 0.04 sec\n",
            "Epoch 885, Loss(train/val) 0.48107/0.25252. Took 0.05 sec\n",
            "Epoch 886, Loss(train/val) 0.48270/0.25283. Took 0.06 sec\n",
            "Epoch 887, Loss(train/val) 0.48215/0.25233. Took 0.05 sec\n",
            "Epoch 888, Loss(train/val) 0.47231/0.25219. Took 0.04 sec\n",
            "Epoch 889, Loss(train/val) 0.47947/0.25208. Took 0.05 sec\n",
            "Epoch 890, Loss(train/val) 0.47991/0.25187. Took 0.04 sec\n",
            "Epoch 891, Loss(train/val) 0.47438/0.25192. Took 0.05 sec\n",
            "Epoch 892, Loss(train/val) 0.48224/0.25189. Took 0.04 sec\n",
            "Epoch 893, Loss(train/val) 0.47662/0.25166. Took 0.05 sec\n",
            "Epoch 894, Loss(train/val) 0.48872/0.25174. Took 0.05 sec\n",
            "Epoch 895, Loss(train/val) 0.47458/0.25181. Took 0.05 sec\n",
            "Epoch 896, Loss(train/val) 0.47989/0.25181. Took 0.05 sec\n",
            "Epoch 897, Loss(train/val) 0.47587/0.25199. Took 0.05 sec\n",
            "Epoch 898, Loss(train/val) 0.47399/0.25194. Took 0.05 sec\n",
            "Epoch 899, Loss(train/val) 0.47411/0.25239. Took 0.04 sec\n",
            "Epoch 900, Loss(train/val) 0.47020/0.25236. Took 0.05 sec\n",
            "Epoch 901, Loss(train/val) 0.47305/0.25128. Took 0.05 sec\n",
            "Epoch 902, Loss(train/val) 0.48160/0.25084. Took 0.04 sec\n",
            "Epoch 903, Loss(train/val) 0.47823/0.25108. Took 0.05 sec\n",
            "Epoch 904, Loss(train/val) 0.47706/0.25118. Took 0.05 sec\n",
            "Epoch 905, Loss(train/val) 0.48214/0.25091. Took 0.04 sec\n",
            "Epoch 906, Loss(train/val) 0.47629/0.25068. Took 0.05 sec\n",
            "Epoch 907, Loss(train/val) 0.47676/0.25077. Took 0.05 sec\n",
            "Epoch 908, Loss(train/val) 0.47972/0.25087. Took 0.04 sec\n",
            "Epoch 909, Loss(train/val) 0.48254/0.25062. Took 0.05 sec\n",
            "Epoch 910, Loss(train/val) 0.47487/0.25051. Took 0.04 sec\n",
            "Epoch 911, Loss(train/val) 0.47725/0.25049. Took 0.05 sec\n",
            "Epoch 912, Loss(train/val) 0.46410/0.25048. Took 0.04 sec\n",
            "Epoch 913, Loss(train/val) 0.47143/0.25049. Took 0.04 sec\n",
            "Epoch 914, Loss(train/val) 0.48184/0.25039. Took 0.05 sec\n",
            "Epoch 915, Loss(train/val) 0.45646/0.25002. Took 0.04 sec\n",
            "Epoch 916, Loss(train/val) 0.47873/0.24983. Took 0.05 sec\n",
            "Epoch 917, Loss(train/val) 0.46525/0.24972. Took 0.04 sec\n",
            "Epoch 918, Loss(train/val) 0.46284/0.24991. Took 0.04 sec\n",
            "Epoch 919, Loss(train/val) 0.47552/0.24963. Took 0.04 sec\n",
            "Epoch 920, Loss(train/val) 0.47507/0.24965. Took 0.05 sec\n",
            "Epoch 921, Loss(train/val) 0.47948/0.24942. Took 0.05 sec\n",
            "Epoch 922, Loss(train/val) 0.47395/0.24925. Took 0.04 sec\n",
            "Epoch 923, Loss(train/val) 0.47494/0.24923. Took 0.04 sec\n",
            "Epoch 924, Loss(train/val) 0.47414/0.24911. Took 0.05 sec\n",
            "Epoch 925, Loss(train/val) 0.46903/0.24917. Took 0.05 sec\n",
            "Epoch 926, Loss(train/val) 0.45820/0.24908. Took 0.05 sec\n",
            "Epoch 927, Loss(train/val) 0.47065/0.24902. Took 0.04 sec\n",
            "Epoch 928, Loss(train/val) 0.47043/0.24911. Took 0.05 sec\n",
            "Epoch 929, Loss(train/val) 0.46499/0.24891. Took 0.05 sec\n",
            "Epoch 930, Loss(train/val) 0.46905/0.24880. Took 0.04 sec\n",
            "Epoch 931, Loss(train/val) 0.45878/0.24902. Took 0.05 sec\n",
            "Epoch 932, Loss(train/val) 0.47270/0.24904. Took 0.05 sec\n",
            "Epoch 933, Loss(train/val) 0.47108/0.24919. Took 0.04 sec\n",
            "Epoch 934, Loss(train/val) 0.47167/0.24883. Took 0.05 sec\n",
            "Epoch 935, Loss(train/val) 0.47991/0.24910. Took 0.04 sec\n",
            "Epoch 936, Loss(train/val) 0.45479/0.24935. Took 0.05 sec\n",
            "Epoch 937, Loss(train/val) 0.47246/0.24877. Took 0.05 sec\n",
            "Epoch 938, Loss(train/val) 0.47603/0.24913. Took 0.05 sec\n",
            "Epoch 939, Loss(train/val) 0.46097/0.24918. Took 0.05 sec\n",
            "Epoch 940, Loss(train/val) 0.48207/0.24869. Took 0.04 sec\n",
            "Epoch 941, Loss(train/val) 0.47732/0.24845. Took 0.05 sec\n",
            "Epoch 942, Loss(train/val) 0.46816/0.24812. Took 0.05 sec\n",
            "Epoch 943, Loss(train/val) 0.46703/0.24780. Took 0.04 sec\n",
            "Epoch 944, Loss(train/val) 0.46921/0.24794. Took 0.04 sec\n",
            "Epoch 945, Loss(train/val) 0.46708/0.24848. Took 0.04 sec\n",
            "Epoch 946, Loss(train/val) 0.46741/0.24807. Took 0.05 sec\n",
            "Epoch 947, Loss(train/val) 0.47363/0.24809. Took 0.05 sec\n",
            "Epoch 948, Loss(train/val) 0.46928/0.24784. Took 0.04 sec\n",
            "Epoch 949, Loss(train/val) 0.46815/0.24737. Took 0.04 sec\n",
            "Epoch 950, Loss(train/val) 0.46489/0.24753. Took 0.05 sec\n",
            "Epoch 951, Loss(train/val) 0.46590/0.24735. Took 0.07 sec\n",
            "Epoch 952, Loss(train/val) 0.47151/0.24737. Took 0.05 sec\n",
            "Epoch 953, Loss(train/val) 0.46969/0.24700. Took 0.04 sec\n",
            "Epoch 954, Loss(train/val) 0.46647/0.24670. Took 0.04 sec\n",
            "Epoch 955, Loss(train/val) 0.47274/0.24674. Took 0.04 sec\n",
            "Epoch 956, Loss(train/val) 0.47184/0.24668. Took 0.05 sec\n",
            "Epoch 957, Loss(train/val) 0.46810/0.24655. Took 0.05 sec\n",
            "Epoch 958, Loss(train/val) 0.46656/0.24629. Took 0.05 sec\n",
            "Epoch 959, Loss(train/val) 0.46232/0.24608. Took 0.04 sec\n",
            "Epoch 960, Loss(train/val) 0.45480/0.24604. Took 0.04 sec\n",
            "Epoch 961, Loss(train/val) 0.46598/0.24621. Took 0.05 sec\n",
            "Epoch 962, Loss(train/val) 0.47042/0.24595. Took 0.04 sec\n",
            "Epoch 963, Loss(train/val) 0.47408/0.24571. Took 0.05 sec\n",
            "Epoch 964, Loss(train/val) 0.46450/0.24557. Took 0.04 sec\n",
            "Epoch 965, Loss(train/val) 0.47987/0.24543. Took 0.04 sec\n",
            "Epoch 966, Loss(train/val) 0.46383/0.24534. Took 0.05 sec\n",
            "Epoch 967, Loss(train/val) 0.46421/0.24547. Took 0.05 sec\n",
            "Epoch 968, Loss(train/val) 0.46736/0.24564. Took 0.04 sec\n",
            "Epoch 969, Loss(train/val) 0.46724/0.24541. Took 0.04 sec\n",
            "Epoch 970, Loss(train/val) 0.46908/0.24560. Took 0.04 sec\n",
            "Epoch 971, Loss(train/val) 0.46382/0.24531. Took 0.06 sec\n",
            "Epoch 972, Loss(train/val) 0.46712/0.24518. Took 0.05 sec\n",
            "Epoch 973, Loss(train/val) 0.46637/0.24551. Took 0.04 sec\n",
            "Epoch 974, Loss(train/val) 0.45971/0.24522. Took 0.05 sec\n",
            "Epoch 975, Loss(train/val) 0.47162/0.24527. Took 0.04 sec\n",
            "Epoch 976, Loss(train/val) 0.46559/0.24536. Took 0.05 sec\n",
            "Epoch 977, Loss(train/val) 0.47438/0.24586. Took 0.05 sec\n",
            "Epoch 978, Loss(train/val) 0.46742/0.24569. Took 0.05 sec\n",
            "Epoch 979, Loss(train/val) 0.46641/0.24473. Took 0.04 sec\n",
            "Epoch 980, Loss(train/val) 0.45800/0.24402. Took 0.05 sec\n",
            "Epoch 981, Loss(train/val) 0.46632/0.24361. Took 0.05 sec\n",
            "Epoch 982, Loss(train/val) 0.47008/0.24333. Took 0.05 sec\n",
            "Epoch 983, Loss(train/val) 0.46520/0.24344. Took 0.04 sec\n",
            "Epoch 984, Loss(train/val) 0.46452/0.24373. Took 0.05 sec\n",
            "Epoch 985, Loss(train/val) 0.46412/0.24346. Took 0.05 sec\n",
            "Epoch 986, Loss(train/val) 0.46588/0.24361. Took 0.05 sec\n",
            "Epoch 987, Loss(train/val) 0.45616/0.24337. Took 0.04 sec\n",
            "Epoch 988, Loss(train/val) 0.46739/0.24334. Took 0.04 sec\n",
            "Epoch 989, Loss(train/val) 0.47381/0.24382. Took 0.05 sec\n",
            "Epoch 990, Loss(train/val) 0.45851/0.24386. Took 0.04 sec\n",
            "Epoch 991, Loss(train/val) 0.45573/0.24366. Took 0.05 sec\n",
            "Epoch 992, Loss(train/val) 0.46350/0.24288. Took 0.05 sec\n",
            "Epoch 993, Loss(train/val) 0.46345/0.24261. Took 0.05 sec\n",
            "Epoch 994, Loss(train/val) 0.46515/0.24252. Took 0.04 sec\n",
            "Epoch 995, Loss(train/val) 0.46288/0.24286. Took 0.04 sec\n",
            "Epoch 996, Loss(train/val) 0.46641/0.24303. Took 0.05 sec\n",
            "Epoch 997, Loss(train/val) 0.45798/0.24356. Took 0.05 sec\n",
            "Epoch 998, Loss(train/val) 0.46297/0.24380. Took 0.05 sec\n",
            "Epoch 999, Loss(train/val) 0.46669/0.24451. Took 0.04 sec\n",
            "Epoch 1000, Loss(train/val) 0.46220/0.24435. Took 0.05 sec\n",
            "Epoch 1001, Loss(train/val) 0.46847/0.24417. Took 0.05 sec\n",
            "Epoch 1002, Loss(train/val) 0.45402/0.24420. Took 0.05 sec\n",
            "Epoch 1003, Loss(train/val) 0.45663/0.24331. Took 0.05 sec\n",
            "Epoch 1004, Loss(train/val) 0.45101/0.24303. Took 0.04 sec\n",
            "Epoch 1005, Loss(train/val) 0.46422/0.24263. Took 0.05 sec\n",
            "Epoch 1006, Loss(train/val) 0.45610/0.24263. Took 0.05 sec\n",
            "Epoch 1007, Loss(train/val) 0.46189/0.24206. Took 0.05 sec\n",
            "Epoch 1008, Loss(train/val) 0.45911/0.24191. Took 0.04 sec\n",
            "Epoch 1009, Loss(train/val) 0.45396/0.24191. Took 0.05 sec\n",
            "Epoch 1010, Loss(train/val) 0.44868/0.24156. Took 0.04 sec\n",
            "Epoch 1011, Loss(train/val) 0.45607/0.24114. Took 0.05 sec\n",
            "Epoch 1012, Loss(train/val) 0.46467/0.24119. Took 0.04 sec\n",
            "Epoch 1013, Loss(train/val) 0.46465/0.24146. Took 0.04 sec\n",
            "Epoch 1014, Loss(train/val) 0.44949/0.24092. Took 0.04 sec\n",
            "Epoch 1015, Loss(train/val) 0.46180/0.24078. Took 0.06 sec\n",
            "Epoch 1016, Loss(train/val) 0.46226/0.24067. Took 0.05 sec\n",
            "Epoch 1017, Loss(train/val) 0.45336/0.24061. Took 0.05 sec\n",
            "Epoch 1018, Loss(train/val) 0.46740/0.24023. Took 0.04 sec\n",
            "Epoch 1019, Loss(train/val) 0.45846/0.24025. Took 0.05 sec\n",
            "Epoch 1020, Loss(train/val) 0.47079/0.24017. Took 0.05 sec\n",
            "Epoch 1021, Loss(train/val) 0.45786/0.24008. Took 0.05 sec\n",
            "Epoch 1022, Loss(train/val) 0.46559/0.23961. Took 0.05 sec\n",
            "Epoch 1023, Loss(train/val) 0.45933/0.23948. Took 0.05 sec\n",
            "Epoch 1024, Loss(train/val) 0.46018/0.23942. Took 0.05 sec\n",
            "Epoch 1025, Loss(train/val) 0.46008/0.23938. Took 0.04 sec\n",
            "Epoch 1026, Loss(train/val) 0.45682/0.23956. Took 0.05 sec\n",
            "Epoch 1027, Loss(train/val) 0.45153/0.23921. Took 0.05 sec\n",
            "Epoch 1028, Loss(train/val) 0.45414/0.23933. Took 0.04 sec\n",
            "Epoch 1029, Loss(train/val) 0.45276/0.23932. Took 0.05 sec\n",
            "Epoch 1030, Loss(train/val) 0.45329/0.23931. Took 0.05 sec\n",
            "Epoch 1031, Loss(train/val) 0.45545/0.23953. Took 0.05 sec\n",
            "Epoch 1032, Loss(train/val) 0.45456/0.23927. Took 0.05 sec\n",
            "Epoch 1033, Loss(train/val) 0.45395/0.23898. Took 0.04 sec\n",
            "Epoch 1034, Loss(train/val) 0.46496/0.23891. Took 0.04 sec\n",
            "Epoch 1035, Loss(train/val) 0.45660/0.23856. Took 0.04 sec\n",
            "Epoch 1036, Loss(train/val) 0.45808/0.23848. Took 0.06 sec\n",
            "Epoch 1037, Loss(train/val) 0.45997/0.23829. Took 0.04 sec\n",
            "Epoch 1038, Loss(train/val) 0.45787/0.23805. Took 0.04 sec\n",
            "Epoch 1039, Loss(train/val) 0.45970/0.23809. Took 0.04 sec\n",
            "Epoch 1040, Loss(train/val) 0.45615/0.23794. Took 0.05 sec\n",
            "Epoch 1041, Loss(train/val) 0.45935/0.23809. Took 0.05 sec\n",
            "Epoch 1042, Loss(train/val) 0.45341/0.23860. Took 0.05 sec\n",
            "Epoch 1043, Loss(train/val) 0.45035/0.23880. Took 0.05 sec\n",
            "Epoch 1044, Loss(train/val) 0.45698/0.23873. Took 0.05 sec\n",
            "Epoch 1045, Loss(train/val) 0.44877/0.23778. Took 0.04 sec\n",
            "Epoch 1046, Loss(train/val) 0.45765/0.23733. Took 0.05 sec\n",
            "Epoch 1047, Loss(train/val) 0.45759/0.23699. Took 0.05 sec\n",
            "Epoch 1048, Loss(train/val) 0.45703/0.23682. Took 0.05 sec\n",
            "Epoch 1049, Loss(train/val) 0.46366/0.23663. Took 0.04 sec\n",
            "Epoch 1050, Loss(train/val) 0.46148/0.23709. Took 0.04 sec\n",
            "Epoch 1051, Loss(train/val) 0.45509/0.23729. Took 0.05 sec\n",
            "Epoch 1052, Loss(train/val) 0.46283/0.23704. Took 0.05 sec\n",
            "Epoch 1053, Loss(train/val) 0.44920/0.23678. Took 0.04 sec\n",
            "Epoch 1054, Loss(train/val) 0.45976/0.23662. Took 0.04 sec\n",
            "Epoch 1055, Loss(train/val) 0.45028/0.23690. Took 0.04 sec\n",
            "Epoch 1056, Loss(train/val) 0.44113/0.23697. Took 0.05 sec\n",
            "Epoch 1057, Loss(train/val) 0.44683/0.23725. Took 0.05 sec\n",
            "Epoch 1058, Loss(train/val) 0.45141/0.23681. Took 0.04 sec\n",
            "Epoch 1059, Loss(train/val) 0.45456/0.23684. Took 0.04 sec\n",
            "Epoch 1060, Loss(train/val) 0.45122/0.23670. Took 0.05 sec\n",
            "Epoch 1061, Loss(train/val) 0.45462/0.23657. Took 0.05 sec\n",
            "Epoch 1062, Loss(train/val) 0.44630/0.23671. Took 0.05 sec\n",
            "Epoch 1063, Loss(train/val) 0.45509/0.23659. Took 0.04 sec\n",
            "Epoch 1064, Loss(train/val) 0.45076/0.23689. Took 0.04 sec\n",
            "Epoch 1065, Loss(train/val) 0.45261/0.23670. Took 0.04 sec\n",
            "Epoch 1066, Loss(train/val) 0.44968/0.23672. Took 0.06 sec\n",
            "Epoch 1067, Loss(train/val) 0.45724/0.23621. Took 0.05 sec\n",
            "Epoch 1068, Loss(train/val) 0.45285/0.23599. Took 0.05 sec\n",
            "Epoch 1069, Loss(train/val) 0.44456/0.23603. Took 0.05 sec\n",
            "Epoch 1070, Loss(train/val) 0.45536/0.23581. Took 0.04 sec\n",
            "Epoch 1071, Loss(train/val) 0.44704/0.23599. Took 0.05 sec\n",
            "Epoch 1072, Loss(train/val) 0.45014/0.23591. Took 0.04 sec\n",
            "Epoch 1073, Loss(train/val) 0.45171/0.23588. Took 0.05 sec\n",
            "Epoch 1074, Loss(train/val) 0.45517/0.23555. Took 0.04 sec\n",
            "Epoch 1075, Loss(train/val) 0.44843/0.23534. Took 0.04 sec\n",
            "Epoch 1076, Loss(train/val) 0.44602/0.23552. Took 0.05 sec\n",
            "Epoch 1077, Loss(train/val) 0.45083/0.23561. Took 0.05 sec\n",
            "Epoch 1078, Loss(train/val) 0.44240/0.23561. Took 0.05 sec\n",
            "Epoch 1079, Loss(train/val) 0.45191/0.23534. Took 0.05 sec\n",
            "Epoch 1080, Loss(train/val) 0.45104/0.23541. Took 0.05 sec\n",
            "Epoch 1081, Loss(train/val) 0.45088/0.23528. Took 0.06 sec\n",
            "Epoch 1082, Loss(train/val) 0.44869/0.23534. Took 0.05 sec\n",
            "Epoch 1083, Loss(train/val) 0.44303/0.23587. Took 0.05 sec\n",
            "Epoch 1084, Loss(train/val) 0.44952/0.23594. Took 0.05 sec\n",
            "Epoch 1085, Loss(train/val) 0.45201/0.23593. Took 0.05 sec\n",
            "Epoch 1086, Loss(train/val) 0.44205/0.23585. Took 0.05 sec\n",
            "Epoch 1087, Loss(train/val) 0.44863/0.23589. Took 0.05 sec\n",
            "Epoch 1088, Loss(train/val) 0.44815/0.23506. Took 0.04 sec\n",
            "Epoch 1089, Loss(train/val) 0.45538/0.23517. Took 0.05 sec\n",
            "Epoch 1090, Loss(train/val) 0.44751/0.23565. Took 0.05 sec\n",
            "Epoch 1091, Loss(train/val) 0.44759/0.23536. Took 0.05 sec\n",
            "Epoch 1092, Loss(train/val) 0.44840/0.23464. Took 0.05 sec\n",
            "Epoch 1093, Loss(train/val) 0.44797/0.23479. Took 0.05 sec\n",
            "Epoch 1094, Loss(train/val) 0.44974/0.23508. Took 0.05 sec\n",
            "Epoch 1095, Loss(train/val) 0.44874/0.23485. Took 0.04 sec\n",
            "Epoch 1096, Loss(train/val) 0.43746/0.23497. Took 0.06 sec\n",
            "Epoch 1097, Loss(train/val) 0.45275/0.23535. Took 0.04 sec\n",
            "Epoch 1098, Loss(train/val) 0.45402/0.23707. Took 0.05 sec\n",
            "Epoch 1099, Loss(train/val) 0.44800/0.23639. Took 0.05 sec\n",
            "Epoch 1100, Loss(train/val) 0.44337/0.23562. Took 0.06 sec\n",
            "Epoch 1101, Loss(train/val) 0.44869/0.23548. Took 0.05 sec\n",
            "Epoch 1102, Loss(train/val) 0.44787/0.23524. Took 0.04 sec\n",
            "Epoch 1103, Loss(train/val) 0.45027/0.23533. Took 0.05 sec\n",
            "Epoch 1104, Loss(train/val) 0.45899/0.23528. Took 0.05 sec\n",
            "Epoch 1105, Loss(train/val) 0.43374/0.23553. Took 0.05 sec\n",
            "Epoch 1106, Loss(train/val) 0.43743/0.23511. Took 0.05 sec\n",
            "Epoch 1107, Loss(train/val) 0.44179/0.23485. Took 0.04 sec\n",
            "Epoch 1108, Loss(train/val) 0.45219/0.23411. Took 0.04 sec\n",
            "Epoch 1109, Loss(train/val) 0.44250/0.23413. Took 0.05 sec\n",
            "Epoch 1110, Loss(train/val) 0.45228/0.23413. Took 0.05 sec\n",
            "Epoch 1111, Loss(train/val) 0.44839/0.23410. Took 0.05 sec\n",
            "Epoch 1112, Loss(train/val) 0.45255/0.23377. Took 0.05 sec\n",
            "Epoch 1113, Loss(train/val) 0.43973/0.23363. Took 0.05 sec\n",
            "Epoch 1114, Loss(train/val) 0.44755/0.23359. Took 0.04 sec\n",
            "Epoch 1115, Loss(train/val) 0.44577/0.23378. Took 0.05 sec\n",
            "Epoch 1116, Loss(train/val) 0.44462/0.23360. Took 0.05 sec\n",
            "Epoch 1117, Loss(train/val) 0.45097/0.23392. Took 0.04 sec\n",
            "Epoch 1118, Loss(train/val) 0.44880/0.23402. Took 0.04 sec\n",
            "Epoch 1119, Loss(train/val) 0.44715/0.23396. Took 0.04 sec\n",
            "Epoch 1120, Loss(train/val) 0.43436/0.23348. Took 0.04 sec\n",
            "Epoch 1121, Loss(train/val) 0.44130/0.23358. Took 0.05 sec\n",
            "Epoch 1122, Loss(train/val) 0.44026/0.23353. Took 0.05 sec\n",
            "Epoch 1123, Loss(train/val) 0.45905/0.23361. Took 0.04 sec\n",
            "Epoch 1124, Loss(train/val) 0.44719/0.23284. Took 0.04 sec\n",
            "Epoch 1125, Loss(train/val) 0.45118/0.23246. Took 0.05 sec\n",
            "Epoch 1126, Loss(train/val) 0.44127/0.23272. Took 0.05 sec\n",
            "Epoch 1127, Loss(train/val) 0.43850/0.23268. Took 0.05 sec\n",
            "Epoch 1128, Loss(train/val) 0.45022/0.23231. Took 0.04 sec\n",
            "Epoch 1129, Loss(train/val) 0.43711/0.23246. Took 0.04 sec\n",
            "Epoch 1130, Loss(train/val) 0.44398/0.23240. Took 0.05 sec\n",
            "Epoch 1131, Loss(train/val) 0.44077/0.23262. Took 0.05 sec\n",
            "Epoch 1132, Loss(train/val) 0.44988/0.23309. Took 0.05 sec\n",
            "Epoch 1133, Loss(train/val) 0.42131/0.23295. Took 0.04 sec\n",
            "Epoch 1134, Loss(train/val) 0.44175/0.23321. Took 0.04 sec\n",
            "Epoch 1135, Loss(train/val) 0.44426/0.23290. Took 0.04 sec\n",
            "Epoch 1136, Loss(train/val) 0.44388/0.23241. Took 0.05 sec\n",
            "Epoch 1137, Loss(train/val) 0.44093/0.23189. Took 0.04 sec\n",
            "Epoch 1138, Loss(train/val) 0.44227/0.23191. Took 0.04 sec\n",
            "Epoch 1139, Loss(train/val) 0.44231/0.23194. Took 0.04 sec\n",
            "Epoch 1140, Loss(train/val) 0.44142/0.23253. Took 0.04 sec\n",
            "Epoch 1141, Loss(train/val) 0.44850/0.23190. Took 0.06 sec\n",
            "Epoch 1142, Loss(train/val) 0.43420/0.23176. Took 0.04 sec\n",
            "Epoch 1143, Loss(train/val) 0.43918/0.23181. Took 0.05 sec\n",
            "Epoch 1144, Loss(train/val) 0.44808/0.23191. Took 0.04 sec\n",
            "Epoch 1145, Loss(train/val) 0.44266/0.23150. Took 0.04 sec\n",
            "Epoch 1146, Loss(train/val) 0.42740/0.23186. Took 0.05 sec\n",
            "Epoch 1147, Loss(train/val) 0.43705/0.23144. Took 0.04 sec\n",
            "Epoch 1148, Loss(train/val) 0.43230/0.23169. Took 0.04 sec\n",
            "Epoch 1149, Loss(train/val) 0.43667/0.23201. Took 0.05 sec\n",
            "Epoch 1150, Loss(train/val) 0.44345/0.23271. Took 0.05 sec\n",
            "Epoch 1151, Loss(train/val) 0.43220/0.23236. Took 0.05 sec\n",
            "Epoch 1152, Loss(train/val) 0.44627/0.23172. Took 0.05 sec\n",
            "Epoch 1153, Loss(train/val) 0.43697/0.23177. Took 0.05 sec\n",
            "Epoch 1154, Loss(train/val) 0.44201/0.23178. Took 0.04 sec\n",
            "Epoch 1155, Loss(train/val) 0.44160/0.23146. Took 0.04 sec\n",
            "Epoch 1156, Loss(train/val) 0.44241/0.23191. Took 0.05 sec\n",
            "Epoch 1157, Loss(train/val) 0.44179/0.23217. Took 0.05 sec\n",
            "Epoch 1158, Loss(train/val) 0.43756/0.23201. Took 0.04 sec\n",
            "Epoch 1159, Loss(train/val) 0.43370/0.23191. Took 0.04 sec\n",
            "Epoch 1160, Loss(train/val) 0.44674/0.23267. Took 0.05 sec\n",
            "Epoch 1161, Loss(train/val) 0.44017/0.23211. Took 0.05 sec\n",
            "Epoch 1162, Loss(train/val) 0.43193/0.23157. Took 0.04 sec\n",
            "Epoch 1163, Loss(train/val) 0.44153/0.23159. Took 0.05 sec\n",
            "Epoch 1164, Loss(train/val) 0.43247/0.23140. Took 0.05 sec\n",
            "Epoch 1165, Loss(train/val) 0.44193/0.23111. Took 0.05 sec\n",
            "Epoch 1166, Loss(train/val) 0.44652/0.23061. Took 0.05 sec\n",
            "Epoch 1167, Loss(train/val) 0.43880/0.23042. Took 0.05 sec\n",
            "Epoch 1168, Loss(train/val) 0.43746/0.23032. Took 0.05 sec\n",
            "Epoch 1169, Loss(train/val) 0.43798/0.23063. Took 0.05 sec\n",
            "Epoch 1170, Loss(train/val) 0.42880/0.23041. Took 0.05 sec\n",
            "Epoch 1171, Loss(train/val) 0.43788/0.23023. Took 0.05 sec\n",
            "Epoch 1172, Loss(train/val) 0.43510/0.23038. Took 0.05 sec\n",
            "Epoch 1173, Loss(train/val) 0.43564/0.23061. Took 0.05 sec\n",
            "Epoch 1174, Loss(train/val) 0.42964/0.23042. Took 0.06 sec\n",
            "Epoch 1175, Loss(train/val) 0.43333/0.23101. Took 0.04 sec\n",
            "Epoch 1176, Loss(train/val) 0.43949/0.23026. Took 0.05 sec\n",
            "Epoch 1177, Loss(train/val) 0.43616/0.23017. Took 0.05 sec\n",
            "Epoch 1178, Loss(train/val) 0.43104/0.23052. Took 0.05 sec\n",
            "Epoch 1179, Loss(train/val) 0.44066/0.23093. Took 0.04 sec\n",
            "Epoch 1180, Loss(train/val) 0.43650/0.23062. Took 0.05 sec\n",
            "Epoch 1181, Loss(train/val) 0.43706/0.23063. Took 0.05 sec\n",
            "Epoch 1182, Loss(train/val) 0.44914/0.23086. Took 0.05 sec\n",
            "Epoch 1183, Loss(train/val) 0.43937/0.23043. Took 0.05 sec\n",
            "Epoch 1184, Loss(train/val) 0.43309/0.23027. Took 0.05 sec\n",
            "Epoch 1185, Loss(train/val) 0.42368/0.23023. Took 0.04 sec\n",
            "Epoch 1186, Loss(train/val) 0.43372/0.23033. Took 0.06 sec\n",
            "Epoch 1187, Loss(train/val) 0.44012/0.23012. Took 0.05 sec\n",
            "Epoch 1188, Loss(train/val) 0.42637/0.23023. Took 0.05 sec\n",
            "Epoch 1189, Loss(train/val) 0.43572/0.23023. Took 0.05 sec\n",
            "Epoch 1190, Loss(train/val) 0.43249/0.23014. Took 0.05 sec\n",
            "Epoch 1191, Loss(train/val) 0.43212/0.22983. Took 0.05 sec\n",
            "Epoch 1192, Loss(train/val) 0.42970/0.22967. Took 0.05 sec\n",
            "Epoch 1193, Loss(train/val) 0.42122/0.22991. Took 0.05 sec\n",
            "Epoch 1194, Loss(train/val) 0.42665/0.22978. Took 0.05 sec\n",
            "Epoch 1195, Loss(train/val) 0.43588/0.22956. Took 0.05 sec\n",
            "Epoch 1196, Loss(train/val) 0.43099/0.22964. Took 0.05 sec\n",
            "Epoch 1197, Loss(train/val) 0.43433/0.22940. Took 0.04 sec\n",
            "Epoch 1198, Loss(train/val) 0.43619/0.22979. Took 0.04 sec\n",
            "Epoch 1199, Loss(train/val) 0.43413/0.22913. Took 0.05 sec\n",
            "Epoch 1200, Loss(train/val) 0.43664/0.22900. Took 0.05 sec\n",
            "Epoch 1201, Loss(train/val) 0.43360/0.22842. Took 0.05 sec\n",
            "Epoch 1202, Loss(train/val) 0.43687/0.22847. Took 0.05 sec\n",
            "Epoch 1203, Loss(train/val) 0.42779/0.22886. Took 0.05 sec\n",
            "Epoch 1204, Loss(train/val) 0.43441/0.22921. Took 0.05 sec\n",
            "Epoch 1205, Loss(train/val) 0.43243/0.22965. Took 0.05 sec\n",
            "Epoch 1206, Loss(train/val) 0.42544/0.22877. Took 0.05 sec\n",
            "Epoch 1207, Loss(train/val) 0.43472/0.22872. Took 0.06 sec\n",
            "Epoch 1208, Loss(train/val) 0.42260/0.22810. Took 0.05 sec\n",
            "Epoch 1209, Loss(train/val) 0.43563/0.22768. Took 0.05 sec\n",
            "Epoch 1210, Loss(train/val) 0.43530/0.22751. Took 0.04 sec\n",
            "Epoch 1211, Loss(train/val) 0.42658/0.22773. Took 0.05 sec\n",
            "Epoch 1212, Loss(train/val) 0.43518/0.22755. Took 0.05 sec\n",
            "Epoch 1213, Loss(train/val) 0.42610/0.22861. Took 0.05 sec\n",
            "Epoch 1214, Loss(train/val) 0.43336/0.22823. Took 0.04 sec\n",
            "Epoch 1215, Loss(train/val) 0.41866/0.22761. Took 0.05 sec\n",
            "Epoch 1216, Loss(train/val) 0.42831/0.22718. Took 0.05 sec\n",
            "Epoch 1217, Loss(train/val) 0.43214/0.22698. Took 0.04 sec\n",
            "Epoch 1218, Loss(train/val) 0.42865/0.22695. Took 0.04 sec\n",
            "Epoch 1219, Loss(train/val) 0.42900/0.22698. Took 0.05 sec\n",
            "Epoch 1220, Loss(train/val) 0.42705/0.22746. Took 0.05 sec\n",
            "Epoch 1221, Loss(train/val) 0.43549/0.22776. Took 0.05 sec\n",
            "Epoch 1222, Loss(train/val) 0.42741/0.22770. Took 0.04 sec\n",
            "Epoch 1223, Loss(train/val) 0.43148/0.22757. Took 0.04 sec\n",
            "Epoch 1224, Loss(train/val) 0.43239/0.22695. Took 0.05 sec\n",
            "Epoch 1225, Loss(train/val) 0.43067/0.22708. Took 0.05 sec\n",
            "Epoch 1226, Loss(train/val) 0.42448/0.22695. Took 0.05 sec\n",
            "Epoch 1227, Loss(train/val) 0.43061/0.22706. Took 0.04 sec\n",
            "Epoch 1228, Loss(train/val) 0.44179/0.22729. Took 0.05 sec\n",
            "Epoch 1229, Loss(train/val) 0.42329/0.22751. Took 0.05 sec\n",
            "Epoch 1230, Loss(train/val) 0.42296/0.22740. Took 0.05 sec\n",
            "Epoch 1231, Loss(train/val) 0.43477/0.22743. Took 0.06 sec\n",
            "Epoch 1232, Loss(train/val) 0.43333/0.22776. Took 0.05 sec\n",
            "Epoch 1233, Loss(train/val) 0.42509/0.22836. Took 0.05 sec\n",
            "Epoch 1234, Loss(train/val) 0.42007/0.22786. Took 0.05 sec\n",
            "Epoch 1235, Loss(train/val) 0.42620/0.22724. Took 0.05 sec\n",
            "Epoch 1236, Loss(train/val) 0.43029/0.22703. Took 0.05 sec\n",
            "Epoch 1237, Loss(train/val) 0.43127/0.22670. Took 0.05 sec\n",
            "Epoch 1238, Loss(train/val) 0.42802/0.22645. Took 0.05 sec\n",
            "Epoch 1239, Loss(train/val) 0.42826/0.22649. Took 0.04 sec\n",
            "Epoch 1240, Loss(train/val) 0.43046/0.22706. Took 0.05 sec\n",
            "Epoch 1241, Loss(train/val) 0.43014/0.22713. Took 0.05 sec\n",
            "Epoch 1242, Loss(train/val) 0.42209/0.22675. Took 0.04 sec\n",
            "Epoch 1243, Loss(train/val) 0.42483/0.22673. Took 0.04 sec\n",
            "Epoch 1244, Loss(train/val) 0.43015/0.22653. Took 0.05 sec\n",
            "Epoch 1245, Loss(train/val) 0.42519/0.22696. Took 0.05 sec\n",
            "Epoch 1246, Loss(train/val) 0.43440/0.22712. Took 0.05 sec\n",
            "Epoch 1247, Loss(train/val) 0.43651/0.22657. Took 0.04 sec\n",
            "Epoch 1248, Loss(train/val) 0.42723/0.22647. Took 0.04 sec\n",
            "Epoch 1249, Loss(train/val) 0.42717/0.22642. Took 0.04 sec\n",
            "Epoch 1250, Loss(train/val) 0.42368/0.22666. Took 0.05 sec\n",
            "Epoch 1251, Loss(train/val) 0.43369/0.22640. Took 0.05 sec\n",
            "Epoch 1252, Loss(train/val) 0.42828/0.22610. Took 0.04 sec\n",
            "Epoch 1253, Loss(train/val) 0.42179/0.22601. Took 0.04 sec\n",
            "Epoch 1254, Loss(train/val) 0.42063/0.22547. Took 0.05 sec\n",
            "Epoch 1255, Loss(train/val) 0.43381/0.22523. Took 0.04 sec\n",
            "Epoch 1256, Loss(train/val) 0.42385/0.22552. Took 0.05 sec\n",
            "Epoch 1257, Loss(train/val) 0.41537/0.22582. Took 0.05 sec\n",
            "Epoch 1258, Loss(train/val) 0.42235/0.22541. Took 0.05 sec\n",
            "Epoch 1259, Loss(train/val) 0.42197/0.22532. Took 0.04 sec\n",
            "Epoch 1260, Loss(train/val) 0.41897/0.22547. Took 0.04 sec\n",
            "Epoch 1261, Loss(train/val) 0.41326/0.22519. Took 0.05 sec\n",
            "Epoch 1262, Loss(train/val) 0.42447/0.22469. Took 0.05 sec\n",
            "Epoch 1263, Loss(train/val) 0.42636/0.22452. Took 0.05 sec\n",
            "Epoch 1264, Loss(train/val) 0.41850/0.22429. Took 0.05 sec\n",
            "Epoch 1265, Loss(train/val) 0.41743/0.22424. Took 0.05 sec\n",
            "Epoch 1266, Loss(train/val) 0.43030/0.22409. Took 0.05 sec\n",
            "Epoch 1267, Loss(train/val) 0.41785/0.22432. Took 0.05 sec\n",
            "Epoch 1268, Loss(train/val) 0.41864/0.22469. Took 0.05 sec\n",
            "Epoch 1269, Loss(train/val) 0.42904/0.22493. Took 0.04 sec\n",
            "Epoch 1270, Loss(train/val) 0.42997/0.22529. Took 0.05 sec\n",
            "Epoch 1271, Loss(train/val) 0.42903/0.22570. Took 0.06 sec\n",
            "Epoch 1272, Loss(train/val) 0.42511/0.22545. Took 0.05 sec\n",
            "Epoch 1273, Loss(train/val) 0.43083/0.22494. Took 0.05 sec\n",
            "Epoch 1274, Loss(train/val) 0.42282/0.22467. Took 0.05 sec\n",
            "Epoch 1275, Loss(train/val) 0.41880/0.22456. Took 0.05 sec\n",
            "Epoch 1276, Loss(train/val) 0.42737/0.22440. Took 0.06 sec\n",
            "Epoch 1277, Loss(train/val) 0.42245/0.22442. Took 0.05 sec\n",
            "Epoch 1278, Loss(train/val) 0.42322/0.22453. Took 0.04 sec\n",
            "Epoch 1279, Loss(train/val) 0.42387/0.22458. Took 0.05 sec\n",
            "Epoch 1280, Loss(train/val) 0.42798/0.22437. Took 0.04 sec\n",
            "Epoch 1281, Loss(train/val) 0.42265/0.22435. Took 0.05 sec\n",
            "Epoch 1282, Loss(train/val) 0.42135/0.22470. Took 0.05 sec\n",
            "Epoch 1283, Loss(train/val) 0.42954/0.22451. Took 0.05 sec\n",
            "Epoch 1284, Loss(train/val) 0.40609/0.22430. Took 0.05 sec\n",
            "Epoch 1285, Loss(train/val) 0.42924/0.22436. Took 0.04 sec\n",
            "Epoch 1286, Loss(train/val) 0.42101/0.22462. Took 0.05 sec\n",
            "Epoch 1287, Loss(train/val) 0.42499/0.22398. Took 0.05 sec\n",
            "Epoch 1288, Loss(train/val) 0.42187/0.22363. Took 0.04 sec\n",
            "Epoch 1289, Loss(train/val) 0.42603/0.22398. Took 0.04 sec\n",
            "Epoch 1290, Loss(train/val) 0.42407/0.22342. Took 0.04 sec\n",
            "Epoch 1291, Loss(train/val) 0.41572/0.22344. Took 0.05 sec\n",
            "Epoch 1292, Loss(train/val) 0.41599/0.22326. Took 0.04 sec\n",
            "Epoch 1293, Loss(train/val) 0.42191/0.22325. Took 0.05 sec\n",
            "Epoch 1294, Loss(train/val) 0.41957/0.22306. Took 0.04 sec\n",
            "Epoch 1295, Loss(train/val) 0.41394/0.22291. Took 0.05 sec\n",
            "Epoch 1296, Loss(train/val) 0.42315/0.22284. Took 0.05 sec\n",
            "Epoch 1297, Loss(train/val) 0.42075/0.22293. Took 0.04 sec\n",
            "Epoch 1298, Loss(train/val) 0.42049/0.22311. Took 0.05 sec\n",
            "Epoch 1299, Loss(train/val) 0.41974/0.22282. Took 0.04 sec\n",
            "Epoch 1300, Loss(train/val) 0.42683/0.22279. Took 0.05 sec\n",
            "Epoch 1301, Loss(train/val) 0.43073/0.22257. Took 0.06 sec\n",
            "Epoch 1302, Loss(train/val) 0.42301/0.22272. Took 0.04 sec\n",
            "Epoch 1303, Loss(train/val) 0.42574/0.22247. Took 0.04 sec\n",
            "Epoch 1304, Loss(train/val) 0.42092/0.22261. Took 0.05 sec\n",
            "Epoch 1305, Loss(train/val) 0.41839/0.22275. Took 0.04 sec\n",
            "Epoch 1306, Loss(train/val) 0.41585/0.22278. Took 0.05 sec\n",
            "Epoch 1307, Loss(train/val) 0.42061/0.22366. Took 0.05 sec\n",
            "Epoch 1308, Loss(train/val) 0.41616/0.22403. Took 0.05 sec\n",
            "Epoch 1309, Loss(train/val) 0.41187/0.22389. Took 0.04 sec\n",
            "Epoch 1310, Loss(train/val) 0.42101/0.22390. Took 0.05 sec\n",
            "Epoch 1311, Loss(train/val) 0.41859/0.22373. Took 0.05 sec\n",
            "Epoch 1312, Loss(train/val) 0.41857/0.22289. Took 0.05 sec\n",
            "Epoch 1313, Loss(train/val) 0.41983/0.22241. Took 0.05 sec\n",
            "Epoch 1314, Loss(train/val) 0.42100/0.22244. Took 0.05 sec\n",
            "Epoch 1315, Loss(train/val) 0.41881/0.22254. Took 0.05 sec\n",
            "Epoch 1316, Loss(train/val) 0.41493/0.22233. Took 0.05 sec\n",
            "Epoch 1317, Loss(train/val) 0.41149/0.22246. Took 0.05 sec\n",
            "Epoch 1318, Loss(train/val) 0.42225/0.22233. Took 0.04 sec\n",
            "Epoch 1319, Loss(train/val) 0.41432/0.22237. Took 0.04 sec\n",
            "Epoch 1320, Loss(train/val) 0.41836/0.22202. Took 0.05 sec\n",
            "Epoch 1321, Loss(train/val) 0.41994/0.22162. Took 0.06 sec\n",
            "Epoch 1322, Loss(train/val) 0.41210/0.22182. Took 0.04 sec\n",
            "Epoch 1323, Loss(train/val) 0.42012/0.22168. Took 0.05 sec\n",
            "Epoch 1324, Loss(train/val) 0.41557/0.22170. Took 0.04 sec\n",
            "Epoch 1325, Loss(train/val) 0.41712/0.22143. Took 0.05 sec\n",
            "Epoch 1326, Loss(train/val) 0.42596/0.22075. Took 0.05 sec\n",
            "Epoch 1327, Loss(train/val) 0.41638/0.22082. Took 0.05 sec\n",
            "Epoch 1328, Loss(train/val) 0.41601/0.22103. Took 0.04 sec\n",
            "Epoch 1329, Loss(train/val) 0.41840/0.22119. Took 0.04 sec\n",
            "Epoch 1330, Loss(train/val) 0.41702/0.22139. Took 0.05 sec\n",
            "Epoch 1331, Loss(train/val) 0.40562/0.22165. Took 0.05 sec\n",
            "Epoch 1332, Loss(train/val) 0.41171/0.22167. Took 0.04 sec\n",
            "Epoch 1333, Loss(train/val) 0.42054/0.22247. Took 0.04 sec\n",
            "Epoch 1334, Loss(train/val) 0.43798/0.22153. Took 0.04 sec\n",
            "Epoch 1335, Loss(train/val) 0.41703/0.22106. Took 0.04 sec\n",
            "Epoch 1336, Loss(train/val) 0.41738/0.22041. Took 0.06 sec\n",
            "Epoch 1337, Loss(train/val) 0.42262/0.22032. Took 0.05 sec\n",
            "Epoch 1338, Loss(train/val) 0.40701/0.22035. Took 0.05 sec\n",
            "Epoch 1339, Loss(train/val) 0.41396/0.21999. Took 0.06 sec\n",
            "Epoch 1340, Loss(train/val) 0.41783/0.22018. Took 0.06 sec\n",
            "Epoch 1341, Loss(train/val) 0.41059/0.22033. Took 0.05 sec\n",
            "Epoch 1342, Loss(train/val) 0.41395/0.22023. Took 0.04 sec\n",
            "Epoch 1343, Loss(train/val) 0.41147/0.22046. Took 0.04 sec\n",
            "Epoch 1344, Loss(train/val) 0.41646/0.22066. Took 0.04 sec\n",
            "Epoch 1345, Loss(train/val) 0.42062/0.22069. Took 0.05 sec\n",
            "Epoch 1346, Loss(train/val) 0.42074/0.22103. Took 0.05 sec\n",
            "Epoch 1347, Loss(train/val) 0.42851/0.22073. Took 0.05 sec\n",
            "Epoch 1348, Loss(train/val) 0.41619/0.22078. Took 0.05 sec\n",
            "Epoch 1349, Loss(train/val) 0.41405/0.22047. Took 0.04 sec\n",
            "Epoch 1350, Loss(train/val) 0.41703/0.22074. Took 0.05 sec\n",
            "Epoch 1351, Loss(train/val) 0.41680/0.22043. Took 0.05 sec\n",
            "Epoch 1352, Loss(train/val) 0.40086/0.22029. Took 0.05 sec\n",
            "Epoch 1353, Loss(train/val) 0.41519/0.22022. Took 0.05 sec\n",
            "Epoch 1354, Loss(train/val) 0.42236/0.22028. Took 0.05 sec\n",
            "Epoch 1355, Loss(train/val) 0.41771/0.22017. Took 0.05 sec\n",
            "Epoch 1356, Loss(train/val) 0.39974/0.21985. Took 0.05 sec\n",
            "Epoch 1357, Loss(train/val) 0.41839/0.21978. Took 0.05 sec\n",
            "Epoch 1358, Loss(train/val) 0.40683/0.21924. Took 0.05 sec\n",
            "Epoch 1359, Loss(train/val) 0.41881/0.21905. Took 0.05 sec\n",
            "Epoch 1360, Loss(train/val) 0.41631/0.21923. Took 0.05 sec\n",
            "Epoch 1361, Loss(train/val) 0.41712/0.21950. Took 0.05 sec\n",
            "Epoch 1362, Loss(train/val) 0.41093/0.21925. Took 0.05 sec\n",
            "Epoch 1363, Loss(train/val) 0.40950/0.21922. Took 0.05 sec\n",
            "Epoch 1364, Loss(train/val) 0.41194/0.21971. Took 0.05 sec\n",
            "Epoch 1365, Loss(train/val) 0.41416/0.21980. Took 0.05 sec\n",
            "Epoch 1366, Loss(train/val) 0.41640/0.22021. Took 0.05 sec\n",
            "Epoch 1367, Loss(train/val) 0.39633/0.21997. Took 0.05 sec\n",
            "Epoch 1368, Loss(train/val) 0.40971/0.21966. Took 0.04 sec\n",
            "Epoch 1369, Loss(train/val) 0.41350/0.21926. Took 0.04 sec\n",
            "Epoch 1370, Loss(train/val) 0.40660/0.21920. Took 0.05 sec\n",
            "Epoch 1371, Loss(train/val) 0.41500/0.21903. Took 0.05 sec\n",
            "Epoch 1372, Loss(train/val) 0.40519/0.21910. Took 0.05 sec\n",
            "Epoch 1373, Loss(train/val) 0.41058/0.21886. Took 0.05 sec\n",
            "Epoch 1374, Loss(train/val) 0.41093/0.21887. Took 0.04 sec\n",
            "Epoch 1375, Loss(train/val) 0.39518/0.21899. Took 0.05 sec\n",
            "Epoch 1376, Loss(train/val) 0.41799/0.21865. Took 0.05 sec\n",
            "Epoch 1377, Loss(train/val) 0.40814/0.21824. Took 0.05 sec\n",
            "Epoch 1378, Loss(train/val) 0.41253/0.21760. Took 0.06 sec\n",
            "Epoch 1379, Loss(train/val) 0.40528/0.21740. Took 0.05 sec\n",
            "Epoch 1380, Loss(train/val) 0.40531/0.21782. Took 0.06 sec\n",
            "Epoch 1381, Loss(train/val) 0.40069/0.21830. Took 0.05 sec\n",
            "Epoch 1382, Loss(train/val) 0.41293/0.21776. Took 0.05 sec\n",
            "Epoch 1383, Loss(train/val) 0.40702/0.21770. Took 0.05 sec\n",
            "Epoch 1384, Loss(train/val) 0.39894/0.21757. Took 0.05 sec\n",
            "Epoch 1385, Loss(train/val) 0.41293/0.21757. Took 0.05 sec\n",
            "Epoch 1386, Loss(train/val) 0.41102/0.21790. Took 0.05 sec\n",
            "Epoch 1387, Loss(train/val) 0.41225/0.21819. Took 0.05 sec\n",
            "Epoch 1388, Loss(train/val) 0.41119/0.21804. Took 0.05 sec\n",
            "Epoch 1389, Loss(train/val) 0.41084/0.21820. Took 0.05 sec\n",
            "Epoch 1390, Loss(train/val) 0.41493/0.21779. Took 0.06 sec\n",
            "Epoch 1391, Loss(train/val) 0.40550/0.21751. Took 0.05 sec\n",
            "Epoch 1392, Loss(train/val) 0.41448/0.21778. Took 0.05 sec\n",
            "Epoch 1393, Loss(train/val) 0.40161/0.21724. Took 0.06 sec\n",
            "Epoch 1394, Loss(train/val) 0.41364/0.21681. Took 0.06 sec\n",
            "Epoch 1395, Loss(train/val) 0.41425/0.21656. Took 0.05 sec\n",
            "Epoch 1396, Loss(train/val) 0.40865/0.21663. Took 0.05 sec\n",
            "Epoch 1397, Loss(train/val) 0.40716/0.21688. Took 0.05 sec\n",
            "Epoch 1398, Loss(train/val) 0.42217/0.21708. Took 0.07 sec\n",
            "Epoch 1399, Loss(train/val) 0.40856/0.21704. Took 0.05 sec\n",
            "Epoch 1400, Loss(train/val) 0.41490/0.21661. Took 0.06 sec\n",
            "Epoch 1401, Loss(train/val) 0.40792/0.21647. Took 0.05 sec\n",
            "Epoch 1402, Loss(train/val) 0.40220/0.21681. Took 0.05 sec\n",
            "Epoch 1403, Loss(train/val) 0.41740/0.21687. Took 0.05 sec\n",
            "Epoch 1404, Loss(train/val) 0.40385/0.21666. Took 0.05 sec\n",
            "Epoch 1405, Loss(train/val) 0.40344/0.21697. Took 0.05 sec\n",
            "Epoch 1406, Loss(train/val) 0.40913/0.21711. Took 0.05 sec\n",
            "Epoch 1407, Loss(train/val) 0.41217/0.21728. Took 0.05 sec\n",
            "Epoch 1408, Loss(train/val) 0.39572/0.21733. Took 0.05 sec\n",
            "Epoch 1409, Loss(train/val) 0.40993/0.21741. Took 0.04 sec\n",
            "Epoch 1410, Loss(train/val) 0.40735/0.21733. Took 0.04 sec\n",
            "Epoch 1411, Loss(train/val) 0.39845/0.21760. Took 0.05 sec\n",
            "Epoch 1412, Loss(train/val) 0.40175/0.21710. Took 0.06 sec\n",
            "Epoch 1413, Loss(train/val) 0.39983/0.21725. Took 0.05 sec\n",
            "Epoch 1414, Loss(train/val) 0.40247/0.21714. Took 0.04 sec\n",
            "Epoch 1415, Loss(train/val) 0.40231/0.21677. Took 0.04 sec\n",
            "Epoch 1416, Loss(train/val) 0.40655/0.21598. Took 0.04 sec\n",
            "Epoch 1417, Loss(train/val) 0.40568/0.21589. Took 0.05 sec\n",
            "Epoch 1418, Loss(train/val) 0.40306/0.21596. Took 0.05 sec\n",
            "Epoch 1419, Loss(train/val) 0.41175/0.21567. Took 0.06 sec\n",
            "Epoch 1420, Loss(train/val) 0.40894/0.21546. Took 0.05 sec\n",
            "Epoch 1421, Loss(train/val) 0.39905/0.21554. Took 0.05 sec\n",
            "Epoch 1422, Loss(train/val) 0.40164/0.21538. Took 0.05 sec\n",
            "Epoch 1423, Loss(train/val) 0.39913/0.21507. Took 0.05 sec\n",
            "Epoch 1424, Loss(train/val) 0.40757/0.21520. Took 0.05 sec\n",
            "Epoch 1425, Loss(train/val) 0.39669/0.21564. Took 0.05 sec\n",
            "Epoch 1426, Loss(train/val) 0.40313/0.21521. Took 0.05 sec\n",
            "Epoch 1427, Loss(train/val) 0.40285/0.21513. Took 0.04 sec\n",
            "Epoch 1428, Loss(train/val) 0.40197/0.21507. Took 0.05 sec\n",
            "Epoch 1429, Loss(train/val) 0.40438/0.21518. Took 0.04 sec\n",
            "Epoch 1430, Loss(train/val) 0.41291/0.21547. Took 0.04 sec\n",
            "Epoch 1431, Loss(train/val) 0.40386/0.21492. Took 0.05 sec\n",
            "Epoch 1432, Loss(train/val) 0.40252/0.21487. Took 0.05 sec\n",
            "Epoch 1433, Loss(train/val) 0.39318/0.21508. Took 0.04 sec\n",
            "Epoch 1434, Loss(train/val) 0.40484/0.21462. Took 0.04 sec\n",
            "Epoch 1435, Loss(train/val) 0.40555/0.21433. Took 0.05 sec\n",
            "Epoch 1436, Loss(train/val) 0.40728/0.21417. Took 0.06 sec\n",
            "Epoch 1437, Loss(train/val) 0.40150/0.21407. Took 0.05 sec\n",
            "Epoch 1438, Loss(train/val) 0.39827/0.21414. Took 0.05 sec\n",
            "Epoch 1439, Loss(train/val) 0.39899/0.21381. Took 0.05 sec\n",
            "Epoch 1440, Loss(train/val) 0.40257/0.21363. Took 0.05 sec\n",
            "Epoch 1441, Loss(train/val) 0.40110/0.21351. Took 0.05 sec\n",
            "Epoch 1442, Loss(train/val) 0.41248/0.21385. Took 0.04 sec\n",
            "Epoch 1443, Loss(train/val) 0.38726/0.21375. Took 0.04 sec\n",
            "Epoch 1444, Loss(train/val) 0.40344/0.21372. Took 0.05 sec\n",
            "Epoch 1445, Loss(train/val) 0.40348/0.21366. Took 0.04 sec\n",
            "Epoch 1446, Loss(train/val) 0.40564/0.21371. Took 0.05 sec\n",
            "Epoch 1447, Loss(train/val) 0.40917/0.21392. Took 0.05 sec\n",
            "Epoch 1448, Loss(train/val) 0.40127/0.21376. Took 0.05 sec\n",
            "Epoch 1449, Loss(train/val) 0.40749/0.21397. Took 0.05 sec\n",
            "Epoch 1450, Loss(train/val) 0.40336/0.21390. Took 0.05 sec\n",
            "Epoch 1451, Loss(train/val) 0.40107/0.21353. Took 0.05 sec\n",
            "Epoch 1452, Loss(train/val) 0.40080/0.21310. Took 0.05 sec\n",
            "Epoch 1453, Loss(train/val) 0.40367/0.21282. Took 0.04 sec\n",
            "Epoch 1454, Loss(train/val) 0.39215/0.21280. Took 0.04 sec\n",
            "Epoch 1455, Loss(train/val) 0.38957/0.21273. Took 0.05 sec\n",
            "Epoch 1456, Loss(train/val) 0.39146/0.21294. Took 0.05 sec\n",
            "Epoch 1457, Loss(train/val) 0.40567/0.21294. Took 0.05 sec\n",
            "Epoch 1458, Loss(train/val) 0.40229/0.21247. Took 0.05 sec\n",
            "Epoch 1459, Loss(train/val) 0.40102/0.21269. Took 0.04 sec\n",
            "Epoch 1460, Loss(train/val) 0.40519/0.21243. Took 0.04 sec\n",
            "Epoch 1461, Loss(train/val) 0.39953/0.21273. Took 0.06 sec\n",
            "Epoch 1462, Loss(train/val) 0.39093/0.21289. Took 0.05 sec\n",
            "Epoch 1463, Loss(train/val) 0.38141/0.21303. Took 0.05 sec\n",
            "Epoch 1464, Loss(train/val) 0.39819/0.21294. Took 0.05 sec\n",
            "Epoch 1465, Loss(train/val) 0.40123/0.21301. Took 0.05 sec\n",
            "Epoch 1466, Loss(train/val) 0.40221/0.21307. Took 0.05 sec\n",
            "Epoch 1467, Loss(train/val) 0.40022/0.21320. Took 0.05 sec\n",
            "Epoch 1468, Loss(train/val) 0.39864/0.21273. Took 0.05 sec\n",
            "Epoch 1469, Loss(train/val) 0.39248/0.21254. Took 0.05 sec\n",
            "Epoch 1470, Loss(train/val) 0.40137/0.21240. Took 0.05 sec\n",
            "Epoch 1471, Loss(train/val) 0.39967/0.21270. Took 0.05 sec\n",
            "Epoch 1472, Loss(train/val) 0.39650/0.21321. Took 0.04 sec\n",
            "Epoch 1473, Loss(train/val) 0.39020/0.21310. Took 0.04 sec\n",
            "Epoch 1474, Loss(train/val) 0.40233/0.21334. Took 0.05 sec\n",
            "Epoch 1475, Loss(train/val) 0.39260/0.21231. Took 0.05 sec\n",
            "Epoch 1476, Loss(train/val) 0.39146/0.21200. Took 0.05 sec\n",
            "Epoch 1477, Loss(train/val) 0.40900/0.21142. Took 0.04 sec\n",
            "Epoch 1478, Loss(train/val) 0.40134/0.21164. Took 0.05 sec\n",
            "Epoch 1479, Loss(train/val) 0.39862/0.21124. Took 0.05 sec\n",
            "Epoch 1480, Loss(train/val) 0.38996/0.21111. Took 0.04 sec\n",
            "Epoch 1481, Loss(train/val) 0.40021/0.21113. Took 0.05 sec\n",
            "Epoch 1482, Loss(train/val) 0.39654/0.21092. Took 0.06 sec\n",
            "Epoch 1483, Loss(train/val) 0.39379/0.21071. Took 0.05 sec\n",
            "Epoch 1484, Loss(train/val) 0.39604/0.21091. Took 0.04 sec\n",
            "Epoch 1485, Loss(train/val) 0.39495/0.21105. Took 0.05 sec\n",
            "Epoch 1486, Loss(train/val) 0.39492/0.21096. Took 0.05 sec\n",
            "Epoch 1487, Loss(train/val) 0.39551/0.21092. Took 0.05 sec\n",
            "Epoch 1488, Loss(train/val) 0.39793/0.21097. Took 0.04 sec\n",
            "Epoch 1489, Loss(train/val) 0.39216/0.21054. Took 0.05 sec\n",
            "Epoch 1490, Loss(train/val) 0.39182/0.21034. Took 0.05 sec\n",
            "Epoch 1491, Loss(train/val) 0.38738/0.21023. Took 0.05 sec\n",
            "Epoch 1492, Loss(train/val) 0.39417/0.21023. Took 0.04 sec\n",
            "Epoch 1493, Loss(train/val) 0.39461/0.21040. Took 0.04 sec\n",
            "Epoch 1494, Loss(train/val) 0.39814/0.21085. Took 0.04 sec\n",
            "Epoch 1495, Loss(train/val) 0.39623/0.21108. Took 0.05 sec\n",
            "Epoch 1496, Loss(train/val) 0.39117/0.21074. Took 0.05 sec\n",
            "Epoch 1497, Loss(train/val) 0.39546/0.21075. Took 0.04 sec\n",
            "Epoch 1498, Loss(train/val) 0.39714/0.21052. Took 0.05 sec\n",
            "Epoch 1499, Loss(train/val) 0.38666/0.21009. Took 0.04 sec\n",
            "Epoch 1500, Loss(train/val) 0.39666/0.20997. Took 0.05 sec\n",
            "Epoch 1501, Loss(train/val) 0.37820/0.20997. Took 0.05 sec\n",
            "Epoch 1502, Loss(train/val) 0.39201/0.20938. Took 0.05 sec\n",
            "Epoch 1503, Loss(train/val) 0.39148/0.20898. Took 0.04 sec\n",
            "Epoch 1504, Loss(train/val) 0.39337/0.20888. Took 0.05 sec\n",
            "Epoch 1505, Loss(train/val) 0.39451/0.20923. Took 0.05 sec\n",
            "Epoch 1506, Loss(train/val) 0.39116/0.20916. Took 0.05 sec\n",
            "Epoch 1507, Loss(train/val) 0.38592/0.20934. Took 0.05 sec\n",
            "Epoch 1508, Loss(train/val) 0.40012/0.20910. Took 0.05 sec\n",
            "Epoch 1509, Loss(train/val) 0.39213/0.20846. Took 0.05 sec\n",
            "Epoch 1510, Loss(train/val) 0.39474/0.20856. Took 0.06 sec\n",
            "Epoch 1511, Loss(train/val) 0.38560/0.20830. Took 0.05 sec\n",
            "Epoch 1512, Loss(train/val) 0.39369/0.20861. Took 0.05 sec\n",
            "Epoch 1513, Loss(train/val) 0.38443/0.20837. Took 0.04 sec\n",
            "Epoch 1514, Loss(train/val) 0.39638/0.20826. Took 0.04 sec\n",
            "Epoch 1515, Loss(train/val) 0.39162/0.20816. Took 0.05 sec\n",
            "Epoch 1516, Loss(train/val) 0.39404/0.20858. Took 0.05 sec\n",
            "Epoch 1517, Loss(train/val) 0.39482/0.20803. Took 0.04 sec\n",
            "Epoch 1518, Loss(train/val) 0.37925/0.20861. Took 0.05 sec\n",
            "Epoch 1519, Loss(train/val) 0.39061/0.20825. Took 0.05 sec\n",
            "Epoch 1520, Loss(train/val) 0.38707/0.20834. Took 0.05 sec\n",
            "Epoch 1521, Loss(train/val) 0.38584/0.20785. Took 0.05 sec\n",
            "Epoch 1522, Loss(train/val) 0.39149/0.20762. Took 0.04 sec\n",
            "Epoch 1523, Loss(train/val) 0.40042/0.20782. Took 0.04 sec\n",
            "Epoch 1524, Loss(train/val) 0.38272/0.20797. Took 0.05 sec\n",
            "Epoch 1525, Loss(train/val) 0.38135/0.20837. Took 0.05 sec\n",
            "Epoch 1526, Loss(train/val) 0.38438/0.20735. Took 0.05 sec\n",
            "Epoch 1527, Loss(train/val) 0.39084/0.20687. Took 0.05 sec\n",
            "Epoch 1528, Loss(train/val) 0.38914/0.20679. Took 0.05 sec\n",
            "Epoch 1529, Loss(train/val) 0.38662/0.20658. Took 0.05 sec\n",
            "Epoch 1530, Loss(train/val) 0.38708/0.20652. Took 0.05 sec\n",
            "Epoch 1531, Loss(train/val) 0.37571/0.20676. Took 0.06 sec\n",
            "Epoch 1532, Loss(train/val) 0.38402/0.20673. Took 0.05 sec\n",
            "Epoch 1533, Loss(train/val) 0.39419/0.20662. Took 0.05 sec\n",
            "Epoch 1534, Loss(train/val) 0.38523/0.20685. Took 0.04 sec\n",
            "Epoch 1535, Loss(train/val) 0.38440/0.20680. Took 0.05 sec\n",
            "Epoch 1536, Loss(train/val) 0.38434/0.20685. Took 0.05 sec\n",
            "Epoch 1537, Loss(train/val) 0.38868/0.20713. Took 0.05 sec\n",
            "Epoch 1538, Loss(train/val) 0.39127/0.20724. Took 0.04 sec\n",
            "Epoch 1539, Loss(train/val) 0.38635/0.20656. Took 0.04 sec\n",
            "Epoch 1540, Loss(train/val) 0.39126/0.20654. Took 0.05 sec\n",
            "Epoch 1541, Loss(train/val) 0.38557/0.20604. Took 0.06 sec\n",
            "Epoch 1542, Loss(train/val) 0.38757/0.20585. Took 0.05 sec\n",
            "Epoch 1543, Loss(train/val) 0.38521/0.20605. Took 0.05 sec\n",
            "Epoch 1544, Loss(train/val) 0.38759/0.20646. Took 0.05 sec\n",
            "Epoch 1545, Loss(train/val) 0.38095/0.20627. Took 0.05 sec\n",
            "Epoch 1546, Loss(train/val) 0.38873/0.20643. Took 0.06 sec\n",
            "Epoch 1547, Loss(train/val) 0.37825/0.20668. Took 0.05 sec\n",
            "Epoch 1548, Loss(train/val) 0.38368/0.20655. Took 0.05 sec\n",
            "Epoch 1549, Loss(train/val) 0.38464/0.20639. Took 0.05 sec\n",
            "Epoch 1550, Loss(train/val) 0.38238/0.20633. Took 0.05 sec\n",
            "Epoch 1551, Loss(train/val) 0.37808/0.20607. Took 0.05 sec\n",
            "Epoch 1552, Loss(train/val) 0.38567/0.20587. Took 0.05 sec\n",
            "Epoch 1553, Loss(train/val) 0.38008/0.20505. Took 0.04 sec\n",
            "Epoch 1554, Loss(train/val) 0.38672/0.20437. Took 0.04 sec\n",
            "Epoch 1555, Loss(train/val) 0.39052/0.20373. Took 0.04 sec\n",
            "Epoch 1556, Loss(train/val) 0.38542/0.20405. Took 0.05 sec\n",
            "Epoch 1557, Loss(train/val) 0.38307/0.20375. Took 0.04 sec\n",
            "Epoch 1558, Loss(train/val) 0.38034/0.20387. Took 0.05 sec\n",
            "Epoch 1559, Loss(train/val) 0.38218/0.20386. Took 0.04 sec\n",
            "Epoch 1560, Loss(train/val) 0.38791/0.20345. Took 0.04 sec\n",
            "Epoch 1561, Loss(train/val) 0.38966/0.20347. Took 0.05 sec\n",
            "Epoch 1562, Loss(train/val) 0.37068/0.20321. Took 0.04 sec\n",
            "Epoch 1563, Loss(train/val) 0.37994/0.20335. Took 0.04 sec\n",
            "Epoch 1564, Loss(train/val) 0.38040/0.20351. Took 0.05 sec\n",
            "Epoch 1565, Loss(train/val) 0.38231/0.20330. Took 0.04 sec\n",
            "Epoch 1566, Loss(train/val) 0.38230/0.20298. Took 0.05 sec\n",
            "Epoch 1567, Loss(train/val) 0.39242/0.20318. Took 0.05 sec\n",
            "Epoch 1568, Loss(train/val) 0.38413/0.20308. Took 0.05 sec\n",
            "Epoch 1569, Loss(train/val) 0.37701/0.20296. Took 0.04 sec\n",
            "Epoch 1570, Loss(train/val) 0.38460/0.20280. Took 0.05 sec\n",
            "Epoch 1571, Loss(train/val) 0.38697/0.20275. Took 0.05 sec\n",
            "Epoch 1572, Loss(train/val) 0.38792/0.20277. Took 0.04 sec\n",
            "Epoch 1573, Loss(train/val) 0.38448/0.20241. Took 0.05 sec\n",
            "Epoch 1574, Loss(train/val) 0.37913/0.20203. Took 0.05 sec\n",
            "Epoch 1575, Loss(train/val) 0.37944/0.20185. Took 0.05 sec\n",
            "Epoch 1576, Loss(train/val) 0.38015/0.20154. Took 0.05 sec\n",
            "Epoch 1577, Loss(train/val) 0.37872/0.20153. Took 0.05 sec\n",
            "Epoch 1578, Loss(train/val) 0.38366/0.20173. Took 0.05 sec\n",
            "Epoch 1579, Loss(train/val) 0.37390/0.20188. Took 0.05 sec\n",
            "Epoch 1580, Loss(train/val) 0.38652/0.20159. Took 0.05 sec\n",
            "Epoch 1581, Loss(train/val) 0.38165/0.20140. Took 0.05 sec\n",
            "Epoch 1582, Loss(train/val) 0.38020/0.20110. Took 0.05 sec\n",
            "Epoch 1583, Loss(train/val) 0.37338/0.20104. Took 0.05 sec\n",
            "Epoch 1584, Loss(train/val) 0.37644/0.20119. Took 0.05 sec\n",
            "Epoch 1585, Loss(train/val) 0.37285/0.20067. Took 0.05 sec\n",
            "Epoch 1586, Loss(train/val) 0.37865/0.20068. Took 0.05 sec\n",
            "Epoch 1587, Loss(train/val) 0.37106/0.20080. Took 0.05 sec\n",
            "Epoch 1588, Loss(train/val) 0.38552/0.20073. Took 0.05 sec\n",
            "Epoch 1589, Loss(train/val) 0.37349/0.20034. Took 0.05 sec\n",
            "Epoch 1590, Loss(train/val) 0.37290/0.20032. Took 0.05 sec\n",
            "Epoch 1591, Loss(train/val) 0.36969/0.20044. Took 0.05 sec\n",
            "Epoch 1592, Loss(train/val) 0.36905/0.20079. Took 0.05 sec\n",
            "Epoch 1593, Loss(train/val) 0.37514/0.20044. Took 0.05 sec\n",
            "Epoch 1594, Loss(train/val) 0.37306/0.20017. Took 0.05 sec\n",
            "Epoch 1595, Loss(train/val) 0.38300/0.19966. Took 0.04 sec\n",
            "Epoch 1596, Loss(train/val) 0.37498/0.19941. Took 0.05 sec\n",
            "Epoch 1597, Loss(train/val) 0.36963/0.19929. Took 0.04 sec\n",
            "Epoch 1598, Loss(train/val) 0.37358/0.19921. Took 0.04 sec\n",
            "Epoch 1599, Loss(train/val) 0.37643/0.19946. Took 0.05 sec\n",
            "Epoch 1600, Loss(train/val) 0.37150/0.19917. Took 0.05 sec\n",
            "Epoch 1601, Loss(train/val) 0.37530/0.19901. Took 0.05 sec\n",
            "Epoch 1602, Loss(train/val) 0.36460/0.19868. Took 0.05 sec\n",
            "Epoch 1603, Loss(train/val) 0.38067/0.19857. Took 0.04 sec\n",
            "Epoch 1604, Loss(train/val) 0.37294/0.19861. Took 0.05 sec\n",
            "Epoch 1605, Loss(train/val) 0.37205/0.19853. Took 0.05 sec\n",
            "Epoch 1606, Loss(train/val) 0.37927/0.19823. Took 0.05 sec\n",
            "Epoch 1607, Loss(train/val) 0.37462/0.19811. Took 0.05 sec\n",
            "Epoch 1608, Loss(train/val) 0.37559/0.19734. Took 0.05 sec\n",
            "Epoch 1609, Loss(train/val) 0.36581/0.19660. Took 0.05 sec\n",
            "Epoch 1610, Loss(train/val) 0.37110/0.19650. Took 0.07 sec\n",
            "Epoch 1611, Loss(train/val) 0.38845/0.19652. Took 0.05 sec\n",
            "Epoch 1612, Loss(train/val) 0.36438/0.19649. Took 0.05 sec\n",
            "Epoch 1613, Loss(train/val) 0.37116/0.19660. Took 0.04 sec\n",
            "Epoch 1614, Loss(train/val) 0.37092/0.19614. Took 0.05 sec\n",
            "Epoch 1615, Loss(train/val) 0.37471/0.19657. Took 0.05 sec\n",
            "Epoch 1616, Loss(train/val) 0.36644/0.19618. Took 0.04 sec\n",
            "Epoch 1617, Loss(train/val) 0.37319/0.19612. Took 0.04 sec\n",
            "Epoch 1618, Loss(train/val) 0.36673/0.19625. Took 0.05 sec\n",
            "Epoch 1619, Loss(train/val) 0.38061/0.19563. Took 0.04 sec\n",
            "Epoch 1620, Loss(train/val) 0.37001/0.19497. Took 0.05 sec\n",
            "Epoch 1621, Loss(train/val) 0.37967/0.19483. Took 0.04 sec\n",
            "Epoch 1622, Loss(train/val) 0.37074/0.19468. Took 0.04 sec\n",
            "Epoch 1623, Loss(train/val) 0.36084/0.19460. Took 0.04 sec\n",
            "Epoch 1624, Loss(train/val) 0.37151/0.19442. Took 0.04 sec\n",
            "Epoch 1625, Loss(train/val) 0.37027/0.19437. Took 0.05 sec\n",
            "Epoch 1626, Loss(train/val) 0.37194/0.19419. Took 0.04 sec\n",
            "Epoch 1627, Loss(train/val) 0.37822/0.19406. Took 0.05 sec\n",
            "Epoch 1628, Loss(train/val) 0.36557/0.19430. Took 0.04 sec\n",
            "Epoch 1629, Loss(train/val) 0.36695/0.19413. Took 0.04 sec\n",
            "Epoch 1630, Loss(train/val) 0.36943/0.19433. Took 0.05 sec\n",
            "Epoch 1631, Loss(train/val) 0.36613/0.19439. Took 0.04 sec\n",
            "Epoch 1632, Loss(train/val) 0.36632/0.19411. Took 0.05 sec\n",
            "Epoch 1633, Loss(train/val) 0.37624/0.19370. Took 0.04 sec\n",
            "Epoch 1634, Loss(train/val) 0.37256/0.19313. Took 0.05 sec\n",
            "Epoch 1635, Loss(train/val) 0.36371/0.19306. Took 0.06 sec\n",
            "Epoch 1636, Loss(train/val) 0.36013/0.19284. Took 0.05 sec\n",
            "Epoch 1637, Loss(train/val) 0.35751/0.19265. Took 0.04 sec\n",
            "Epoch 1638, Loss(train/val) 0.36975/0.19283. Took 0.05 sec\n",
            "Epoch 1639, Loss(train/val) 0.35821/0.19253. Took 0.04 sec\n",
            "Epoch 1640, Loss(train/val) 0.36485/0.19216. Took 0.05 sec\n",
            "Epoch 1641, Loss(train/val) 0.36438/0.19174. Took 0.04 sec\n",
            "Epoch 1642, Loss(train/val) 0.36610/0.19166. Took 0.04 sec\n",
            "Epoch 1643, Loss(train/val) 0.36166/0.19143. Took 0.05 sec\n",
            "Epoch 1644, Loss(train/val) 0.35715/0.19149. Took 0.04 sec\n",
            "Epoch 1645, Loss(train/val) 0.36405/0.19114. Took 0.05 sec\n",
            "Epoch 1646, Loss(train/val) 0.36118/0.19130. Took 0.05 sec\n",
            "Epoch 1647, Loss(train/val) 0.36947/0.19113. Took 0.04 sec\n",
            "Epoch 1648, Loss(train/val) 0.36386/0.19120. Took 0.04 sec\n",
            "Epoch 1649, Loss(train/val) 0.36010/0.19116. Took 0.04 sec\n",
            "Epoch 1650, Loss(train/val) 0.36742/0.19121. Took 0.05 sec\n",
            "Epoch 1651, Loss(train/val) 0.36177/0.19090. Took 0.04 sec\n",
            "Epoch 1652, Loss(train/val) 0.36300/0.19083. Took 0.04 sec\n",
            "Epoch 1653, Loss(train/val) 0.35458/0.19073. Took 0.05 sec\n",
            "Epoch 1654, Loss(train/val) 0.35459/0.19111. Took 0.05 sec\n",
            "Epoch 1655, Loss(train/val) 0.35665/0.19125. Took 0.05 sec\n",
            "Epoch 1656, Loss(train/val) 0.35421/0.19137. Took 0.05 sec\n",
            "Epoch 1657, Loss(train/val) 0.36758/0.19090. Took 0.06 sec\n",
            "Epoch 1658, Loss(train/val) 0.35604/0.19098. Took 0.04 sec\n",
            "Epoch 1659, Loss(train/val) 0.35964/0.19037. Took 0.05 sec\n",
            "Epoch 1660, Loss(train/val) 0.36552/0.18964. Took 0.06 sec\n",
            "Epoch 1661, Loss(train/val) 0.34859/0.18930. Took 0.04 sec\n",
            "Epoch 1662, Loss(train/val) 0.35320/0.18936. Took 0.04 sec\n",
            "Epoch 1663, Loss(train/val) 0.36013/0.18869. Took 0.05 sec\n",
            "Epoch 1664, Loss(train/val) 0.35702/0.18847. Took 0.04 sec\n",
            "Epoch 1665, Loss(train/val) 0.35994/0.18806. Took 0.05 sec\n",
            "Epoch 1666, Loss(train/val) 0.35425/0.18760. Took 0.05 sec\n",
            "Epoch 1667, Loss(train/val) 0.35405/0.18759. Took 0.05 sec\n",
            "Epoch 1668, Loss(train/val) 0.36465/0.18718. Took 0.06 sec\n",
            "Epoch 1669, Loss(train/val) 0.35882/0.18694. Took 0.05 sec\n",
            "Epoch 1670, Loss(train/val) 0.36447/0.18688. Took 0.05 sec\n",
            "Epoch 1671, Loss(train/val) 0.35349/0.18691. Took 0.05 sec\n",
            "Epoch 1672, Loss(train/val) 0.35324/0.18688. Took 0.05 sec\n",
            "Epoch 1673, Loss(train/val) 0.35351/0.18667. Took 0.04 sec\n",
            "Epoch 1674, Loss(train/val) 0.35737/0.18661. Took 0.05 sec\n",
            "Epoch 1675, Loss(train/val) 0.36199/0.18630. Took 0.05 sec\n",
            "Epoch 1676, Loss(train/val) 0.36506/0.18619. Took 0.04 sec\n",
            "Epoch 1677, Loss(train/val) 0.35827/0.18572. Took 0.04 sec\n",
            "Epoch 1678, Loss(train/val) 0.35784/0.18550. Took 0.05 sec\n",
            "Epoch 1679, Loss(train/val) 0.35775/0.18529. Took 0.05 sec\n",
            "Epoch 1680, Loss(train/val) 0.34829/0.18503. Took 0.05 sec\n",
            "Epoch 1681, Loss(train/val) 0.35691/0.18519. Took 0.04 sec\n",
            "Epoch 1682, Loss(train/val) 0.35188/0.18490. Took 0.04 sec\n",
            "Epoch 1683, Loss(train/val) 0.35281/0.18496. Took 0.05 sec\n",
            "Epoch 1684, Loss(train/val) 0.35148/0.18493. Took 0.04 sec\n",
            "Epoch 1685, Loss(train/val) 0.35446/0.18488. Took 0.05 sec\n",
            "Epoch 1686, Loss(train/val) 0.35223/0.18463. Took 0.04 sec\n",
            "Epoch 1687, Loss(train/val) 0.35010/0.18441. Took 0.05 sec\n",
            "Epoch 1688, Loss(train/val) 0.34546/0.18448. Took 0.05 sec\n",
            "Epoch 1689, Loss(train/val) 0.37054/0.18391. Took 0.05 sec\n",
            "Epoch 1690, Loss(train/val) 0.34744/0.18355. Took 0.05 sec\n",
            "Epoch 1691, Loss(train/val) 0.35335/0.18337. Took 0.05 sec\n",
            "Epoch 1692, Loss(train/val) 0.35202/0.18315. Took 0.05 sec\n",
            "Epoch 1693, Loss(train/val) 0.34975/0.18293. Took 0.05 sec\n",
            "Epoch 1694, Loss(train/val) 0.35737/0.18252. Took 0.04 sec\n",
            "Epoch 1695, Loss(train/val) 0.35226/0.18239. Took 0.05 sec\n",
            "Epoch 1696, Loss(train/val) 0.35366/0.18243. Took 0.04 sec\n",
            "Epoch 1697, Loss(train/val) 0.34848/0.18249. Took 0.05 sec\n",
            "Epoch 1698, Loss(train/val) 0.35240/0.18256. Took 0.04 sec\n",
            "Epoch 1699, Loss(train/val) 0.35483/0.18275. Took 0.05 sec\n",
            "Epoch 1700, Loss(train/val) 0.34976/0.18222. Took 0.06 sec\n",
            "Epoch 1701, Loss(train/val) 0.34941/0.18204. Took 0.05 sec\n",
            "Epoch 1702, Loss(train/val) 0.34626/0.18136. Took 0.04 sec\n",
            "Epoch 1703, Loss(train/val) 0.33620/0.18128. Took 0.05 sec\n",
            "Epoch 1704, Loss(train/val) 0.34682/0.18128. Took 0.05 sec\n",
            "Epoch 1705, Loss(train/val) 0.35937/0.18105. Took 0.05 sec\n",
            "Epoch 1706, Loss(train/val) 0.35189/0.18064. Took 0.05 sec\n",
            "Epoch 1707, Loss(train/val) 0.34609/0.18076. Took 0.04 sec\n",
            "Epoch 1708, Loss(train/val) 0.34990/0.18051. Took 0.05 sec\n",
            "Epoch 1709, Loss(train/val) 0.34871/0.18062. Took 0.05 sec\n",
            "Epoch 1710, Loss(train/val) 0.35365/0.18043. Took 0.06 sec\n",
            "Epoch 1711, Loss(train/val) 0.35458/0.17997. Took 0.05 sec\n",
            "Epoch 1712, Loss(train/val) 0.34648/0.17988. Took 0.05 sec\n",
            "Epoch 1713, Loss(train/val) 0.34701/0.17961. Took 0.04 sec\n",
            "Epoch 1714, Loss(train/val) 0.34433/0.17947. Took 0.04 sec\n",
            "Epoch 1715, Loss(train/val) 0.34342/0.17925. Took 0.05 sec\n",
            "Epoch 1716, Loss(train/val) 0.34241/0.17913. Took 0.05 sec\n",
            "Epoch 1717, Loss(train/val) 0.34637/0.17913. Took 0.05 sec\n",
            "Epoch 1718, Loss(train/val) 0.34159/0.17899. Took 0.04 sec\n",
            "Epoch 1719, Loss(train/val) 0.33716/0.17902. Took 0.04 sec\n",
            "Epoch 1720, Loss(train/val) 0.34128/0.17883. Took 0.06 sec\n",
            "Epoch 1721, Loss(train/val) 0.33785/0.17879. Took 0.05 sec\n",
            "Epoch 1722, Loss(train/val) 0.34164/0.17848. Took 0.04 sec\n",
            "Epoch 1723, Loss(train/val) 0.35193/0.17823. Took 0.05 sec\n",
            "Epoch 1724, Loss(train/val) 0.34508/0.17784. Took 0.04 sec\n",
            "Epoch 1725, Loss(train/val) 0.35084/0.17794. Took 0.05 sec\n",
            "Epoch 1726, Loss(train/val) 0.33561/0.17828. Took 0.05 sec\n",
            "Epoch 1727, Loss(train/val) 0.34273/0.17836. Took 0.05 sec\n",
            "Epoch 1728, Loss(train/val) 0.34098/0.17792. Took 0.04 sec\n",
            "Epoch 1729, Loss(train/val) 0.33987/0.17694. Took 0.05 sec\n",
            "Epoch 1730, Loss(train/val) 0.34553/0.17651. Took 0.05 sec\n",
            "Epoch 1731, Loss(train/val) 0.33939/0.17661. Took 0.05 sec\n",
            "Epoch 1732, Loss(train/val) 0.33678/0.17627. Took 0.04 sec\n",
            "Epoch 1733, Loss(train/val) 0.32982/0.17629. Took 0.04 sec\n",
            "Epoch 1734, Loss(train/val) 0.34287/0.17628. Took 0.04 sec\n",
            "Epoch 1735, Loss(train/val) 0.34205/0.17591. Took 0.05 sec\n",
            "Epoch 1736, Loss(train/val) 0.33590/0.17574. Took 0.04 sec\n",
            "Epoch 1737, Loss(train/val) 0.33156/0.17580. Took 0.04 sec\n",
            "Epoch 1738, Loss(train/val) 0.33465/0.17569. Took 0.05 sec\n",
            "Epoch 1739, Loss(train/val) 0.32954/0.17599. Took 0.04 sec\n",
            "Epoch 1740, Loss(train/val) 0.32500/0.17682. Took 0.06 sec\n",
            "Epoch 1741, Loss(train/val) 0.34116/0.17615. Took 0.05 sec\n",
            "Epoch 1742, Loss(train/val) 0.33169/0.17641. Took 0.04 sec\n",
            "Epoch 1743, Loss(train/val) 0.33115/0.17588. Took 0.05 sec\n",
            "Epoch 1744, Loss(train/val) 0.34183/0.17530. Took 0.04 sec\n",
            "Epoch 1745, Loss(train/val) 0.34848/0.17515. Took 0.05 sec\n",
            "Epoch 1746, Loss(train/val) 0.34185/0.17530. Took 0.04 sec\n",
            "Epoch 1747, Loss(train/val) 0.33488/0.17521. Took 0.04 sec\n",
            "Epoch 1748, Loss(train/val) 0.34321/0.17462. Took 0.05 sec\n",
            "Epoch 1749, Loss(train/val) 0.34030/0.17470. Took 0.05 sec\n",
            "Epoch 1750, Loss(train/val) 0.35999/0.17435. Took 0.05 sec\n",
            "Epoch 1751, Loss(train/val) 0.31984/0.17464. Took 0.05 sec\n",
            "Epoch 1752, Loss(train/val) 0.34191/0.17583. Took 0.04 sec\n",
            "Epoch 1753, Loss(train/val) 0.33471/0.17570. Took 0.05 sec\n",
            "Epoch 1754, Loss(train/val) 0.34181/0.17584. Took 0.05 sec\n",
            "Epoch 1755, Loss(train/val) 0.32913/0.17573. Took 0.05 sec\n",
            "Epoch 1756, Loss(train/val) 0.33308/0.17468. Took 0.05 sec\n",
            "Epoch 1757, Loss(train/val) 0.32836/0.17360. Took 0.04 sec\n",
            "Epoch 1758, Loss(train/val) 0.33809/0.17316. Took 0.04 sec\n",
            "Epoch 1759, Loss(train/val) 0.33285/0.17302. Took 0.05 sec\n",
            "Epoch 1760, Loss(train/val) 0.33331/0.17255. Took 0.05 sec\n",
            "Epoch 1761, Loss(train/val) 0.33301/0.17244. Took 0.05 sec\n",
            "Epoch 1762, Loss(train/val) 0.33376/0.17252. Took 0.05 sec\n",
            "Epoch 1763, Loss(train/val) 0.33155/0.17255. Took 0.04 sec\n",
            "Epoch 1764, Loss(train/val) 0.33224/0.17269. Took 0.04 sec\n",
            "Epoch 1765, Loss(train/val) 0.32938/0.17253. Took 0.06 sec\n",
            "Epoch 1766, Loss(train/val) 0.34216/0.17250. Took 0.04 sec\n",
            "Epoch 1767, Loss(train/val) 0.33776/0.17197. Took 0.05 sec\n",
            "Epoch 1768, Loss(train/val) 0.32445/0.17165. Took 0.04 sec\n",
            "Epoch 1769, Loss(train/val) 0.33118/0.17171. Took 0.04 sec\n",
            "Epoch 1770, Loss(train/val) 0.33575/0.17170. Took 0.05 sec\n",
            "Epoch 1771, Loss(train/val) 0.33926/0.17265. Took 0.05 sec\n",
            "Epoch 1772, Loss(train/val) 0.32943/0.17355. Took 0.05 sec\n",
            "Epoch 1773, Loss(train/val) 0.33693/0.17336. Took 0.05 sec\n",
            "Epoch 1774, Loss(train/val) 0.32008/0.17279. Took 0.05 sec\n",
            "Epoch 1775, Loss(train/val) 0.33966/0.17201. Took 0.05 sec\n",
            "Epoch 1776, Loss(train/val) 0.33702/0.17203. Took 0.05 sec\n",
            "Epoch 1777, Loss(train/val) 0.32797/0.17204. Took 0.05 sec\n",
            "Epoch 1778, Loss(train/val) 0.32514/0.17209. Took 0.05 sec\n",
            "Epoch 1779, Loss(train/val) 0.33691/0.17215. Took 0.05 sec\n",
            "Epoch 1780, Loss(train/val) 0.33207/0.17164. Took 0.05 sec\n",
            "Epoch 1781, Loss(train/val) 0.33070/0.17135. Took 0.05 sec\n",
            "Epoch 1782, Loss(train/val) 0.32842/0.17206. Took 0.05 sec\n",
            "Epoch 1783, Loss(train/val) 0.33069/0.17189. Took 0.05 sec\n",
            "Epoch 1784, Loss(train/val) 0.33373/0.17214. Took 0.05 sec\n",
            "Epoch 1785, Loss(train/val) 0.32534/0.17132. Took 0.05 sec\n",
            "Epoch 1786, Loss(train/val) 0.32522/0.17149. Took 0.05 sec\n",
            "Epoch 1787, Loss(train/val) 0.33614/0.17242. Took 0.04 sec\n",
            "Epoch 1788, Loss(train/val) 0.33067/0.17233. Took 0.04 sec\n",
            "Epoch 1789, Loss(train/val) 0.33229/0.17330. Took 0.05 sec\n",
            "Epoch 1790, Loss(train/val) 0.32762/0.17304. Took 0.05 sec\n",
            "Epoch 1791, Loss(train/val) 0.32732/0.17351. Took 0.05 sec\n",
            "Epoch 1792, Loss(train/val) 0.34605/0.17282. Took 0.05 sec\n",
            "Epoch 1793, Loss(train/val) 0.32842/0.17248. Took 0.05 sec\n",
            "Epoch 1794, Loss(train/val) 0.32093/0.17204. Took 0.05 sec\n",
            "Epoch 1795, Loss(train/val) 0.33430/0.17115. Took 0.05 sec\n",
            "Epoch 1796, Loss(train/val) 0.33086/0.17114. Took 0.05 sec\n",
            "Epoch 1797, Loss(train/val) 0.32846/0.17062. Took 0.04 sec\n",
            "Epoch 1798, Loss(train/val) 0.31985/0.17023. Took 0.05 sec\n",
            "Epoch 1799, Loss(train/val) 0.32663/0.17018. Took 0.06 sec\n",
            "Epoch 1800, Loss(train/val) 0.32375/0.16979. Took 0.05 sec\n",
            "Epoch 1801, Loss(train/val) 0.32588/0.17005. Took 0.05 sec\n",
            "Epoch 1802, Loss(train/val) 0.32571/0.16971. Took 0.04 sec\n",
            "Epoch 1803, Loss(train/val) 0.32324/0.16951. Took 0.04 sec\n",
            "Epoch 1804, Loss(train/val) 0.33100/0.16971. Took 0.05 sec\n",
            "Epoch 1805, Loss(train/val) 0.33020/0.16947. Took 0.05 sec\n",
            "Epoch 1806, Loss(train/val) 0.32878/0.16921. Took 0.05 sec\n",
            "Epoch 1807, Loss(train/val) 0.32770/0.16916. Took 0.05 sec\n",
            "Epoch 1808, Loss(train/val) 0.32410/0.16956. Took 0.05 sec\n",
            "Epoch 1809, Loss(train/val) 0.31756/0.17025. Took 0.05 sec\n",
            "Epoch 1810, Loss(train/val) 0.31979/0.16975. Took 0.05 sec\n",
            "Epoch 1811, Loss(train/val) 0.32294/0.16999. Took 0.05 sec\n",
            "Epoch 1812, Loss(train/val) 0.31723/0.17160. Took 0.04 sec\n",
            "Epoch 1813, Loss(train/val) 0.32828/0.17121. Took 0.04 sec\n",
            "Epoch 1814, Loss(train/val) 0.32329/0.17059. Took 0.05 sec\n",
            "Epoch 1815, Loss(train/val) 0.31770/0.17131. Took 0.05 sec\n",
            "Epoch 1816, Loss(train/val) 0.31901/0.17104. Took 0.05 sec\n",
            "Epoch 1817, Loss(train/val) 0.32937/0.17081. Took 0.04 sec\n",
            "Epoch 1818, Loss(train/val) 0.32658/0.17099. Took 0.05 sec\n",
            "Epoch 1819, Loss(train/val) 0.32555/0.17083. Took 0.05 sec\n",
            "Epoch 1820, Loss(train/val) 0.31127/0.17248. Took 0.05 sec\n",
            "Epoch 1821, Loss(train/val) 0.33821/0.17433. Took 0.05 sec\n",
            "Epoch 1822, Loss(train/val) 0.32845/0.17311. Took 0.05 sec\n",
            "Epoch 1823, Loss(train/val) 0.31935/0.17326. Took 0.04 sec\n",
            "Epoch 1824, Loss(train/val) 0.32210/0.17146. Took 0.05 sec\n",
            "Epoch 1825, Loss(train/val) 0.31894/0.17029. Took 0.05 sec\n",
            "Epoch 1826, Loss(train/val) 0.31942/0.17013. Took 0.05 sec\n",
            "Epoch 1827, Loss(train/val) 0.32361/0.16978. Took 0.04 sec\n",
            "Epoch 1828, Loss(train/val) 0.33872/0.16964. Took 0.04 sec\n",
            "Epoch 1829, Loss(train/val) 0.32386/0.16906. Took 0.06 sec\n",
            "Epoch 1830, Loss(train/val) 0.32466/0.16925. Took 0.05 sec\n",
            "Epoch 1831, Loss(train/val) 0.32314/0.16833. Took 0.05 sec\n",
            "Epoch 1832, Loss(train/val) 0.31590/0.16862. Took 0.05 sec\n",
            "Epoch 1833, Loss(train/val) 0.32168/0.16892. Took 0.05 sec\n",
            "Epoch 1834, Loss(train/val) 0.31903/0.16884. Took 0.05 sec\n",
            "Epoch 1835, Loss(train/val) 0.31447/0.16946. Took 0.05 sec\n",
            "Epoch 1836, Loss(train/val) 0.33374/0.16914. Took 0.05 sec\n",
            "Epoch 1837, Loss(train/val) 0.32498/0.16976. Took 0.05 sec\n",
            "Epoch 1838, Loss(train/val) 0.31763/0.17024. Took 0.04 sec\n",
            "Epoch 1839, Loss(train/val) 0.32678/0.17122. Took 0.05 sec\n",
            "Epoch 1840, Loss(train/val) 0.31605/0.16952. Took 0.05 sec\n",
            "Epoch 1841, Loss(train/val) 0.31963/0.16831. Took 0.04 sec\n",
            "Epoch 1842, Loss(train/val) 0.32181/0.16827. Took 0.04 sec\n",
            "Epoch 1843, Loss(train/val) 0.31590/0.16869. Took 0.05 sec\n",
            "Epoch 1844, Loss(train/val) 0.31178/0.16922. Took 0.04 sec\n",
            "Epoch 1845, Loss(train/val) 0.32245/0.16977. Took 0.05 sec\n",
            "Epoch 1846, Loss(train/val) 0.31635/0.16994. Took 0.05 sec\n",
            "Epoch 1847, Loss(train/val) 0.32434/0.16935. Took 0.04 sec\n",
            "Epoch 1848, Loss(train/val) 0.32130/0.16915. Took 0.04 sec\n",
            "Epoch 1849, Loss(train/val) 0.31315/0.16944. Took 0.04 sec\n",
            "Epoch 1850, Loss(train/val) 0.31963/0.16924. Took 0.06 sec\n",
            "Epoch 1851, Loss(train/val) 0.31737/0.16893. Took 0.04 sec\n",
            "Epoch 1852, Loss(train/val) 0.31707/0.16883. Took 0.04 sec\n",
            "Epoch 1853, Loss(train/val) 0.32380/0.16973. Took 0.04 sec\n",
            "Epoch 1854, Loss(train/val) 0.32115/0.17039. Took 0.04 sec\n",
            "Epoch 1855, Loss(train/val) 0.31541/0.17061. Took 0.05 sec\n",
            "Epoch 1856, Loss(train/val) 0.31633/0.17132. Took 0.05 sec\n",
            "Epoch 1857, Loss(train/val) 0.32101/0.17054. Took 0.04 sec\n",
            "Epoch 1858, Loss(train/val) 0.33023/0.16940. Took 0.05 sec\n",
            "Epoch 1859, Loss(train/val) 0.30902/0.16860. Took 0.04 sec\n",
            "Epoch 1860, Loss(train/val) 0.31334/0.16793. Took 0.05 sec\n",
            "Epoch 1861, Loss(train/val) 0.32280/0.16756. Took 0.04 sec\n",
            "Epoch 1862, Loss(train/val) 0.31562/0.16771. Took 0.05 sec\n",
            "Epoch 1863, Loss(train/val) 0.31434/0.16790. Took 0.05 sec\n",
            "Epoch 1864, Loss(train/val) 0.32631/0.16821. Took 0.05 sec\n",
            "Epoch 1865, Loss(train/val) 0.31599/0.16853. Took 0.05 sec\n",
            "Epoch 1866, Loss(train/val) 0.31379/0.16894. Took 0.04 sec\n",
            "Epoch 1867, Loss(train/val) 0.31752/0.16876. Took 0.05 sec\n",
            "Epoch 1868, Loss(train/val) 0.31898/0.16969. Took 0.04 sec\n",
            "Epoch 1869, Loss(train/val) 0.31864/0.16938. Took 0.04 sec\n",
            "Epoch 1870, Loss(train/val) 0.32215/0.17002. Took 0.05 sec\n",
            "Epoch 1871, Loss(train/val) 0.30846/0.17089. Took 0.04 sec\n",
            "Epoch 1872, Loss(train/val) 0.31984/0.17172. Took 0.05 sec\n",
            "Epoch 1873, Loss(train/val) 0.31818/0.17051. Took 0.04 sec\n",
            "Epoch 1874, Loss(train/val) 0.32318/0.16946. Took 0.04 sec\n",
            "Epoch 1875, Loss(train/val) 0.30813/0.16835. Took 0.06 sec\n",
            "Epoch 1876, Loss(train/val) 0.31696/0.16804. Took 0.05 sec\n",
            "Epoch 1877, Loss(train/val) 0.31781/0.16871. Took 0.05 sec\n",
            "Epoch 1878, Loss(train/val) 0.31816/0.16822. Took 0.05 sec\n",
            "Epoch 1879, Loss(train/val) 0.31227/0.16816. Took 0.05 sec\n",
            "Epoch 1880, Loss(train/val) 0.29978/0.16860. Took 0.05 sec\n",
            "Epoch 1881, Loss(train/val) 0.31452/0.16807. Took 0.05 sec\n",
            "Epoch 1882, Loss(train/val) 0.31315/0.16813. Took 0.05 sec\n",
            "Epoch 1883, Loss(train/val) 0.33232/0.16855. Took 0.05 sec\n",
            "Epoch 1884, Loss(train/val) 0.31597/0.16915. Took 0.05 sec\n",
            "Epoch 1885, Loss(train/val) 0.31199/0.16851. Took 0.05 sec\n",
            "Epoch 1886, Loss(train/val) 0.30852/0.16910. Took 0.05 sec\n",
            "Epoch 1887, Loss(train/val) 0.31000/0.16789. Took 0.05 sec\n",
            "Epoch 1888, Loss(train/val) 0.31153/0.16765. Took 0.05 sec\n",
            "Epoch 1889, Loss(train/val) 0.31130/0.16765. Took 0.05 sec\n",
            "Epoch 1890, Loss(train/val) 0.31561/0.16814. Took 0.05 sec\n",
            "Epoch 1891, Loss(train/val) 0.31608/0.16781. Took 0.04 sec\n",
            "Epoch 1892, Loss(train/val) 0.31541/0.16731. Took 0.05 sec\n",
            "Epoch 1893, Loss(train/val) 0.32238/0.16715. Took 0.05 sec\n",
            "Epoch 1894, Loss(train/val) 0.31248/0.16714. Took 0.04 sec\n",
            "Epoch 1895, Loss(train/val) 0.31180/0.16730. Took 0.05 sec\n",
            "Epoch 1896, Loss(train/val) 0.31445/0.16724. Took 0.04 sec\n",
            "Epoch 1897, Loss(train/val) 0.32001/0.16737. Took 0.04 sec\n",
            "Epoch 1898, Loss(train/val) 0.30877/0.16787. Took 0.04 sec\n",
            "Epoch 1899, Loss(train/val) 0.30606/0.16773. Took 0.04 sec\n",
            "Epoch 1900, Loss(train/val) 0.30651/0.16787. Took 0.06 sec\n",
            "Epoch 1901, Loss(train/val) 0.31514/0.16729. Took 0.05 sec\n",
            "Epoch 1902, Loss(train/val) 0.31033/0.16707. Took 0.04 sec\n",
            "Epoch 1903, Loss(train/val) 0.30879/0.16853. Took 0.04 sec\n",
            "Epoch 1904, Loss(train/val) 0.31910/0.16839. Took 0.04 sec\n",
            "Epoch 1905, Loss(train/val) 0.31437/0.16910. Took 0.05 sec\n",
            "Epoch 1906, Loss(train/val) 0.31447/0.16916. Took 0.05 sec\n",
            "Epoch 1907, Loss(train/val) 0.31561/0.16952. Took 0.04 sec\n",
            "Epoch 1908, Loss(train/val) 0.31348/0.16909. Took 0.04 sec\n",
            "Epoch 1909, Loss(train/val) 0.30962/0.16859. Took 0.05 sec\n",
            "Epoch 1910, Loss(train/val) 0.30592/0.16824. Took 0.05 sec\n",
            "Epoch 1911, Loss(train/val) 0.30587/0.16873. Took 0.04 sec\n",
            "Epoch 1912, Loss(train/val) 0.31438/0.16821. Took 0.04 sec\n",
            "Epoch 1913, Loss(train/val) 0.31249/0.16856. Took 0.05 sec\n",
            "Epoch 1914, Loss(train/val) 0.31646/0.16897. Took 0.05 sec\n",
            "Epoch 1915, Loss(train/val) 0.31014/0.16780. Took 0.07 sec\n",
            "Epoch 1916, Loss(train/val) 0.31023/0.16740. Took 0.04 sec\n",
            "Epoch 1917, Loss(train/val) 0.30990/0.16714. Took 0.04 sec\n",
            "Epoch 1918, Loss(train/val) 0.32556/0.16681. Took 0.04 sec\n",
            "Epoch 1919, Loss(train/val) 0.30549/0.16690. Took 0.05 sec\n",
            "Epoch 1920, Loss(train/val) 0.31655/0.16688. Took 0.05 sec\n",
            "Epoch 1921, Loss(train/val) 0.31254/0.16699. Took 0.04 sec\n",
            "Epoch 1922, Loss(train/val) 0.31864/0.16709. Took 0.04 sec\n",
            "Epoch 1923, Loss(train/val) 0.30836/0.16695. Took 0.05 sec\n",
            "Epoch 1924, Loss(train/val) 0.32738/0.16771. Took 0.05 sec\n",
            "Epoch 1925, Loss(train/val) 0.30909/0.16739. Took 0.05 sec\n",
            "Epoch 1926, Loss(train/val) 0.30078/0.16769. Took 0.05 sec\n",
            "Epoch 1927, Loss(train/val) 0.31883/0.16697. Took 0.05 sec\n",
            "Epoch 1928, Loss(train/val) 0.30643/0.16683. Took 0.05 sec\n",
            "Epoch 1929, Loss(train/val) 0.31273/0.16653. Took 0.05 sec\n",
            "Epoch 1930, Loss(train/val) 0.30910/0.16686. Took 0.05 sec\n",
            "Epoch 1931, Loss(train/val) 0.31316/0.16701. Took 0.05 sec\n",
            "Epoch 1932, Loss(train/val) 0.32600/0.16684. Took 0.04 sec\n",
            "Epoch 1933, Loss(train/val) 0.30880/0.16710. Took 0.04 sec\n",
            "Epoch 1934, Loss(train/val) 0.31685/0.16777. Took 0.04 sec\n",
            "Epoch 1935, Loss(train/val) 0.31110/0.16808. Took 0.05 sec\n",
            "Epoch 1936, Loss(train/val) 0.31887/0.16794. Took 0.05 sec\n",
            "Epoch 1937, Loss(train/val) 0.31561/0.16717. Took 0.04 sec\n",
            "Epoch 1938, Loss(train/val) 0.30684/0.16767. Took 0.05 sec\n",
            "Epoch 1939, Loss(train/val) 0.31291/0.16836. Took 0.04 sec\n",
            "Epoch 1940, Loss(train/val) 0.31109/0.16920. Took 0.06 sec\n",
            "Epoch 1941, Loss(train/val) 0.32674/0.17023. Took 0.04 sec\n",
            "Epoch 1942, Loss(train/val) 0.28859/0.16782. Took 0.05 sec\n",
            "Epoch 1943, Loss(train/val) 0.30172/0.16680. Took 0.05 sec\n",
            "Epoch 1944, Loss(train/val) 0.31360/0.16694. Took 0.04 sec\n",
            "Epoch 1945, Loss(train/val) 0.30973/0.16698. Took 0.06 sec\n",
            "Epoch 1946, Loss(train/val) 0.31545/0.16738. Took 0.05 sec\n",
            "Epoch 1947, Loss(train/val) 0.30435/0.16783. Took 0.05 sec\n",
            "Epoch 1948, Loss(train/val) 0.30407/0.16834. Took 0.05 sec\n",
            "Epoch 1949, Loss(train/val) 0.31007/0.16775. Took 0.05 sec\n",
            "Epoch 1950, Loss(train/val) 0.30624/0.16838. Took 0.05 sec\n",
            "Epoch 1951, Loss(train/val) 0.30072/0.16874. Took 0.05 sec\n",
            "Epoch 1952, Loss(train/val) 0.30961/0.16803. Took 0.05 sec\n",
            "Epoch 1953, Loss(train/val) 0.30655/0.16783. Took 0.04 sec\n",
            "Epoch 1954, Loss(train/val) 0.30630/0.16742. Took 0.04 sec\n",
            "Epoch 1955, Loss(train/val) 0.30652/0.16755. Took 0.05 sec\n",
            "Epoch 1956, Loss(train/val) 0.30821/0.16787. Took 0.04 sec\n",
            "Epoch 1957, Loss(train/val) 0.30710/0.16857. Took 0.04 sec\n",
            "Epoch 1958, Loss(train/val) 0.30415/0.16769. Took 0.05 sec\n",
            "Epoch 1959, Loss(train/val) 0.29300/0.16785. Took 0.04 sec\n",
            "Epoch 1960, Loss(train/val) 0.30146/0.16704. Took 0.05 sec\n",
            "Epoch 1961, Loss(train/val) 0.31240/0.16668. Took 0.05 sec\n",
            "Epoch 1962, Loss(train/val) 0.30429/0.16654. Took 0.04 sec\n",
            "Epoch 1963, Loss(train/val) 0.29688/0.16644. Took 0.04 sec\n",
            "Epoch 1964, Loss(train/val) 0.30824/0.16622. Took 0.05 sec\n",
            "Epoch 1965, Loss(train/val) 0.30344/0.16621. Took 0.05 sec\n",
            "Epoch 1966, Loss(train/val) 0.30404/0.16626. Took 0.05 sec\n",
            "Epoch 1967, Loss(train/val) 0.30731/0.16571. Took 0.05 sec\n",
            "Epoch 1968, Loss(train/val) 0.30367/0.16577. Took 0.05 sec\n",
            "Epoch 1969, Loss(train/val) 0.30887/0.16573. Took 0.04 sec\n",
            "Epoch 1970, Loss(train/val) 0.29918/0.16577. Took 0.05 sec\n",
            "Epoch 1971, Loss(train/val) 0.30418/0.16595. Took 0.04 sec\n",
            "Epoch 1972, Loss(train/val) 0.32258/0.16615. Took 0.05 sec\n",
            "Epoch 1973, Loss(train/val) 0.31678/0.16612. Took 0.04 sec\n",
            "Epoch 1974, Loss(train/val) 0.30714/0.16603. Took 0.05 sec\n",
            "Epoch 1975, Loss(train/val) 0.30445/0.16605. Took 0.05 sec\n",
            "Epoch 1976, Loss(train/val) 0.30093/0.16600. Took 0.04 sec\n",
            "Epoch 1977, Loss(train/val) 0.30512/0.16572. Took 0.04 sec\n",
            "Epoch 1978, Loss(train/val) 0.31662/0.16589. Took 0.04 sec\n",
            "Epoch 1979, Loss(train/val) 0.29972/0.16601. Took 0.04 sec\n",
            "Epoch 1980, Loss(train/val) 0.30189/0.16607. Took 0.06 sec\n",
            "Epoch 1981, Loss(train/val) 0.31040/0.16626. Took 0.04 sec\n",
            "Epoch 1982, Loss(train/val) 0.30303/0.16688. Took 0.04 sec\n",
            "Epoch 1983, Loss(train/val) 0.31205/0.16800. Took 0.05 sec\n",
            "Epoch 1984, Loss(train/val) 0.29383/0.16852. Took 0.05 sec\n",
            "Epoch 1985, Loss(train/val) 0.30488/0.16756. Took 0.05 sec\n",
            "Epoch 1986, Loss(train/val) 0.29750/0.16840. Took 0.05 sec\n",
            "Epoch 1987, Loss(train/val) 0.30082/0.16703. Took 0.05 sec\n",
            "Epoch 1988, Loss(train/val) 0.30696/0.16637. Took 0.05 sec\n",
            "Epoch 1989, Loss(train/val) 0.30384/0.16621. Took 0.05 sec\n",
            "Epoch 1990, Loss(train/val) 0.30314/0.16566. Took 0.06 sec\n",
            "Epoch 1991, Loss(train/val) 0.30755/0.16577. Took 0.05 sec\n",
            "Epoch 1992, Loss(train/val) 0.29685/0.16554. Took 0.05 sec\n",
            "Epoch 1993, Loss(train/val) 0.30206/0.16559. Took 0.05 sec\n",
            "Epoch 1994, Loss(train/val) 0.30199/0.16585. Took 0.04 sec\n",
            "Epoch 1995, Loss(train/val) 0.30282/0.16642. Took 0.05 sec\n",
            "Epoch 1996, Loss(train/val) 0.30285/0.16625. Took 0.04 sec\n",
            "Epoch 1997, Loss(train/val) 0.30826/0.16714. Took 0.04 sec\n",
            "Epoch 1998, Loss(train/val) 0.29810/0.16710. Took 0.04 sec\n",
            "Epoch 1999, Loss(train/val) 0.29666/0.16663. Took 0.04 sec\n",
            "Epoch 2000, Loss(train/val) 0.29726/0.16673. Took 0.05 sec\n",
            "Epoch 2001, Loss(train/val) 0.30539/0.16718. Took 0.05 sec\n",
            "Epoch 2002, Loss(train/val) 0.29521/0.16726. Took 0.04 sec\n",
            "Epoch 2003, Loss(train/val) 0.30224/0.16673. Took 0.05 sec\n",
            "Epoch 2004, Loss(train/val) 0.31031/0.16550. Took 0.05 sec\n",
            "Epoch 2005, Loss(train/val) 0.32041/0.16514. Took 0.05 sec\n",
            "Epoch 2006, Loss(train/val) 0.30646/0.16469. Took 0.04 sec\n",
            "Epoch 2007, Loss(train/val) 0.29737/0.16477. Took 0.04 sec\n",
            "Epoch 2008, Loss(train/val) 0.30456/0.16466. Took 0.05 sec\n",
            "Epoch 2009, Loss(train/val) 0.29763/0.16489. Took 0.05 sec\n",
            "Epoch 2010, Loss(train/val) 0.29583/0.16509. Took 0.05 sec\n",
            "Epoch 2011, Loss(train/val) 0.29772/0.16500. Took 0.04 sec\n",
            "Epoch 2012, Loss(train/val) 0.29561/0.16536. Took 0.05 sec\n",
            "Epoch 2013, Loss(train/val) 0.29034/0.16569. Took 0.04 sec\n",
            "Epoch 2014, Loss(train/val) 0.30053/0.16591. Took 0.05 sec\n",
            "Epoch 2015, Loss(train/val) 0.29669/0.16598. Took 0.05 sec\n",
            "Epoch 2016, Loss(train/val) 0.29727/0.16562. Took 0.05 sec\n",
            "Epoch 2017, Loss(train/val) 0.29069/0.16544. Took 0.04 sec\n",
            "Epoch 2018, Loss(train/val) 0.31410/0.16531. Took 0.04 sec\n",
            "Epoch 2019, Loss(train/val) 0.30432/0.16524. Took 0.04 sec\n",
            "Epoch 2020, Loss(train/val) 0.29672/0.16548. Took 0.05 sec\n",
            "Epoch 2021, Loss(train/val) 0.29398/0.16697. Took 0.05 sec\n",
            "Epoch 2022, Loss(train/val) 0.30082/0.16751. Took 0.04 sec\n",
            "Epoch 2023, Loss(train/val) 0.31344/0.16661. Took 0.05 sec\n",
            "Epoch 2024, Loss(train/val) 0.30534/0.16636. Took 0.04 sec\n",
            "Epoch 2025, Loss(train/val) 0.29351/0.16650. Took 0.06 sec\n",
            "Epoch 2026, Loss(train/val) 0.30780/0.16571. Took 0.05 sec\n",
            "Epoch 2027, Loss(train/val) 0.30870/0.16540. Took 0.04 sec\n",
            "Epoch 2028, Loss(train/val) 0.29340/0.16577. Took 0.05 sec\n",
            "Epoch 2029, Loss(train/val) 0.29579/0.16583. Took 0.04 sec\n",
            "Epoch 2030, Loss(train/val) 0.29432/0.16574. Took 0.05 sec\n",
            "Epoch 2031, Loss(train/val) 0.29894/0.16565. Took 0.04 sec\n",
            "Epoch 2032, Loss(train/val) 0.29679/0.16560. Took 0.04 sec\n",
            "Epoch 2033, Loss(train/val) 0.29746/0.16559. Took 0.05 sec\n",
            "Epoch 2034, Loss(train/val) 0.30178/0.16593. Took 0.04 sec\n",
            "Epoch 2035, Loss(train/val) 0.30069/0.16619. Took 0.05 sec\n",
            "Epoch 2036, Loss(train/val) 0.30401/0.16570. Took 0.05 sec\n",
            "Epoch 2037, Loss(train/val) 0.30398/0.16523. Took 0.05 sec\n",
            "Epoch 2038, Loss(train/val) 0.29687/0.16518. Took 0.05 sec\n",
            "Epoch 2039, Loss(train/val) 0.29323/0.16492. Took 0.05 sec\n",
            "Epoch 2040, Loss(train/val) 0.29351/0.16487. Took 0.06 sec\n",
            "Epoch 2041, Loss(train/val) 0.29369/0.16491. Took 0.05 sec\n",
            "Epoch 2042, Loss(train/val) 0.29906/0.16471. Took 0.04 sec\n",
            "Epoch 2043, Loss(train/val) 0.29826/0.16457. Took 0.04 sec\n",
            "Epoch 2044, Loss(train/val) 0.30956/0.16432. Took 0.05 sec\n",
            "Epoch 2045, Loss(train/val) 0.29978/0.16436. Took 0.06 sec\n",
            "Epoch 2046, Loss(train/val) 0.30475/0.16462. Took 0.05 sec\n",
            "Epoch 2047, Loss(train/val) 0.31233/0.16472. Took 0.05 sec\n",
            "Epoch 2048, Loss(train/val) 0.29923/0.16413. Took 0.05 sec\n",
            "Epoch 2049, Loss(train/val) 0.29575/0.16413. Took 0.05 sec\n",
            "Epoch 2050, Loss(train/val) 0.29590/0.16390. Took 0.07 sec\n",
            "Epoch 2051, Loss(train/val) 0.29306/0.16389. Took 0.05 sec\n",
            "Epoch 2052, Loss(train/val) 0.29910/0.16392. Took 0.05 sec\n",
            "Epoch 2053, Loss(train/val) 0.30099/0.16417. Took 0.05 sec\n",
            "Epoch 2054, Loss(train/val) 0.29781/0.16459. Took 0.05 sec\n",
            "Epoch 2055, Loss(train/val) 0.30662/0.16501. Took 0.05 sec\n",
            "Epoch 2056, Loss(train/val) 0.29747/0.16562. Took 0.05 sec\n",
            "Epoch 2057, Loss(train/val) 0.28971/0.16628. Took 0.05 sec\n",
            "Epoch 2058, Loss(train/val) 0.29014/0.16548. Took 0.05 sec\n",
            "Epoch 2059, Loss(train/val) 0.29237/0.16556. Took 0.04 sec\n",
            "Epoch 2060, Loss(train/val) 0.29469/0.16528. Took 0.05 sec\n",
            "Epoch 2061, Loss(train/val) 0.29835/0.16511. Took 0.05 sec\n",
            "Epoch 2062, Loss(train/val) 0.29448/0.16499. Took 0.05 sec\n",
            "Epoch 2063, Loss(train/val) 0.30372/0.16496. Took 0.05 sec\n",
            "Epoch 2064, Loss(train/val) 0.29465/0.16502. Took 0.05 sec\n",
            "Epoch 2065, Loss(train/val) 0.29270/0.16441. Took 0.05 sec\n",
            "Epoch 2066, Loss(train/val) 0.29588/0.16409. Took 0.05 sec\n",
            "Epoch 2067, Loss(train/val) 0.29823/0.16431. Took 0.04 sec\n",
            "Epoch 2068, Loss(train/val) 0.29407/0.16467. Took 0.04 sec\n",
            "Epoch 2069, Loss(train/val) 0.29619/0.16443. Took 0.04 sec\n",
            "Epoch 2070, Loss(train/val) 0.29993/0.16404. Took 0.05 sec\n",
            "Epoch 2071, Loss(train/val) 0.29985/0.16385. Took 0.05 sec\n",
            "Epoch 2072, Loss(train/val) 0.29506/0.16358. Took 0.04 sec\n",
            "Epoch 2073, Loss(train/val) 0.29574/0.16348. Took 0.05 sec\n",
            "Epoch 2074, Loss(train/val) 0.30279/0.16317. Took 0.05 sec\n",
            "Epoch 2075, Loss(train/val) 0.29945/0.16325. Took 0.05 sec\n",
            "Epoch 2076, Loss(train/val) 0.27734/0.16352. Took 0.05 sec\n",
            "Epoch 2077, Loss(train/val) 0.29519/0.16412. Took 0.04 sec\n",
            "Epoch 2078, Loss(train/val) 0.29210/0.16405. Took 0.05 sec\n",
            "Epoch 2079, Loss(train/val) 0.30299/0.16417. Took 0.04 sec\n",
            "Epoch 2080, Loss(train/val) 0.29139/0.16426. Took 0.05 sec\n",
            "Epoch 2081, Loss(train/val) 0.30037/0.16418. Took 0.04 sec\n",
            "Epoch 2082, Loss(train/val) 0.29084/0.16427. Took 0.04 sec\n",
            "Epoch 2083, Loss(train/val) 0.29622/0.16441. Took 0.04 sec\n",
            "Epoch 2084, Loss(train/val) 0.29108/0.16447. Took 0.04 sec\n",
            "Epoch 2085, Loss(train/val) 0.29122/0.16440. Took 0.05 sec\n",
            "Epoch 2086, Loss(train/val) 0.28980/0.16429. Took 0.06 sec\n",
            "Epoch 2087, Loss(train/val) 0.29967/0.16412. Took 0.04 sec\n",
            "Epoch 2088, Loss(train/val) 0.29531/0.16374. Took 0.04 sec\n",
            "Epoch 2089, Loss(train/val) 0.29719/0.16354. Took 0.05 sec\n",
            "Epoch 2090, Loss(train/val) 0.29133/0.16346. Took 0.06 sec\n",
            "Epoch 2091, Loss(train/val) 0.29834/0.16364. Took 0.05 sec\n",
            "Epoch 2092, Loss(train/val) 0.30233/0.16373. Took 0.05 sec\n",
            "Epoch 2093, Loss(train/val) 0.28871/0.16391. Took 0.05 sec\n",
            "Epoch 2094, Loss(train/val) 0.29017/0.16375. Took 0.05 sec\n",
            "Epoch 2095, Loss(train/val) 0.28241/0.16413. Took 0.05 sec\n",
            "Epoch 2096, Loss(train/val) 0.30532/0.16411. Took 0.05 sec\n",
            "Epoch 2097, Loss(train/val) 0.28533/0.16436. Took 0.05 sec\n",
            "Epoch 2098, Loss(train/val) 0.28756/0.16437. Took 0.04 sec\n",
            "Epoch 2099, Loss(train/val) 0.30677/0.16385. Took 0.05 sec\n",
            "Epoch 2100, Loss(train/val) 0.28855/0.16374. Took 0.05 sec\n",
            "Epoch 2101, Loss(train/val) 0.29315/0.16351. Took 0.05 sec\n",
            "Epoch 2102, Loss(train/val) 0.29500/0.16381. Took 0.04 sec\n",
            "Epoch 2103, Loss(train/val) 0.28861/0.16359. Took 0.05 sec\n",
            "Epoch 2104, Loss(train/val) 0.28994/0.16401. Took 0.05 sec\n",
            "Epoch 2105, Loss(train/val) 0.29590/0.16463. Took 0.05 sec\n",
            "Epoch 2106, Loss(train/val) 0.29330/0.16537. Took 0.04 sec\n",
            "Epoch 2107, Loss(train/val) 0.28462/0.16586. Took 0.05 sec\n",
            "Epoch 2108, Loss(train/val) 0.29071/0.16499. Took 0.05 sec\n",
            "Epoch 2109, Loss(train/val) 0.28719/0.16441. Took 0.05 sec\n",
            "Epoch 2110, Loss(train/val) 0.29091/0.16392. Took 0.05 sec\n",
            "Epoch 2111, Loss(train/val) 0.29191/0.16383. Took 0.05 sec\n",
            "Epoch 2112, Loss(train/val) 0.29173/0.16308. Took 0.05 sec\n",
            "Epoch 2113, Loss(train/val) 0.28039/0.16329. Took 0.05 sec\n",
            "Epoch 2114, Loss(train/val) 0.28360/0.16347. Took 0.05 sec\n",
            "Epoch 2115, Loss(train/val) 0.29379/0.16323. Took 0.05 sec\n",
            "Epoch 2116, Loss(train/val) 0.30058/0.16361. Took 0.05 sec\n",
            "Epoch 2117, Loss(train/val) 0.29309/0.16363. Took 0.04 sec\n",
            "Epoch 2118, Loss(train/val) 0.29216/0.16358. Took 0.05 sec\n",
            "Epoch 2119, Loss(train/val) 0.29017/0.16376. Took 0.05 sec\n",
            "Epoch 2120, Loss(train/val) 0.29424/0.16361. Took 0.05 sec\n",
            "Epoch 2121, Loss(train/val) 0.28541/0.16379. Took 0.05 sec\n",
            "Epoch 2122, Loss(train/val) 0.29330/0.16378. Took 0.04 sec\n",
            "Epoch 2123, Loss(train/val) 0.29172/0.16311. Took 0.04 sec\n",
            "Epoch 2124, Loss(train/val) 0.29296/0.16330. Took 0.05 sec\n",
            "Epoch 2125, Loss(train/val) 0.28886/0.16301. Took 0.05 sec\n",
            "Epoch 2126, Loss(train/val) 0.28399/0.16313. Took 0.05 sec\n",
            "Epoch 2127, Loss(train/val) 0.28562/0.16327. Took 0.04 sec\n",
            "Epoch 2128, Loss(train/val) 0.28728/0.16389. Took 0.05 sec\n",
            "Epoch 2129, Loss(train/val) 0.28075/0.16388. Took 0.05 sec\n",
            "Epoch 2130, Loss(train/val) 0.29734/0.16399. Took 0.05 sec\n",
            "Epoch 2131, Loss(train/val) 0.28106/0.16395. Took 0.05 sec\n",
            "Epoch 2132, Loss(train/val) 0.29679/0.16436. Took 0.05 sec\n",
            "Epoch 2133, Loss(train/val) 0.28992/0.16309. Took 0.05 sec\n",
            "Epoch 2134, Loss(train/val) 0.28895/0.16254. Took 0.05 sec\n",
            "Epoch 2135, Loss(train/val) 0.28649/0.16236. Took 0.05 sec\n",
            "Epoch 2136, Loss(train/val) 0.28088/0.16254. Took 0.05 sec\n",
            "Epoch 2137, Loss(train/val) 0.28520/0.16309. Took 0.05 sec\n",
            "Epoch 2138, Loss(train/val) 0.29324/0.16329. Took 0.05 sec\n",
            "Epoch 2139, Loss(train/val) 0.28788/0.16312. Took 0.05 sec\n",
            "Epoch 2140, Loss(train/val) 0.29516/0.16335. Took 0.05 sec\n",
            "Epoch 2141, Loss(train/val) 0.30249/0.16310. Took 0.05 sec\n",
            "Epoch 2142, Loss(train/val) 0.28544/0.16274. Took 0.05 sec\n",
            "Epoch 2143, Loss(train/val) 0.28480/0.16286. Took 0.05 sec\n",
            "Epoch 2144, Loss(train/val) 0.28772/0.16287. Took 0.05 sec\n",
            "Epoch 2145, Loss(train/val) 0.28926/0.16305. Took 0.05 sec\n",
            "Epoch 2146, Loss(train/val) 0.29948/0.16293. Took 0.05 sec\n",
            "Epoch 2147, Loss(train/val) 0.29216/0.16234. Took 0.05 sec\n",
            "Epoch 2148, Loss(train/val) 0.29432/0.16201. Took 0.05 sec\n",
            "Epoch 2149, Loss(train/val) 0.29248/0.16192. Took 0.06 sec\n",
            "Epoch 2150, Loss(train/val) 0.29427/0.16196. Took 0.05 sec\n",
            "Epoch 2151, Loss(train/val) 0.28506/0.16190. Took 0.05 sec\n",
            "Epoch 2152, Loss(train/val) 0.29882/0.16206. Took 0.04 sec\n",
            "Epoch 2153, Loss(train/val) 0.29006/0.16231. Took 0.05 sec\n",
            "Epoch 2154, Loss(train/val) 0.27954/0.16231. Took 0.04 sec\n",
            "Epoch 2155, Loss(train/val) 0.29363/0.16237. Took 0.05 sec\n",
            "Epoch 2156, Loss(train/val) 0.28528/0.16277. Took 0.05 sec\n",
            "Epoch 2157, Loss(train/val) 0.29486/0.16211. Took 0.05 sec\n",
            "Epoch 2158, Loss(train/val) 0.27588/0.16224. Took 0.05 sec\n",
            "Epoch 2159, Loss(train/val) 0.28718/0.16211. Took 0.05 sec\n",
            "Epoch 2160, Loss(train/val) 0.28855/0.16220. Took 0.05 sec\n",
            "Epoch 2161, Loss(train/val) 0.29023/0.16220. Took 0.05 sec\n",
            "Epoch 2162, Loss(train/val) 0.28643/0.16260. Took 0.05 sec\n",
            "Epoch 2163, Loss(train/val) 0.29125/0.16276. Took 0.05 sec\n",
            "Epoch 2164, Loss(train/val) 0.28339/0.16302. Took 0.05 sec\n",
            "Epoch 2165, Loss(train/val) 0.28645/0.16321. Took 0.05 sec\n",
            "Epoch 2166, Loss(train/val) 0.29392/0.16358. Took 0.05 sec\n",
            "Epoch 2167, Loss(train/val) 0.29087/0.16344. Took 0.05 sec\n",
            "Epoch 2168, Loss(train/val) 0.29315/0.16282. Took 0.05 sec\n",
            "Epoch 2169, Loss(train/val) 0.28330/0.16269. Took 0.05 sec\n",
            "Epoch 2170, Loss(train/val) 0.29629/0.16239. Took 0.06 sec\n",
            "Epoch 2171, Loss(train/val) 0.27705/0.16232. Took 0.04 sec\n",
            "Epoch 2172, Loss(train/val) 0.27785/0.16293. Took 0.05 sec\n",
            "Epoch 2173, Loss(train/val) 0.28737/0.16239. Took 0.05 sec\n",
            "Epoch 2174, Loss(train/val) 0.28874/0.16221. Took 0.05 sec\n",
            "Epoch 2175, Loss(train/val) 0.28913/0.16241. Took 0.05 sec\n",
            "Epoch 2176, Loss(train/val) 0.28889/0.16257. Took 0.05 sec\n",
            "Epoch 2177, Loss(train/val) 0.28381/0.16241. Took 0.04 sec\n",
            "Epoch 2178, Loss(train/val) 0.28239/0.16226. Took 0.05 sec\n",
            "Epoch 2179, Loss(train/val) 0.28781/0.16201. Took 0.05 sec\n",
            "Epoch 2180, Loss(train/val) 0.28039/0.16164. Took 0.06 sec\n",
            "Epoch 2181, Loss(train/val) 0.28338/0.16172. Took 0.05 sec\n",
            "Epoch 2182, Loss(train/val) 0.28417/0.16143. Took 0.05 sec\n",
            "Epoch 2183, Loss(train/val) 0.28063/0.16137. Took 0.05 sec\n",
            "Epoch 2184, Loss(train/val) 0.28193/0.16131. Took 0.05 sec\n",
            "Epoch 2185, Loss(train/val) 0.28321/0.16144. Took 0.05 sec\n",
            "Epoch 2186, Loss(train/val) 0.28523/0.16127. Took 0.05 sec\n",
            "Epoch 2187, Loss(train/val) 0.27663/0.16204. Took 0.04 sec\n",
            "Epoch 2188, Loss(train/val) 0.28301/0.16256. Took 0.05 sec\n",
            "Epoch 2189, Loss(train/val) 0.28061/0.16277. Took 0.05 sec\n",
            "Epoch 2190, Loss(train/val) 0.28262/0.16284. Took 0.05 sec\n",
            "Epoch 2191, Loss(train/val) 0.28439/0.16228. Took 0.05 sec\n",
            "Epoch 2192, Loss(train/val) 0.28079/0.16260. Took 0.05 sec\n",
            "Epoch 2193, Loss(train/val) 0.28056/0.16268. Took 0.04 sec\n",
            "Epoch 2194, Loss(train/val) 0.28504/0.16231. Took 0.04 sec\n",
            "Epoch 2195, Loss(train/val) 0.27324/0.16239. Took 0.05 sec\n",
            "Epoch 2196, Loss(train/val) 0.28632/0.16210. Took 0.05 sec\n",
            "Epoch 2197, Loss(train/val) 0.27923/0.16196. Took 0.05 sec\n",
            "Epoch 2198, Loss(train/val) 0.28308/0.16196. Took 0.04 sec\n",
            "Epoch 2199, Loss(train/val) 0.28156/0.16243. Took 0.05 sec\n",
            "Epoch 2200, Loss(train/val) 0.28828/0.16162. Took 0.05 sec\n",
            "Epoch 2201, Loss(train/val) 0.29003/0.16176. Took 0.05 sec\n",
            "Epoch 2202, Loss(train/val) 0.29858/0.16115. Took 0.05 sec\n",
            "Epoch 2203, Loss(train/val) 0.28800/0.16124. Took 0.05 sec\n",
            "Epoch 2204, Loss(train/val) 0.28144/0.16081. Took 0.04 sec\n",
            "Epoch 2205, Loss(train/val) 0.29748/0.16110. Took 0.05 sec\n",
            "Epoch 2206, Loss(train/val) 0.28280/0.16068. Took 0.05 sec\n",
            "Epoch 2207, Loss(train/val) 0.27596/0.16081. Took 0.05 sec\n",
            "Epoch 2208, Loss(train/val) 0.30054/0.16077. Took 0.05 sec\n",
            "Epoch 2209, Loss(train/val) 0.28403/0.16076. Took 0.04 sec\n",
            "Epoch 2210, Loss(train/val) 0.26738/0.16106. Took 0.05 sec\n",
            "Epoch 2211, Loss(train/val) 0.27850/0.16116. Took 0.05 sec\n",
            "Epoch 2212, Loss(train/val) 0.28212/0.16127. Took 0.04 sec\n",
            "Epoch 2213, Loss(train/val) 0.28174/0.16159. Took 0.05 sec\n",
            "Epoch 2214, Loss(train/val) 0.27930/0.16195. Took 0.05 sec\n",
            "Epoch 2215, Loss(train/val) 0.28444/0.16166. Took 0.05 sec\n",
            "Epoch 2216, Loss(train/val) 0.27970/0.16193. Took 0.05 sec\n",
            "Epoch 2217, Loss(train/val) 0.27707/0.16218. Took 0.04 sec\n",
            "Epoch 2218, Loss(train/val) 0.29744/0.16216. Took 0.04 sec\n",
            "Epoch 2219, Loss(train/val) 0.27566/0.16193. Took 0.04 sec\n",
            "Epoch 2220, Loss(train/val) 0.27921/0.16208. Took 0.05 sec\n",
            "Epoch 2221, Loss(train/val) 0.28398/0.16105. Took 0.05 sec\n",
            "Epoch 2222, Loss(train/val) 0.29287/0.16113. Took 0.05 sec\n",
            "Epoch 2223, Loss(train/val) 0.28650/0.16084. Took 0.04 sec\n",
            "Epoch 2224, Loss(train/val) 0.27932/0.16077. Took 0.04 sec\n",
            "Epoch 2225, Loss(train/val) 0.28583/0.16076. Took 0.05 sec\n",
            "Epoch 2226, Loss(train/val) 0.28098/0.16095. Took 0.05 sec\n",
            "Epoch 2227, Loss(train/val) 0.26714/0.16096. Took 0.05 sec\n",
            "Epoch 2228, Loss(train/val) 0.28165/0.16081. Took 0.05 sec\n",
            "Epoch 2229, Loss(train/val) 0.28619/0.16082. Took 0.05 sec\n",
            "Epoch 2230, Loss(train/val) 0.27573/0.16050. Took 0.06 sec\n",
            "Epoch 2231, Loss(train/val) 0.27676/0.16049. Took 0.05 sec\n",
            "Epoch 2232, Loss(train/val) 0.28148/0.16054. Took 0.05 sec\n",
            "Epoch 2233, Loss(train/val) 0.27680/0.16079. Took 0.05 sec\n",
            "Epoch 2234, Loss(train/val) 0.28032/0.16101. Took 0.05 sec\n",
            "Epoch 2235, Loss(train/val) 0.28400/0.16119. Took 0.06 sec\n",
            "Epoch 2236, Loss(train/val) 0.30091/0.16129. Took 0.05 sec\n",
            "Epoch 2237, Loss(train/val) 0.27947/0.16087. Took 0.05 sec\n",
            "Epoch 2238, Loss(train/val) 0.27489/0.16084. Took 0.04 sec\n",
            "Epoch 2239, Loss(train/val) 0.28401/0.16094. Took 0.05 sec\n",
            "Epoch 2240, Loss(train/val) 0.27755/0.16091. Took 0.05 sec\n",
            "Epoch 2241, Loss(train/val) 0.26831/0.16110. Took 0.05 sec\n",
            "Epoch 2242, Loss(train/val) 0.29006/0.16088. Took 0.05 sec\n",
            "Epoch 2243, Loss(train/val) 0.28103/0.16070. Took 0.05 sec\n",
            "Epoch 2244, Loss(train/val) 0.28526/0.16058. Took 0.04 sec\n",
            "Epoch 2245, Loss(train/val) 0.27971/0.16037. Took 0.06 sec\n",
            "Epoch 2246, Loss(train/val) 0.27531/0.16025. Took 0.05 sec\n",
            "Epoch 2247, Loss(train/val) 0.27561/0.16044. Took 0.05 sec\n",
            "Epoch 2248, Loss(train/val) 0.28568/0.16046. Took 0.05 sec\n",
            "Epoch 2249, Loss(train/val) 0.27845/0.16042. Took 0.05 sec\n",
            "Epoch 2250, Loss(train/val) 0.27061/0.16049. Took 0.05 sec\n",
            "Epoch 2251, Loss(train/val) 0.27910/0.16055. Took 0.05 sec\n",
            "Epoch 2252, Loss(train/val) 0.28574/0.16040. Took 0.05 sec\n",
            "Epoch 2253, Loss(train/val) 0.28589/0.16023. Took 0.05 sec\n",
            "Epoch 2254, Loss(train/val) 0.28544/0.15998. Took 0.06 sec\n",
            "Epoch 2255, Loss(train/val) 0.27537/0.16010. Took 0.07 sec\n",
            "Epoch 2256, Loss(train/val) 0.27202/0.16017. Took 0.05 sec\n",
            "Epoch 2257, Loss(train/val) 0.28497/0.16008. Took 0.05 sec\n",
            "Epoch 2258, Loss(train/val) 0.27427/0.16029. Took 0.06 sec\n",
            "Epoch 2259, Loss(train/val) 0.27500/0.16041. Took 0.05 sec\n",
            "Epoch 2260, Loss(train/val) 0.27724/0.16024. Took 0.05 sec\n",
            "Epoch 2261, Loss(train/val) 0.27605/0.16008. Took 0.05 sec\n",
            "Epoch 2262, Loss(train/val) 0.27482/0.16011. Took 0.05 sec\n",
            "Epoch 2263, Loss(train/val) 0.28221/0.15970. Took 0.05 sec\n",
            "Epoch 2264, Loss(train/val) 0.27382/0.15978. Took 0.05 sec\n",
            "Epoch 2265, Loss(train/val) 0.27152/0.16008. Took 0.05 sec\n",
            "Epoch 2266, Loss(train/val) 0.27396/0.16009. Took 0.05 sec\n",
            "Epoch 2267, Loss(train/val) 0.27395/0.16018. Took 0.05 sec\n",
            "Epoch 2268, Loss(train/val) 0.28561/0.15983. Took 0.07 sec\n",
            "Epoch 2269, Loss(train/val) 0.26459/0.15980. Took 0.05 sec\n",
            "Epoch 2270, Loss(train/val) 0.27839/0.15976. Took 0.05 sec\n",
            "Epoch 2271, Loss(train/val) 0.27238/0.16011. Took 0.05 sec\n",
            "Epoch 2272, Loss(train/val) 0.27448/0.16029. Took 0.05 sec\n",
            "Epoch 2273, Loss(train/val) 0.27929/0.16030. Took 0.05 sec\n",
            "Epoch 2274, Loss(train/val) 0.27620/0.16035. Took 0.05 sec\n",
            "Epoch 2275, Loss(train/val) 0.28295/0.16005. Took 0.05 sec\n",
            "Epoch 2276, Loss(train/val) 0.28452/0.16010. Took 0.04 sec\n",
            "Epoch 2277, Loss(train/val) 0.27525/0.16009. Took 0.04 sec\n",
            "Epoch 2278, Loss(train/val) 0.27527/0.15976. Took 0.06 sec\n",
            "Epoch 2279, Loss(train/val) 0.27190/0.15955. Took 0.05 sec\n",
            "Epoch 2280, Loss(train/val) 0.28545/0.15920. Took 0.05 sec\n",
            "Epoch 2281, Loss(train/val) 0.28242/0.15906. Took 0.05 sec\n",
            "Epoch 2282, Loss(train/val) 0.27341/0.15926. Took 0.05 sec\n",
            "Epoch 2283, Loss(train/val) 0.27674/0.15986. Took 0.05 sec\n",
            "Epoch 2284, Loss(train/val) 0.26326/0.16101. Took 0.05 sec\n",
            "Epoch 2285, Loss(train/val) 0.27159/0.16110. Took 0.05 sec\n",
            "Epoch 2286, Loss(train/val) 0.27569/0.16048. Took 0.04 sec\n",
            "Epoch 2287, Loss(train/val) 0.27258/0.16046. Took 0.05 sec\n",
            "Epoch 2288, Loss(train/val) 0.27545/0.16027. Took 0.06 sec\n",
            "Epoch 2289, Loss(train/val) 0.29017/0.16063. Took 0.05 sec\n",
            "Epoch 2290, Loss(train/val) 0.28480/0.16019. Took 0.04 sec\n",
            "Epoch 2291, Loss(train/val) 0.27211/0.16037. Took 0.05 sec\n",
            "Epoch 2292, Loss(train/val) 0.27016/0.16014. Took 0.05 sec\n",
            "Epoch 2293, Loss(train/val) 0.27853/0.15986. Took 0.05 sec\n",
            "Epoch 2294, Loss(train/val) 0.28389/0.15989. Took 0.04 sec\n",
            "Epoch 2295, Loss(train/val) 0.28273/0.15982. Took 0.05 sec\n",
            "Epoch 2296, Loss(train/val) 0.26653/0.15934. Took 0.06 sec\n",
            "Epoch 2297, Loss(train/val) 0.28416/0.15903. Took 0.05 sec\n",
            "Epoch 2298, Loss(train/val) 0.27864/0.15937. Took 0.05 sec\n",
            "Epoch 2299, Loss(train/val) 0.27250/0.15907. Took 0.05 sec\n",
            "Epoch 2300, Loss(train/val) 0.27880/0.15915. Took 0.04 sec\n",
            "Epoch 2301, Loss(train/val) 0.27099/0.15896. Took 0.04 sec\n",
            "Epoch 2302, Loss(train/val) 0.28050/0.15913. Took 0.05 sec\n",
            "Epoch 2303, Loss(train/val) 0.28082/0.15893. Took 0.05 sec\n",
            "Epoch 2304, Loss(train/val) 0.27574/0.15895. Took 0.05 sec\n",
            "Epoch 2305, Loss(train/val) 0.27127/0.15898. Took 0.05 sec\n",
            "Epoch 2306, Loss(train/val) 0.27753/0.15912. Took 0.04 sec\n",
            "Epoch 2307, Loss(train/val) 0.28370/0.15934. Took 0.04 sec\n",
            "Epoch 2308, Loss(train/val) 0.27306/0.15918. Took 0.05 sec\n",
            "Epoch 2309, Loss(train/val) 0.28847/0.15949. Took 0.05 sec\n",
            "Epoch 2310, Loss(train/val) 0.27251/0.15921. Took 0.04 sec\n",
            "Epoch 2311, Loss(train/val) 0.27591/0.15919. Took 0.05 sec\n",
            "Epoch 2312, Loss(train/val) 0.28033/0.15952. Took 0.04 sec\n",
            "Epoch 2313, Loss(train/val) 0.28469/0.15953. Took 0.05 sec\n",
            "Epoch 2314, Loss(train/val) 0.26972/0.15921. Took 0.05 sec\n",
            "Epoch 2315, Loss(train/val) 0.27475/0.15944. Took 0.05 sec\n",
            "Epoch 2316, Loss(train/val) 0.27166/0.15996. Took 0.05 sec\n",
            "Epoch 2317, Loss(train/val) 0.27278/0.16036. Took 0.05 sec\n",
            "Epoch 2318, Loss(train/val) 0.28048/0.15998. Took 0.06 sec\n",
            "Epoch 2319, Loss(train/val) 0.27443/0.15987. Took 0.05 sec\n",
            "Epoch 2320, Loss(train/val) 0.28031/0.16065. Took 0.05 sec\n",
            "Epoch 2321, Loss(train/val) 0.27544/0.16017. Took 0.05 sec\n",
            "Epoch 2322, Loss(train/val) 0.27350/0.16001. Took 0.05 sec\n",
            "Epoch 2323, Loss(train/val) 0.27210/0.15950. Took 0.06 sec\n",
            "Epoch 2324, Loss(train/val) 0.28292/0.15954. Took 0.05 sec\n",
            "Epoch 2325, Loss(train/val) 0.28009/0.15887. Took 0.05 sec\n",
            "Epoch 2326, Loss(train/val) 0.27945/0.15880. Took 0.05 sec\n",
            "Epoch 2327, Loss(train/val) 0.27416/0.15879. Took 0.05 sec\n",
            "Epoch 2328, Loss(train/val) 0.27842/0.15918. Took 0.05 sec\n",
            "Epoch 2329, Loss(train/val) 0.27872/0.15895. Took 0.05 sec\n",
            "Epoch 2330, Loss(train/val) 0.26773/0.15881. Took 0.05 sec\n",
            "Epoch 2331, Loss(train/val) 0.26641/0.15884. Took 0.05 sec\n",
            "Epoch 2332, Loss(train/val) 0.27458/0.15907. Took 0.05 sec\n",
            "Epoch 2333, Loss(train/val) 0.26519/0.15900. Took 0.05 sec\n",
            "Epoch 2334, Loss(train/val) 0.27406/0.15921. Took 0.05 sec\n",
            "Epoch 2335, Loss(train/val) 0.26552/0.15880. Took 0.05 sec\n",
            "Epoch 2336, Loss(train/val) 0.28562/0.15859. Took 0.05 sec\n",
            "Epoch 2337, Loss(train/val) 0.26586/0.15863. Took 0.05 sec\n",
            "Epoch 2338, Loss(train/val) 0.26917/0.15878. Took 0.05 sec\n",
            "Epoch 2339, Loss(train/val) 0.28391/0.15884. Took 0.05 sec\n",
            "Epoch 2340, Loss(train/val) 0.26806/0.15951. Took 0.05 sec\n",
            "Epoch 2341, Loss(train/val) 0.27646/0.15923. Took 0.05 sec\n",
            "Epoch 2342, Loss(train/val) 0.26536/0.15930. Took 0.04 sec\n",
            "Epoch 2343, Loss(train/val) 0.26044/0.15884. Took 0.05 sec\n",
            "Epoch 2344, Loss(train/val) 0.27786/0.15853. Took 0.05 sec\n",
            "Epoch 2345, Loss(train/val) 0.26843/0.15859. Took 0.05 sec\n",
            "Epoch 2346, Loss(train/val) 0.27030/0.15916. Took 0.04 sec\n",
            "Epoch 2347, Loss(train/val) 0.27588/0.15982. Took 0.05 sec\n",
            "Epoch 2348, Loss(train/val) 0.27594/0.15936. Took 0.05 sec\n",
            "Epoch 2349, Loss(train/val) 0.27540/0.15927. Took 0.04 sec\n",
            "Epoch 2350, Loss(train/val) 0.27179/0.15952. Took 0.05 sec\n",
            "Epoch 2351, Loss(train/val) 0.27732/0.15906. Took 0.05 sec\n",
            "Epoch 2352, Loss(train/val) 0.27024/0.15858. Took 0.05 sec\n",
            "Epoch 2353, Loss(train/val) 0.26969/0.15858. Took 0.06 sec\n",
            "Epoch 2354, Loss(train/val) 0.26808/0.15832. Took 0.05 sec\n",
            "Epoch 2355, Loss(train/val) 0.26966/0.15851. Took 0.05 sec\n",
            "Epoch 2356, Loss(train/val) 0.26830/0.15868. Took 0.04 sec\n",
            "Epoch 2357, Loss(train/val) 0.27311/0.15898. Took 0.05 sec\n",
            "Epoch 2358, Loss(train/val) 0.26012/0.15889. Took 0.05 sec\n",
            "Epoch 2359, Loss(train/val) 0.26602/0.15906. Took 0.05 sec\n",
            "Epoch 2360, Loss(train/val) 0.29058/0.15977. Took 0.05 sec\n",
            "Epoch 2361, Loss(train/val) 0.27326/0.15908. Took 0.05 sec\n",
            "Epoch 2362, Loss(train/val) 0.26901/0.15841. Took 0.05 sec\n",
            "Epoch 2363, Loss(train/val) 0.26165/0.15855. Took 0.06 sec\n",
            "Epoch 2364, Loss(train/val) 0.26866/0.15825. Took 0.05 sec\n",
            "Epoch 2365, Loss(train/val) 0.27787/0.15870. Took 0.04 sec\n",
            "Epoch 2366, Loss(train/val) 0.27099/0.15849. Took 0.05 sec\n",
            "Epoch 2367, Loss(train/val) 0.27187/0.15846. Took 0.04 sec\n",
            "Epoch 2368, Loss(train/val) 0.28083/0.15819. Took 0.05 sec\n",
            "Epoch 2369, Loss(train/val) 0.29058/0.15813. Took 0.05 sec\n",
            "Epoch 2370, Loss(train/val) 0.26492/0.15791. Took 0.05 sec\n",
            "Epoch 2371, Loss(train/val) 0.26827/0.15783. Took 0.04 sec\n",
            "Epoch 2372, Loss(train/val) 0.27119/0.15785. Took 0.05 sec\n",
            "Epoch 2373, Loss(train/val) 0.26714/0.15792. Took 0.05 sec\n",
            "Epoch 2374, Loss(train/val) 0.27961/0.15773. Took 0.05 sec\n",
            "Epoch 2375, Loss(train/val) 0.26840/0.15827. Took 0.04 sec\n",
            "Epoch 2376, Loss(train/val) 0.27152/0.15931. Took 0.05 sec\n",
            "Epoch 2377, Loss(train/val) 0.26825/0.15887. Took 0.05 sec\n",
            "Epoch 2378, Loss(train/val) 0.26035/0.15885. Took 0.06 sec\n",
            "Epoch 2379, Loss(train/val) 0.26560/0.15888. Took 0.05 sec\n",
            "Epoch 2380, Loss(train/val) 0.27570/0.15817. Took 0.05 sec\n",
            "Epoch 2381, Loss(train/val) 0.30691/0.15833. Took 0.05 sec\n",
            "Epoch 2382, Loss(train/val) 0.26938/0.15818. Took 0.05 sec\n",
            "Epoch 2383, Loss(train/val) 0.27786/0.15832. Took 0.05 sec\n",
            "Epoch 2384, Loss(train/val) 0.26322/0.15829. Took 0.05 sec\n",
            "Epoch 2385, Loss(train/val) 0.27386/0.15808. Took 0.04 sec\n",
            "Epoch 2386, Loss(train/val) 0.26503/0.15820. Took 0.05 sec\n",
            "Epoch 2387, Loss(train/val) 0.27095/0.15794. Took 0.04 sec\n",
            "Epoch 2388, Loss(train/val) 0.26247/0.15892. Took 0.06 sec\n",
            "Epoch 2389, Loss(train/val) 0.26231/0.15957. Took 0.04 sec\n",
            "Epoch 2390, Loss(train/val) 0.26872/0.16016. Took 0.04 sec\n",
            "Epoch 2391, Loss(train/val) 0.26327/0.15980. Took 0.05 sec\n",
            "Epoch 2392, Loss(train/val) 0.26940/0.15889. Took 0.05 sec\n",
            "Epoch 2393, Loss(train/val) 0.27113/0.15851. Took 0.05 sec\n",
            "Epoch 2394, Loss(train/val) 0.26589/0.15854. Took 0.04 sec\n",
            "Epoch 2395, Loss(train/val) 0.26654/0.15855. Took 0.04 sec\n",
            "Epoch 2396, Loss(train/val) 0.27033/0.15903. Took 0.05 sec\n",
            "Epoch 2397, Loss(train/val) 0.26503/0.15895. Took 0.05 sec\n",
            "Epoch 2398, Loss(train/val) 0.27481/0.15893. Took 0.05 sec\n",
            "Epoch 2399, Loss(train/val) 0.26670/0.15835. Took 0.05 sec\n",
            "Epoch 2400, Loss(train/val) 0.26210/0.15791. Took 0.04 sec\n",
            "Epoch 2401, Loss(train/val) 0.26349/0.15789. Took 0.04 sec\n",
            "Epoch 2402, Loss(train/val) 0.27250/0.15753. Took 0.05 sec\n",
            "Epoch 2403, Loss(train/val) 0.27998/0.15769. Took 0.05 sec\n",
            "Epoch 2404, Loss(train/val) 0.26424/0.15750. Took 0.05 sec\n",
            "Epoch 2405, Loss(train/val) 0.26286/0.15716. Took 0.05 sec\n",
            "Epoch 2406, Loss(train/val) 0.26808/0.15730. Took 0.05 sec\n",
            "Epoch 2407, Loss(train/val) 0.26970/0.15796. Took 0.04 sec\n",
            "Epoch 2408, Loss(train/val) 0.28007/0.15760. Took 0.05 sec\n",
            "Epoch 2409, Loss(train/val) 0.26732/0.15802. Took 0.04 sec\n",
            "Epoch 2410, Loss(train/val) 0.25813/0.15832. Took 0.04 sec\n",
            "Epoch 2411, Loss(train/val) 0.27237/0.15854. Took 0.05 sec\n",
            "Epoch 2412, Loss(train/val) 0.26177/0.15812. Took 0.04 sec\n",
            "Epoch 2413, Loss(train/val) 0.25931/0.15845. Took 0.05 sec\n",
            "Epoch 2414, Loss(train/val) 0.26608/0.15844. Took 0.05 sec\n",
            "Epoch 2415, Loss(train/val) 0.27046/0.15800. Took 0.05 sec\n",
            "Epoch 2416, Loss(train/val) 0.29310/0.15778. Took 0.04 sec\n",
            "Epoch 2417, Loss(train/val) 0.26727/0.15738. Took 0.04 sec\n",
            "Epoch 2418, Loss(train/val) 0.26444/0.15726. Took 0.05 sec\n",
            "Epoch 2419, Loss(train/val) 0.27121/0.15723. Took 0.04 sec\n",
            "Epoch 2420, Loss(train/val) 0.27559/0.15726. Took 0.05 sec\n",
            "Epoch 2421, Loss(train/val) 0.26874/0.15707. Took 0.05 sec\n",
            "Epoch 2422, Loss(train/val) 0.27264/0.15681. Took 0.05 sec\n",
            "Epoch 2423, Loss(train/val) 0.25405/0.15709. Took 0.05 sec\n",
            "Epoch 2424, Loss(train/val) 0.26440/0.15696. Took 0.05 sec\n",
            "Epoch 2425, Loss(train/val) 0.25755/0.15746. Took 0.04 sec\n",
            "Epoch 2426, Loss(train/val) 0.27097/0.15852. Took 0.04 sec\n",
            "Epoch 2427, Loss(train/val) 0.26816/0.15857. Took 0.04 sec\n",
            "Epoch 2428, Loss(train/val) 0.26210/0.15853. Took 0.05 sec\n",
            "Epoch 2429, Loss(train/val) 0.25381/0.15819. Took 0.05 sec\n",
            "Epoch 2430, Loss(train/val) 0.27985/0.15753. Took 0.04 sec\n",
            "Epoch 2431, Loss(train/val) 0.26019/0.15757. Took 0.05 sec\n",
            "Epoch 2432, Loss(train/val) 0.26265/0.15821. Took 0.04 sec\n",
            "Epoch 2433, Loss(train/val) 0.26258/0.15889. Took 0.05 sec\n",
            "Epoch 2434, Loss(train/val) 0.27046/0.15913. Took 0.05 sec\n",
            "Epoch 2435, Loss(train/val) 0.25650/0.15962. Took 0.05 sec\n",
            "Epoch 2436, Loss(train/val) 0.26353/0.15913. Took 0.04 sec\n",
            "Epoch 2437, Loss(train/val) 0.26207/0.15828. Took 0.04 sec\n",
            "Epoch 2438, Loss(train/val) 0.25971/0.15759. Took 0.06 sec\n",
            "Epoch 2439, Loss(train/val) 0.26877/0.15753. Took 0.05 sec\n",
            "Epoch 2440, Loss(train/val) 0.26429/0.15733. Took 0.05 sec\n",
            "Epoch 2441, Loss(train/val) 0.26569/0.15726. Took 0.05 sec\n",
            "Epoch 2442, Loss(train/val) 0.26574/0.15736. Took 0.05 sec\n",
            "Epoch 2443, Loss(train/val) 0.26352/0.15732. Took 0.05 sec\n",
            "Epoch 2444, Loss(train/val) 0.27346/0.15786. Took 0.05 sec\n",
            "Epoch 2445, Loss(train/val) 0.25547/0.15762. Took 0.05 sec\n",
            "Epoch 2446, Loss(train/val) 0.26742/0.15710. Took 0.05 sec\n",
            "Epoch 2447, Loss(train/val) 0.26686/0.15716. Took 0.05 sec\n",
            "Epoch 2448, Loss(train/val) 0.28139/0.15725. Took 0.05 sec\n",
            "Epoch 2449, Loss(train/val) 0.25942/0.15759. Took 0.05 sec\n",
            "Epoch 2450, Loss(train/val) 0.27303/0.15729. Took 0.05 sec\n",
            "Epoch 2451, Loss(train/val) 0.26201/0.15723. Took 0.05 sec\n",
            "Epoch 2452, Loss(train/val) 0.27918/0.15749. Took 0.05 sec\n",
            "Epoch 2453, Loss(train/val) 0.27107/0.15716. Took 0.05 sec\n",
            "Epoch 2454, Loss(train/val) 0.27490/0.15716. Took 0.05 sec\n",
            "Epoch 2455, Loss(train/val) 0.26516/0.15736. Took 0.05 sec\n",
            "Epoch 2456, Loss(train/val) 0.27120/0.15742. Took 0.05 sec\n",
            "Epoch 2457, Loss(train/val) 0.25561/0.15699. Took 0.05 sec\n",
            "Epoch 2458, Loss(train/val) 0.26711/0.15698. Took 0.05 sec\n",
            "Epoch 2459, Loss(train/val) 0.27697/0.15715. Took 0.05 sec\n",
            "Epoch 2460, Loss(train/val) 0.26544/0.15665. Took 0.05 sec\n",
            "Epoch 2461, Loss(train/val) 0.26590/0.15657. Took 0.04 sec\n",
            "Epoch 2462, Loss(train/val) 0.27045/0.15642. Took 0.05 sec\n",
            "Epoch 2463, Loss(train/val) 0.27101/0.15659. Took 0.05 sec\n",
            "Epoch 2464, Loss(train/val) 0.25508/0.15726. Took 0.05 sec\n",
            "Epoch 2465, Loss(train/val) 0.26413/0.15741. Took 0.05 sec\n",
            "Epoch 2466, Loss(train/val) 0.25741/0.15781. Took 0.05 sec\n",
            "Epoch 2467, Loss(train/val) 0.26139/0.15857. Took 0.05 sec\n",
            "Epoch 2468, Loss(train/val) 0.26838/0.15864. Took 0.05 sec\n",
            "Epoch 2469, Loss(train/val) 0.25466/0.15780. Took 0.05 sec\n",
            "Epoch 2470, Loss(train/val) 0.25868/0.15727. Took 0.05 sec\n",
            "Epoch 2471, Loss(train/val) 0.26309/0.15715. Took 0.04 sec\n",
            "Epoch 2472, Loss(train/val) 0.25716/0.15746. Took 0.04 sec\n",
            "Epoch 2473, Loss(train/val) 0.27115/0.15731. Took 0.05 sec\n",
            "Epoch 2474, Loss(train/val) 0.26372/0.15715. Took 0.05 sec\n",
            "Epoch 2475, Loss(train/val) 0.26649/0.15703. Took 0.04 sec\n",
            "Epoch 2476, Loss(train/val) 0.26143/0.15674. Took 0.04 sec\n",
            "Epoch 2477, Loss(train/val) 0.27130/0.15670. Took 0.04 sec\n",
            "Epoch 2478, Loss(train/val) 0.26299/0.15689. Took 0.05 sec\n",
            "Epoch 2479, Loss(train/val) 0.26350/0.15660. Took 0.04 sec\n",
            "Epoch 2480, Loss(train/val) 0.25676/0.15675. Took 0.04 sec\n",
            "Epoch 2481, Loss(train/val) 0.25829/0.15708. Took 0.05 sec\n",
            "Epoch 2482, Loss(train/val) 0.26684/0.15718. Took 0.04 sec\n",
            "Epoch 2483, Loss(train/val) 0.28251/0.15732. Took 0.06 sec\n",
            "Epoch 2484, Loss(train/val) 0.26804/0.15708. Took 0.05 sec\n",
            "Epoch 2485, Loss(train/val) 0.26211/0.15712. Took 0.05 sec\n",
            "Epoch 2486, Loss(train/val) 0.26502/0.15700. Took 0.05 sec\n",
            "Epoch 2487, Loss(train/val) 0.26204/0.15712. Took 0.04 sec\n",
            "Epoch 2488, Loss(train/val) 0.25667/0.15726. Took 0.06 sec\n",
            "Epoch 2489, Loss(train/val) 0.25825/0.15746. Took 0.04 sec\n",
            "Epoch 2490, Loss(train/val) 0.25881/0.15690. Took 0.04 sec\n",
            "Epoch 2491, Loss(train/val) 0.28014/0.15692. Took 0.06 sec\n",
            "Epoch 2492, Loss(train/val) 0.27521/0.15694. Took 0.04 sec\n",
            "Epoch 2493, Loss(train/val) 0.25994/0.15763. Took 0.05 sec\n",
            "Epoch 2494, Loss(train/val) 0.25673/0.15820. Took 0.05 sec\n",
            "Epoch 2495, Loss(train/val) 0.25581/0.15717. Took 0.04 sec\n",
            "Epoch 2496, Loss(train/val) 0.25465/0.15720. Took 0.04 sec\n",
            "Epoch 2497, Loss(train/val) 0.25951/0.15728. Took 0.04 sec\n",
            "Epoch 2498, Loss(train/val) 0.25277/0.15714. Took 0.05 sec\n",
            "Epoch 2499, Loss(train/val) 0.26275/0.15724. Took 0.04 sec\n",
            "Epoch 2500, Loss(train/val) 0.26561/0.15778. Took 0.04 sec\n",
            "Epoch 2501, Loss(train/val) 0.25009/0.15842. Took 0.04 sec\n",
            "Epoch 2502, Loss(train/val) 0.27203/0.15832. Took 0.04 sec\n",
            "Epoch 2503, Loss(train/val) 0.28839/0.15738. Took 0.05 sec\n",
            "Epoch 2504, Loss(train/val) 0.26139/0.15647. Took 0.04 sec\n",
            "Epoch 2505, Loss(train/val) 0.26134/0.15635. Took 0.05 sec\n",
            "Epoch 2506, Loss(train/val) 0.25764/0.15629. Took 0.04 sec\n",
            "Epoch 2507, Loss(train/val) 0.25976/0.15640. Took 0.04 sec\n",
            "Epoch 2508, Loss(train/val) 0.27353/0.15655. Took 0.05 sec\n",
            "Epoch 2509, Loss(train/val) 0.26146/0.15675. Took 0.05 sec\n",
            "Epoch 2510, Loss(train/val) 0.26019/0.15646. Took 0.04 sec\n",
            "Epoch 2511, Loss(train/val) 0.25988/0.15620. Took 0.05 sec\n",
            "Epoch 2512, Loss(train/val) 0.25660/0.15616. Took 0.04 sec\n",
            "Epoch 2513, Loss(train/val) 0.25920/0.15610. Took 0.05 sec\n",
            "Epoch 2514, Loss(train/val) 0.25525/0.15644. Took 0.04 sec\n",
            "Epoch 2515, Loss(train/val) 0.26387/0.15691. Took 0.05 sec\n",
            "Epoch 2516, Loss(train/val) 0.25478/0.15653. Took 0.05 sec\n",
            "Epoch 2517, Loss(train/val) 0.29596/0.15609. Took 0.04 sec\n",
            "Epoch 2518, Loss(train/val) 0.26461/0.15648. Took 0.05 sec\n",
            "Epoch 2519, Loss(train/val) 0.25954/0.15645. Took 0.04 sec\n",
            "Epoch 2520, Loss(train/val) 0.25782/0.15643. Took 0.04 sec\n",
            "Epoch 2521, Loss(train/val) 0.25464/0.15670. Took 0.04 sec\n",
            "Epoch 2522, Loss(train/val) 0.25692/0.15738. Took 0.04 sec\n",
            "Epoch 2523, Loss(train/val) 0.25578/0.15740. Took 0.05 sec\n",
            "Epoch 2524, Loss(train/val) 0.26627/0.15706. Took 0.05 sec\n",
            "Epoch 2525, Loss(train/val) 0.25903/0.15699. Took 0.04 sec\n",
            "Epoch 2526, Loss(train/val) 0.25569/0.15664. Took 0.05 sec\n",
            "Epoch 2527, Loss(train/val) 0.25338/0.15685. Took 0.04 sec\n",
            "Epoch 2528, Loss(train/val) 0.25430/0.15707. Took 0.05 sec\n",
            "Epoch 2529, Loss(train/val) 0.25460/0.15701. Took 0.04 sec\n",
            "Epoch 2530, Loss(train/val) 0.26075/0.15694. Took 0.05 sec\n",
            "Epoch 2531, Loss(train/val) 0.25100/0.15689. Took 0.05 sec\n",
            "Epoch 2532, Loss(train/val) 0.25486/0.15718. Took 0.04 sec\n",
            "Epoch 2533, Loss(train/val) 0.25266/0.15717. Took 0.05 sec\n",
            "Epoch 2534, Loss(train/val) 0.25189/0.15652. Took 0.05 sec\n",
            "Epoch 2535, Loss(train/val) 0.26363/0.15597. Took 0.05 sec\n",
            "Epoch 2536, Loss(train/val) 0.26101/0.15614. Took 0.04 sec\n",
            "Epoch 2537, Loss(train/val) 0.25770/0.15599. Took 0.05 sec\n",
            "Epoch 2538, Loss(train/val) 0.25988/0.15603. Took 0.05 sec\n",
            "Epoch 2539, Loss(train/val) 0.25662/0.15613. Took 0.05 sec\n",
            "Epoch 2540, Loss(train/val) 0.27054/0.15718. Took 0.04 sec\n",
            "Epoch 2541, Loss(train/val) 0.26435/0.15636. Took 0.04 sec\n",
            "Epoch 2542, Loss(train/val) 0.25970/0.15593. Took 0.05 sec\n",
            "Epoch 2543, Loss(train/val) 0.25137/0.15587. Took 0.05 sec\n",
            "Epoch 2544, Loss(train/val) 0.25371/0.15609. Took 0.05 sec\n",
            "Epoch 2545, Loss(train/val) 0.26308/0.15692. Took 0.04 sec\n",
            "Epoch 2546, Loss(train/val) 0.25714/0.15691. Took 0.05 sec\n",
            "Epoch 2547, Loss(train/val) 0.24820/0.15667. Took 0.04 sec\n",
            "Epoch 2548, Loss(train/val) 0.26755/0.15672. Took 0.05 sec\n",
            "Epoch 2549, Loss(train/val) 0.24651/0.15670. Took 0.04 sec\n",
            "Epoch 2550, Loss(train/val) 0.26051/0.15723. Took 0.05 sec\n",
            "Epoch 2551, Loss(train/val) 0.25561/0.15694. Took 0.04 sec\n",
            "Epoch 2552, Loss(train/val) 0.25839/0.15639. Took 0.05 sec\n",
            "Epoch 2553, Loss(train/val) 0.26252/0.15641. Took 0.06 sec\n",
            "Epoch 2554, Loss(train/val) 0.25619/0.15642. Took 0.04 sec\n",
            "Epoch 2555, Loss(train/val) 0.27116/0.15710. Took 0.05 sec\n",
            "Epoch 2556, Loss(train/val) 0.27246/0.15715. Took 0.05 sec\n",
            "Epoch 2557, Loss(train/val) 0.25835/0.15602. Took 0.05 sec\n",
            "Epoch 2558, Loss(train/val) 0.25755/0.15642. Took 0.06 sec\n",
            "Epoch 2559, Loss(train/val) 0.26355/0.15756. Took 0.05 sec\n",
            "Epoch 2560, Loss(train/val) 0.24733/0.15715. Took 0.05 sec\n",
            "Epoch 2561, Loss(train/val) 0.25491/0.15715. Took 0.05 sec\n",
            "Epoch 2562, Loss(train/val) 0.25764/0.15657. Took 0.05 sec\n",
            "Epoch 2563, Loss(train/val) 0.26319/0.15571. Took 0.05 sec\n",
            "Epoch 2564, Loss(train/val) 0.26036/0.15540. Took 0.04 sec\n",
            "Epoch 2565, Loss(train/val) 0.25180/0.15575. Took 0.05 sec\n",
            "Epoch 2566, Loss(train/val) 0.25688/0.15570. Took 0.04 sec\n",
            "Epoch 2567, Loss(train/val) 0.25665/0.15582. Took 0.04 sec\n",
            "Epoch 2568, Loss(train/val) 0.26359/0.15621. Took 0.05 sec\n",
            "Epoch 2569, Loss(train/val) 0.25678/0.15647. Took 0.05 sec\n",
            "Epoch 2570, Loss(train/val) 0.26464/0.15665. Took 0.04 sec\n",
            "Epoch 2571, Loss(train/val) 0.26428/0.15653. Took 0.04 sec\n",
            "Epoch 2572, Loss(train/val) 0.25267/0.15709. Took 0.05 sec\n",
            "Epoch 2573, Loss(train/val) 0.25654/0.15762. Took 0.05 sec\n",
            "Epoch 2574, Loss(train/val) 0.25929/0.15751. Took 0.06 sec\n",
            "Epoch 2575, Loss(train/val) 0.26409/0.15615. Took 0.04 sec\n",
            "Epoch 2576, Loss(train/val) 0.25165/0.15584. Took 0.05 sec\n",
            "Epoch 2577, Loss(train/val) 0.25210/0.15570. Took 0.04 sec\n",
            "Epoch 2578, Loss(train/val) 0.25771/0.15604. Took 0.05 sec\n",
            "Epoch 2579, Loss(train/val) 0.25574/0.15606. Took 0.04 sec\n",
            "Epoch 2580, Loss(train/val) 0.25091/0.15637. Took 0.04 sec\n",
            "Epoch 2581, Loss(train/val) 0.25486/0.15679. Took 0.04 sec\n",
            "Epoch 2582, Loss(train/val) 0.26088/0.15674. Took 0.04 sec\n",
            "Epoch 2583, Loss(train/val) 0.25279/0.15641. Took 0.05 sec\n",
            "Epoch 2584, Loss(train/val) 0.25255/0.15627. Took 0.05 sec\n",
            "Epoch 2585, Loss(train/val) 0.25812/0.15639. Took 0.05 sec\n",
            "Epoch 2586, Loss(train/val) 0.24460/0.15601. Took 0.05 sec\n",
            "Epoch 2587, Loss(train/val) 0.26031/0.15702. Took 0.05 sec\n",
            "Epoch 2588, Loss(train/val) 0.26315/0.15812. Took 0.07 sec\n",
            "Epoch 2589, Loss(train/val) 0.25439/0.15684. Took 0.06 sec\n",
            "Epoch 2590, Loss(train/val) 0.25562/0.15577. Took 0.05 sec\n",
            "Epoch 2591, Loss(train/val) 0.26161/0.15568. Took 0.05 sec\n",
            "Epoch 2592, Loss(train/val) 0.26014/0.15628. Took 0.04 sec\n",
            "Epoch 2593, Loss(train/val) 0.24576/0.15612. Took 0.05 sec\n",
            "Epoch 2594, Loss(train/val) 0.24315/0.15609. Took 0.05 sec\n",
            "Epoch 2595, Loss(train/val) 0.25294/0.15622. Took 0.06 sec\n",
            "Epoch 2596, Loss(train/val) 0.25077/0.15646. Took 0.05 sec\n",
            "Epoch 2597, Loss(train/val) 0.25658/0.15696. Took 0.05 sec\n",
            "Epoch 2598, Loss(train/val) 0.25888/0.15688. Took 0.05 sec\n",
            "Epoch 2599, Loss(train/val) 0.25508/0.15726. Took 0.04 sec\n",
            "Epoch 2600, Loss(train/val) 0.25796/0.15656. Took 0.04 sec\n",
            "Epoch 2601, Loss(train/val) 0.26150/0.15649. Took 0.04 sec\n",
            "Epoch 2602, Loss(train/val) 0.26025/0.15619. Took 0.04 sec\n",
            "Epoch 2603, Loss(train/val) 0.26302/0.15691. Took 0.05 sec\n",
            "Epoch 2604, Loss(train/val) 0.25274/0.15746. Took 0.04 sec\n",
            "Epoch 2605, Loss(train/val) 0.25001/0.15774. Took 0.04 sec\n",
            "Epoch 2606, Loss(train/val) 0.26945/0.15767. Took 0.05 sec\n",
            "Epoch 2607, Loss(train/val) 0.24651/0.15692. Took 0.05 sec\n",
            "Epoch 2608, Loss(train/val) 0.25952/0.15674. Took 0.05 sec\n",
            "Epoch 2609, Loss(train/val) 0.25345/0.15602. Took 0.04 sec\n",
            "Epoch 2610, Loss(train/val) 0.25128/0.15569. Took 0.04 sec\n",
            "Epoch 2611, Loss(train/val) 0.24664/0.15541. Took 0.04 sec\n",
            "Epoch 2612, Loss(train/val) 0.24717/0.15555. Took 0.04 sec\n",
            "Epoch 2613, Loss(train/val) 0.24898/0.15564. Took 0.06 sec\n",
            "Epoch 2614, Loss(train/val) 0.25586/0.15583. Took 0.05 sec\n",
            "Epoch 2615, Loss(train/val) 0.26752/0.15571. Took 0.05 sec\n",
            "Epoch 2616, Loss(train/val) 0.25439/0.15540. Took 0.05 sec\n",
            "Epoch 2617, Loss(train/val) 0.24899/0.15544. Took 0.05 sec\n",
            "Epoch 2618, Loss(train/val) 0.25085/0.15547. Took 0.05 sec\n",
            "Epoch 2619, Loss(train/val) 0.25355/0.15549. Took 0.04 sec\n",
            "Epoch 2620, Loss(train/val) 0.28838/0.15678. Took 0.04 sec\n",
            "Epoch 2621, Loss(train/val) 0.25248/0.15624. Took 0.04 sec\n",
            "Epoch 2622, Loss(train/val) 0.25501/0.15627. Took 0.04 sec\n",
            "Epoch 2623, Loss(train/val) 0.26520/0.15644. Took 0.05 sec\n",
            "Epoch 2624, Loss(train/val) 0.24819/0.15666. Took 0.04 sec\n",
            "Epoch 2625, Loss(train/val) 0.25352/0.15757. Took 0.04 sec\n",
            "Epoch 2626, Loss(train/val) 0.26662/0.15730. Took 0.04 sec\n",
            "Epoch 2627, Loss(train/val) 0.25617/0.15659. Took 0.05 sec\n",
            "Epoch 2628, Loss(train/val) 0.25605/0.15608. Took 0.05 sec\n",
            "Epoch 2629, Loss(train/val) 0.27582/0.15639. Took 0.04 sec\n",
            "Epoch 2630, Loss(train/val) 0.25185/0.15590. Took 0.04 sec\n",
            "Epoch 2631, Loss(train/val) 0.25411/0.15572. Took 0.04 sec\n",
            "Epoch 2632, Loss(train/val) 0.25159/0.15542. Took 0.05 sec\n",
            "Epoch 2633, Loss(train/val) 0.24424/0.15543. Took 0.06 sec\n",
            "Epoch 2634, Loss(train/val) 0.25356/0.15515. Took 0.05 sec\n",
            "Epoch 2635, Loss(train/val) 0.26062/0.15494. Took 0.05 sec\n",
            "Epoch 2636, Loss(train/val) 0.25103/0.15489. Took 0.05 sec\n",
            "Epoch 2637, Loss(train/val) 0.25276/0.15505. Took 0.05 sec\n",
            "Epoch 2638, Loss(train/val) 0.24382/0.15546. Took 0.07 sec\n",
            "Epoch 2639, Loss(train/val) 0.24743/0.15569. Took 0.05 sec\n",
            "Epoch 2640, Loss(train/val) 0.25736/0.15558. Took 0.05 sec\n",
            "Epoch 2641, Loss(train/val) 0.24954/0.15609. Took 0.05 sec\n",
            "Epoch 2642, Loss(train/val) 0.25990/0.15673. Took 0.05 sec\n",
            "Epoch 2643, Loss(train/val) 0.25357/0.15664. Took 0.06 sec\n",
            "Epoch 2644, Loss(train/val) 0.25413/0.15599. Took 0.05 sec\n",
            "Epoch 2645, Loss(train/val) 0.25298/0.15581. Took 0.05 sec\n",
            "Epoch 2646, Loss(train/val) 0.23497/0.15588. Took 0.05 sec\n",
            "Epoch 2647, Loss(train/val) 0.26034/0.15563. Took 0.05 sec\n",
            "Epoch 2648, Loss(train/val) 0.24321/0.15635. Took 0.06 sec\n",
            "Epoch 2649, Loss(train/val) 0.24506/0.15631. Took 0.05 sec\n",
            "Epoch 2650, Loss(train/val) 0.25143/0.15659. Took 0.06 sec\n",
            "Epoch 2651, Loss(train/val) 0.24948/0.15583. Took 0.05 sec\n",
            "Epoch 2652, Loss(train/val) 0.25030/0.15568. Took 0.06 sec\n",
            "Epoch 2653, Loss(train/val) 0.27808/0.15587. Took 0.05 sec\n",
            "Epoch 2654, Loss(train/val) 0.25938/0.15578. Took 0.05 sec\n",
            "Epoch 2655, Loss(train/val) 0.26279/0.15523. Took 0.05 sec\n",
            "Epoch 2656, Loss(train/val) 0.25130/0.15542. Took 0.05 sec\n",
            "Epoch 2657, Loss(train/val) 0.25386/0.15517. Took 0.05 sec\n",
            "Epoch 2658, Loss(train/val) 0.26341/0.15502. Took 0.05 sec\n",
            "Epoch 2659, Loss(train/val) 0.26403/0.15493. Took 0.04 sec\n",
            "Epoch 2660, Loss(train/val) 0.24997/0.15523. Took 0.05 sec\n",
            "Epoch 2661, Loss(train/val) 0.25272/0.15542. Took 0.05 sec\n",
            "Epoch 2662, Loss(train/val) 0.25170/0.15562. Took 0.04 sec\n",
            "Epoch 2663, Loss(train/val) 0.25670/0.15502. Took 0.04 sec\n",
            "Epoch 2664, Loss(train/val) 0.24912/0.15501. Took 0.05 sec\n",
            "Epoch 2665, Loss(train/val) 0.26825/0.15555. Took 0.05 sec\n",
            "Epoch 2666, Loss(train/val) 0.24329/0.15541. Took 0.06 sec\n",
            "Epoch 2667, Loss(train/val) 0.25071/0.15544. Took 0.05 sec\n",
            "Epoch 2668, Loss(train/val) 0.24761/0.15507. Took 0.05 sec\n",
            "Epoch 2669, Loss(train/val) 0.25005/0.15497. Took 0.05 sec\n",
            "Epoch 2670, Loss(train/val) 0.25243/0.15511. Took 0.05 sec\n",
            "Epoch 2671, Loss(train/val) 0.25581/0.15523. Took 0.05 sec\n",
            "Epoch 2672, Loss(train/val) 0.24903/0.15513. Took 0.05 sec\n",
            "Epoch 2673, Loss(train/val) 0.24795/0.15495. Took 0.05 sec\n",
            "Epoch 2674, Loss(train/val) 0.25985/0.15517. Took 0.05 sec\n",
            "Epoch 2675, Loss(train/val) 0.24656/0.15550. Took 0.05 sec\n",
            "Epoch 2676, Loss(train/val) 0.25116/0.15570. Took 0.05 sec\n",
            "Epoch 2677, Loss(train/val) 0.26046/0.15525. Took 0.05 sec\n",
            "Epoch 2678, Loss(train/val) 0.24826/0.15530. Took 0.06 sec\n",
            "Epoch 2679, Loss(train/val) 0.24599/0.15560. Took 0.05 sec\n",
            "Epoch 2680, Loss(train/val) 0.25661/0.15606. Took 0.05 sec\n",
            "Epoch 2681, Loss(train/val) 0.25286/0.15580. Took 0.05 sec\n",
            "Epoch 2682, Loss(train/val) 0.24778/0.15570. Took 0.05 sec\n",
            "Epoch 2683, Loss(train/val) 0.24880/0.15557. Took 0.05 sec\n",
            "Epoch 2684, Loss(train/val) 0.25161/0.15546. Took 0.05 sec\n",
            "Epoch 2685, Loss(train/val) 0.26059/0.15558. Took 0.04 sec\n",
            "Epoch 2686, Loss(train/val) 0.25133/0.15539. Took 0.05 sec\n",
            "Epoch 2687, Loss(train/val) 0.24598/0.15523. Took 0.04 sec\n",
            "Epoch 2688, Loss(train/val) 0.24075/0.15539. Took 0.05 sec\n",
            "Epoch 2689, Loss(train/val) 0.24243/0.15512. Took 0.04 sec\n",
            "Epoch 2690, Loss(train/val) 0.25148/0.15496. Took 0.05 sec\n",
            "Epoch 2691, Loss(train/val) 0.25902/0.15523. Took 0.06 sec\n",
            "Epoch 2692, Loss(train/val) 0.24065/0.15517. Took 0.05 sec\n",
            "Epoch 2693, Loss(train/val) 0.24767/0.15503. Took 0.05 sec\n",
            "Epoch 2694, Loss(train/val) 0.24150/0.15537. Took 0.05 sec\n",
            "Epoch 2695, Loss(train/val) 0.23935/0.15547. Took 0.05 sec\n",
            "Epoch 2696, Loss(train/val) 0.24945/0.15534. Took 0.05 sec\n",
            "Epoch 2697, Loss(train/val) 0.24253/0.15501. Took 0.05 sec\n",
            "Epoch 2698, Loss(train/val) 0.24221/0.15517. Took 0.05 sec\n",
            "Epoch 2699, Loss(train/val) 0.24435/0.15526. Took 0.06 sec\n",
            "Epoch 2700, Loss(train/val) 0.24629/0.15508. Took 0.05 sec\n",
            "Epoch 2701, Loss(train/val) 0.24766/0.15549. Took 0.05 sec\n",
            "Epoch 2702, Loss(train/val) 0.25300/0.15526. Took 0.05 sec\n",
            "Epoch 2703, Loss(train/val) 0.24264/0.15552. Took 0.05 sec\n",
            "Epoch 2704, Loss(train/val) 0.24705/0.15587. Took 0.05 sec\n",
            "Epoch 2705, Loss(train/val) 0.25025/0.15596. Took 0.05 sec\n",
            "Epoch 2706, Loss(train/val) 0.25668/0.15560. Took 0.05 sec\n",
            "Epoch 2707, Loss(train/val) 0.24994/0.15554. Took 0.04 sec\n",
            "Epoch 2708, Loss(train/val) 0.26206/0.15516. Took 0.05 sec\n",
            "Epoch 2709, Loss(train/val) 0.24449/0.15509. Took 0.05 sec\n",
            "Epoch 2710, Loss(train/val) 0.25019/0.15531. Took 0.04 sec\n",
            "Epoch 2711, Loss(train/val) 0.25517/0.15542. Took 0.06 sec\n",
            "Epoch 2712, Loss(train/val) 0.25461/0.15498. Took 0.05 sec\n",
            "Epoch 2713, Loss(train/val) 0.24215/0.15545. Took 0.04 sec\n",
            "Epoch 2714, Loss(train/val) 0.24367/0.15566. Took 0.05 sec\n",
            "Epoch 2715, Loss(train/val) 0.24981/0.15584. Took 0.04 sec\n",
            "Epoch 2716, Loss(train/val) 0.24564/0.15528. Took 0.05 sec\n",
            "Epoch 2717, Loss(train/val) 0.25613/0.15487. Took 0.05 sec\n",
            "Epoch 2718, Loss(train/val) 0.24980/0.15467. Took 0.04 sec\n",
            "Epoch 2719, Loss(train/val) 0.24656/0.15478. Took 0.05 sec\n",
            "Epoch 2720, Loss(train/val) 0.24404/0.15489. Took 0.05 sec\n",
            "Epoch 2721, Loss(train/val) 0.25483/0.15545. Took 0.05 sec\n",
            "Epoch 2722, Loss(train/val) 0.25879/0.15518. Took 0.05 sec\n",
            "Epoch 2723, Loss(train/val) 0.24973/0.15545. Took 0.05 sec\n",
            "Epoch 2724, Loss(train/val) 0.25682/0.15525. Took 0.04 sec\n",
            "Epoch 2725, Loss(train/val) 0.25650/0.15585. Took 0.05 sec\n",
            "Epoch 2726, Loss(train/val) 0.25432/0.15519. Took 0.05 sec\n",
            "Epoch 2727, Loss(train/val) 0.26166/0.15480. Took 0.05 sec\n",
            "Epoch 2728, Loss(train/val) 0.25564/0.15476. Took 0.05 sec\n",
            "Epoch 2729, Loss(train/val) 0.26777/0.15473. Took 0.05 sec\n",
            "Epoch 2730, Loss(train/val) 0.25154/0.15470. Took 0.05 sec\n",
            "Epoch 2731, Loss(train/val) 0.27717/0.15573. Took 0.05 sec\n",
            "Epoch 2732, Loss(train/val) 0.25584/0.15495. Took 0.04 sec\n",
            "Epoch 2733, Loss(train/val) 0.24209/0.15541. Took 0.04 sec\n",
            "Epoch 2734, Loss(train/val) 0.26415/0.15606. Took 0.04 sec\n",
            "Epoch 2735, Loss(train/val) 0.25541/0.15535. Took 0.05 sec\n",
            "Epoch 2736, Loss(train/val) 0.25060/0.15500. Took 0.05 sec\n",
            "Epoch 2737, Loss(train/val) 0.24617/0.15517. Took 0.05 sec\n",
            "Epoch 2738, Loss(train/val) 0.26421/0.15497. Took 0.05 sec\n",
            "Epoch 2739, Loss(train/val) 0.25351/0.15560. Took 0.05 sec\n",
            "Epoch 2740, Loss(train/val) 0.24749/0.15551. Took 0.05 sec\n",
            "Epoch 2741, Loss(train/val) 0.24833/0.15513. Took 0.06 sec\n",
            "Epoch 2742, Loss(train/val) 0.28620/0.15588. Took 0.05 sec\n",
            "Epoch 2743, Loss(train/val) 0.26625/0.15580. Took 0.04 sec\n",
            "Epoch 2744, Loss(train/val) 0.25066/0.15573. Took 0.04 sec\n",
            "Epoch 2745, Loss(train/val) 0.24544/0.15507. Took 0.04 sec\n",
            "Epoch 2746, Loss(train/val) 0.25724/0.15468. Took 0.05 sec\n",
            "Epoch 2747, Loss(train/val) 0.25210/0.15441. Took 0.05 sec\n",
            "Epoch 2748, Loss(train/val) 0.25913/0.15444. Took 0.05 sec\n",
            "Epoch 2749, Loss(train/val) 0.24257/0.15491. Took 0.04 sec\n",
            "Epoch 2750, Loss(train/val) 0.24673/0.15545. Took 0.04 sec\n",
            "Epoch 2751, Loss(train/val) 0.24818/0.15539. Took 0.05 sec\n",
            "Epoch 2752, Loss(train/val) 0.25751/0.15614. Took 0.05 sec\n",
            "Epoch 2753, Loss(train/val) 0.24796/0.15591. Took 0.04 sec\n",
            "Epoch 2754, Loss(train/val) 0.25244/0.15564. Took 0.04 sec\n",
            "Epoch 2755, Loss(train/val) 0.24030/0.15493. Took 0.05 sec\n",
            "Epoch 2756, Loss(train/val) 0.24224/0.15488. Took 0.05 sec\n",
            "Epoch 2757, Loss(train/val) 0.25179/0.15460. Took 0.05 sec\n",
            "Epoch 2758, Loss(train/val) 0.23932/0.15518. Took 0.05 sec\n",
            "Epoch 2759, Loss(train/val) 0.24010/0.15511. Took 0.05 sec\n",
            "Epoch 2760, Loss(train/val) 0.24157/0.15534. Took 0.05 sec\n",
            "Epoch 2761, Loss(train/val) 0.24938/0.15555. Took 0.05 sec\n",
            "Epoch 2762, Loss(train/val) 0.25286/0.15508. Took 0.05 sec\n",
            "Epoch 2763, Loss(train/val) 0.24974/0.15531. Took 0.05 sec\n",
            "Epoch 2764, Loss(train/val) 0.25324/0.15554. Took 0.04 sec\n",
            "Epoch 2765, Loss(train/val) 0.26545/0.15521. Took 0.04 sec\n",
            "Epoch 2766, Loss(train/val) 0.24085/0.15483. Took 0.05 sec\n",
            "Epoch 2767, Loss(train/val) 0.25531/0.15484. Took 0.04 sec\n",
            "Epoch 2768, Loss(train/val) 0.26540/0.15501. Took 0.04 sec\n",
            "Epoch 2769, Loss(train/val) 0.24614/0.15539. Took 0.05 sec\n",
            "Epoch 2770, Loss(train/val) 0.25149/0.15502. Took 0.05 sec\n",
            "Epoch 2771, Loss(train/val) 0.25710/0.15520. Took 0.05 sec\n",
            "Epoch 2772, Loss(train/val) 0.24409/0.15441. Took 0.05 sec\n",
            "Epoch 2773, Loss(train/val) 0.24168/0.15456. Took 0.05 sec\n",
            "Epoch 2774, Loss(train/val) 0.24276/0.15459. Took 0.04 sec\n",
            "Epoch 2775, Loss(train/val) 0.25716/0.15499. Took 0.06 sec\n",
            "Epoch 2776, Loss(train/val) 0.24593/0.15465. Took 0.05 sec\n",
            "Epoch 2777, Loss(train/val) 0.24633/0.15492. Took 0.05 sec\n",
            "Epoch 2778, Loss(train/val) 0.23462/0.15522. Took 0.05 sec\n",
            "Epoch 2779, Loss(train/val) 0.24371/0.15501. Took 0.05 sec\n",
            "Epoch 2780, Loss(train/val) 0.24538/0.15537. Took 0.05 sec\n",
            "Epoch 2781, Loss(train/val) 0.26515/0.15538. Took 0.05 sec\n",
            "Epoch 2782, Loss(train/val) 0.24669/0.15494. Took 0.06 sec\n",
            "Epoch 2783, Loss(train/val) 0.24039/0.15495. Took 0.05 sec\n",
            "Epoch 2784, Loss(train/val) 0.24871/0.15496. Took 0.06 sec\n",
            "Epoch 2785, Loss(train/val) 0.24719/0.15474. Took 0.05 sec\n",
            "Epoch 2786, Loss(train/val) 0.24701/0.15511. Took 0.05 sec\n",
            "Epoch 2787, Loss(train/val) 0.24710/0.15511. Took 0.05 sec\n",
            "Epoch 2788, Loss(train/val) 0.25455/0.15500. Took 0.05 sec\n",
            "Epoch 2789, Loss(train/val) 0.24196/0.15477. Took 0.04 sec\n",
            "Epoch 2790, Loss(train/val) 0.25247/0.15458. Took 0.05 sec\n",
            "Epoch 2791, Loss(train/val) 0.24486/0.15483. Took 0.05 sec\n",
            "Epoch 2792, Loss(train/val) 0.25384/0.15527. Took 0.05 sec\n",
            "Epoch 2793, Loss(train/val) 0.25694/0.15587. Took 0.05 sec\n",
            "Epoch 2794, Loss(train/val) 0.24768/0.15579. Took 0.05 sec\n",
            "Epoch 2795, Loss(train/val) 0.23935/0.15495. Took 0.06 sec\n",
            "Epoch 2796, Loss(train/val) 0.25105/0.15451. Took 0.05 sec\n",
            "Epoch 2797, Loss(train/val) 0.24683/0.15439. Took 0.05 sec\n",
            "Epoch 2798, Loss(train/val) 0.24123/0.15479. Took 0.05 sec\n",
            "Epoch 2799, Loss(train/val) 0.25745/0.15580. Took 0.05 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var1 = 'l2'\n",
        "df = load_exp_result('exp4_l2')\n",
        "\n",
        "#plot_acc(var1, var2, df)\n",
        "plot_loss_variation(var1, df, sharey=False) #sharey를 True로 하면 모둔 subplot의 y축의 스케일이 같아집니다.\n",
        "#plot_acc_variation(var1, var2, df, margin_titles=True, sharey=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "G2olMR9y1iOR",
        "outputId": "8ee46556-b054-4b48-9be0-047930a24d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 923.375x216 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "### l2 = 0.00001"
      ],
      "metadata": {
        "id": "Hq_l9mRq0lNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Dropout"
      ],
      "metadata": {
        "id": "zliqZBy90t7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 666\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp5_dropout1\"\n",
        "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "args.batch_size = 193\n",
        "args.x_frames = 4\n",
        "args.y_frames = 4\n",
        "trainset = MyDataset(args.x_frames, args.y_frames, mydata, 0, int(0.6*len(mydata)))\n",
        "valset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.6*len(mydata)), int(0.8*len(mydata)))\n",
        "testset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.8*len(mydata)), int(len(mydata)))\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
        "\n",
        "args.input_dim = 1\n",
        "args.hid_dim = 16\n",
        "args.n_layers = 8\n",
        "\n",
        "args.l2 = 0.00001\n",
        "args.dropout = 0.0\n",
        "args.use_bn = True\n",
        "\n",
        "args.optim = 'Adam'\n",
        "args.lr = 0.00005\n",
        "args.epoch = 2800\n",
        "\n",
        "name_var1 = 'dropout'\n",
        "list_var1 = [0.0, 0.2, 0.4, 0.6, 0.8]\n",
        "\n",
        "for var1 in list_var1:\n",
        "    setattr(args, name_var1, var1)\n",
        "    print(args)\n",
        "                \n",
        "    setting, result = experiment(partition, deepcopy(args))\n",
        "    save_exp_result(setting, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnXpUU4bDuUh",
        "outputId": "3ac5d162-4048-4737-f496-184e8da30f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Epoch 601, Loss(train/val) 0.56332/0.28251. Took 0.05 sec\n",
            "Epoch 602, Loss(train/val) 0.53156/0.27958. Took 0.05 sec\n",
            "Epoch 603, Loss(train/val) 0.54083/0.28326. Took 0.05 sec\n",
            "Epoch 604, Loss(train/val) 0.55543/0.28879. Took 0.06 sec\n",
            "Epoch 605, Loss(train/val) 0.55941/0.29313. Took 0.05 sec\n",
            "Epoch 606, Loss(train/val) 0.55897/0.28360. Took 0.05 sec\n",
            "Epoch 607, Loss(train/val) 0.56134/0.27717. Took 0.06 sec\n",
            "Epoch 608, Loss(train/val) 0.56825/0.27703. Took 0.05 sec\n",
            "Epoch 609, Loss(train/val) 0.52418/0.27977. Took 0.05 sec\n",
            "Epoch 610, Loss(train/val) 0.54266/0.27773. Took 0.05 sec\n",
            "Epoch 611, Loss(train/val) 0.53986/0.27790. Took 0.05 sec\n",
            "Epoch 612, Loss(train/val) 0.56873/0.27361. Took 0.06 sec\n",
            "Epoch 613, Loss(train/val) 0.54834/0.27158. Took 0.05 sec\n",
            "Epoch 614, Loss(train/val) 0.55543/0.27110. Took 0.05 sec\n",
            "Epoch 615, Loss(train/val) 0.54405/0.27034. Took 0.04 sec\n",
            "Epoch 616, Loss(train/val) 0.53875/0.26962. Took 0.05 sec\n",
            "Epoch 617, Loss(train/val) 0.56395/0.26870. Took 0.05 sec\n",
            "Epoch 618, Loss(train/val) 0.55099/0.26915. Took 0.05 sec\n",
            "Epoch 619, Loss(train/val) 0.55437/0.26756. Took 0.05 sec\n",
            "Epoch 620, Loss(train/val) 0.54921/0.26683. Took 0.05 sec\n",
            "Epoch 621, Loss(train/val) 0.55685/0.26624. Took 0.05 sec\n",
            "Epoch 622, Loss(train/val) 0.53027/0.26531. Took 0.05 sec\n",
            "Epoch 623, Loss(train/val) 0.53175/0.26475. Took 0.05 sec\n",
            "Epoch 624, Loss(train/val) 0.53677/0.26362. Took 0.05 sec\n",
            "Epoch 625, Loss(train/val) 0.57526/0.26516. Took 0.05 sec\n",
            "Epoch 626, Loss(train/val) 0.54912/0.26753. Took 0.05 sec\n",
            "Epoch 627, Loss(train/val) 0.56008/0.26499. Took 0.06 sec\n",
            "Epoch 628, Loss(train/val) 0.54965/0.26410. Took 0.05 sec\n",
            "Epoch 629, Loss(train/val) 0.54144/0.26411. Took 0.05 sec\n",
            "Epoch 630, Loss(train/val) 0.55659/0.26581. Took 0.05 sec\n",
            "Epoch 631, Loss(train/val) 0.54695/0.26485. Took 0.05 sec\n",
            "Epoch 632, Loss(train/val) 0.56304/0.26303. Took 0.05 sec\n",
            "Epoch 633, Loss(train/val) 0.56336/0.26261. Took 0.06 sec\n",
            "Epoch 634, Loss(train/val) 0.53915/0.26274. Took 0.05 sec\n",
            "Epoch 635, Loss(train/val) 0.57105/0.26400. Took 0.04 sec\n",
            "Epoch 636, Loss(train/val) 0.56829/0.26488. Took 0.05 sec\n",
            "Epoch 637, Loss(train/val) 0.54190/0.26541. Took 0.05 sec\n",
            "Epoch 638, Loss(train/val) 0.55344/0.26618. Took 0.05 sec\n",
            "Epoch 639, Loss(train/val) 0.53796/0.26985. Took 0.05 sec\n",
            "Epoch 640, Loss(train/val) 0.54755/0.27048. Took 0.04 sec\n",
            "Epoch 641, Loss(train/val) 0.54044/0.26916. Took 0.04 sec\n",
            "Epoch 642, Loss(train/val) 0.54601/0.26465. Took 0.05 sec\n",
            "Epoch 643, Loss(train/val) 0.56678/0.26305. Took 0.05 sec\n",
            "Epoch 644, Loss(train/val) 0.54232/0.26258. Took 0.04 sec\n",
            "Epoch 645, Loss(train/val) 0.52456/0.26159. Took 0.05 sec\n",
            "Epoch 646, Loss(train/val) 0.53655/0.26125. Took 0.05 sec\n",
            "Epoch 647, Loss(train/val) 0.55058/0.26122. Took 0.06 sec\n",
            "Epoch 648, Loss(train/val) 0.56821/0.26122. Took 0.05 sec\n",
            "Epoch 649, Loss(train/val) 0.55750/0.26098. Took 0.05 sec\n",
            "Epoch 650, Loss(train/val) 0.54212/0.26102. Took 0.05 sec\n",
            "Epoch 651, Loss(train/val) 0.55863/0.26263. Took 0.05 sec\n",
            "Epoch 652, Loss(train/val) 0.54781/0.26656. Took 0.05 sec\n",
            "Epoch 653, Loss(train/val) 0.51779/0.26942. Took 0.05 sec\n",
            "Epoch 654, Loss(train/val) 0.52735/0.26992. Took 0.05 sec\n",
            "Epoch 655, Loss(train/val) 0.54556/0.26748. Took 0.05 sec\n",
            "Epoch 656, Loss(train/val) 0.55204/0.26807. Took 0.04 sec\n",
            "Epoch 657, Loss(train/val) 0.54752/0.27179. Took 0.05 sec\n",
            "Epoch 658, Loss(train/val) 0.53331/0.27371. Took 0.05 sec\n",
            "Epoch 659, Loss(train/val) 0.55055/0.27100. Took 0.05 sec\n",
            "Epoch 660, Loss(train/val) 0.53944/0.26832. Took 0.05 sec\n",
            "Epoch 661, Loss(train/val) 0.54313/0.26519. Took 0.04 sec\n",
            "Epoch 662, Loss(train/val) 0.52604/0.26198. Took 0.05 sec\n",
            "Epoch 663, Loss(train/val) 0.54486/0.26244. Took 0.05 sec\n",
            "Epoch 664, Loss(train/val) 0.51828/0.26359. Took 0.05 sec\n",
            "Epoch 665, Loss(train/val) 0.52331/0.26676. Took 0.05 sec\n",
            "Epoch 666, Loss(train/val) 0.54287/0.27532. Took 0.05 sec\n",
            "Epoch 667, Loss(train/val) 0.53081/0.28717. Took 0.05 sec\n",
            "Epoch 668, Loss(train/val) 0.54742/0.29212. Took 0.05 sec\n",
            "Epoch 669, Loss(train/val) 0.52659/0.29372. Took 0.05 sec\n",
            "Epoch 670, Loss(train/val) 0.53379/0.29270. Took 0.05 sec\n",
            "Epoch 671, Loss(train/val) 0.53066/0.28147. Took 0.05 sec\n",
            "Epoch 672, Loss(train/val) 0.53388/0.27308. Took 0.05 sec\n",
            "Epoch 673, Loss(train/val) 0.53188/0.26921. Took 0.05 sec\n",
            "Epoch 674, Loss(train/val) 0.53075/0.26554. Took 0.05 sec\n",
            "Epoch 675, Loss(train/val) 0.54526/0.26336. Took 0.05 sec\n",
            "Epoch 676, Loss(train/val) 0.56195/0.26001. Took 0.05 sec\n",
            "Epoch 677, Loss(train/val) 0.55201/0.25923. Took 0.05 sec\n",
            "Epoch 678, Loss(train/val) 0.54717/0.25841. Took 0.04 sec\n",
            "Epoch 679, Loss(train/val) 0.55824/0.25777. Took 0.05 sec\n",
            "Epoch 680, Loss(train/val) 0.54683/0.25704. Took 0.05 sec\n",
            "Epoch 681, Loss(train/val) 0.52084/0.25707. Took 0.05 sec\n",
            "Epoch 682, Loss(train/val) 0.51696/0.25852. Took 0.05 sec\n",
            "Epoch 683, Loss(train/val) 0.55158/0.26025. Took 0.05 sec\n",
            "Epoch 684, Loss(train/val) 0.55854/0.26054. Took 0.05 sec\n",
            "Epoch 685, Loss(train/val) 0.54482/0.26021. Took 0.05 sec\n",
            "Epoch 686, Loss(train/val) 0.52601/0.26225. Took 0.04 sec\n",
            "Epoch 687, Loss(train/val) 0.52642/0.26863. Took 0.05 sec\n",
            "Epoch 688, Loss(train/val) 0.51610/0.27463. Took 0.04 sec\n",
            "Epoch 689, Loss(train/val) 0.53951/0.28044. Took 0.05 sec\n",
            "Epoch 690, Loss(train/val) 0.53039/0.27504. Took 0.05 sec\n",
            "Epoch 691, Loss(train/val) 0.54943/0.26397. Took 0.05 sec\n",
            "Epoch 692, Loss(train/val) 0.53341/0.25881. Took 0.05 sec\n",
            "Epoch 693, Loss(train/val) 0.52832/0.25548. Took 0.05 sec\n",
            "Epoch 694, Loss(train/val) 0.52590/0.25449. Took 0.05 sec\n",
            "Epoch 695, Loss(train/val) 0.52114/0.25480. Took 0.04 sec\n",
            "Epoch 696, Loss(train/val) 0.53351/0.25405. Took 0.05 sec\n",
            "Epoch 697, Loss(train/val) 0.53122/0.25443. Took 0.06 sec\n",
            "Epoch 698, Loss(train/val) 0.53997/0.25266. Took 0.04 sec\n",
            "Epoch 699, Loss(train/val) 0.54133/0.25289. Took 0.04 sec\n",
            "Epoch 700, Loss(train/val) 0.50603/0.25170. Took 0.05 sec\n",
            "Epoch 701, Loss(train/val) 0.52490/0.25194. Took 0.04 sec\n",
            "Epoch 702, Loss(train/val) 0.52869/0.25262. Took 0.05 sec\n",
            "Epoch 703, Loss(train/val) 0.53591/0.25655. Took 0.04 sec\n",
            "Epoch 704, Loss(train/val) 0.53727/0.26191. Took 0.05 sec\n",
            "Epoch 705, Loss(train/val) 0.52458/0.27570. Took 0.04 sec\n",
            "Epoch 706, Loss(train/val) 0.52746/0.28554. Took 0.05 sec\n",
            "Epoch 707, Loss(train/val) 0.53302/0.29581. Took 0.05 sec\n",
            "Epoch 708, Loss(train/val) 0.53886/0.29583. Took 0.05 sec\n",
            "Epoch 709, Loss(train/val) 0.53240/0.28707. Took 0.05 sec\n",
            "Epoch 710, Loss(train/val) 0.52023/0.27404. Took 0.04 sec\n",
            "Epoch 711, Loss(train/val) 0.53979/0.26716. Took 0.04 sec\n",
            "Epoch 712, Loss(train/val) 0.52916/0.26283. Took 0.05 sec\n",
            "Epoch 713, Loss(train/val) 0.50262/0.26015. Took 0.05 sec\n",
            "Epoch 714, Loss(train/val) 0.54259/0.25881. Took 0.05 sec\n",
            "Epoch 715, Loss(train/val) 0.55720/0.25764. Took 0.05 sec\n",
            "Epoch 716, Loss(train/val) 0.52022/0.25601. Took 0.04 sec\n",
            "Epoch 717, Loss(train/val) 0.51067/0.25852. Took 0.05 sec\n",
            "Epoch 718, Loss(train/val) 0.53243/0.26081. Took 0.05 sec\n",
            "Epoch 719, Loss(train/val) 0.51770/0.26193. Took 0.05 sec\n",
            "Epoch 720, Loss(train/val) 0.53019/0.26091. Took 0.04 sec\n",
            "Epoch 721, Loss(train/val) 0.51934/0.25928. Took 0.05 sec\n",
            "Epoch 722, Loss(train/val) 0.53186/0.25819. Took 0.05 sec\n",
            "Epoch 723, Loss(train/val) 0.54907/0.25353. Took 0.04 sec\n",
            "Epoch 724, Loss(train/val) 0.51481/0.25203. Took 0.04 sec\n",
            "Epoch 725, Loss(train/val) 0.52699/0.25110. Took 0.05 sec\n",
            "Epoch 726, Loss(train/val) 0.50062/0.25122. Took 0.05 sec\n",
            "Epoch 727, Loss(train/val) 0.53722/0.25145. Took 0.05 sec\n",
            "Epoch 728, Loss(train/val) 0.51853/0.25077. Took 0.04 sec\n",
            "Epoch 729, Loss(train/val) 0.51251/0.25030. Took 0.04 sec\n",
            "Epoch 730, Loss(train/val) 0.54279/0.25028. Took 0.04 sec\n",
            "Epoch 731, Loss(train/val) 0.52530/0.25197. Took 0.04 sec\n",
            "Epoch 732, Loss(train/val) 0.53737/0.25247. Took 0.05 sec\n",
            "Epoch 733, Loss(train/val) 0.52276/0.25452. Took 0.05 sec\n",
            "Epoch 734, Loss(train/val) 0.52935/0.25924. Took 0.05 sec\n",
            "Epoch 735, Loss(train/val) 0.53794/0.26222. Took 0.05 sec\n",
            "Epoch 736, Loss(train/val) 0.52897/0.26156. Took 0.05 sec\n",
            "Epoch 737, Loss(train/val) 0.51671/0.25817. Took 0.05 sec\n",
            "Epoch 738, Loss(train/val) 0.52144/0.25443. Took 0.05 sec\n",
            "Epoch 739, Loss(train/val) 0.53230/0.25320. Took 0.04 sec\n",
            "Epoch 740, Loss(train/val) 0.51541/0.25045. Took 0.05 sec\n",
            "Epoch 741, Loss(train/val) 0.53840/0.25212. Took 0.04 sec\n",
            "Epoch 742, Loss(train/val) 0.52818/0.25082. Took 0.05 sec\n",
            "Epoch 743, Loss(train/val) 0.52610/0.25044. Took 0.05 sec\n",
            "Epoch 744, Loss(train/val) 0.52247/0.25133. Took 0.04 sec\n",
            "Epoch 745, Loss(train/val) 0.52597/0.25445. Took 0.05 sec\n",
            "Epoch 746, Loss(train/val) 0.51788/0.25910. Took 0.05 sec\n",
            "Epoch 747, Loss(train/val) 0.53159/0.26298. Took 0.06 sec\n",
            "Epoch 748, Loss(train/val) 0.51557/0.26455. Took 0.05 sec\n",
            "Epoch 749, Loss(train/val) 0.51805/0.26545. Took 0.04 sec\n",
            "Epoch 750, Loss(train/val) 0.51746/0.25707. Took 0.04 sec\n",
            "Epoch 751, Loss(train/val) 0.50063/0.25427. Took 0.04 sec\n",
            "Epoch 752, Loss(train/val) 0.53862/0.25426. Took 0.05 sec\n",
            "Epoch 753, Loss(train/val) 0.50555/0.25321. Took 0.05 sec\n",
            "Epoch 754, Loss(train/val) 0.51084/0.24866. Took 0.05 sec\n",
            "Epoch 755, Loss(train/val) 0.52725/0.24797. Took 0.04 sec\n",
            "Epoch 756, Loss(train/val) 0.52276/0.24713. Took 0.04 sec\n",
            "Epoch 757, Loss(train/val) 0.50834/0.24616. Took 0.05 sec\n",
            "Epoch 758, Loss(train/val) 0.51613/0.24585. Took 0.04 sec\n",
            "Epoch 759, Loss(train/val) 0.48648/0.24566. Took 0.05 sec\n",
            "Epoch 760, Loss(train/val) 0.50017/0.24630. Took 0.05 sec\n",
            "Epoch 761, Loss(train/val) 0.51654/0.24717. Took 0.04 sec\n",
            "Epoch 762, Loss(train/val) 0.52333/0.25017. Took 0.06 sec\n",
            "Epoch 763, Loss(train/val) 0.51893/0.25282. Took 0.05 sec\n",
            "Epoch 764, Loss(train/val) 0.53035/0.25666. Took 0.04 sec\n",
            "Epoch 765, Loss(train/val) 0.53526/0.25506. Took 0.05 sec\n",
            "Epoch 766, Loss(train/val) 0.50615/0.25701. Took 0.04 sec\n",
            "Epoch 767, Loss(train/val) 0.50282/0.26364. Took 0.05 sec\n",
            "Epoch 768, Loss(train/val) 0.49707/0.26549. Took 0.05 sec\n",
            "Epoch 769, Loss(train/val) 0.51145/0.26953. Took 0.05 sec\n",
            "Epoch 770, Loss(train/val) 0.51531/0.27000. Took 0.04 sec\n",
            "Epoch 771, Loss(train/val) 0.49787/0.26621. Took 0.05 sec\n",
            "Epoch 772, Loss(train/val) 0.52069/0.26753. Took 0.05 sec\n",
            "Epoch 773, Loss(train/val) 0.51122/0.26245. Took 0.05 sec\n",
            "Epoch 774, Loss(train/val) 0.52102/0.26163. Took 0.05 sec\n",
            "Epoch 775, Loss(train/val) 0.51206/0.26104. Took 0.05 sec\n",
            "Epoch 776, Loss(train/val) 0.52364/0.25490. Took 0.05 sec\n",
            "Epoch 777, Loss(train/val) 0.49089/0.24710. Took 0.05 sec\n",
            "Epoch 778, Loss(train/val) 0.52248/0.24428. Took 0.07 sec\n",
            "Epoch 779, Loss(train/val) 0.51970/0.24700. Took 0.07 sec\n",
            "Epoch 780, Loss(train/val) 0.51632/0.25027. Took 0.07 sec\n",
            "Epoch 781, Loss(train/val) 0.51774/0.25273. Took 0.08 sec\n",
            "Epoch 782, Loss(train/val) 0.49577/0.25342. Took 0.07 sec\n",
            "Epoch 783, Loss(train/val) 0.51499/0.25548. Took 0.08 sec\n",
            "Epoch 784, Loss(train/val) 0.50415/0.26046. Took 0.07 sec\n",
            "Epoch 785, Loss(train/val) 0.50811/0.25947. Took 0.07 sec\n",
            "Epoch 786, Loss(train/val) 0.50573/0.25343. Took 0.08 sec\n",
            "Epoch 787, Loss(train/val) 0.50690/0.24783. Took 0.08 sec\n",
            "Epoch 788, Loss(train/val) 0.49406/0.24573. Took 0.08 sec\n",
            "Epoch 789, Loss(train/val) 0.51723/0.24684. Took 0.08 sec\n",
            "Epoch 790, Loss(train/val) 0.54228/0.24691. Took 0.08 sec\n",
            "Epoch 791, Loss(train/val) 0.50622/0.24499. Took 0.08 sec\n",
            "Epoch 792, Loss(train/val) 0.53553/0.24465. Took 0.09 sec\n",
            "Epoch 793, Loss(train/val) 0.50626/0.24405. Took 0.07 sec\n",
            "Epoch 794, Loss(train/val) 0.51896/0.24476. Took 0.08 sec\n",
            "Epoch 795, Loss(train/val) 0.51160/0.24626. Took 0.08 sec\n",
            "Epoch 796, Loss(train/val) 0.50880/0.24613. Took 0.08 sec\n",
            "Epoch 797, Loss(train/val) 0.50363/0.24807. Took 0.07 sec\n",
            "Epoch 798, Loss(train/val) 0.49400/0.24783. Took 0.08 sec\n",
            "Epoch 799, Loss(train/val) 0.49356/0.24643. Took 0.07 sec\n",
            "Epoch 800, Loss(train/val) 0.49649/0.24703. Took 0.07 sec\n",
            "Epoch 801, Loss(train/val) 0.53785/0.24874. Took 0.07 sec\n",
            "Epoch 802, Loss(train/val) 0.49895/0.25225. Took 0.08 sec\n",
            "Epoch 803, Loss(train/val) 0.52282/0.24956. Took 0.07 sec\n",
            "Epoch 804, Loss(train/val) 0.50103/0.24950. Took 0.07 sec\n",
            "Epoch 805, Loss(train/val) 0.49516/0.25077. Took 0.08 sec\n",
            "Epoch 806, Loss(train/val) 0.50400/0.24650. Took 0.07 sec\n",
            "Epoch 807, Loss(train/val) 0.48444/0.24238. Took 0.07 sec\n",
            "Epoch 808, Loss(train/val) 0.50952/0.24101. Took 0.05 sec\n",
            "Epoch 809, Loss(train/val) 0.52617/0.24329. Took 0.05 sec\n",
            "Epoch 810, Loss(train/val) 0.50770/0.24346. Took 0.05 sec\n",
            "Epoch 811, Loss(train/val) 0.50897/0.24539. Took 0.05 sec\n",
            "Epoch 812, Loss(train/val) 0.51919/0.24620. Took 0.05 sec\n",
            "Epoch 813, Loss(train/val) 0.50158/0.24543. Took 0.05 sec\n",
            "Epoch 814, Loss(train/val) 0.50020/0.24190. Took 0.06 sec\n",
            "Epoch 815, Loss(train/val) 0.46647/0.24395. Took 0.05 sec\n",
            "Epoch 816, Loss(train/val) 0.52046/0.24490. Took 0.05 sec\n",
            "Epoch 817, Loss(train/val) 0.51785/0.24257. Took 0.05 sec\n",
            "Epoch 818, Loss(train/val) 0.50076/0.24008. Took 0.05 sec\n",
            "Epoch 819, Loss(train/val) 0.51227/0.24346. Took 0.05 sec\n",
            "Epoch 820, Loss(train/val) 0.49880/0.25003. Took 0.05 sec\n",
            "Epoch 821, Loss(train/val) 0.51378/0.25133. Took 0.04 sec\n",
            "Epoch 822, Loss(train/val) 0.51002/0.25135. Took 0.04 sec\n",
            "Epoch 823, Loss(train/val) 0.45356/0.24389. Took 0.05 sec\n",
            "Epoch 824, Loss(train/val) 0.49597/0.24090. Took 0.06 sec\n",
            "Epoch 825, Loss(train/val) 0.49182/0.24170. Took 0.04 sec\n",
            "Epoch 826, Loss(train/val) 0.50203/0.24248. Took 0.05 sec\n",
            "Epoch 827, Loss(train/val) 0.49764/0.24306. Took 0.04 sec\n",
            "Epoch 828, Loss(train/val) 0.50456/0.24272. Took 0.04 sec\n",
            "Epoch 829, Loss(train/val) 0.51906/0.24189. Took 0.06 sec\n",
            "Epoch 830, Loss(train/val) 0.50303/0.24271. Took 0.05 sec\n",
            "Epoch 831, Loss(train/val) 0.50713/0.24329. Took 0.05 sec\n",
            "Epoch 832, Loss(train/val) 0.51236/0.24258. Took 0.05 sec\n",
            "Epoch 833, Loss(train/val) 0.49835/0.24231. Took 0.05 sec\n",
            "Epoch 834, Loss(train/val) 0.51614/0.24224. Took 0.06 sec\n",
            "Epoch 835, Loss(train/val) 0.49477/0.23986. Took 0.05 sec\n",
            "Epoch 836, Loss(train/val) 0.51399/0.23816. Took 0.05 sec\n",
            "Epoch 837, Loss(train/val) 0.50721/0.24043. Took 0.05 sec\n",
            "Epoch 838, Loss(train/val) 0.49870/0.24233. Took 0.05 sec\n",
            "Epoch 839, Loss(train/val) 0.49618/0.24202. Took 0.05 sec\n",
            "Epoch 840, Loss(train/val) 0.53148/0.24152. Took 0.05 sec\n",
            "Epoch 841, Loss(train/val) 0.50613/0.23926. Took 0.05 sec\n",
            "Epoch 842, Loss(train/val) 0.51235/0.23944. Took 0.05 sec\n",
            "Epoch 843, Loss(train/val) 0.47497/0.23904. Took 0.05 sec\n",
            "Epoch 844, Loss(train/val) 0.50332/0.23939. Took 0.06 sec\n",
            "Epoch 845, Loss(train/val) 0.51125/0.23949. Took 0.05 sec\n",
            "Epoch 846, Loss(train/val) 0.49433/0.23964. Took 0.05 sec\n",
            "Epoch 847, Loss(train/val) 0.49563/0.24480. Took 0.05 sec\n",
            "Epoch 848, Loss(train/val) 0.50871/0.24922. Took 0.05 sec\n",
            "Epoch 849, Loss(train/val) 0.50407/0.25628. Took 0.05 sec\n",
            "Epoch 850, Loss(train/val) 0.49718/0.25427. Took 0.05 sec\n",
            "Epoch 851, Loss(train/val) 0.50246/0.25137. Took 0.04 sec\n",
            "Epoch 852, Loss(train/val) 0.49358/0.24487. Took 0.04 sec\n",
            "Epoch 853, Loss(train/val) 0.51593/0.24184. Took 0.04 sec\n",
            "Epoch 854, Loss(train/val) 0.50625/0.24033. Took 0.05 sec\n",
            "Epoch 855, Loss(train/val) 0.48361/0.23847. Took 0.05 sec\n",
            "Epoch 856, Loss(train/val) 0.47997/0.23837. Took 0.05 sec\n",
            "Epoch 857, Loss(train/val) 0.46823/0.23931. Took 0.04 sec\n",
            "Epoch 858, Loss(train/val) 0.49057/0.24008. Took 0.04 sec\n",
            "Epoch 859, Loss(train/val) 0.48007/0.23899. Took 0.05 sec\n",
            "Epoch 860, Loss(train/val) 0.49054/0.23835. Took 0.05 sec\n",
            "Epoch 861, Loss(train/val) 0.51205/0.23742. Took 0.05 sec\n",
            "Epoch 862, Loss(train/val) 0.51729/0.23714. Took 0.05 sec\n",
            "Epoch 863, Loss(train/val) 0.50599/0.23762. Took 0.05 sec\n",
            "Epoch 864, Loss(train/val) 0.48451/0.23784. Took 0.05 sec\n",
            "Epoch 865, Loss(train/val) 0.51251/0.23756. Took 0.05 sec\n",
            "Epoch 866, Loss(train/val) 0.48258/0.23822. Took 0.05 sec\n",
            "Epoch 867, Loss(train/val) 0.48259/0.23876. Took 0.04 sec\n",
            "Epoch 868, Loss(train/val) 0.51363/0.23793. Took 0.04 sec\n",
            "Epoch 869, Loss(train/val) 0.52343/0.23710. Took 0.05 sec\n",
            "Epoch 870, Loss(train/val) 0.50083/0.23871. Took 0.04 sec\n",
            "Epoch 871, Loss(train/val) 0.51115/0.23778. Took 0.05 sec\n",
            "Epoch 872, Loss(train/val) 0.47846/0.23718. Took 0.05 sec\n",
            "Epoch 873, Loss(train/val) 0.51164/0.24095. Took 0.04 sec\n",
            "Epoch 874, Loss(train/val) 0.48860/0.23917. Took 0.05 sec\n",
            "Epoch 875, Loss(train/val) 0.52688/0.23872. Took 0.05 sec\n",
            "Epoch 876, Loss(train/val) 0.51212/0.23987. Took 0.05 sec\n",
            "Epoch 877, Loss(train/val) 0.51690/0.23728. Took 0.04 sec\n",
            "Epoch 878, Loss(train/val) 0.47407/0.23529. Took 0.04 sec\n",
            "Epoch 879, Loss(train/val) 0.53743/0.23478. Took 0.05 sec\n",
            "Epoch 880, Loss(train/val) 0.49694/0.23523. Took 0.05 sec\n",
            "Epoch 881, Loss(train/val) 0.47493/0.23572. Took 0.05 sec\n",
            "Epoch 882, Loss(train/val) 0.46920/0.23800. Took 0.05 sec\n",
            "Epoch 883, Loss(train/val) 0.48433/0.24084. Took 0.05 sec\n",
            "Epoch 884, Loss(train/val) 0.49083/0.24035. Took 0.05 sec\n",
            "Epoch 885, Loss(train/val) 0.50503/0.24039. Took 0.05 sec\n",
            "Epoch 886, Loss(train/val) 0.51301/0.23965. Took 0.05 sec\n",
            "Epoch 887, Loss(train/val) 0.48676/0.23809. Took 0.04 sec\n",
            "Epoch 888, Loss(train/val) 0.48052/0.23855. Took 0.04 sec\n",
            "Epoch 889, Loss(train/val) 0.52143/0.23568. Took 0.05 sec\n",
            "Epoch 890, Loss(train/val) 0.49305/0.23607. Took 0.05 sec\n",
            "Epoch 891, Loss(train/val) 0.48215/0.23519. Took 0.04 sec\n",
            "Epoch 892, Loss(train/val) 0.50517/0.23807. Took 0.05 sec\n",
            "Epoch 893, Loss(train/val) 0.48368/0.23662. Took 0.06 sec\n",
            "Epoch 894, Loss(train/val) 0.49622/0.23958. Took 0.05 sec\n",
            "Epoch 895, Loss(train/val) 0.51414/0.23939. Took 0.04 sec\n",
            "Epoch 896, Loss(train/val) 0.49266/0.24141. Took 0.05 sec\n",
            "Epoch 897, Loss(train/val) 0.50228/0.24679. Took 0.05 sec\n",
            "Epoch 898, Loss(train/val) 0.47920/0.24948. Took 0.05 sec\n",
            "Epoch 899, Loss(train/val) 0.51192/0.25144. Took 0.05 sec\n",
            "Epoch 900, Loss(train/val) 0.49936/0.25026. Took 0.05 sec\n",
            "Epoch 901, Loss(train/val) 0.48709/0.25449. Took 0.05 sec\n",
            "Epoch 902, Loss(train/val) 0.48552/0.25846. Took 0.05 sec\n",
            "Epoch 903, Loss(train/val) 0.49758/0.26603. Took 0.05 sec\n",
            "Epoch 904, Loss(train/val) 0.49050/0.26191. Took 0.05 sec\n",
            "Epoch 905, Loss(train/val) 0.49028/0.24406. Took 0.05 sec\n",
            "Epoch 906, Loss(train/val) 0.48166/0.23545. Took 0.04 sec\n",
            "Epoch 907, Loss(train/val) 0.48564/0.23282. Took 0.05 sec\n",
            "Epoch 908, Loss(train/val) 0.49920/0.23311. Took 0.05 sec\n",
            "Epoch 909, Loss(train/val) 0.50613/0.23990. Took 0.05 sec\n",
            "Epoch 910, Loss(train/val) 0.49736/0.24946. Took 0.04 sec\n",
            "Epoch 911, Loss(train/val) 0.50268/0.25757. Took 0.04 sec\n",
            "Epoch 912, Loss(train/val) 0.47304/0.26272. Took 0.04 sec\n",
            "Epoch 913, Loss(train/val) 0.49313/0.25942. Took 0.04 sec\n",
            "Epoch 914, Loss(train/val) 0.51297/0.25754. Took 0.06 sec\n",
            "Epoch 915, Loss(train/val) 0.46959/0.25190. Took 0.04 sec\n",
            "Epoch 916, Loss(train/val) 0.48858/0.24307. Took 0.05 sec\n",
            "Epoch 917, Loss(train/val) 0.47154/0.23891. Took 0.05 sec\n",
            "Epoch 918, Loss(train/val) 0.49630/0.23611. Took 0.04 sec\n",
            "Epoch 919, Loss(train/val) 0.48856/0.23414. Took 0.05 sec\n",
            "Epoch 920, Loss(train/val) 0.52648/0.23477. Took 0.04 sec\n",
            "Epoch 921, Loss(train/val) 0.47359/0.23455. Took 0.05 sec\n",
            "Epoch 922, Loss(train/val) 0.50776/0.23385. Took 0.05 sec\n",
            "Epoch 923, Loss(train/val) 0.49036/0.23421. Took 0.05 sec\n",
            "Epoch 924, Loss(train/val) 0.50472/0.23486. Took 0.06 sec\n",
            "Epoch 925, Loss(train/val) 0.48087/0.23552. Took 0.05 sec\n",
            "Epoch 926, Loss(train/val) 0.46778/0.23903. Took 0.05 sec\n",
            "Epoch 927, Loss(train/val) 0.49097/0.23953. Took 0.04 sec\n",
            "Epoch 928, Loss(train/val) 0.48016/0.23627. Took 0.05 sec\n",
            "Epoch 929, Loss(train/val) 0.49299/0.23596. Took 0.05 sec\n",
            "Epoch 930, Loss(train/val) 0.49869/0.23683. Took 0.05 sec\n",
            "Epoch 931, Loss(train/val) 0.47513/0.23427. Took 0.05 sec\n",
            "Epoch 932, Loss(train/val) 0.48219/0.23082. Took 0.05 sec\n",
            "Epoch 933, Loss(train/val) 0.49796/0.22852. Took 0.04 sec\n",
            "Epoch 934, Loss(train/val) 0.46708/0.22743. Took 0.05 sec\n",
            "Epoch 935, Loss(train/val) 0.49927/0.22911. Took 0.05 sec\n",
            "Epoch 936, Loss(train/val) 0.47908/0.23244. Took 0.05 sec\n",
            "Epoch 937, Loss(train/val) 0.49683/0.23465. Took 0.05 sec\n",
            "Epoch 938, Loss(train/val) 0.48286/0.23615. Took 0.05 sec\n",
            "Epoch 939, Loss(train/val) 0.47804/0.23502. Took 0.05 sec\n",
            "Epoch 940, Loss(train/val) 0.49282/0.23317. Took 0.05 sec\n",
            "Epoch 941, Loss(train/val) 0.48318/0.23279. Took 0.05 sec\n",
            "Epoch 942, Loss(train/val) 0.48330/0.23028. Took 0.05 sec\n",
            "Epoch 943, Loss(train/val) 0.51384/0.22943. Took 0.05 sec\n",
            "Epoch 944, Loss(train/val) 0.47172/0.23194. Took 0.05 sec\n",
            "Epoch 945, Loss(train/val) 0.49334/0.23830. Took 0.05 sec\n",
            "Epoch 946, Loss(train/val) 0.48270/0.23388. Took 0.04 sec\n",
            "Epoch 947, Loss(train/val) 0.50142/0.23499. Took 0.04 sec\n",
            "Epoch 948, Loss(train/val) 0.47087/0.23795. Took 0.05 sec\n",
            "Epoch 949, Loss(train/val) 0.44882/0.24190. Took 0.05 sec\n",
            "Epoch 950, Loss(train/val) 0.49020/0.24207. Took 0.04 sec\n",
            "Epoch 951, Loss(train/val) 0.49222/0.23739. Took 0.05 sec\n",
            "Epoch 952, Loss(train/val) 0.49378/0.23710. Took 0.04 sec\n",
            "Epoch 953, Loss(train/val) 0.49313/0.23592. Took 0.05 sec\n",
            "Epoch 954, Loss(train/val) 0.48324/0.23668. Took 0.06 sec\n",
            "Epoch 955, Loss(train/val) 0.48526/0.23520. Took 0.05 sec\n",
            "Epoch 956, Loss(train/val) 0.49691/0.23338. Took 0.04 sec\n",
            "Epoch 957, Loss(train/val) 0.48332/0.23397. Took 0.05 sec\n",
            "Epoch 958, Loss(train/val) 0.50337/0.22956. Took 0.05 sec\n",
            "Epoch 959, Loss(train/val) 0.45514/0.22894. Took 0.05 sec\n",
            "Epoch 960, Loss(train/val) 0.47017/0.22900. Took 0.04 sec\n",
            "Epoch 961, Loss(train/val) 0.46482/0.23071. Took 0.05 sec\n",
            "Epoch 962, Loss(train/val) 0.49080/0.23021. Took 0.05 sec\n",
            "Epoch 963, Loss(train/val) 0.48962/0.22992. Took 0.04 sec\n",
            "Epoch 964, Loss(train/val) 0.49652/0.22774. Took 0.05 sec\n",
            "Epoch 965, Loss(train/val) 0.49879/0.22680. Took 0.04 sec\n",
            "Epoch 966, Loss(train/val) 0.48028/0.22764. Took 0.05 sec\n",
            "Epoch 967, Loss(train/val) 0.48894/0.22706. Took 0.04 sec\n",
            "Epoch 968, Loss(train/val) 0.46491/0.22674. Took 0.04 sec\n",
            "Epoch 969, Loss(train/val) 0.46472/0.22687. Took 0.05 sec\n",
            "Epoch 970, Loss(train/val) 0.49889/0.22681. Took 0.05 sec\n",
            "Epoch 971, Loss(train/val) 0.47573/0.22679. Took 0.04 sec\n",
            "Epoch 972, Loss(train/val) 0.50267/0.22831. Took 0.04 sec\n",
            "Epoch 973, Loss(train/val) 0.49282/0.23417. Took 0.05 sec\n",
            "Epoch 974, Loss(train/val) 0.50028/0.23822. Took 0.05 sec\n",
            "Epoch 975, Loss(train/val) 0.49773/0.23341. Took 0.05 sec\n",
            "Epoch 976, Loss(train/val) 0.49790/0.22891. Took 0.04 sec\n",
            "Epoch 977, Loss(train/val) 0.51115/0.22753. Took 0.04 sec\n",
            "Epoch 978, Loss(train/val) 0.47814/0.22898. Took 0.05 sec\n",
            "Epoch 979, Loss(train/val) 0.50912/0.22853. Took 0.05 sec\n",
            "Epoch 980, Loss(train/val) 0.51250/0.22953. Took 0.05 sec\n",
            "Epoch 981, Loss(train/val) 0.49568/0.22801. Took 0.05 sec\n",
            "Epoch 982, Loss(train/val) 0.47597/0.22744. Took 0.05 sec\n",
            "Epoch 983, Loss(train/val) 0.49004/0.22777. Took 0.05 sec\n",
            "Epoch 984, Loss(train/val) 0.46809/0.22856. Took 0.05 sec\n",
            "Epoch 985, Loss(train/val) 0.47745/0.22939. Took 0.04 sec\n",
            "Epoch 986, Loss(train/val) 0.48079/0.22953. Took 0.04 sec\n",
            "Epoch 987, Loss(train/val) 0.48498/0.23047. Took 0.06 sec\n",
            "Epoch 988, Loss(train/val) 0.48906/0.22839. Took 0.05 sec\n",
            "Epoch 989, Loss(train/val) 0.48168/0.22619. Took 0.05 sec\n",
            "Epoch 990, Loss(train/val) 0.48723/0.22440. Took 0.05 sec\n",
            "Epoch 991, Loss(train/val) 0.47598/0.22274. Took 0.05 sec\n",
            "Epoch 992, Loss(train/val) 0.48391/0.22187. Took 0.04 sec\n",
            "Epoch 993, Loss(train/val) 0.48198/0.22149. Took 0.05 sec\n",
            "Epoch 994, Loss(train/val) 0.47707/0.22177. Took 0.05 sec\n",
            "Epoch 995, Loss(train/val) 0.46863/0.22046. Took 0.05 sec\n",
            "Epoch 996, Loss(train/val) 0.46149/0.22141. Took 0.04 sec\n",
            "Epoch 997, Loss(train/val) 0.46913/0.22223. Took 0.05 sec\n",
            "Epoch 998, Loss(train/val) 0.47819/0.22834. Took 0.05 sec\n",
            "Epoch 999, Loss(train/val) 0.47781/0.23405. Took 0.06 sec\n",
            "Epoch 1000, Loss(train/val) 0.46613/0.23619. Took 0.05 sec\n",
            "Epoch 1001, Loss(train/val) 0.48325/0.23239. Took 0.05 sec\n",
            "Epoch 1002, Loss(train/val) 0.48338/0.23057. Took 0.05 sec\n",
            "Epoch 1003, Loss(train/val) 0.48490/0.23037. Took 0.04 sec\n",
            "Epoch 1004, Loss(train/val) 0.49424/0.23160. Took 0.05 sec\n",
            "Epoch 1005, Loss(train/val) 0.48958/0.22680. Took 0.05 sec\n",
            "Epoch 1006, Loss(train/val) 0.45523/0.22412. Took 0.05 sec\n",
            "Epoch 1007, Loss(train/val) 0.49093/0.22376. Took 0.05 sec\n",
            "Epoch 1008, Loss(train/val) 0.45405/0.22438. Took 0.05 sec\n",
            "Epoch 1009, Loss(train/val) 0.48533/0.22575. Took 0.05 sec\n",
            "Epoch 1010, Loss(train/val) 0.45641/0.22614. Took 0.05 sec\n",
            "Epoch 1011, Loss(train/val) 0.47797/0.22571. Took 0.05 sec\n",
            "Epoch 1012, Loss(train/val) 0.46430/0.22404. Took 0.04 sec\n",
            "Epoch 1013, Loss(train/val) 0.49241/0.22433. Took 0.05 sec\n",
            "Epoch 1014, Loss(train/val) 0.50210/0.22358. Took 0.05 sec\n",
            "Epoch 1015, Loss(train/val) 0.46890/0.22441. Took 0.05 sec\n",
            "Epoch 1016, Loss(train/val) 0.46958/0.22560. Took 0.05 sec\n",
            "Epoch 1017, Loss(train/val) 0.46730/0.22866. Took 0.04 sec\n",
            "Epoch 1018, Loss(train/val) 0.50023/0.23296. Took 0.05 sec\n",
            "Epoch 1019, Loss(train/val) 0.45448/0.23792. Took 0.05 sec\n",
            "Epoch 1020, Loss(train/val) 0.49990/0.24567. Took 0.05 sec\n",
            "Epoch 1021, Loss(train/val) 0.45678/0.24598. Took 0.05 sec\n",
            "Epoch 1022, Loss(train/val) 0.51222/0.24096. Took 0.04 sec\n",
            "Epoch 1023, Loss(train/val) 0.49889/0.23267. Took 0.04 sec\n",
            "Epoch 1024, Loss(train/val) 0.49320/0.22998. Took 0.06 sec\n",
            "Epoch 1025, Loss(train/val) 0.46902/0.22277. Took 0.05 sec\n",
            "Epoch 1026, Loss(train/val) 0.48223/0.22262. Took 0.05 sec\n",
            "Epoch 1027, Loss(train/val) 0.46044/0.22286. Took 0.05 sec\n",
            "Epoch 1028, Loss(train/val) 0.47208/0.22392. Took 0.05 sec\n",
            "Epoch 1029, Loss(train/val) 0.48087/0.22440. Took 0.05 sec\n",
            "Epoch 1030, Loss(train/val) 0.48591/0.22540. Took 0.05 sec\n",
            "Epoch 1031, Loss(train/val) 0.47580/0.22615. Took 0.04 sec\n",
            "Epoch 1032, Loss(train/val) 0.48374/0.22596. Took 0.05 sec\n",
            "Epoch 1033, Loss(train/val) 0.46116/0.22614. Took 0.05 sec\n",
            "Epoch 1034, Loss(train/val) 0.48249/0.22777. Took 0.05 sec\n",
            "Epoch 1035, Loss(train/val) 0.49768/0.22589. Took 0.04 sec\n",
            "Epoch 1036, Loss(train/val) 0.49055/0.22478. Took 0.05 sec\n",
            "Epoch 1037, Loss(train/val) 0.48812/0.22377. Took 0.05 sec\n",
            "Epoch 1038, Loss(train/val) 0.49028/0.22218. Took 0.04 sec\n",
            "Epoch 1039, Loss(train/val) 0.50984/0.22157. Took 0.05 sec\n",
            "Epoch 1040, Loss(train/val) 0.48775/0.22210. Took 0.05 sec\n",
            "Epoch 1041, Loss(train/val) 0.48564/0.22338. Took 0.05 sec\n",
            "Epoch 1042, Loss(train/val) 0.47618/0.22358. Took 0.05 sec\n",
            "Epoch 1043, Loss(train/val) 0.47047/0.22403. Took 0.05 sec\n",
            "Epoch 1044, Loss(train/val) 0.49645/0.22424. Took 0.06 sec\n",
            "Epoch 1045, Loss(train/val) 0.47081/0.22151. Took 0.05 sec\n",
            "Epoch 1046, Loss(train/val) 0.49658/0.22099. Took 0.05 sec\n",
            "Epoch 1047, Loss(train/val) 0.47823/0.22237. Took 0.05 sec\n",
            "Epoch 1048, Loss(train/val) 0.48940/0.22363. Took 0.05 sec\n",
            "Epoch 1049, Loss(train/val) 0.46544/0.22356. Took 0.05 sec\n",
            "Epoch 1050, Loss(train/val) 0.46607/0.22411. Took 0.04 sec\n",
            "Epoch 1051, Loss(train/val) 0.46668/0.22466. Took 0.04 sec\n",
            "Epoch 1052, Loss(train/val) 0.46895/0.22468. Took 0.05 sec\n",
            "Epoch 1053, Loss(train/val) 0.47025/0.22374. Took 0.05 sec\n",
            "Epoch 1054, Loss(train/val) 0.46764/0.22405. Took 0.05 sec\n",
            "Epoch 1055, Loss(train/val) 0.47234/0.22333. Took 0.04 sec\n",
            "Epoch 1056, Loss(train/val) 0.46724/0.22361. Took 0.04 sec\n",
            "Epoch 1057, Loss(train/val) 0.47327/0.22815. Took 0.04 sec\n",
            "Epoch 1058, Loss(train/val) 0.45985/0.23280. Took 0.04 sec\n",
            "Epoch 1059, Loss(train/val) 0.48385/0.23859. Took 0.05 sec\n",
            "Epoch 1060, Loss(train/val) 0.46227/0.23516. Took 0.05 sec\n",
            "Epoch 1061, Loss(train/val) 0.47562/0.23589. Took 0.05 sec\n",
            "Epoch 1062, Loss(train/val) 0.45350/0.22959. Took 0.05 sec\n",
            "Epoch 1063, Loss(train/val) 0.48183/0.22550. Took 0.06 sec\n",
            "Epoch 1064, Loss(train/val) 0.46094/0.22503. Took 0.05 sec\n",
            "Epoch 1065, Loss(train/val) 0.46616/0.22191. Took 0.05 sec\n",
            "Epoch 1066, Loss(train/val) 0.47453/0.22127. Took 0.04 sec\n",
            "Epoch 1067, Loss(train/val) 0.47196/0.22095. Took 0.04 sec\n",
            "Epoch 1068, Loss(train/val) 0.46603/0.22109. Took 0.05 sec\n",
            "Epoch 1069, Loss(train/val) 0.48564/0.22068. Took 0.05 sec\n",
            "Epoch 1070, Loss(train/val) 0.47362/0.22205. Took 0.04 sec\n",
            "Epoch 1071, Loss(train/val) 0.48596/0.22026. Took 0.04 sec\n",
            "Epoch 1072, Loss(train/val) 0.49819/0.21794. Took 0.05 sec\n",
            "Epoch 1073, Loss(train/val) 0.44961/0.21706. Took 0.04 sec\n",
            "Epoch 1074, Loss(train/val) 0.47629/0.21577. Took 0.05 sec\n",
            "Epoch 1075, Loss(train/val) 0.46077/0.21613. Took 0.05 sec\n",
            "Epoch 1076, Loss(train/val) 0.46754/0.21692. Took 0.04 sec\n",
            "Epoch 1077, Loss(train/val) 0.47468/0.21927. Took 0.04 sec\n",
            "Epoch 1078, Loss(train/val) 0.47437/0.21959. Took 0.05 sec\n",
            "Epoch 1079, Loss(train/val) 0.48768/0.21874. Took 0.05 sec\n",
            "Epoch 1080, Loss(train/val) 0.47513/0.21939. Took 0.04 sec\n",
            "Epoch 1081, Loss(train/val) 0.50531/0.22151. Took 0.05 sec\n",
            "Epoch 1082, Loss(train/val) 0.46023/0.22591. Took 0.05 sec\n",
            "Epoch 1083, Loss(train/val) 0.48080/0.22208. Took 0.05 sec\n",
            "Epoch 1084, Loss(train/val) 0.46597/0.21764. Took 0.05 sec\n",
            "Epoch 1085, Loss(train/val) 0.47235/0.21763. Took 0.05 sec\n",
            "Epoch 1086, Loss(train/val) 0.47548/0.21841. Took 0.05 sec\n",
            "Epoch 1087, Loss(train/val) 0.47260/0.21942. Took 0.05 sec\n",
            "Epoch 1088, Loss(train/val) 0.44944/0.21818. Took 0.05 sec\n",
            "Epoch 1089, Loss(train/val) 0.47055/0.21701. Took 0.05 sec\n",
            "Epoch 1090, Loss(train/val) 0.46976/0.21494. Took 0.05 sec\n",
            "Epoch 1091, Loss(train/val) 0.46165/0.21630. Took 0.05 sec\n",
            "Epoch 1092, Loss(train/val) 0.46034/0.21690. Took 0.05 sec\n",
            "Epoch 1093, Loss(train/val) 0.47005/0.21733. Took 0.04 sec\n",
            "Epoch 1094, Loss(train/val) 0.46341/0.21882. Took 0.05 sec\n",
            "Epoch 1095, Loss(train/val) 0.46512/0.21858. Took 0.05 sec\n",
            "Epoch 1096, Loss(train/val) 0.46295/0.21828. Took 0.05 sec\n",
            "Epoch 1097, Loss(train/val) 0.46638/0.21895. Took 0.04 sec\n",
            "Epoch 1098, Loss(train/val) 0.47380/0.22081. Took 0.04 sec\n",
            "Epoch 1099, Loss(train/val) 0.45492/0.22232. Took 0.06 sec\n",
            "Epoch 1100, Loss(train/val) 0.45956/0.22291. Took 0.05 sec\n",
            "Epoch 1101, Loss(train/val) 0.44737/0.22319. Took 0.04 sec\n",
            "Epoch 1102, Loss(train/val) 0.47050/0.22416. Took 0.04 sec\n",
            "Epoch 1103, Loss(train/val) 0.47655/0.22541. Took 0.05 sec\n",
            "Epoch 1104, Loss(train/val) 0.46967/0.22571. Took 0.05 sec\n",
            "Epoch 1105, Loss(train/val) 0.45306/0.22522. Took 0.05 sec\n",
            "Epoch 1106, Loss(train/val) 0.45931/0.22282. Took 0.06 sec\n",
            "Epoch 1107, Loss(train/val) 0.45707/0.22030. Took 0.05 sec\n",
            "Epoch 1108, Loss(train/val) 0.48264/0.21897. Took 0.05 sec\n",
            "Epoch 1109, Loss(train/val) 0.45924/0.21974. Took 0.05 sec\n",
            "Epoch 1110, Loss(train/val) 0.46780/0.22121. Took 0.05 sec\n",
            "Epoch 1111, Loss(train/val) 0.47845/0.22390. Took 0.05 sec\n",
            "Epoch 1112, Loss(train/val) 0.48624/0.22381. Took 0.05 sec\n",
            "Epoch 1113, Loss(train/val) 0.47778/0.22379. Took 0.05 sec\n",
            "Epoch 1114, Loss(train/val) 0.46819/0.21892. Took 0.05 sec\n",
            "Epoch 1115, Loss(train/val) 0.45991/0.21614. Took 0.05 sec\n",
            "Epoch 1116, Loss(train/val) 0.46185/0.21664. Took 0.05 sec\n",
            "Epoch 1117, Loss(train/val) 0.46533/0.21798. Took 0.04 sec\n",
            "Epoch 1118, Loss(train/val) 0.45920/0.22097. Took 0.04 sec\n",
            "Epoch 1119, Loss(train/val) 0.46405/0.22502. Took 0.07 sec\n",
            "Epoch 1120, Loss(train/val) 0.47065/0.22065. Took 0.05 sec\n",
            "Epoch 1121, Loss(train/val) 0.45573/0.21787. Took 0.05 sec\n",
            "Epoch 1122, Loss(train/val) 0.49143/0.21671. Took 0.05 sec\n",
            "Epoch 1123, Loss(train/val) 0.46045/0.21701. Took 0.05 sec\n",
            "Epoch 1124, Loss(train/val) 0.47308/0.21661. Took 0.05 sec\n",
            "Epoch 1125, Loss(train/val) 0.47955/0.21701. Took 0.05 sec\n",
            "Epoch 1126, Loss(train/val) 0.44107/0.21649. Took 0.05 sec\n",
            "Epoch 1127, Loss(train/val) 0.48381/0.21580. Took 0.06 sec\n",
            "Epoch 1128, Loss(train/val) 0.52105/0.21514. Took 0.05 sec\n",
            "Epoch 1129, Loss(train/val) 0.45045/0.21645. Took 0.05 sec\n",
            "Epoch 1130, Loss(train/val) 0.47641/0.22040. Took 0.05 sec\n",
            "Epoch 1131, Loss(train/val) 0.47351/0.22113. Took 0.05 sec\n",
            "Epoch 1132, Loss(train/val) 0.47840/0.21992. Took 0.05 sec\n",
            "Epoch 1133, Loss(train/val) 0.46203/0.22084. Took 0.05 sec\n",
            "Epoch 1134, Loss(train/val) 0.49027/0.22192. Took 0.05 sec\n",
            "Epoch 1135, Loss(train/val) 0.49305/0.21947. Took 0.05 sec\n",
            "Epoch 1136, Loss(train/val) 0.44247/0.22261. Took 0.05 sec\n",
            "Epoch 1137, Loss(train/val) 0.48143/0.22087. Took 0.05 sec\n",
            "Epoch 1138, Loss(train/val) 0.47623/0.22185. Took 0.05 sec\n",
            "Epoch 1139, Loss(train/val) 0.46596/0.23030. Took 0.06 sec\n",
            "Epoch 1140, Loss(train/val) 0.46282/0.24475. Took 0.05 sec\n",
            "Epoch 1141, Loss(train/val) 0.46197/0.24253. Took 0.05 sec\n",
            "Epoch 1142, Loss(train/val) 0.48037/0.22546. Took 0.05 sec\n",
            "Epoch 1143, Loss(train/val) 0.45984/0.21816. Took 0.05 sec\n",
            "Epoch 1144, Loss(train/val) 0.47051/0.21554. Took 0.05 sec\n",
            "Epoch 1145, Loss(train/val) 0.49850/0.21623. Took 0.05 sec\n",
            "Epoch 1146, Loss(train/val) 0.48835/0.21767. Took 0.05 sec\n",
            "Epoch 1147, Loss(train/val) 0.45670/0.21992. Took 0.04 sec\n",
            "Epoch 1148, Loss(train/val) 0.47101/0.22129. Took 0.05 sec\n",
            "Epoch 1149, Loss(train/val) 0.45232/0.21861. Took 0.06 sec\n",
            "Epoch 1150, Loss(train/val) 0.48613/0.21496. Took 0.05 sec\n",
            "Epoch 1151, Loss(train/val) 0.47702/0.21546. Took 0.04 sec\n",
            "Epoch 1152, Loss(train/val) 0.48514/0.21672. Took 0.04 sec\n",
            "Epoch 1153, Loss(train/val) 0.46418/0.21614. Took 0.05 sec\n",
            "Epoch 1154, Loss(train/val) 0.49135/0.21685. Took 0.05 sec\n",
            "Epoch 1155, Loss(train/val) 0.45435/0.21858. Took 0.04 sec\n",
            "Epoch 1156, Loss(train/val) 0.46972/0.22314. Took 0.04 sec\n",
            "Epoch 1157, Loss(train/val) 0.44216/0.22468. Took 0.05 sec\n",
            "Epoch 1158, Loss(train/val) 0.47417/0.22464. Took 0.04 sec\n",
            "Epoch 1159, Loss(train/val) 0.45626/0.22503. Took 0.05 sec\n",
            "Epoch 1160, Loss(train/val) 0.46371/0.22736. Took 0.04 sec\n",
            "Epoch 1161, Loss(train/val) 0.44723/0.22103. Took 0.04 sec\n",
            "Epoch 1162, Loss(train/val) 0.45363/0.21628. Took 0.05 sec\n",
            "Epoch 1163, Loss(train/val) 0.45663/0.21579. Took 0.04 sec\n",
            "Epoch 1164, Loss(train/val) 0.44164/0.21580. Took 0.06 sec\n",
            "Epoch 1165, Loss(train/val) 0.46723/0.21575. Took 0.05 sec\n",
            "Epoch 1166, Loss(train/val) 0.44544/0.21502. Took 0.04 sec\n",
            "Epoch 1167, Loss(train/val) 0.45800/0.21572. Took 0.05 sec\n",
            "Epoch 1168, Loss(train/val) 0.46249/0.21566. Took 0.04 sec\n",
            "Epoch 1169, Loss(train/val) 0.49305/0.21276. Took 0.06 sec\n",
            "Epoch 1170, Loss(train/val) 0.48037/0.21182. Took 0.05 sec\n",
            "Epoch 1171, Loss(train/val) 0.44830/0.21145. Took 0.04 sec\n",
            "Epoch 1172, Loss(train/val) 0.45027/0.21112. Took 0.05 sec\n",
            "Epoch 1173, Loss(train/val) 0.47347/0.21170. Took 0.04 sec\n",
            "Epoch 1174, Loss(train/val) 0.45463/0.21142. Took 0.05 sec\n",
            "Epoch 1175, Loss(train/val) 0.45734/0.21076. Took 0.05 sec\n",
            "Epoch 1176, Loss(train/val) 0.47113/0.21155. Took 0.05 sec\n",
            "Epoch 1177, Loss(train/val) 0.45640/0.21135. Took 0.05 sec\n",
            "Epoch 1178, Loss(train/val) 0.47935/0.21141. Took 0.05 sec\n",
            "Epoch 1179, Loss(train/val) 0.48557/0.21183. Took 0.05 sec\n",
            "Epoch 1180, Loss(train/val) 0.42810/0.21313. Took 0.04 sec\n",
            "Epoch 1181, Loss(train/val) 0.45832/0.21544. Took 0.04 sec\n",
            "Epoch 1182, Loss(train/val) 0.46768/0.21777. Took 0.05 sec\n",
            "Epoch 1183, Loss(train/val) 0.47738/0.21787. Took 0.05 sec\n",
            "Epoch 1184, Loss(train/val) 0.45002/0.21845. Took 0.05 sec\n",
            "Epoch 1185, Loss(train/val) 0.44945/0.22506. Took 0.04 sec\n",
            "Epoch 1186, Loss(train/val) 0.47979/0.22212. Took 0.05 sec\n",
            "Epoch 1187, Loss(train/val) 0.45944/0.22178. Took 0.05 sec\n",
            "Epoch 1188, Loss(train/val) 0.46094/0.21629. Took 0.05 sec\n",
            "Epoch 1189, Loss(train/val) 0.47714/0.21108. Took 0.05 sec\n",
            "Epoch 1190, Loss(train/val) 0.47492/0.21055. Took 0.05 sec\n",
            "Epoch 1191, Loss(train/val) 0.43814/0.21140. Took 0.05 sec\n",
            "Epoch 1192, Loss(train/val) 0.46047/0.21192. Took 0.05 sec\n",
            "Epoch 1193, Loss(train/val) 0.46215/0.21238. Took 0.05 sec\n",
            "Epoch 1194, Loss(train/val) 0.46282/0.21321. Took 0.06 sec\n",
            "Epoch 1195, Loss(train/val) 0.45800/0.21462. Took 0.05 sec\n",
            "Epoch 1196, Loss(train/val) 0.45285/0.21291. Took 0.05 sec\n",
            "Epoch 1197, Loss(train/val) 0.44436/0.21300. Took 0.04 sec\n",
            "Epoch 1198, Loss(train/val) 0.48424/0.21239. Took 0.05 sec\n",
            "Epoch 1199, Loss(train/val) 0.47342/0.21248. Took 0.05 sec\n",
            "Epoch 1200, Loss(train/val) 0.46294/0.21160. Took 0.04 sec\n",
            "Epoch 1201, Loss(train/val) 0.44932/0.21110. Took 0.05 sec\n",
            "Epoch 1202, Loss(train/val) 0.47557/0.21122. Took 0.04 sec\n",
            "Epoch 1203, Loss(train/val) 0.44428/0.21276. Took 0.04 sec\n",
            "Epoch 1204, Loss(train/val) 0.44760/0.21401. Took 0.06 sec\n",
            "Epoch 1205, Loss(train/val) 0.46618/0.21725. Took 0.04 sec\n",
            "Epoch 1206, Loss(train/val) 0.45448/0.22901. Took 0.05 sec\n",
            "Epoch 1207, Loss(train/val) 0.47140/0.24060. Took 0.05 sec\n",
            "Epoch 1208, Loss(train/val) 0.45539/0.24560. Took 0.05 sec\n",
            "Epoch 1209, Loss(train/val) 0.49836/0.24160. Took 0.06 sec\n",
            "Epoch 1210, Loss(train/val) 0.46533/0.23838. Took 0.05 sec\n",
            "Epoch 1211, Loss(train/val) 0.45601/0.23062. Took 0.04 sec\n",
            "Epoch 1212, Loss(train/val) 0.45799/0.22987. Took 0.05 sec\n",
            "Epoch 1213, Loss(train/val) 0.43726/0.22879. Took 0.05 sec\n",
            "Epoch 1214, Loss(train/val) 0.46554/0.22652. Took 0.05 sec\n",
            "Epoch 1215, Loss(train/val) 0.47724/0.21886. Took 0.05 sec\n",
            "Epoch 1216, Loss(train/val) 0.43800/0.21393. Took 0.05 sec\n",
            "Epoch 1217, Loss(train/val) 0.46421/0.21410. Took 0.05 sec\n",
            "Epoch 1218, Loss(train/val) 0.44805/0.21641. Took 0.05 sec\n",
            "Epoch 1219, Loss(train/val) 0.44219/0.21386. Took 0.06 sec\n",
            "Epoch 1220, Loss(train/val) 0.47045/0.21167. Took 0.04 sec\n",
            "Epoch 1221, Loss(train/val) 0.47363/0.21039. Took 0.04 sec\n",
            "Epoch 1222, Loss(train/val) 0.45514/0.21040. Took 0.04 sec\n",
            "Epoch 1223, Loss(train/val) 0.44108/0.21182. Took 0.04 sec\n",
            "Epoch 1224, Loss(train/val) 0.46618/0.21208. Took 0.06 sec\n",
            "Epoch 1225, Loss(train/val) 0.45164/0.21283. Took 0.05 sec\n",
            "Epoch 1226, Loss(train/val) 0.44980/0.21377. Took 0.04 sec\n",
            "Epoch 1227, Loss(train/val) 0.45447/0.21565. Took 0.05 sec\n",
            "Epoch 1228, Loss(train/val) 0.47946/0.21619. Took 0.05 sec\n",
            "Epoch 1229, Loss(train/val) 0.45344/0.21367. Took 0.05 sec\n",
            "Epoch 1230, Loss(train/val) 0.46659/0.21369. Took 0.05 sec\n",
            "Epoch 1231, Loss(train/val) 0.45204/0.21597. Took 0.05 sec\n",
            "Epoch 1232, Loss(train/val) 0.45817/0.22482. Took 0.05 sec\n",
            "Epoch 1233, Loss(train/val) 0.44014/0.23462. Took 0.05 sec\n",
            "Epoch 1234, Loss(train/val) 0.44997/0.22958. Took 0.06 sec\n",
            "Epoch 1235, Loss(train/val) 0.45642/0.22453. Took 0.05 sec\n",
            "Epoch 1236, Loss(train/val) 0.45047/0.22463. Took 0.05 sec\n",
            "Epoch 1237, Loss(train/val) 0.44510/0.22071. Took 0.05 sec\n",
            "Epoch 1238, Loss(train/val) 0.47039/0.21633. Took 0.04 sec\n",
            "Epoch 1239, Loss(train/val) 0.43748/0.21321. Took 0.05 sec\n",
            "Epoch 1240, Loss(train/val) 0.45138/0.21171. Took 0.05 sec\n",
            "Epoch 1241, Loss(train/val) 0.45039/0.21090. Took 0.04 sec\n",
            "Epoch 1242, Loss(train/val) 0.43322/0.20957. Took 0.04 sec\n",
            "Epoch 1243, Loss(train/val) 0.42750/0.21082. Took 0.04 sec\n",
            "Epoch 1244, Loss(train/val) 0.44566/0.21063. Took 0.05 sec\n",
            "Epoch 1245, Loss(train/val) 0.46276/0.20903. Took 0.05 sec\n",
            "Epoch 1246, Loss(train/val) 0.46453/0.20944. Took 0.04 sec\n",
            "Epoch 1247, Loss(train/val) 0.48260/0.21143. Took 0.04 sec\n",
            "Epoch 1248, Loss(train/val) 0.46883/0.21318. Took 0.04 sec\n",
            "Epoch 1249, Loss(train/val) 0.46099/0.21261. Took 0.05 sec\n",
            "Epoch 1250, Loss(train/val) 0.45565/0.21259. Took 0.04 sec\n",
            "Epoch 1251, Loss(train/val) 0.42296/0.21136. Took 0.04 sec\n",
            "Epoch 1252, Loss(train/val) 0.47337/0.20855. Took 0.04 sec\n",
            "Epoch 1253, Loss(train/val) 0.45758/0.20726. Took 0.04 sec\n",
            "Epoch 1254, Loss(train/val) 0.42968/0.20685. Took 0.06 sec\n",
            "Epoch 1255, Loss(train/val) 0.46202/0.20728. Took 0.05 sec\n",
            "Epoch 1256, Loss(train/val) 0.46858/0.20870. Took 0.05 sec\n",
            "Epoch 1257, Loss(train/val) 0.44383/0.20841. Took 0.05 sec\n",
            "Epoch 1258, Loss(train/val) 0.45106/0.20645. Took 0.05 sec\n",
            "Epoch 1259, Loss(train/val) 0.47182/0.20499. Took 0.05 sec\n",
            "Epoch 1260, Loss(train/val) 0.45415/0.20404. Took 0.05 sec\n",
            "Epoch 1261, Loss(train/val) 0.47210/0.20448. Took 0.05 sec\n",
            "Epoch 1262, Loss(train/val) 0.43342/0.20777. Took 0.04 sec\n",
            "Epoch 1263, Loss(train/val) 0.43970/0.20607. Took 0.05 sec\n",
            "Epoch 1264, Loss(train/val) 0.45886/0.20463. Took 0.05 sec\n",
            "Epoch 1265, Loss(train/val) 0.43945/0.20780. Took 0.04 sec\n",
            "Epoch 1266, Loss(train/val) 0.45559/0.21447. Took 0.04 sec\n",
            "Epoch 1267, Loss(train/val) 0.44843/0.21638. Took 0.04 sec\n",
            "Epoch 1268, Loss(train/val) 0.47413/0.21387. Took 0.04 sec\n",
            "Epoch 1269, Loss(train/val) 0.47223/0.21040. Took 0.05 sec\n",
            "Epoch 1270, Loss(train/val) 0.46749/0.20989. Took 0.04 sec\n",
            "Epoch 1271, Loss(train/val) 0.45385/0.21060. Took 0.04 sec\n",
            "Epoch 1272, Loss(train/val) 0.47186/0.20777. Took 0.05 sec\n",
            "Epoch 1273, Loss(train/val) 0.45412/0.20601. Took 0.05 sec\n",
            "Epoch 1274, Loss(train/val) 0.47963/0.20553. Took 0.05 sec\n",
            "Epoch 1275, Loss(train/val) 0.44792/0.20409. Took 0.05 sec\n",
            "Epoch 1276, Loss(train/val) 0.45516/0.20346. Took 0.06 sec\n",
            "Epoch 1277, Loss(train/val) 0.43783/0.20322. Took 0.05 sec\n",
            "Epoch 1278, Loss(train/val) 0.46850/0.20402. Took 0.04 sec\n",
            "Epoch 1279, Loss(train/val) 0.43271/0.20412. Took 0.05 sec\n",
            "Epoch 1280, Loss(train/val) 0.44482/0.20472. Took 0.05 sec\n",
            "Epoch 1281, Loss(train/val) 0.43351/0.20632. Took 0.04 sec\n",
            "Epoch 1282, Loss(train/val) 0.45494/0.20781. Took 0.04 sec\n",
            "Epoch 1283, Loss(train/val) 0.44270/0.21031. Took 0.05 sec\n",
            "Epoch 1284, Loss(train/val) 0.43943/0.21644. Took 0.05 sec\n",
            "Epoch 1285, Loss(train/val) 0.47390/0.21670. Took 0.04 sec\n",
            "Epoch 1286, Loss(train/val) 0.47390/0.21875. Took 0.05 sec\n",
            "Epoch 1287, Loss(train/val) 0.46365/0.21862. Took 0.04 sec\n",
            "Epoch 1288, Loss(train/val) 0.45858/0.21211. Took 0.05 sec\n",
            "Epoch 1289, Loss(train/val) 0.43045/0.20861. Took 0.05 sec\n",
            "Epoch 1290, Loss(train/val) 0.47653/0.20795. Took 0.05 sec\n",
            "Epoch 1291, Loss(train/val) 0.44493/0.20734. Took 0.04 sec\n",
            "Epoch 1292, Loss(train/val) 0.45636/0.20692. Took 0.04 sec\n",
            "Epoch 1293, Loss(train/val) 0.44995/0.20620. Took 0.05 sec\n",
            "Epoch 1294, Loss(train/val) 0.44406/0.20713. Took 0.05 sec\n",
            "Epoch 1295, Loss(train/val) 0.44587/0.20800. Took 0.05 sec\n",
            "Epoch 1296, Loss(train/val) 0.46704/0.21259. Took 0.05 sec\n",
            "Epoch 1297, Loss(train/val) 0.43160/0.21624. Took 0.04 sec\n",
            "Epoch 1298, Loss(train/val) 0.46352/0.21944. Took 0.06 sec\n",
            "Epoch 1299, Loss(train/val) 0.46206/0.21436. Took 0.05 sec\n",
            "Epoch 1300, Loss(train/val) 0.46491/0.21185. Took 0.05 sec\n",
            "Epoch 1301, Loss(train/val) 0.49199/0.21642. Took 0.04 sec\n",
            "Epoch 1302, Loss(train/val) 0.47906/0.22002. Took 0.04 sec\n",
            "Epoch 1303, Loss(train/val) 0.44822/0.21448. Took 0.04 sec\n",
            "Epoch 1304, Loss(train/val) 0.43971/0.21442. Took 0.06 sec\n",
            "Epoch 1305, Loss(train/val) 0.44739/0.21083. Took 0.05 sec\n",
            "Epoch 1306, Loss(train/val) 0.43363/0.20813. Took 0.05 sec\n",
            "Epoch 1307, Loss(train/val) 0.43806/0.21169. Took 0.04 sec\n",
            "Epoch 1308, Loss(train/val) 0.44354/0.21462. Took 0.04 sec\n",
            "Epoch 1309, Loss(train/val) 0.45332/0.22266. Took 0.05 sec\n",
            "Epoch 1310, Loss(train/val) 0.45966/0.22916. Took 0.04 sec\n",
            "Epoch 1311, Loss(train/val) 0.47875/0.23185. Took 0.04 sec\n",
            "Epoch 1312, Loss(train/val) 0.44085/0.22721. Took 0.04 sec\n",
            "Epoch 1313, Loss(train/val) 0.45775/0.21604. Took 0.04 sec\n",
            "Epoch 1314, Loss(train/val) 0.43870/0.21105. Took 0.05 sec\n",
            "Epoch 1315, Loss(train/val) 0.44736/0.20857. Took 0.05 sec\n",
            "Epoch 1316, Loss(train/val) 0.44738/0.20587. Took 0.05 sec\n",
            "Epoch 1317, Loss(train/val) 0.44724/0.20448. Took 0.04 sec\n",
            "Epoch 1318, Loss(train/val) 0.44672/0.20464. Took 0.04 sec\n",
            "Epoch 1319, Loss(train/val) 0.44427/0.20500. Took 0.06 sec\n",
            "Epoch 1320, Loss(train/val) 0.47741/0.20529. Took 0.05 sec\n",
            "Epoch 1321, Loss(train/val) 0.44993/0.20547. Took 0.05 sec\n",
            "Epoch 1322, Loss(train/val) 0.45566/0.20762. Took 0.04 sec\n",
            "Epoch 1323, Loss(train/val) 0.45317/0.20614. Took 0.04 sec\n",
            "Epoch 1324, Loss(train/val) 0.44309/0.20525. Took 0.05 sec\n",
            "Epoch 1325, Loss(train/val) 0.45635/0.20436. Took 0.05 sec\n",
            "Epoch 1326, Loss(train/val) 0.46305/0.20346. Took 0.05 sec\n",
            "Epoch 1327, Loss(train/val) 0.47876/0.20238. Took 0.04 sec\n",
            "Epoch 1328, Loss(train/val) 0.45380/0.20165. Took 0.04 sec\n",
            "Epoch 1329, Loss(train/val) 0.45268/0.20138. Took 0.05 sec\n",
            "Epoch 1330, Loss(train/val) 0.46156/0.20375. Took 0.05 sec\n",
            "Epoch 1331, Loss(train/val) 0.46056/0.20530. Took 0.04 sec\n",
            "Epoch 1332, Loss(train/val) 0.45354/0.20571. Took 0.04 sec\n",
            "Epoch 1333, Loss(train/val) 0.43128/0.20547. Took 0.04 sec\n",
            "Epoch 1334, Loss(train/val) 0.46751/0.20138. Took 0.05 sec\n",
            "Epoch 1335, Loss(train/val) 0.43056/0.20025. Took 0.05 sec\n",
            "Epoch 1336, Loss(train/val) 0.44925/0.20125. Took 0.05 sec\n",
            "Epoch 1337, Loss(train/val) 0.46067/0.20333. Took 0.04 sec\n",
            "Epoch 1338, Loss(train/val) 0.46841/0.20554. Took 0.05 sec\n",
            "Epoch 1339, Loss(train/val) 0.43854/0.20569. Took 0.05 sec\n",
            "Epoch 1340, Loss(train/val) 0.42630/0.20542. Took 0.05 sec\n",
            "Epoch 1341, Loss(train/val) 0.45406/0.20450. Took 0.06 sec\n",
            "Epoch 1342, Loss(train/val) 0.43393/0.20035. Took 0.05 sec\n",
            "Epoch 1343, Loss(train/val) 0.44334/0.20032. Took 0.05 sec\n",
            "Epoch 1344, Loss(train/val) 0.43701/0.20098. Took 0.05 sec\n",
            "Epoch 1345, Loss(train/val) 0.44813/0.20108. Took 0.04 sec\n",
            "Epoch 1346, Loss(train/val) 0.44365/0.20341. Took 0.05 sec\n",
            "Epoch 1347, Loss(train/val) 0.47964/0.20381. Took 0.04 sec\n",
            "Epoch 1348, Loss(train/val) 0.44470/0.20337. Took 0.04 sec\n",
            "Epoch 1349, Loss(train/val) 0.47301/0.20342. Took 0.05 sec\n",
            "Epoch 1350, Loss(train/val) 0.46365/0.20453. Took 0.04 sec\n",
            "Epoch 1351, Loss(train/val) 0.44438/0.20446. Took 0.05 sec\n",
            "Epoch 1352, Loss(train/val) 0.43052/0.20667. Took 0.04 sec\n",
            "Epoch 1353, Loss(train/val) 0.45555/0.20703. Took 0.04 sec\n",
            "Epoch 1354, Loss(train/val) 0.46401/0.21177. Took 0.05 sec\n",
            "Epoch 1355, Loss(train/val) 0.44364/0.21079. Took 0.04 sec\n",
            "Epoch 1356, Loss(train/val) 0.43719/0.21378. Took 0.05 sec\n",
            "Epoch 1357, Loss(train/val) 0.43975/0.21670. Took 0.05 sec\n",
            "Epoch 1358, Loss(train/val) 0.45450/0.21720. Took 0.05 sec\n",
            "Epoch 1359, Loss(train/val) 0.41437/0.21880. Took 0.05 sec\n",
            "Epoch 1360, Loss(train/val) 0.45510/0.22030. Took 0.05 sec\n",
            "Epoch 1361, Loss(train/val) 0.44780/0.22700. Took 0.04 sec\n",
            "Epoch 1362, Loss(train/val) 0.46142/0.22748. Took 0.05 sec\n",
            "Epoch 1363, Loss(train/val) 0.44320/0.22606. Took 0.04 sec\n",
            "Epoch 1364, Loss(train/val) 0.42550/0.21886. Took 0.05 sec\n",
            "Epoch 1365, Loss(train/val) 0.43395/0.21011. Took 0.04 sec\n",
            "Epoch 1366, Loss(train/val) 0.45101/0.20403. Took 0.05 sec\n",
            "Epoch 1367, Loss(train/val) 0.43283/0.20069. Took 0.05 sec\n",
            "Epoch 1368, Loss(train/val) 0.45146/0.19980. Took 0.05 sec\n",
            "Epoch 1369, Loss(train/val) 0.45609/0.19890. Took 0.05 sec\n",
            "Epoch 1370, Loss(train/val) 0.43460/0.19871. Took 0.04 sec\n",
            "Epoch 1371, Loss(train/val) 0.45042/0.19886. Took 0.05 sec\n",
            "Epoch 1372, Loss(train/val) 0.45423/0.20039. Took 0.04 sec\n",
            "Epoch 1373, Loss(train/val) 0.43541/0.20260. Took 0.04 sec\n",
            "Epoch 1374, Loss(train/val) 0.44307/0.20326. Took 0.05 sec\n",
            "Epoch 1375, Loss(train/val) 0.43021/0.20131. Took 0.04 sec\n",
            "Epoch 1376, Loss(train/val) 0.47447/0.20070. Took 0.05 sec\n",
            "Epoch 1377, Loss(train/val) 0.46109/0.20280. Took 0.04 sec\n",
            "Epoch 1378, Loss(train/val) 0.43564/0.20555. Took 0.05 sec\n",
            "Epoch 1379, Loss(train/val) 0.43488/0.20805. Took 0.06 sec\n",
            "Epoch 1380, Loss(train/val) 0.44625/0.20924. Took 0.05 sec\n",
            "Epoch 1381, Loss(train/val) 0.43340/0.20510. Took 0.05 sec\n",
            "Epoch 1382, Loss(train/val) 0.46140/0.20090. Took 0.04 sec\n",
            "Epoch 1383, Loss(train/val) 0.43359/0.20045. Took 0.05 sec\n",
            "Epoch 1384, Loss(train/val) 0.44524/0.19968. Took 0.06 sec\n",
            "Epoch 1385, Loss(train/val) 0.42983/0.19981. Took 0.05 sec\n",
            "Epoch 1386, Loss(train/val) 0.44235/0.20045. Took 0.04 sec\n",
            "Epoch 1387, Loss(train/val) 0.46184/0.20071. Took 0.05 sec\n",
            "Epoch 1388, Loss(train/val) 0.46256/0.20162. Took 0.05 sec\n",
            "Epoch 1389, Loss(train/val) 0.43239/0.20241. Took 0.05 sec\n",
            "Epoch 1390, Loss(train/val) 0.45555/0.20133. Took 0.05 sec\n",
            "Epoch 1391, Loss(train/val) 0.44203/0.20307. Took 0.05 sec\n",
            "Epoch 1392, Loss(train/val) 0.44277/0.20196. Took 0.05 sec\n",
            "Epoch 1393, Loss(train/val) 0.44883/0.20214. Took 0.05 sec\n",
            "Epoch 1394, Loss(train/val) 0.44358/0.19997. Took 0.05 sec\n",
            "Epoch 1395, Loss(train/val) 0.44673/0.19968. Took 0.04 sec\n",
            "Epoch 1396, Loss(train/val) 0.43394/0.19867. Took 0.04 sec\n",
            "Epoch 1397, Loss(train/val) 0.41716/0.19953. Took 0.04 sec\n",
            "Epoch 1398, Loss(train/val) 0.45542/0.20273. Took 0.05 sec\n",
            "Epoch 1399, Loss(train/val) 0.43884/0.20018. Took 0.05 sec\n",
            "Epoch 1400, Loss(train/val) 0.45955/0.19630. Took 0.05 sec\n",
            "Epoch 1401, Loss(train/val) 0.43737/0.19694. Took 0.05 sec\n",
            "Epoch 1402, Loss(train/val) 0.44551/0.19744. Took 0.04 sec\n",
            "Epoch 1403, Loss(train/val) 0.46324/0.19777. Took 0.05 sec\n",
            "Epoch 1404, Loss(train/val) 0.43857/0.20374. Took 0.06 sec\n",
            "Epoch 1405, Loss(train/val) 0.43582/0.21270. Took 0.06 sec\n",
            "Epoch 1406, Loss(train/val) 0.43527/0.21723. Took 0.05 sec\n",
            "Epoch 1407, Loss(train/val) 0.45998/0.22307. Took 0.05 sec\n",
            "Epoch 1408, Loss(train/val) 0.43148/0.21736. Took 0.04 sec\n",
            "Epoch 1409, Loss(train/val) 0.46283/0.21207. Took 0.05 sec\n",
            "Epoch 1410, Loss(train/val) 0.46358/0.20431. Took 0.04 sec\n",
            "Epoch 1411, Loss(train/val) 0.42004/0.19737. Took 0.05 sec\n",
            "Epoch 1412, Loss(train/val) 0.46327/0.19686. Took 0.05 sec\n",
            "Epoch 1413, Loss(train/val) 0.45076/0.19861. Took 0.05 sec\n",
            "Epoch 1414, Loss(train/val) 0.43012/0.19878. Took 0.05 sec\n",
            "Epoch 1415, Loss(train/val) 0.44072/0.20007. Took 0.05 sec\n",
            "Epoch 1416, Loss(train/val) 0.41835/0.19748. Took 0.05 sec\n",
            "Epoch 1417, Loss(train/val) 0.46298/0.19606. Took 0.05 sec\n",
            "Epoch 1418, Loss(train/val) 0.45342/0.19591. Took 0.05 sec\n",
            "Epoch 1419, Loss(train/val) 0.44618/0.19569. Took 0.06 sec\n",
            "Epoch 1420, Loss(train/val) 0.44192/0.19789. Took 0.05 sec\n",
            "Epoch 1421, Loss(train/val) 0.43035/0.19862. Took 0.05 sec\n",
            "Epoch 1422, Loss(train/val) 0.44094/0.20006. Took 0.05 sec\n",
            "Epoch 1423, Loss(train/val) 0.44235/0.20224. Took 0.05 sec\n",
            "Epoch 1424, Loss(train/val) 0.44528/0.19894. Took 0.05 sec\n",
            "Epoch 1425, Loss(train/val) 0.43734/0.19634. Took 0.05 sec\n",
            "Epoch 1426, Loss(train/val) 0.42550/0.19573. Took 0.05 sec\n",
            "Epoch 1427, Loss(train/val) 0.44071/0.19680. Took 0.05 sec\n",
            "Epoch 1428, Loss(train/val) 0.46208/0.19675. Took 0.05 sec\n",
            "Epoch 1429, Loss(train/val) 0.47012/0.19828. Took 0.06 sec\n",
            "Epoch 1430, Loss(train/val) 0.46064/0.20082. Took 0.05 sec\n",
            "Epoch 1431, Loss(train/val) 0.43465/0.20228. Took 0.04 sec\n",
            "Epoch 1432, Loss(train/val) 0.43699/0.20197. Took 0.05 sec\n",
            "Epoch 1433, Loss(train/val) 0.44715/0.19842. Took 0.04 sec\n",
            "Epoch 1434, Loss(train/val) 0.44259/0.20099. Took 0.05 sec\n",
            "Epoch 1435, Loss(train/val) 0.44206/0.20432. Took 0.05 sec\n",
            "Epoch 1436, Loss(train/val) 0.42719/0.20128. Took 0.05 sec\n",
            "Epoch 1437, Loss(train/val) 0.45849/0.19594. Took 0.04 sec\n",
            "Epoch 1438, Loss(train/val) 0.44842/0.19504. Took 0.04 sec\n",
            "Epoch 1439, Loss(train/val) 0.42590/0.19594. Took 0.05 sec\n",
            "Epoch 1440, Loss(train/val) 0.44894/0.19639. Took 0.05 sec\n",
            "Epoch 1441, Loss(train/val) 0.43987/0.19306. Took 0.05 sec\n",
            "Epoch 1442, Loss(train/val) 0.44430/0.19407. Took 0.05 sec\n",
            "Epoch 1443, Loss(train/val) 0.42878/0.19366. Took 0.04 sec\n",
            "Epoch 1444, Loss(train/val) 0.48243/0.19553. Took 0.06 sec\n",
            "Epoch 1445, Loss(train/val) 0.43208/0.19649. Took 0.05 sec\n",
            "Epoch 1446, Loss(train/val) 0.44414/0.19548. Took 0.05 sec\n",
            "Epoch 1447, Loss(train/val) 0.43255/0.19273. Took 0.05 sec\n",
            "Epoch 1448, Loss(train/val) 0.41447/0.19301. Took 0.05 sec\n",
            "Epoch 1449, Loss(train/val) 0.44220/0.19431. Took 0.05 sec\n",
            "Epoch 1450, Loss(train/val) 0.43370/0.19713. Took 0.05 sec\n",
            "Epoch 1451, Loss(train/val) 0.43409/0.20185. Took 0.05 sec\n",
            "Epoch 1452, Loss(train/val) 0.43377/0.20952. Took 0.04 sec\n",
            "Epoch 1453, Loss(train/val) 0.43788/0.21106. Took 0.04 sec\n",
            "Epoch 1454, Loss(train/val) 0.41472/0.21316. Took 0.05 sec\n",
            "Epoch 1455, Loss(train/val) 0.42174/0.21950. Took 0.05 sec\n",
            "Epoch 1456, Loss(train/val) 0.41496/0.23095. Took 0.04 sec\n",
            "Epoch 1457, Loss(train/val) 0.43232/0.24269. Took 0.04 sec\n",
            "Epoch 1458, Loss(train/val) 0.41887/0.24686. Took 0.04 sec\n",
            "Epoch 1459, Loss(train/val) 0.42958/0.24710. Took 0.05 sec\n",
            "Epoch 1460, Loss(train/val) 0.43195/0.24419. Took 0.05 sec\n",
            "Epoch 1461, Loss(train/val) 0.43523/0.24359. Took 0.04 sec\n",
            "Epoch 1462, Loss(train/val) 0.40681/0.24908. Took 0.04 sec\n",
            "Epoch 1463, Loss(train/val) 0.43281/0.25136. Took 0.04 sec\n",
            "Epoch 1464, Loss(train/val) 0.43498/0.23262. Took 0.05 sec\n",
            "Epoch 1465, Loss(train/val) 0.42463/0.22073. Took 0.04 sec\n",
            "Epoch 1466, Loss(train/val) 0.44255/0.21100. Took 0.04 sec\n",
            "Epoch 1467, Loss(train/val) 0.42312/0.20677. Took 0.05 sec\n",
            "Epoch 1468, Loss(train/val) 0.42906/0.19642. Took 0.05 sec\n",
            "Epoch 1469, Loss(train/val) 0.43228/0.19074. Took 0.06 sec\n",
            "Epoch 1470, Loss(train/val) 0.43902/0.18864. Took 0.04 sec\n",
            "Epoch 1471, Loss(train/val) 0.43645/0.18898. Took 0.05 sec\n",
            "Epoch 1472, Loss(train/val) 0.45925/0.19200. Took 0.04 sec\n",
            "Epoch 1473, Loss(train/val) 0.41445/0.19546. Took 0.05 sec\n",
            "Epoch 1474, Loss(train/val) 0.44596/0.19748. Took 0.06 sec\n",
            "Epoch 1475, Loss(train/val) 0.42536/0.19457. Took 0.05 sec\n",
            "Epoch 1476, Loss(train/val) 0.40529/0.19253. Took 0.05 sec\n",
            "Epoch 1477, Loss(train/val) 0.43360/0.19314. Took 0.04 sec\n",
            "Epoch 1478, Loss(train/val) 0.42911/0.18915. Took 0.05 sec\n",
            "Epoch 1479, Loss(train/val) 0.41291/0.18755. Took 0.05 sec\n",
            "Epoch 1480, Loss(train/val) 0.41307/0.18623. Took 0.04 sec\n",
            "Epoch 1481, Loss(train/val) 0.42417/0.18770. Took 0.04 sec\n",
            "Epoch 1482, Loss(train/val) 0.44233/0.19320. Took 0.05 sec\n",
            "Epoch 1483, Loss(train/val) 0.41588/0.20066. Took 0.05 sec\n",
            "Epoch 1484, Loss(train/val) 0.43751/0.20268. Took 0.05 sec\n",
            "Epoch 1485, Loss(train/val) 0.41746/0.19269. Took 0.05 sec\n",
            "Epoch 1486, Loss(train/val) 0.43394/0.18938. Took 0.05 sec\n",
            "Epoch 1487, Loss(train/val) 0.41251/0.18997. Took 0.05 sec\n",
            "Epoch 1488, Loss(train/val) 0.43166/0.18947. Took 0.05 sec\n",
            "Epoch 1489, Loss(train/val) 0.39069/0.19044. Took 0.05 sec\n",
            "Epoch 1490, Loss(train/val) 0.44066/0.19336. Took 0.05 sec\n",
            "Epoch 1491, Loss(train/val) 0.44269/0.20068. Took 0.04 sec\n",
            "Epoch 1492, Loss(train/val) 0.42738/0.20995. Took 0.05 sec\n",
            "Epoch 1493, Loss(train/val) 0.41821/0.21389. Took 0.04 sec\n",
            "Epoch 1494, Loss(train/val) 0.45603/0.20964. Took 0.06 sec\n",
            "Epoch 1495, Loss(train/val) 0.42620/0.20025. Took 0.05 sec\n",
            "Epoch 1496, Loss(train/val) 0.42037/0.19534. Took 0.04 sec\n",
            "Epoch 1497, Loss(train/val) 0.44645/0.19532. Took 0.05 sec\n",
            "Epoch 1498, Loss(train/val) 0.45745/0.19545. Took 0.04 sec\n",
            "Epoch 1499, Loss(train/val) 0.38878/0.19389. Took 0.05 sec\n",
            "Epoch 1500, Loss(train/val) 0.44099/0.19826. Took 0.04 sec\n",
            "Epoch 1501, Loss(train/val) 0.38796/0.19993. Took 0.04 sec\n",
            "Epoch 1502, Loss(train/val) 0.43790/0.20069. Took 0.05 sec\n",
            "Epoch 1503, Loss(train/val) 0.42894/0.19976. Took 0.05 sec\n",
            "Epoch 1504, Loss(train/val) 0.41196/0.20232. Took 0.05 sec\n",
            "Epoch 1505, Loss(train/val) 0.44891/0.20560. Took 0.05 sec\n",
            "Epoch 1506, Loss(train/val) 0.42362/0.20480. Took 0.04 sec\n",
            "Epoch 1507, Loss(train/val) 0.40803/0.19916. Took 0.05 sec\n",
            "Epoch 1508, Loss(train/val) 0.44072/0.19360. Took 0.05 sec\n",
            "Epoch 1509, Loss(train/val) 0.43250/0.18936. Took 0.05 sec\n",
            "Epoch 1510, Loss(train/val) 0.44428/0.18880. Took 0.05 sec\n",
            "Epoch 1511, Loss(train/val) 0.42377/0.18927. Took 0.05 sec\n",
            "Epoch 1512, Loss(train/val) 0.42137/0.18823. Took 0.05 sec\n",
            "Epoch 1513, Loss(train/val) 0.43267/0.18833. Took 0.05 sec\n",
            "Epoch 1514, Loss(train/val) 0.44441/0.18554. Took 0.05 sec\n",
            "Epoch 1515, Loss(train/val) 0.46796/0.18851. Took 0.04 sec\n",
            "Epoch 1516, Loss(train/val) 0.41649/0.19417. Took 0.05 sec\n",
            "Epoch 1517, Loss(train/val) 0.40786/0.19359. Took 0.04 sec\n",
            "Epoch 1518, Loss(train/val) 0.41354/0.19167. Took 0.05 sec\n",
            "Epoch 1519, Loss(train/val) 0.44090/0.18830. Took 0.05 sec\n",
            "Epoch 1520, Loss(train/val) 0.41739/0.18699. Took 0.05 sec\n",
            "Epoch 1521, Loss(train/val) 0.43431/0.18585. Took 0.04 sec\n",
            "Epoch 1522, Loss(train/val) 0.42977/0.18600. Took 0.05 sec\n",
            "Epoch 1523, Loss(train/val) 0.40891/0.18620. Took 0.05 sec\n",
            "Epoch 1524, Loss(train/val) 0.41867/0.18915. Took 0.06 sec\n",
            "Epoch 1525, Loss(train/val) 0.41589/0.19173. Took 0.05 sec\n",
            "Epoch 1526, Loss(train/val) 0.41401/0.18925. Took 0.04 sec\n",
            "Epoch 1527, Loss(train/val) 0.39940/0.18447. Took 0.05 sec\n",
            "Epoch 1528, Loss(train/val) 0.41001/0.18534. Took 0.05 sec\n",
            "Epoch 1529, Loss(train/val) 0.44762/0.19053. Took 0.05 sec\n",
            "Epoch 1530, Loss(train/val) 0.43529/0.18316. Took 0.05 sec\n",
            "Epoch 1531, Loss(train/val) 0.42032/0.18073. Took 0.05 sec\n",
            "Epoch 1532, Loss(train/val) 0.43066/0.18042. Took 0.05 sec\n",
            "Epoch 1533, Loss(train/val) 0.44009/0.18259. Took 0.05 sec\n",
            "Epoch 1534, Loss(train/val) 0.40974/0.18713. Took 0.06 sec\n",
            "Epoch 1535, Loss(train/val) 0.41673/0.19255. Took 0.05 sec\n",
            "Epoch 1536, Loss(train/val) 0.40912/0.20948. Took 0.05 sec\n",
            "Epoch 1537, Loss(train/val) 0.42723/0.22859. Took 0.05 sec\n",
            "Epoch 1538, Loss(train/val) 0.42414/0.24225. Took 0.05 sec\n",
            "Epoch 1539, Loss(train/val) 0.41946/0.23288. Took 0.05 sec\n",
            "Epoch 1540, Loss(train/val) 0.42121/0.23271. Took 0.05 sec\n",
            "Epoch 1541, Loss(train/val) 0.43214/0.21762. Took 0.05 sec\n",
            "Epoch 1542, Loss(train/val) 0.40382/0.20387. Took 0.05 sec\n",
            "Epoch 1543, Loss(train/val) 0.43507/0.18906. Took 0.06 sec\n",
            "Epoch 1544, Loss(train/val) 0.44495/0.18295. Took 0.05 sec\n",
            "Epoch 1545, Loss(train/val) 0.42072/0.18098. Took 0.05 sec\n",
            "Epoch 1546, Loss(train/val) 0.43309/0.18383. Took 0.05 sec\n",
            "Epoch 1547, Loss(train/val) 0.41064/0.18645. Took 0.06 sec\n",
            "Epoch 1548, Loss(train/val) 0.42972/0.18886. Took 0.05 sec\n",
            "Epoch 1549, Loss(train/val) 0.42953/0.19140. Took 0.05 sec\n",
            "Epoch 1550, Loss(train/val) 0.40909/0.19334. Took 0.05 sec\n",
            "Epoch 1551, Loss(train/val) 0.39708/0.19043. Took 0.05 sec\n",
            "Epoch 1552, Loss(train/val) 0.41548/0.19074. Took 0.07 sec\n",
            "Epoch 1553, Loss(train/val) 0.44287/0.18174. Took 0.05 sec\n",
            "Epoch 1554, Loss(train/val) 0.41307/0.17891. Took 0.05 sec\n",
            "Epoch 1555, Loss(train/val) 0.43590/0.17905. Took 0.05 sec\n",
            "Epoch 1556, Loss(train/val) 0.42197/0.18639. Took 0.05 sec\n",
            "Epoch 1557, Loss(train/val) 0.41484/0.18961. Took 0.06 sec\n",
            "Epoch 1558, Loss(train/val) 0.42430/0.18757. Took 0.06 sec\n",
            "Epoch 1559, Loss(train/val) 0.43851/0.17796. Took 0.05 sec\n",
            "Epoch 1560, Loss(train/val) 0.42606/0.18140. Took 0.06 sec\n",
            "Epoch 1561, Loss(train/val) 0.42942/0.19208. Took 0.05 sec\n",
            "Epoch 1562, Loss(train/val) 0.43404/0.19649. Took 0.05 sec\n",
            "Epoch 1563, Loss(train/val) 0.43378/0.21126. Took 0.04 sec\n",
            "Epoch 1564, Loss(train/val) 0.43263/0.21344. Took 0.05 sec\n",
            "Epoch 1565, Loss(train/val) 0.41653/0.20783. Took 0.05 sec\n",
            "Epoch 1566, Loss(train/val) 0.41637/0.21078. Took 0.04 sec\n",
            "Epoch 1567, Loss(train/val) 0.41699/0.21965. Took 0.05 sec\n",
            "Epoch 1568, Loss(train/val) 0.43722/0.21026. Took 0.05 sec\n",
            "Epoch 1569, Loss(train/val) 0.40454/0.19303. Took 0.05 sec\n",
            "Epoch 1570, Loss(train/val) 0.47992/0.18153. Took 0.05 sec\n",
            "Epoch 1571, Loss(train/val) 0.43267/0.17831. Took 0.05 sec\n",
            "Epoch 1572, Loss(train/val) 0.42290/0.17845. Took 0.06 sec\n",
            "Epoch 1573, Loss(train/val) 0.41356/0.17889. Took 0.05 sec\n",
            "Epoch 1574, Loss(train/val) 0.41689/0.17927. Took 0.05 sec\n",
            "Epoch 1575, Loss(train/val) 0.42396/0.17974. Took 0.05 sec\n",
            "Epoch 1576, Loss(train/val) 0.41948/0.17964. Took 0.05 sec\n",
            "Epoch 1577, Loss(train/val) 0.41144/0.17865. Took 0.05 sec\n",
            "Epoch 1578, Loss(train/val) 0.39370/0.17907. Took 0.04 sec\n",
            "Epoch 1579, Loss(train/val) 0.41051/0.17983. Took 0.04 sec\n",
            "Epoch 1580, Loss(train/val) 0.41951/0.18153. Took 0.05 sec\n",
            "Epoch 1581, Loss(train/val) 0.39985/0.18718. Took 0.05 sec\n",
            "Epoch 1582, Loss(train/val) 0.43286/0.19726. Took 0.05 sec\n",
            "Epoch 1583, Loss(train/val) 0.44758/0.19564. Took 0.05 sec\n",
            "Epoch 1584, Loss(train/val) 0.43494/0.18045. Took 0.05 sec\n",
            "Epoch 1585, Loss(train/val) 0.41409/0.17924. Took 0.05 sec\n",
            "Epoch 1586, Loss(train/val) 0.43154/0.17905. Took 0.05 sec\n",
            "Epoch 1587, Loss(train/val) 0.42083/0.17814. Took 0.04 sec\n",
            "Epoch 1588, Loss(train/val) 0.40664/0.17731. Took 0.05 sec\n",
            "Epoch 1589, Loss(train/val) 0.40315/0.17677. Took 0.05 sec\n",
            "Epoch 1590, Loss(train/val) 0.42753/0.17723. Took 0.05 sec\n",
            "Epoch 1591, Loss(train/val) 0.43625/0.17451. Took 0.04 sec\n",
            "Epoch 1592, Loss(train/val) 0.42581/0.17570. Took 0.05 sec\n",
            "Epoch 1593, Loss(train/val) 0.40918/0.17404. Took 0.05 sec\n",
            "Epoch 1594, Loss(train/val) 0.42895/0.17429. Took 0.05 sec\n",
            "Epoch 1595, Loss(train/val) 0.43654/0.17431. Took 0.05 sec\n",
            "Epoch 1596, Loss(train/val) 0.41284/0.17391. Took 0.05 sec\n",
            "Epoch 1597, Loss(train/val) 0.42196/0.17351. Took 0.05 sec\n",
            "Epoch 1598, Loss(train/val) 0.44228/0.17358. Took 0.05 sec\n",
            "Epoch 1599, Loss(train/val) 0.39089/0.17913. Took 0.05 sec\n",
            "Epoch 1600, Loss(train/val) 0.41096/0.18236. Took 0.05 sec\n",
            "Epoch 1601, Loss(train/val) 0.40446/0.18760. Took 0.05 sec\n",
            "Epoch 1602, Loss(train/val) 0.40593/0.18578. Took 0.05 sec\n",
            "Epoch 1603, Loss(train/val) 0.42779/0.18241. Took 0.04 sec\n",
            "Epoch 1604, Loss(train/val) 0.42500/0.18621. Took 0.05 sec\n",
            "Epoch 1605, Loss(train/val) 0.41611/0.18126. Took 0.05 sec\n",
            "Epoch 1606, Loss(train/val) 0.39586/0.17153. Took 0.05 sec\n",
            "Epoch 1607, Loss(train/val) 0.40153/0.17051. Took 0.04 sec\n",
            "Epoch 1608, Loss(train/val) 0.41526/0.17073. Took 0.05 sec\n",
            "Epoch 1609, Loss(train/val) 0.41395/0.17048. Took 0.05 sec\n",
            "Epoch 1610, Loss(train/val) 0.40774/0.17122. Took 0.05 sec\n",
            "Epoch 1611, Loss(train/val) 0.44037/0.17134. Took 0.05 sec\n",
            "Epoch 1612, Loss(train/val) 0.36779/0.17090. Took 0.04 sec\n",
            "Epoch 1613, Loss(train/val) 0.40040/0.17216. Took 0.04 sec\n",
            "Epoch 1614, Loss(train/val) 0.38790/0.17323. Took 0.05 sec\n",
            "Epoch 1615, Loss(train/val) 0.41794/0.17315. Took 0.06 sec\n",
            "Epoch 1616, Loss(train/val) 0.40897/0.17392. Took 0.04 sec\n",
            "Epoch 1617, Loss(train/val) 0.41413/0.17351. Took 0.04 sec\n",
            "Epoch 1618, Loss(train/val) 0.40962/0.17338. Took 0.04 sec\n",
            "Epoch 1619, Loss(train/val) 0.43614/0.17230. Took 0.04 sec\n",
            "Epoch 1620, Loss(train/val) 0.42108/0.17227. Took 0.05 sec\n",
            "Epoch 1621, Loss(train/val) 0.43856/0.17246. Took 0.05 sec\n",
            "Epoch 1622, Loss(train/val) 0.40672/0.17354. Took 0.05 sec\n",
            "Epoch 1623, Loss(train/val) 0.38604/0.17824. Took 0.04 sec\n",
            "Epoch 1624, Loss(train/val) 0.41701/0.19995. Took 0.04 sec\n",
            "Epoch 1625, Loss(train/val) 0.40432/0.21316. Took 0.05 sec\n",
            "Epoch 1626, Loss(train/val) 0.44910/0.20964. Took 0.05 sec\n",
            "Epoch 1627, Loss(train/val) 0.43830/0.19960. Took 0.04 sec\n",
            "Epoch 1628, Loss(train/val) 0.38646/0.19811. Took 0.05 sec\n",
            "Epoch 1629, Loss(train/val) 0.40785/0.18775. Took 0.05 sec\n",
            "Epoch 1630, Loss(train/val) 0.41484/0.17974. Took 0.05 sec\n",
            "Epoch 1631, Loss(train/val) 0.36985/0.17831. Took 0.05 sec\n",
            "Epoch 1632, Loss(train/val) 0.41240/0.17744. Took 0.05 sec\n",
            "Epoch 1633, Loss(train/val) 0.39840/0.17749. Took 0.05 sec\n",
            "Epoch 1634, Loss(train/val) 0.41168/0.17586. Took 0.04 sec\n",
            "Epoch 1635, Loss(train/val) 0.40055/0.17563. Took 0.05 sec\n",
            "Epoch 1636, Loss(train/val) 0.39406/0.17024. Took 0.05 sec\n",
            "Epoch 1637, Loss(train/val) 0.43501/0.16977. Took 0.05 sec\n",
            "Epoch 1638, Loss(train/val) 0.40761/0.17107. Took 0.05 sec\n",
            "Epoch 1639, Loss(train/val) 0.41842/0.17040. Took 0.05 sec\n",
            "Epoch 1640, Loss(train/val) 0.40748/0.16811. Took 0.05 sec\n",
            "Epoch 1641, Loss(train/val) 0.40103/0.16798. Took 0.05 sec\n",
            "Epoch 1642, Loss(train/val) 0.41318/0.16992. Took 0.04 sec\n",
            "Epoch 1643, Loss(train/val) 0.38961/0.17131. Took 0.04 sec\n",
            "Epoch 1644, Loss(train/val) 0.42127/0.17298. Took 0.04 sec\n",
            "Epoch 1645, Loss(train/val) 0.40000/0.17368. Took 0.05 sec\n",
            "Epoch 1646, Loss(train/val) 0.42910/0.17395. Took 0.04 sec\n",
            "Epoch 1647, Loss(train/val) 0.40230/0.17263. Took 0.05 sec\n",
            "Epoch 1648, Loss(train/val) 0.39561/0.17186. Took 0.04 sec\n",
            "Epoch 1649, Loss(train/val) 0.38935/0.17512. Took 0.04 sec\n",
            "Epoch 1650, Loss(train/val) 0.40994/0.17618. Took 0.05 sec\n",
            "Epoch 1651, Loss(train/val) 0.41683/0.17686. Took 0.05 sec\n",
            "Epoch 1652, Loss(train/val) 0.42106/0.16987. Took 0.04 sec\n",
            "Epoch 1653, Loss(train/val) 0.39950/0.16688. Took 0.04 sec\n",
            "Epoch 1654, Loss(train/val) 0.42938/0.16712. Took 0.04 sec\n",
            "Epoch 1655, Loss(train/val) 0.42251/0.17218. Took 0.05 sec\n",
            "Epoch 1656, Loss(train/val) 0.40018/0.18177. Took 0.04 sec\n",
            "Epoch 1657, Loss(train/val) 0.40734/0.19078. Took 0.05 sec\n",
            "Epoch 1658, Loss(train/val) 0.41758/0.19452. Took 0.05 sec\n",
            "Epoch 1659, Loss(train/val) 0.44861/0.19717. Took 0.04 sec\n",
            "Epoch 1660, Loss(train/val) 0.42137/0.18553. Took 0.05 sec\n",
            "Epoch 1661, Loss(train/val) 0.41456/0.18100. Took 0.05 sec\n",
            "Epoch 1662, Loss(train/val) 0.39723/0.17290. Took 0.05 sec\n",
            "Epoch 1663, Loss(train/val) 0.40996/0.17339. Took 0.04 sec\n",
            "Epoch 1664, Loss(train/val) 0.42132/0.17901. Took 0.04 sec\n",
            "Epoch 1665, Loss(train/val) 0.42591/0.18423. Took 0.05 sec\n",
            "Epoch 1666, Loss(train/val) 0.40558/0.18583. Took 0.05 sec\n",
            "Epoch 1667, Loss(train/val) 0.40995/0.17711. Took 0.05 sec\n",
            "Epoch 1668, Loss(train/val) 0.40724/0.17313. Took 0.04 sec\n",
            "Epoch 1669, Loss(train/val) 0.40987/0.17317. Took 0.05 sec\n",
            "Epoch 1670, Loss(train/val) 0.42428/0.17366. Took 0.05 sec\n",
            "Epoch 1671, Loss(train/val) 0.41107/0.17354. Took 0.04 sec\n",
            "Epoch 1672, Loss(train/val) 0.36488/0.17220. Took 0.04 sec\n",
            "Epoch 1673, Loss(train/val) 0.40827/0.17296. Took 0.05 sec\n",
            "Epoch 1674, Loss(train/val) 0.41439/0.17458. Took 0.04 sec\n",
            "Epoch 1675, Loss(train/val) 0.41451/0.17116. Took 0.05 sec\n",
            "Epoch 1676, Loss(train/val) 0.41366/0.16977. Took 0.05 sec\n",
            "Epoch 1677, Loss(train/val) 0.41542/0.17002. Took 0.05 sec\n",
            "Epoch 1678, Loss(train/val) 0.42895/0.17274. Took 0.05 sec\n",
            "Epoch 1679, Loss(train/val) 0.39437/0.17827. Took 0.05 sec\n",
            "Epoch 1680, Loss(train/val) 0.36660/0.17506. Took 0.05 sec\n",
            "Epoch 1681, Loss(train/val) 0.41045/0.17150. Took 0.05 sec\n",
            "Epoch 1682, Loss(train/val) 0.40737/0.17016. Took 0.04 sec\n",
            "Epoch 1683, Loss(train/val) 0.37311/0.16990. Took 0.05 sec\n",
            "Epoch 1684, Loss(train/val) 0.40615/0.17105. Took 0.05 sec\n",
            "Epoch 1685, Loss(train/val) 0.39533/0.17566. Took 0.05 sec\n",
            "Epoch 1686, Loss(train/val) 0.38071/0.18112. Took 0.05 sec\n",
            "Epoch 1687, Loss(train/val) 0.38807/0.18163. Took 0.05 sec\n",
            "Epoch 1688, Loss(train/val) 0.40434/0.18421. Took 0.05 sec\n",
            "Epoch 1689, Loss(train/val) 0.42303/0.17812. Took 0.05 sec\n",
            "Epoch 1690, Loss(train/val) 0.40216/0.17065. Took 0.05 sec\n",
            "Epoch 1691, Loss(train/val) 0.40963/0.16939. Took 0.05 sec\n",
            "Epoch 1692, Loss(train/val) 0.40097/0.17015. Took 0.05 sec\n",
            "Epoch 1693, Loss(train/val) 0.40389/0.17276. Took 0.05 sec\n",
            "Epoch 1694, Loss(train/val) 0.40892/0.17005. Took 0.04 sec\n",
            "Epoch 1695, Loss(train/val) 0.39510/0.16840. Took 0.05 sec\n",
            "Epoch 1696, Loss(train/val) 0.40824/0.16815. Took 0.05 sec\n",
            "Epoch 1697, Loss(train/val) 0.42784/0.17187. Took 0.05 sec\n",
            "Epoch 1698, Loss(train/val) 0.43274/0.17888. Took 0.05 sec\n",
            "Epoch 1699, Loss(train/val) 0.39117/0.17804. Took 0.05 sec\n",
            "Epoch 1700, Loss(train/val) 0.40588/0.17554. Took 0.06 sec\n",
            "Epoch 1701, Loss(train/val) 0.40754/0.16939. Took 0.05 sec\n",
            "Epoch 1702, Loss(train/val) 0.40720/0.16874. Took 0.04 sec\n",
            "Epoch 1703, Loss(train/val) 0.41376/0.17844. Took 0.05 sec\n",
            "Epoch 1704, Loss(train/val) 0.42506/0.20060. Took 0.05 sec\n",
            "Epoch 1705, Loss(train/val) 0.41028/0.20411. Took 0.05 sec\n",
            "Epoch 1706, Loss(train/val) 0.40776/0.19430. Took 0.04 sec\n",
            "Epoch 1707, Loss(train/val) 0.40053/0.18671. Took 0.05 sec\n",
            "Epoch 1708, Loss(train/val) 0.41088/0.17555. Took 0.04 sec\n",
            "Epoch 1709, Loss(train/val) 0.39998/0.17470. Took 0.05 sec\n",
            "Epoch 1710, Loss(train/val) 0.42374/0.17722. Took 0.05 sec\n",
            "Epoch 1711, Loss(train/val) 0.39774/0.17266. Took 0.04 sec\n",
            "Epoch 1712, Loss(train/val) 0.42216/0.17113. Took 0.05 sec\n",
            "Epoch 1713, Loss(train/val) 0.40769/0.17212. Took 0.05 sec\n",
            "Epoch 1714, Loss(train/val) 0.39899/0.17174. Took 0.05 sec\n",
            "Epoch 1715, Loss(train/val) 0.39277/0.17229. Took 0.05 sec\n",
            "Epoch 1716, Loss(train/val) 0.40079/0.17284. Took 0.05 sec\n",
            "Epoch 1717, Loss(train/val) 0.39537/0.17160. Took 0.05 sec\n",
            "Epoch 1718, Loss(train/val) 0.40989/0.17046. Took 0.04 sec\n",
            "Epoch 1719, Loss(train/val) 0.40450/0.17039. Took 0.05 sec\n",
            "Epoch 1720, Loss(train/val) 0.40337/0.17096. Took 0.05 sec\n",
            "Epoch 1721, Loss(train/val) 0.40750/0.17291. Took 0.05 sec\n",
            "Epoch 1722, Loss(train/val) 0.39807/0.17300. Took 0.06 sec\n",
            "Epoch 1723, Loss(train/val) 0.41585/0.17338. Took 0.04 sec\n",
            "Epoch 1724, Loss(train/val) 0.40805/0.17143. Took 0.05 sec\n",
            "Epoch 1725, Loss(train/val) 0.40374/0.17224. Took 0.05 sec\n",
            "Epoch 1726, Loss(train/val) 0.41296/0.17548. Took 0.05 sec\n",
            "Epoch 1727, Loss(train/val) 0.41400/0.17560. Took 0.05 sec\n",
            "Epoch 1728, Loss(train/val) 0.40345/0.18348. Took 0.04 sec\n",
            "Epoch 1729, Loss(train/val) 0.38432/0.18202. Took 0.05 sec\n",
            "Epoch 1730, Loss(train/val) 0.37797/0.17430. Took 0.05 sec\n",
            "Epoch 1731, Loss(train/val) 0.39974/0.17191. Took 0.04 sec\n",
            "Epoch 1732, Loss(train/val) 0.41430/0.16902. Took 0.04 sec\n",
            "Epoch 1733, Loss(train/val) 0.38278/0.16892. Took 0.04 sec\n",
            "Epoch 1734, Loss(train/val) 0.41799/0.17075. Took 0.05 sec\n",
            "Epoch 1735, Loss(train/val) 0.40075/0.17329. Took 0.05 sec\n",
            "Epoch 1736, Loss(train/val) 0.42288/0.17202. Took 0.05 sec\n",
            "Epoch 1737, Loss(train/val) 0.39174/0.17473. Took 0.05 sec\n",
            "Epoch 1738, Loss(train/val) 0.40209/0.17781. Took 0.04 sec\n",
            "Epoch 1739, Loss(train/val) 0.35930/0.18202. Took 0.04 sec\n",
            "Epoch 1740, Loss(train/val) 0.40546/0.18782. Took 0.05 sec\n",
            "Epoch 1741, Loss(train/val) 0.40914/0.20057. Took 0.05 sec\n",
            "Epoch 1742, Loss(train/val) 0.40494/0.20510. Took 0.04 sec\n",
            "Epoch 1743, Loss(train/val) 0.39595/0.19703. Took 0.04 sec\n",
            "Epoch 1744, Loss(train/val) 0.39141/0.18288. Took 0.05 sec\n",
            "Epoch 1745, Loss(train/val) 0.40134/0.17704. Took 0.05 sec\n",
            "Epoch 1746, Loss(train/val) 0.39135/0.17564. Took 0.05 sec\n",
            "Epoch 1747, Loss(train/val) 0.39825/0.17296. Took 0.04 sec\n",
            "Epoch 1748, Loss(train/val) 0.42231/0.17063. Took 0.04 sec\n",
            "Epoch 1749, Loss(train/val) 0.40666/0.16895. Took 0.04 sec\n",
            "Epoch 1750, Loss(train/val) 0.40161/0.16868. Took 0.05 sec\n",
            "Epoch 1751, Loss(train/val) 0.39019/0.17410. Took 0.05 sec\n",
            "Epoch 1752, Loss(train/val) 0.43898/0.18143. Took 0.05 sec\n",
            "Epoch 1753, Loss(train/val) 0.40143/0.18018. Took 0.05 sec\n",
            "Epoch 1754, Loss(train/val) 0.42371/0.17682. Took 0.05 sec\n",
            "Epoch 1755, Loss(train/val) 0.42608/0.17037. Took 0.06 sec\n",
            "Epoch 1756, Loss(train/val) 0.42193/0.16911. Took 0.05 sec\n",
            "Epoch 1757, Loss(train/val) 0.41044/0.16940. Took 0.05 sec\n",
            "Epoch 1758, Loss(train/val) 0.43580/0.16962. Took 0.05 sec\n",
            "Epoch 1759, Loss(train/val) 0.39442/0.16960. Took 0.05 sec\n",
            "Epoch 1760, Loss(train/val) 0.37385/0.16965. Took 0.05 sec\n",
            "Epoch 1761, Loss(train/val) 0.39767/0.16972. Took 0.05 sec\n",
            "Epoch 1762, Loss(train/val) 0.39470/0.16963. Took 0.05 sec\n",
            "Epoch 1763, Loss(train/val) 0.38329/0.17200. Took 0.05 sec\n",
            "Epoch 1764, Loss(train/val) 0.38699/0.17300. Took 0.04 sec\n",
            "Epoch 1765, Loss(train/val) 0.40727/0.17472. Took 0.06 sec\n",
            "Epoch 1766, Loss(train/val) 0.41720/0.17629. Took 0.05 sec\n",
            "Epoch 1767, Loss(train/val) 0.38629/0.17400. Took 0.05 sec\n",
            "Epoch 1768, Loss(train/val) 0.38336/0.17118. Took 0.04 sec\n",
            "Epoch 1769, Loss(train/val) 0.43054/0.16857. Took 0.04 sec\n",
            "Epoch 1770, Loss(train/val) 0.38683/0.17055. Took 0.05 sec\n",
            "Epoch 1771, Loss(train/val) 0.40440/0.17293. Took 0.05 sec\n",
            "Epoch 1772, Loss(train/val) 0.39581/0.17488. Took 0.05 sec\n",
            "Epoch 1773, Loss(train/val) 0.39930/0.17837. Took 0.05 sec\n",
            "Epoch 1774, Loss(train/val) 0.37788/0.18019. Took 0.05 sec\n",
            "Epoch 1775, Loss(train/val) 0.42931/0.18784. Took 0.05 sec\n",
            "Epoch 1776, Loss(train/val) 0.41182/0.18621. Took 0.05 sec\n",
            "Epoch 1777, Loss(train/val) 0.39061/0.17870. Took 0.04 sec\n",
            "Epoch 1778, Loss(train/val) 0.37373/0.17398. Took 0.04 sec\n",
            "Epoch 1779, Loss(train/val) 0.41908/0.17347. Took 0.05 sec\n",
            "Epoch 1780, Loss(train/val) 0.39454/0.17476. Took 0.05 sec\n",
            "Epoch 1781, Loss(train/val) 0.39667/0.17415. Took 0.05 sec\n",
            "Epoch 1782, Loss(train/val) 0.40849/0.17477. Took 0.04 sec\n",
            "Epoch 1783, Loss(train/val) 0.41535/0.17350. Took 0.05 sec\n",
            "Epoch 1784, Loss(train/val) 0.37609/0.17391. Took 0.05 sec\n",
            "Epoch 1785, Loss(train/val) 0.39345/0.17356. Took 0.05 sec\n",
            "Epoch 1786, Loss(train/val) 0.39345/0.17254. Took 0.05 sec\n",
            "Epoch 1787, Loss(train/val) 0.38289/0.17327. Took 0.05 sec\n",
            "Epoch 1788, Loss(train/val) 0.41243/0.17447. Took 0.04 sec\n",
            "Epoch 1789, Loss(train/val) 0.43170/0.17172. Took 0.04 sec\n",
            "Epoch 1790, Loss(train/val) 0.41026/0.16938. Took 0.05 sec\n",
            "Epoch 1791, Loss(train/val) 0.38364/0.17019. Took 0.05 sec\n",
            "Epoch 1792, Loss(train/val) 0.42328/0.17158. Took 0.05 sec\n",
            "Epoch 1793, Loss(train/val) 0.41157/0.17352. Took 0.05 sec\n",
            "Epoch 1794, Loss(train/val) 0.38685/0.17252. Took 0.05 sec\n",
            "Epoch 1795, Loss(train/val) 0.39047/0.17041. Took 0.05 sec\n",
            "Epoch 1796, Loss(train/val) 0.37776/0.16936. Took 0.05 sec\n",
            "Epoch 1797, Loss(train/val) 0.37359/0.16983. Took 0.04 sec\n",
            "Epoch 1798, Loss(train/val) 0.39901/0.16963. Took 0.05 sec\n",
            "Epoch 1799, Loss(train/val) 0.38601/0.16946. Took 0.05 sec\n",
            "Epoch 1800, Loss(train/val) 0.39130/0.16902. Took 0.05 sec\n",
            "Epoch 1801, Loss(train/val) 0.40884/0.16946. Took 0.05 sec\n",
            "Epoch 1802, Loss(train/val) 0.40018/0.17134. Took 0.05 sec\n",
            "Epoch 1803, Loss(train/val) 0.39577/0.17374. Took 0.04 sec\n",
            "Epoch 1804, Loss(train/val) 0.41412/0.18125. Took 0.05 sec\n",
            "Epoch 1805, Loss(train/val) 0.40986/0.18843. Took 0.05 sec\n",
            "Epoch 1806, Loss(train/val) 0.40035/0.18320. Took 0.04 sec\n",
            "Epoch 1807, Loss(train/val) 0.39845/0.17699. Took 0.04 sec\n",
            "Epoch 1808, Loss(train/val) 0.39731/0.17538. Took 0.06 sec\n",
            "Epoch 1809, Loss(train/val) 0.36335/0.17498. Took 0.04 sec\n",
            "Epoch 1810, Loss(train/val) 0.37715/0.17802. Took 0.05 sec\n",
            "Epoch 1811, Loss(train/val) 0.39437/0.17728. Took 0.04 sec\n",
            "Epoch 1812, Loss(train/val) 0.39334/0.16559. Took 0.04 sec\n",
            "Epoch 1813, Loss(train/val) 0.39022/0.16438. Took 0.04 sec\n",
            "Epoch 1814, Loss(train/val) 0.38514/0.16561. Took 0.05 sec\n",
            "Epoch 1815, Loss(train/val) 0.40725/0.16548. Took 0.05 sec\n",
            "Epoch 1816, Loss(train/val) 0.39861/0.16516. Took 0.04 sec\n",
            "Epoch 1817, Loss(train/val) 0.39827/0.16484. Took 0.04 sec\n",
            "Epoch 1818, Loss(train/val) 0.40208/0.16463. Took 0.05 sec\n",
            "Epoch 1819, Loss(train/val) 0.39202/0.16513. Took 0.05 sec\n",
            "Epoch 1820, Loss(train/val) 0.37473/0.16651. Took 0.05 sec\n",
            "Epoch 1821, Loss(train/val) 0.41018/0.16652. Took 0.05 sec\n",
            "Epoch 1822, Loss(train/val) 0.41512/0.16843. Took 0.04 sec\n",
            "Epoch 1823, Loss(train/val) 0.38433/0.16856. Took 0.04 sec\n",
            "Epoch 1824, Loss(train/val) 0.40580/0.16872. Took 0.05 sec\n",
            "Epoch 1825, Loss(train/val) 0.39416/0.16941. Took 0.05 sec\n",
            "Epoch 1826, Loss(train/val) 0.38161/0.17104. Took 0.04 sec\n",
            "Epoch 1827, Loss(train/val) 0.37195/0.17496. Took 0.04 sec\n",
            "Epoch 1828, Loss(train/val) 0.39952/0.17991. Took 0.04 sec\n",
            "Epoch 1829, Loss(train/val) 0.41210/0.17832. Took 0.05 sec\n",
            "Epoch 1830, Loss(train/val) 0.39567/0.18011. Took 0.06 sec\n",
            "Epoch 1831, Loss(train/val) 0.40435/0.17876. Took 0.05 sec\n",
            "Epoch 1832, Loss(train/val) 0.40820/0.18249. Took 0.04 sec\n",
            "Epoch 1833, Loss(train/val) 0.37891/0.17726. Took 0.05 sec\n",
            "Epoch 1834, Loss(train/val) 0.41450/0.17437. Took 0.04 sec\n",
            "Epoch 1835, Loss(train/val) 0.38457/0.17305. Took 0.06 sec\n",
            "Epoch 1836, Loss(train/val) 0.40268/0.17306. Took 0.05 sec\n",
            "Epoch 1837, Loss(train/val) 0.40242/0.17863. Took 0.04 sec\n",
            "Epoch 1838, Loss(train/val) 0.40185/0.18029. Took 0.06 sec\n",
            "Epoch 1839, Loss(train/val) 0.39307/0.17622. Took 0.04 sec\n",
            "Epoch 1840, Loss(train/val) 0.38042/0.17641. Took 0.05 sec\n",
            "Epoch 1841, Loss(train/val) 0.39442/0.18703. Took 0.04 sec\n",
            "Epoch 1842, Loss(train/val) 0.36046/0.19668. Took 0.04 sec\n",
            "Epoch 1843, Loss(train/val) 0.40597/0.19402. Took 0.04 sec\n",
            "Epoch 1844, Loss(train/val) 0.39836/0.18392. Took 0.04 sec\n",
            "Epoch 1845, Loss(train/val) 0.38841/0.17717. Took 0.05 sec\n",
            "Epoch 1846, Loss(train/val) 0.39376/0.17204. Took 0.05 sec\n",
            "Epoch 1847, Loss(train/val) 0.39670/0.16969. Took 0.04 sec\n",
            "Epoch 1848, Loss(train/val) 0.39395/0.16965. Took 0.04 sec\n",
            "Epoch 1849, Loss(train/val) 0.39680/0.16833. Took 0.05 sec\n",
            "Epoch 1850, Loss(train/val) 0.39325/0.16781. Took 0.05 sec\n",
            "Epoch 1851, Loss(train/val) 0.39403/0.16809. Took 0.05 sec\n",
            "Epoch 1852, Loss(train/val) 0.40315/0.17070. Took 0.04 sec\n",
            "Epoch 1853, Loss(train/val) 0.39669/0.17506. Took 0.04 sec\n",
            "Epoch 1854, Loss(train/val) 0.38858/0.17458. Took 0.05 sec\n",
            "Epoch 1855, Loss(train/val) 0.41727/0.16766. Took 0.05 sec\n",
            "Epoch 1856, Loss(train/val) 0.37586/0.16648. Took 0.05 sec\n",
            "Epoch 1857, Loss(train/val) 0.36494/0.16810. Took 0.05 sec\n",
            "Epoch 1858, Loss(train/val) 0.41212/0.16878. Took 0.05 sec\n",
            "Epoch 1859, Loss(train/val) 0.40231/0.16864. Took 0.05 sec\n",
            "Epoch 1860, Loss(train/val) 0.40638/0.16711. Took 0.05 sec\n",
            "Epoch 1861, Loss(train/val) 0.39356/0.16793. Took 0.05 sec\n",
            "Epoch 1862, Loss(train/val) 0.36824/0.16956. Took 0.05 sec\n",
            "Epoch 1863, Loss(train/val) 0.38744/0.17054. Took 0.05 sec\n",
            "Epoch 1864, Loss(train/val) 0.38612/0.16993. Took 0.04 sec\n",
            "Epoch 1865, Loss(train/val) 0.38141/0.16886. Took 0.05 sec\n",
            "Epoch 1866, Loss(train/val) 0.39420/0.16934. Took 0.05 sec\n",
            "Epoch 1867, Loss(train/val) 0.37420/0.16978. Took 0.05 sec\n",
            "Epoch 1868, Loss(train/val) 0.40684/0.17086. Took 0.05 sec\n",
            "Epoch 1869, Loss(train/val) 0.39358/0.17197. Took 0.04 sec\n",
            "Epoch 1870, Loss(train/val) 0.37989/0.17391. Took 0.05 sec\n",
            "Epoch 1871, Loss(train/val) 0.39838/0.17633. Took 0.04 sec\n",
            "Epoch 1872, Loss(train/val) 0.38599/0.17841. Took 0.04 sec\n",
            "Epoch 1873, Loss(train/val) 0.40511/0.17874. Took 0.05 sec\n",
            "Epoch 1874, Loss(train/val) 0.39704/0.17505. Took 0.05 sec\n",
            "Epoch 1875, Loss(train/val) 0.40264/0.17179. Took 0.05 sec\n",
            "Epoch 1876, Loss(train/val) 0.39962/0.17352. Took 0.05 sec\n",
            "Epoch 1877, Loss(train/val) 0.37624/0.17727. Took 0.05 sec\n",
            "Epoch 1878, Loss(train/val) 0.39945/0.17519. Took 0.05 sec\n",
            "Epoch 1879, Loss(train/val) 0.35280/0.17214. Took 0.05 sec\n",
            "Epoch 1880, Loss(train/val) 0.39798/0.17426. Took 0.05 sec\n",
            "Epoch 1881, Loss(train/val) 0.39001/0.17524. Took 0.05 sec\n",
            "Epoch 1882, Loss(train/val) 0.37047/0.17153. Took 0.05 sec\n",
            "Epoch 1883, Loss(train/val) 0.40464/0.17472. Took 0.05 sec\n",
            "Epoch 1884, Loss(train/val) 0.37637/0.17760. Took 0.05 sec\n",
            "Epoch 1885, Loss(train/val) 0.38982/0.17607. Took 0.05 sec\n",
            "Epoch 1886, Loss(train/val) 0.39380/0.17551. Took 0.04 sec\n",
            "Epoch 1887, Loss(train/val) 0.39345/0.16763. Took 0.04 sec\n",
            "Epoch 1888, Loss(train/val) 0.43868/0.16588. Took 0.05 sec\n",
            "Epoch 1889, Loss(train/val) 0.40070/0.17170. Took 0.04 sec\n",
            "Epoch 1890, Loss(train/val) 0.37694/0.18491. Took 0.05 sec\n",
            "Epoch 1891, Loss(train/val) 0.39299/0.20378. Took 0.04 sec\n",
            "Epoch 1892, Loss(train/val) 0.39792/0.20596. Took 0.04 sec\n",
            "Epoch 1893, Loss(train/val) 0.40978/0.20656. Took 0.04 sec\n",
            "Epoch 1894, Loss(train/val) 0.37756/0.20386. Took 0.05 sec\n",
            "Epoch 1895, Loss(train/val) 0.39662/0.19271. Took 0.05 sec\n",
            "Epoch 1896, Loss(train/val) 0.40762/0.18187. Took 0.04 sec\n",
            "Epoch 1897, Loss(train/val) 0.38638/0.17602. Took 0.04 sec\n",
            "Epoch 1898, Loss(train/val) 0.39003/0.17133. Took 0.05 sec\n",
            "Epoch 1899, Loss(train/val) 0.41353/0.17185. Took 0.05 sec\n",
            "Epoch 1900, Loss(train/val) 0.41892/0.17193. Took 0.05 sec\n",
            "Epoch 1901, Loss(train/val) 0.42042/0.17196. Took 0.05 sec\n",
            "Epoch 1902, Loss(train/val) 0.42122/0.17332. Took 0.05 sec\n",
            "Epoch 1903, Loss(train/val) 0.39410/0.17642. Took 0.05 sec\n",
            "Epoch 1904, Loss(train/val) 0.38122/0.17656. Took 0.05 sec\n",
            "Epoch 1905, Loss(train/val) 0.38539/0.17781. Took 0.05 sec\n",
            "Epoch 1906, Loss(train/val) 0.39528/0.17434. Took 0.05 sec\n",
            "Epoch 1907, Loss(train/val) 0.39807/0.17591. Took 0.05 sec\n",
            "Epoch 1908, Loss(train/val) 0.39215/0.17318. Took 0.05 sec\n",
            "Epoch 1909, Loss(train/val) 0.36582/0.17177. Took 0.05 sec\n",
            "Epoch 1910, Loss(train/val) 0.37165/0.17048. Took 0.05 sec\n",
            "Epoch 1911, Loss(train/val) 0.39823/0.16856. Took 0.05 sec\n",
            "Epoch 1912, Loss(train/val) 0.36186/0.16711. Took 0.05 sec\n",
            "Epoch 1913, Loss(train/val) 0.37695/0.16630. Took 0.04 sec\n",
            "Epoch 1914, Loss(train/val) 0.39312/0.16711. Took 0.04 sec\n",
            "Epoch 1915, Loss(train/val) 0.42179/0.16725. Took 0.06 sec\n",
            "Epoch 1916, Loss(train/val) 0.40106/0.16862. Took 0.04 sec\n",
            "Epoch 1917, Loss(train/val) 0.39503/0.16817. Took 0.05 sec\n",
            "Epoch 1918, Loss(train/val) 0.37903/0.16843. Took 0.05 sec\n",
            "Epoch 1919, Loss(train/val) 0.37634/0.16853. Took 0.05 sec\n",
            "Epoch 1920, Loss(train/val) 0.38493/0.16788. Took 0.06 sec\n",
            "Epoch 1921, Loss(train/val) 0.39785/0.16975. Took 0.05 sec\n",
            "Epoch 1922, Loss(train/val) 0.38743/0.17078. Took 0.04 sec\n",
            "Epoch 1923, Loss(train/val) 0.39148/0.17288. Took 0.04 sec\n",
            "Epoch 1924, Loss(train/val) 0.40303/0.16912. Took 0.04 sec\n",
            "Epoch 1925, Loss(train/val) 0.37619/0.16934. Took 0.05 sec\n",
            "Epoch 1926, Loss(train/val) 0.40750/0.16805. Took 0.05 sec\n",
            "Epoch 1927, Loss(train/val) 0.42488/0.16696. Took 0.05 sec\n",
            "Epoch 1928, Loss(train/val) 0.41265/0.16594. Took 0.05 sec\n",
            "Epoch 1929, Loss(train/val) 0.39166/0.16561. Took 0.05 sec\n",
            "Epoch 1930, Loss(train/val) 0.36524/0.16611. Took 0.05 sec\n",
            "Epoch 1931, Loss(train/val) 0.37105/0.16761. Took 0.04 sec\n",
            "Epoch 1932, Loss(train/val) 0.38102/0.16818. Took 0.04 sec\n",
            "Epoch 1933, Loss(train/val) 0.39527/0.16696. Took 0.05 sec\n",
            "Epoch 1934, Loss(train/val) 0.39603/0.16759. Took 0.04 sec\n",
            "Epoch 1935, Loss(train/val) 0.39470/0.17103. Took 0.05 sec\n",
            "Epoch 1936, Loss(train/val) 0.39451/0.17031. Took 0.05 sec\n",
            "Epoch 1937, Loss(train/val) 0.37247/0.16862. Took 0.06 sec\n",
            "Epoch 1938, Loss(train/val) 0.37621/0.16795. Took 0.05 sec\n",
            "Epoch 1939, Loss(train/val) 0.41611/0.16988. Took 0.05 sec\n",
            "Epoch 1940, Loss(train/val) 0.36765/0.16926. Took 0.05 sec\n",
            "Epoch 1941, Loss(train/val) 0.40637/0.17031. Took 0.05 sec\n",
            "Epoch 1942, Loss(train/val) 0.37764/0.17029. Took 0.05 sec\n",
            "Epoch 1943, Loss(train/val) 0.38043/0.17064. Took 0.04 sec\n",
            "Epoch 1944, Loss(train/val) 0.40901/0.17549. Took 0.05 sec\n",
            "Epoch 1945, Loss(train/val) 0.39163/0.18413. Took 0.06 sec\n",
            "Epoch 1946, Loss(train/val) 0.38365/0.19937. Took 0.05 sec\n",
            "Epoch 1947, Loss(train/val) 0.39865/0.19669. Took 0.05 sec\n",
            "Epoch 1948, Loss(train/val) 0.37695/0.18036. Took 0.04 sec\n",
            "Epoch 1949, Loss(train/val) 0.39529/0.17277. Took 0.04 sec\n",
            "Epoch 1950, Loss(train/val) 0.40361/0.16958. Took 0.05 sec\n",
            "Epoch 1951, Loss(train/val) 0.36986/0.16840. Took 0.05 sec\n",
            "Epoch 1952, Loss(train/val) 0.39776/0.16901. Took 0.05 sec\n",
            "Epoch 1953, Loss(train/val) 0.38168/0.17018. Took 0.04 sec\n",
            "Epoch 1954, Loss(train/val) 0.39862/0.17227. Took 0.04 sec\n",
            "Epoch 1955, Loss(train/val) 0.41091/0.17354. Took 0.06 sec\n",
            "Epoch 1956, Loss(train/val) 0.38327/0.17249. Took 0.05 sec\n",
            "Epoch 1957, Loss(train/val) 0.38211/0.17297. Took 0.05 sec\n",
            "Epoch 1958, Loss(train/val) 0.38483/0.17317. Took 0.05 sec\n",
            "Epoch 1959, Loss(train/val) 0.39559/0.17588. Took 0.04 sec\n",
            "Epoch 1960, Loss(train/val) 0.36839/0.17738. Took 0.05 sec\n",
            "Epoch 1961, Loss(train/val) 0.37195/0.17559. Took 0.05 sec\n",
            "Epoch 1962, Loss(train/val) 0.36844/0.17389. Took 0.04 sec\n",
            "Epoch 1963, Loss(train/val) 0.37935/0.17451. Took 0.05 sec\n",
            "Epoch 1964, Loss(train/val) 0.39473/0.17200. Took 0.04 sec\n",
            "Epoch 1965, Loss(train/val) 0.35225/0.17006. Took 0.06 sec\n",
            "Epoch 1966, Loss(train/val) 0.38934/0.16903. Took 0.05 sec\n",
            "Epoch 1967, Loss(train/val) 0.39202/0.16916. Took 0.04 sec\n",
            "Epoch 1968, Loss(train/val) 0.39655/0.17146. Took 0.05 sec\n",
            "Epoch 1969, Loss(train/val) 0.41076/0.17208. Took 0.04 sec\n",
            "Epoch 1970, Loss(train/val) 0.38513/0.17258. Took 0.05 sec\n",
            "Epoch 1971, Loss(train/val) 0.38041/0.17105. Took 0.04 sec\n",
            "Epoch 1972, Loss(train/val) 0.39296/0.16940. Took 0.05 sec\n",
            "Epoch 1973, Loss(train/val) 0.37548/0.16951. Took 0.04 sec\n",
            "Epoch 1974, Loss(train/val) 0.40074/0.17140. Took 0.04 sec\n",
            "Epoch 1975, Loss(train/val) 0.38416/0.17102. Took 0.05 sec\n",
            "Epoch 1976, Loss(train/val) 0.40139/0.17335. Took 0.04 sec\n",
            "Epoch 1977, Loss(train/val) 0.39881/0.18021. Took 0.04 sec\n",
            "Epoch 1978, Loss(train/val) 0.40219/0.18269. Took 0.05 sec\n",
            "Epoch 1979, Loss(train/val) 0.41211/0.18185. Took 0.05 sec\n",
            "Epoch 1980, Loss(train/val) 0.37875/0.17434. Took 0.06 sec\n",
            "Epoch 1981, Loss(train/val) 0.38589/0.17186. Took 0.05 sec\n",
            "Epoch 1982, Loss(train/val) 0.37313/0.17116. Took 0.04 sec\n",
            "Epoch 1983, Loss(train/val) 0.39575/0.16905. Took 0.04 sec\n",
            "Epoch 1984, Loss(train/val) 0.37420/0.16930. Took 0.05 sec\n",
            "Epoch 1985, Loss(train/val) 0.39277/0.16957. Took 0.05 sec\n",
            "Epoch 1986, Loss(train/val) 0.38432/0.17329. Took 0.04 sec\n",
            "Epoch 1987, Loss(train/val) 0.38920/0.17842. Took 0.05 sec\n",
            "Epoch 1988, Loss(train/val) 0.40503/0.18038. Took 0.05 sec\n",
            "Epoch 1989, Loss(train/val) 0.40306/0.18016. Took 0.05 sec\n",
            "Epoch 1990, Loss(train/val) 0.38000/0.18241. Took 0.05 sec\n",
            "Epoch 1991, Loss(train/val) 0.38243/0.17567. Took 0.05 sec\n",
            "Epoch 1992, Loss(train/val) 0.37880/0.16985. Took 0.05 sec\n",
            "Epoch 1993, Loss(train/val) 0.40199/0.17068. Took 0.05 sec\n",
            "Epoch 1994, Loss(train/val) 0.38316/0.17539. Took 0.04 sec\n",
            "Epoch 1995, Loss(train/val) 0.37902/0.17552. Took 0.05 sec\n",
            "Epoch 1996, Loss(train/val) 0.40622/0.16978. Took 0.05 sec\n",
            "Epoch 1997, Loss(train/val) 0.39162/0.16742. Took 0.05 sec\n",
            "Epoch 1998, Loss(train/val) 0.38578/0.16718. Took 0.04 sec\n",
            "Epoch 1999, Loss(train/val) 0.40151/0.17023. Took 0.04 sec\n",
            "Epoch 2000, Loss(train/val) 0.39059/0.17371. Took 0.05 sec\n",
            "Epoch 2001, Loss(train/val) 0.38346/0.17336. Took 0.04 sec\n",
            "Epoch 2002, Loss(train/val) 0.38549/0.16851. Took 0.05 sec\n",
            "Epoch 2003, Loss(train/val) 0.39041/0.16676. Took 0.04 sec\n",
            "Epoch 2004, Loss(train/val) 0.40497/0.16645. Took 0.05 sec\n",
            "Epoch 2005, Loss(train/val) 0.39201/0.16799. Took 0.05 sec\n",
            "Epoch 2006, Loss(train/val) 0.38874/0.17669. Took 0.05 sec\n",
            "Epoch 2007, Loss(train/val) 0.38621/0.20042. Took 0.05 sec\n",
            "Epoch 2008, Loss(train/val) 0.37215/0.21288. Took 0.05 sec\n",
            "Epoch 2009, Loss(train/val) 0.37281/0.20321. Took 0.05 sec\n",
            "Epoch 2010, Loss(train/val) 0.38642/0.20524. Took 0.05 sec\n",
            "Epoch 2011, Loss(train/val) 0.37613/0.20450. Took 0.05 sec\n",
            "Epoch 2012, Loss(train/val) 0.36311/0.20335. Took 0.05 sec\n",
            "Epoch 2013, Loss(train/val) 0.39149/0.19553. Took 0.05 sec\n",
            "Epoch 2014, Loss(train/val) 0.40392/0.18028. Took 0.05 sec\n",
            "Epoch 2015, Loss(train/val) 0.38811/0.17344. Took 0.05 sec\n",
            "Epoch 2016, Loss(train/val) 0.39498/0.18008. Took 0.04 sec\n",
            "Epoch 2017, Loss(train/val) 0.39355/0.17522. Took 0.04 sec\n",
            "Epoch 2018, Loss(train/val) 0.40785/0.17189. Took 0.04 sec\n",
            "Epoch 2019, Loss(train/val) 0.39393/0.17059. Took 0.05 sec\n",
            "Epoch 2020, Loss(train/val) 0.37680/0.17086. Took 0.06 sec\n",
            "Epoch 2021, Loss(train/val) 0.38186/0.16585. Took 0.04 sec\n",
            "Epoch 2022, Loss(train/val) 0.39225/0.17111. Took 0.04 sec\n",
            "Epoch 2023, Loss(train/val) 0.39378/0.17342. Took 0.05 sec\n",
            "Epoch 2024, Loss(train/val) 0.37207/0.16958. Took 0.04 sec\n",
            "Epoch 2025, Loss(train/val) 0.37380/0.16612. Took 0.05 sec\n",
            "Epoch 2026, Loss(train/val) 0.40967/0.16534. Took 0.04 sec\n",
            "Epoch 2027, Loss(train/val) 0.38881/0.16536. Took 0.04 sec\n",
            "Epoch 2028, Loss(train/val) 0.40694/0.16834. Took 0.05 sec\n",
            "Epoch 2029, Loss(train/val) 0.39469/0.17694. Took 0.04 sec\n",
            "Epoch 2030, Loss(train/val) 0.39566/0.18406. Took 0.05 sec\n",
            "Epoch 2031, Loss(train/val) 0.40043/0.19247. Took 0.05 sec\n",
            "Epoch 2032, Loss(train/val) 0.38148/0.19912. Took 0.04 sec\n",
            "Epoch 2033, Loss(train/val) 0.38361/0.19258. Took 0.04 sec\n",
            "Epoch 2034, Loss(train/val) 0.37763/0.18684. Took 0.05 sec\n",
            "Epoch 2035, Loss(train/val) 0.37191/0.17512. Took 0.05 sec\n",
            "Epoch 2036, Loss(train/val) 0.36185/0.16842. Took 0.05 sec\n",
            "Epoch 2037, Loss(train/val) 0.41879/0.16842. Took 0.04 sec\n",
            "Epoch 2038, Loss(train/val) 0.37541/0.16931. Took 0.04 sec\n",
            "Epoch 2039, Loss(train/val) 0.39205/0.17512. Took 0.05 sec\n",
            "Epoch 2040, Loss(train/val) 0.39773/0.18658. Took 0.05 sec\n",
            "Epoch 2041, Loss(train/val) 0.38157/0.18796. Took 0.04 sec\n",
            "Epoch 2042, Loss(train/val) 0.38578/0.18492. Took 0.05 sec\n",
            "Epoch 2043, Loss(train/val) 0.36933/0.17374. Took 0.05 sec\n",
            "Epoch 2044, Loss(train/val) 0.38615/0.16911. Took 0.05 sec\n",
            "Epoch 2045, Loss(train/val) 0.41787/0.16812. Took 0.06 sec\n",
            "Epoch 2046, Loss(train/val) 0.37062/0.16821. Took 0.05 sec\n",
            "Epoch 2047, Loss(train/val) 0.38854/0.16999. Took 0.05 sec\n",
            "Epoch 2048, Loss(train/val) 0.38702/0.17309. Took 0.05 sec\n",
            "Epoch 2049, Loss(train/val) 0.37437/0.16917. Took 0.05 sec\n",
            "Epoch 2050, Loss(train/val) 0.39941/0.16864. Took 0.06 sec\n",
            "Epoch 2051, Loss(train/val) 0.38074/0.17079. Took 0.05 sec\n",
            "Epoch 2052, Loss(train/val) 0.41115/0.17138. Took 0.05 sec\n",
            "Epoch 2053, Loss(train/val) 0.39036/0.17524. Took 0.05 sec\n",
            "Epoch 2054, Loss(train/val) 0.41734/0.17362. Took 0.06 sec\n",
            "Epoch 2055, Loss(train/val) 0.39922/0.16944. Took 0.05 sec\n",
            "Epoch 2056, Loss(train/val) 0.36948/0.17302. Took 0.05 sec\n",
            "Epoch 2057, Loss(train/val) 0.37594/0.18531. Took 0.05 sec\n",
            "Epoch 2058, Loss(train/val) 0.35926/0.17980. Took 0.05 sec\n",
            "Epoch 2059, Loss(train/val) 0.39237/0.16935. Took 0.05 sec\n",
            "Epoch 2060, Loss(train/val) 0.37328/0.16598. Took 0.05 sec\n",
            "Epoch 2061, Loss(train/val) 0.38492/0.16565. Took 0.04 sec\n",
            "Epoch 2062, Loss(train/val) 0.37123/0.16693. Took 0.05 sec\n",
            "Epoch 2063, Loss(train/val) 0.38598/0.17228. Took 0.05 sec\n",
            "Epoch 2064, Loss(train/val) 0.36708/0.17914. Took 0.05 sec\n",
            "Epoch 2065, Loss(train/val) 0.39560/0.17851. Took 0.05 sec\n",
            "Epoch 2066, Loss(train/val) 0.37811/0.17547. Took 0.04 sec\n",
            "Epoch 2067, Loss(train/val) 0.36774/0.16889. Took 0.04 sec\n",
            "Epoch 2068, Loss(train/val) 0.38947/0.16711. Took 0.05 sec\n",
            "Epoch 2069, Loss(train/val) 0.37475/0.16594. Took 0.05 sec\n",
            "Epoch 2070, Loss(train/val) 0.39801/0.16595. Took 0.04 sec\n",
            "Epoch 2071, Loss(train/val) 0.41427/0.16615. Took 0.05 sec\n",
            "Epoch 2072, Loss(train/val) 0.38530/0.17004. Took 0.05 sec\n",
            "Epoch 2073, Loss(train/val) 0.42198/0.16948. Took 0.05 sec\n",
            "Epoch 2074, Loss(train/val) 0.39767/0.17043. Took 0.06 sec\n",
            "Epoch 2075, Loss(train/val) 0.37129/0.16810. Took 0.05 sec\n",
            "Epoch 2076, Loss(train/val) 0.35655/0.16907. Took 0.05 sec\n",
            "Epoch 2077, Loss(train/val) 0.36875/0.17078. Took 0.05 sec\n",
            "Epoch 2078, Loss(train/val) 0.37995/0.17331. Took 0.04 sec\n",
            "Epoch 2079, Loss(train/val) 0.40692/0.17417. Took 0.05 sec\n",
            "Epoch 2080, Loss(train/val) 0.38061/0.17624. Took 0.05 sec\n",
            "Epoch 2081, Loss(train/val) 0.41363/0.18729. Took 0.05 sec\n",
            "Epoch 2082, Loss(train/val) 0.40969/0.20084. Took 0.05 sec\n",
            "Epoch 2083, Loss(train/val) 0.41474/0.20661. Took 0.05 sec\n",
            "Epoch 2084, Loss(train/val) 0.38696/0.21548. Took 0.05 sec\n",
            "Epoch 2085, Loss(train/val) 0.39838/0.19515. Took 0.05 sec\n",
            "Epoch 2086, Loss(train/val) 0.36785/0.17960. Took 0.04 sec\n",
            "Epoch 2087, Loss(train/val) 0.38551/0.16614. Took 0.05 sec\n",
            "Epoch 2088, Loss(train/val) 0.41490/0.17221. Took 0.05 sec\n",
            "Epoch 2089, Loss(train/val) 0.40577/0.18118. Took 0.06 sec\n",
            "Epoch 2090, Loss(train/val) 0.38062/0.18591. Took 0.05 sec\n",
            "Epoch 2091, Loss(train/val) 0.38948/0.18194. Took 0.05 sec\n",
            "Epoch 2092, Loss(train/val) 0.37703/0.17210. Took 0.05 sec\n",
            "Epoch 2093, Loss(train/val) 0.38819/0.17001. Took 0.04 sec\n",
            "Epoch 2094, Loss(train/val) 0.36742/0.17036. Took 0.05 sec\n",
            "Epoch 2095, Loss(train/val) 0.36899/0.17179. Took 0.05 sec\n",
            "Epoch 2096, Loss(train/val) 0.41328/0.17426. Took 0.05 sec\n",
            "Epoch 2097, Loss(train/val) 0.38743/0.18447. Took 0.04 sec\n",
            "Epoch 2098, Loss(train/val) 0.41829/0.19545. Took 0.05 sec\n",
            "Epoch 2099, Loss(train/val) 0.40401/0.19068. Took 0.06 sec\n",
            "Epoch 2100, Loss(train/val) 0.41067/0.18483. Took 0.04 sec\n",
            "Epoch 2101, Loss(train/val) 0.40557/0.17908. Took 0.04 sec\n",
            "Epoch 2102, Loss(train/val) 0.42449/0.17192. Took 0.04 sec\n",
            "Epoch 2103, Loss(train/val) 0.39176/0.16756. Took 0.04 sec\n",
            "Epoch 2104, Loss(train/val) 0.37468/0.16759. Took 0.06 sec\n",
            "Epoch 2105, Loss(train/val) 0.39994/0.16744. Took 0.04 sec\n",
            "Epoch 2106, Loss(train/val) 0.38114/0.16775. Took 0.04 sec\n",
            "Epoch 2107, Loss(train/val) 0.37465/0.16838. Took 0.05 sec\n",
            "Epoch 2108, Loss(train/val) 0.40404/0.16824. Took 0.06 sec\n",
            "Epoch 2109, Loss(train/val) 0.37349/0.17054. Took 0.06 sec\n",
            "Epoch 2110, Loss(train/val) 0.38046/0.17317. Took 0.05 sec\n",
            "Epoch 2111, Loss(train/val) 0.38894/0.17337. Took 0.04 sec\n",
            "Epoch 2112, Loss(train/val) 0.40591/0.17342. Took 0.05 sec\n",
            "Epoch 2113, Loss(train/val) 0.37635/0.16677. Took 0.05 sec\n",
            "Epoch 2114, Loss(train/val) 0.38655/0.16632. Took 0.05 sec\n",
            "Epoch 2115, Loss(train/val) 0.38524/0.16636. Took 0.05 sec\n",
            "Epoch 2116, Loss(train/val) 0.37056/0.16607. Took 0.05 sec\n",
            "Epoch 2117, Loss(train/val) 0.38672/0.16560. Took 0.05 sec\n",
            "Epoch 2118, Loss(train/val) 0.38129/0.16622. Took 0.05 sec\n",
            "Epoch 2119, Loss(train/val) 0.37995/0.17230. Took 0.05 sec\n",
            "Epoch 2120, Loss(train/val) 0.37813/0.17767. Took 0.05 sec\n",
            "Epoch 2121, Loss(train/val) 0.37963/0.17998. Took 0.04 sec\n",
            "Epoch 2122, Loss(train/val) 0.38740/0.17941. Took 0.04 sec\n",
            "Epoch 2123, Loss(train/val) 0.37588/0.17048. Took 0.05 sec\n",
            "Epoch 2124, Loss(train/val) 0.39829/0.16703. Took 0.05 sec\n",
            "Epoch 2125, Loss(train/val) 0.38462/0.16852. Took 0.05 sec\n",
            "Epoch 2126, Loss(train/val) 0.36578/0.16873. Took 0.05 sec\n",
            "Epoch 2127, Loss(train/val) 0.39684/0.16631. Took 0.05 sec\n",
            "Epoch 2128, Loss(train/val) 0.38260/0.16686. Took 0.05 sec\n",
            "Epoch 2129, Loss(train/val) 0.36974/0.16611. Took 0.06 sec\n",
            "Epoch 2130, Loss(train/val) 0.38824/0.16685. Took 0.05 sec\n",
            "Epoch 2131, Loss(train/val) 0.38667/0.16617. Took 0.04 sec\n",
            "Epoch 2132, Loss(train/val) 0.35786/0.17031. Took 0.05 sec\n",
            "Epoch 2133, Loss(train/val) 0.37447/0.17580. Took 0.05 sec\n",
            "Epoch 2134, Loss(train/val) 0.38139/0.17301. Took 0.06 sec\n",
            "Epoch 2135, Loss(train/val) 0.35210/0.17031. Took 0.05 sec\n",
            "Epoch 2136, Loss(train/val) 0.37235/0.16687. Took 0.05 sec\n",
            "Epoch 2137, Loss(train/val) 0.39365/0.16797. Took 0.05 sec\n",
            "Epoch 2138, Loss(train/val) 0.37271/0.17092. Took 0.05 sec\n",
            "Epoch 2139, Loss(train/val) 0.39395/0.16863. Took 0.05 sec\n",
            "Epoch 2140, Loss(train/val) 0.38891/0.16783. Took 0.05 sec\n",
            "Epoch 2141, Loss(train/val) 0.38558/0.16744. Took 0.05 sec\n",
            "Epoch 2142, Loss(train/val) 0.41917/0.16740. Took 0.05 sec\n",
            "Epoch 2143, Loss(train/val) 0.38232/0.16766. Took 0.05 sec\n",
            "Epoch 2144, Loss(train/val) 0.41715/0.16725. Took 0.05 sec\n",
            "Epoch 2145, Loss(train/val) 0.39348/0.16708. Took 0.05 sec\n",
            "Epoch 2146, Loss(train/val) 0.38476/0.16657. Took 0.04 sec\n",
            "Epoch 2147, Loss(train/val) 0.36644/0.16896. Took 0.04 sec\n",
            "Epoch 2148, Loss(train/val) 0.38429/0.16704. Took 0.04 sec\n",
            "Epoch 2149, Loss(train/val) 0.38322/0.16610. Took 0.05 sec\n",
            "Epoch 2150, Loss(train/val) 0.40405/0.16763. Took 0.06 sec\n",
            "Epoch 2151, Loss(train/val) 0.38454/0.16963. Took 0.04 sec\n",
            "Epoch 2152, Loss(train/val) 0.39222/0.16801. Took 0.05 sec\n",
            "Epoch 2153, Loss(train/val) 0.36093/0.16669. Took 0.05 sec\n",
            "Epoch 2154, Loss(train/val) 0.37355/0.16671. Took 0.06 sec\n",
            "Epoch 2155, Loss(train/val) 0.39225/0.16707. Took 0.05 sec\n",
            "Epoch 2156, Loss(train/val) 0.36255/0.16688. Took 0.05 sec\n",
            "Epoch 2157, Loss(train/val) 0.39432/0.16765. Took 0.05 sec\n",
            "Epoch 2158, Loss(train/val) 0.36778/0.16869. Took 0.04 sec\n",
            "Epoch 2159, Loss(train/val) 0.37590/0.16828. Took 0.05 sec\n",
            "Epoch 2160, Loss(train/val) 0.36795/0.16685. Took 0.05 sec\n",
            "Epoch 2161, Loss(train/val) 0.38775/0.16784. Took 0.05 sec\n",
            "Epoch 2162, Loss(train/val) 0.36171/0.16903. Took 0.04 sec\n",
            "Epoch 2163, Loss(train/val) 0.38931/0.16740. Took 0.04 sec\n",
            "Epoch 2164, Loss(train/val) 0.39183/0.16691. Took 0.05 sec\n",
            "Epoch 2165, Loss(train/val) 0.37029/0.16749. Took 0.05 sec\n",
            "Epoch 2166, Loss(train/val) 0.39430/0.16819. Took 0.05 sec\n",
            "Epoch 2167, Loss(train/val) 0.39152/0.16709. Took 0.05 sec\n",
            "Epoch 2168, Loss(train/val) 0.38688/0.16852. Took 0.05 sec\n",
            "Epoch 2169, Loss(train/val) 0.37843/0.17245. Took 0.05 sec\n",
            "Epoch 2170, Loss(train/val) 0.36221/0.17072. Took 0.05 sec\n",
            "Epoch 2171, Loss(train/val) 0.35972/0.16687. Took 0.04 sec\n",
            "Epoch 2172, Loss(train/val) 0.39238/0.16577. Took 0.05 sec\n",
            "Epoch 2173, Loss(train/val) 0.38346/0.16982. Took 0.05 sec\n",
            "Epoch 2174, Loss(train/val) 0.40866/0.18364. Took 0.05 sec\n",
            "Epoch 2175, Loss(train/val) 0.41525/0.20367. Took 0.05 sec\n",
            "Epoch 2176, Loss(train/val) 0.35842/0.21897. Took 0.04 sec\n",
            "Epoch 2177, Loss(train/val) 0.38201/0.23322. Took 0.05 sec\n",
            "Epoch 2178, Loss(train/val) 0.38393/0.22957. Took 0.05 sec\n",
            "Epoch 2179, Loss(train/val) 0.38370/0.21275. Took 0.05 sec\n",
            "Epoch 2180, Loss(train/val) 0.37393/0.19439. Took 0.05 sec\n",
            "Epoch 2181, Loss(train/val) 0.36694/0.19928. Took 0.05 sec\n",
            "Epoch 2182, Loss(train/val) 0.39466/0.20374. Took 0.04 sec\n",
            "Epoch 2183, Loss(train/val) 0.38131/0.18871. Took 0.04 sec\n",
            "Epoch 2184, Loss(train/val) 0.40027/0.17208. Took 0.05 sec\n",
            "Epoch 2185, Loss(train/val) 0.37518/0.16963. Took 0.05 sec\n",
            "Epoch 2186, Loss(train/val) 0.37335/0.17009. Took 0.04 sec\n",
            "Epoch 2187, Loss(train/val) 0.37709/0.17271. Took 0.04 sec\n",
            "Epoch 2188, Loss(train/val) 0.39114/0.16921. Took 0.04 sec\n",
            "Epoch 2189, Loss(train/val) 0.39057/0.16774. Took 0.05 sec\n",
            "Epoch 2190, Loss(train/val) 0.37094/0.16695. Took 0.05 sec\n",
            "Epoch 2191, Loss(train/val) 0.38001/0.16711. Took 0.05 sec\n",
            "Epoch 2192, Loss(train/val) 0.39487/0.16735. Took 0.05 sec\n",
            "Epoch 2193, Loss(train/val) 0.39218/0.17016. Took 0.06 sec\n",
            "Epoch 2194, Loss(train/val) 0.38439/0.19107. Took 0.05 sec\n",
            "Epoch 2195, Loss(train/val) 0.36795/0.21544. Took 0.04 sec\n",
            "Epoch 2196, Loss(train/val) 0.39520/0.20801. Took 0.05 sec\n",
            "Epoch 2197, Loss(train/val) 0.35477/0.19022. Took 0.05 sec\n",
            "Epoch 2198, Loss(train/val) 0.43026/0.18406. Took 0.04 sec\n",
            "Epoch 2199, Loss(train/val) 0.38275/0.17857. Took 0.05 sec\n",
            "Epoch 2200, Loss(train/val) 0.37628/0.17816. Took 0.04 sec\n",
            "Epoch 2201, Loss(train/val) 0.37905/0.17533. Took 0.04 sec\n",
            "Epoch 2202, Loss(train/val) 0.37257/0.17642. Took 0.05 sec\n",
            "Epoch 2203, Loss(train/val) 0.37383/0.16740. Took 0.05 sec\n",
            "Epoch 2204, Loss(train/val) 0.39948/0.17132. Took 0.05 sec\n",
            "Epoch 2205, Loss(train/val) 0.38561/0.17513. Took 0.05 sec\n",
            "Epoch 2206, Loss(train/val) 0.38143/0.17341. Took 0.04 sec\n",
            "Epoch 2207, Loss(train/val) 0.36206/0.17713. Took 0.05 sec\n",
            "Epoch 2208, Loss(train/val) 0.41027/0.18312. Took 0.04 sec\n",
            "Epoch 2209, Loss(train/val) 0.40836/0.17101. Took 0.05 sec\n",
            "Epoch 2210, Loss(train/val) 0.37191/0.16742. Took 0.04 sec\n",
            "Epoch 2211, Loss(train/val) 0.36202/0.16607. Took 0.04 sec\n",
            "Epoch 2212, Loss(train/val) 0.37728/0.16872. Took 0.04 sec\n",
            "Epoch 2213, Loss(train/val) 0.38750/0.17120. Took 0.05 sec\n",
            "Epoch 2214, Loss(train/val) 0.38702/0.17124. Took 0.05 sec\n",
            "Epoch 2215, Loss(train/val) 0.36010/0.17676. Took 0.05 sec\n",
            "Epoch 2216, Loss(train/val) 0.38554/0.17568. Took 0.04 sec\n",
            "Epoch 2217, Loss(train/val) 0.37440/0.16741. Took 0.04 sec\n",
            "Epoch 2218, Loss(train/val) 0.39867/0.16707. Took 0.05 sec\n",
            "Epoch 2219, Loss(train/val) 0.37665/0.17493. Took 0.05 sec\n",
            "Epoch 2220, Loss(train/val) 0.39957/0.18216. Took 0.05 sec\n",
            "Epoch 2221, Loss(train/val) 0.38280/0.17452. Took 0.04 sec\n",
            "Epoch 2222, Loss(train/val) 0.38099/0.17247. Took 0.04 sec\n",
            "Epoch 2223, Loss(train/val) 0.39216/0.17860. Took 0.04 sec\n",
            "Epoch 2224, Loss(train/val) 0.36697/0.17335. Took 0.06 sec\n",
            "Epoch 2225, Loss(train/val) 0.36302/0.16791. Took 0.04 sec\n",
            "Epoch 2226, Loss(train/val) 0.41221/0.16822. Took 0.05 sec\n",
            "Epoch 2227, Loss(train/val) 0.36936/0.16772. Took 0.04 sec\n",
            "Epoch 2228, Loss(train/val) 0.38097/0.16874. Took 0.05 sec\n",
            "Epoch 2229, Loss(train/val) 0.37344/0.17148. Took 0.06 sec\n",
            "Epoch 2230, Loss(train/val) 0.38097/0.16522. Took 0.05 sec\n",
            "Epoch 2231, Loss(train/val) 0.36752/0.16418. Took 0.05 sec\n",
            "Epoch 2232, Loss(train/val) 0.38964/0.16405. Took 0.04 sec\n",
            "Epoch 2233, Loss(train/val) 0.37382/0.16445. Took 0.05 sec\n",
            "Epoch 2234, Loss(train/val) 0.38038/0.16476. Took 0.05 sec\n",
            "Epoch 2235, Loss(train/val) 0.36439/0.17218. Took 0.05 sec\n",
            "Epoch 2236, Loss(train/val) 0.39249/0.17965. Took 0.05 sec\n",
            "Epoch 2237, Loss(train/val) 0.36634/0.18862. Took 0.05 sec\n",
            "Epoch 2238, Loss(train/val) 0.40398/0.19589. Took 0.05 sec\n",
            "Epoch 2239, Loss(train/val) 0.38859/0.19481. Took 0.05 sec\n",
            "Epoch 2240, Loss(train/val) 0.37521/0.20307. Took 0.05 sec\n",
            "Epoch 2241, Loss(train/val) 0.38447/0.20670. Took 0.05 sec\n",
            "Epoch 2242, Loss(train/val) 0.38599/0.21837. Took 0.04 sec\n",
            "Epoch 2243, Loss(train/val) 0.36976/0.19458. Took 0.04 sec\n",
            "Epoch 2244, Loss(train/val) 0.37424/0.17401. Took 0.05 sec\n",
            "Epoch 2245, Loss(train/val) 0.37452/0.16812. Took 0.04 sec\n",
            "Epoch 2246, Loss(train/val) 0.38868/0.16925. Took 0.04 sec\n",
            "Epoch 2247, Loss(train/val) 0.36742/0.16610. Took 0.04 sec\n",
            "Epoch 2248, Loss(train/val) 0.38939/0.16558. Took 0.05 sec\n",
            "Epoch 2249, Loss(train/val) 0.38253/0.16738. Took 0.05 sec\n",
            "Epoch 2250, Loss(train/val) 0.34696/0.17328. Took 0.04 sec\n",
            "Epoch 2251, Loss(train/val) 0.38350/0.16952. Took 0.05 sec\n",
            "Epoch 2252, Loss(train/val) 0.38250/0.16500. Took 0.04 sec\n",
            "Epoch 2253, Loss(train/val) 0.38598/0.16618. Took 0.04 sec\n",
            "Epoch 2254, Loss(train/val) 0.36391/0.16770. Took 0.05 sec\n",
            "Epoch 2255, Loss(train/val) 0.36419/0.16769. Took 0.05 sec\n",
            "Epoch 2256, Loss(train/val) 0.36408/0.16607. Took 0.05 sec\n",
            "Epoch 2257, Loss(train/val) 0.38630/0.16875. Took 0.04 sec\n",
            "Epoch 2258, Loss(train/val) 0.36603/0.17544. Took 0.06 sec\n",
            "Epoch 2259, Loss(train/val) 0.36747/0.19008. Took 0.05 sec\n",
            "Epoch 2260, Loss(train/val) 0.36211/0.20720. Took 0.05 sec\n",
            "Epoch 2261, Loss(train/val) 0.36328/0.19608. Took 0.05 sec\n",
            "Epoch 2262, Loss(train/val) 0.38613/0.18200. Took 0.05 sec\n",
            "Epoch 2263, Loss(train/val) 0.38221/0.17538. Took 0.05 sec\n",
            "Epoch 2264, Loss(train/val) 0.36632/0.18383. Took 0.05 sec\n",
            "Epoch 2265, Loss(train/val) 0.38909/0.18192. Took 0.05 sec\n",
            "Epoch 2266, Loss(train/val) 0.35968/0.17474. Took 0.05 sec\n",
            "Epoch 2267, Loss(train/val) 0.37811/0.17077. Took 0.04 sec\n",
            "Epoch 2268, Loss(train/val) 0.36834/0.16699. Took 0.04 sec\n",
            "Epoch 2269, Loss(train/val) 0.38581/0.16445. Took 0.05 sec\n",
            "Epoch 2270, Loss(train/val) 0.38408/0.16478. Took 0.05 sec\n",
            "Epoch 2271, Loss(train/val) 0.37496/0.16998. Took 0.05 sec\n",
            "Epoch 2272, Loss(train/val) 0.38855/0.16915. Took 0.05 sec\n",
            "Epoch 2273, Loss(train/val) 0.37402/0.17465. Took 0.04 sec\n",
            "Epoch 2274, Loss(train/val) 0.37689/0.17063. Took 0.05 sec\n",
            "Epoch 2275, Loss(train/val) 0.37471/0.16555. Took 0.05 sec\n",
            "Epoch 2276, Loss(train/val) 0.40169/0.16461. Took 0.05 sec\n",
            "Epoch 2277, Loss(train/val) 0.37498/0.16769. Took 0.04 sec\n",
            "Epoch 2278, Loss(train/val) 0.38563/0.17078. Took 0.05 sec\n",
            "Epoch 2279, Loss(train/val) 0.35951/0.17553. Took 0.06 sec\n",
            "Epoch 2280, Loss(train/val) 0.35644/0.18734. Took 0.05 sec\n",
            "Epoch 2281, Loss(train/val) 0.37926/0.20073. Took 0.05 sec\n",
            "Epoch 2282, Loss(train/val) 0.35251/0.19924. Took 0.05 sec\n",
            "Epoch 2283, Loss(train/val) 0.36541/0.18590. Took 0.05 sec\n",
            "Epoch 2284, Loss(train/val) 0.38764/0.16884. Took 0.05 sec\n",
            "Epoch 2285, Loss(train/val) 0.35502/0.16508. Took 0.05 sec\n",
            "Epoch 2286, Loss(train/val) 0.37141/0.16850. Took 0.05 sec\n",
            "Epoch 2287, Loss(train/val) 0.38202/0.18361. Took 0.04 sec\n",
            "Epoch 2288, Loss(train/val) 0.36051/0.20267. Took 0.05 sec\n",
            "Epoch 2289, Loss(train/val) 0.39422/0.23048. Took 0.05 sec\n",
            "Epoch 2290, Loss(train/val) 0.38819/0.24160. Took 0.04 sec\n",
            "Epoch 2291, Loss(train/val) 0.40277/0.24369. Took 0.05 sec\n",
            "Epoch 2292, Loss(train/val) 0.39625/0.23903. Took 0.04 sec\n",
            "Epoch 2293, Loss(train/val) 0.38550/0.20380. Took 0.05 sec\n",
            "Epoch 2294, Loss(train/val) 0.38348/0.17835. Took 0.05 sec\n",
            "Epoch 2295, Loss(train/val) 0.38430/0.16632. Took 0.05 sec\n",
            "Epoch 2296, Loss(train/val) 0.38069/0.16491. Took 0.04 sec\n",
            "Epoch 2297, Loss(train/val) 0.39700/0.17230. Took 0.05 sec\n",
            "Epoch 2298, Loss(train/val) 0.38710/0.16831. Took 0.04 sec\n",
            "Epoch 2299, Loss(train/val) 0.40009/0.17028. Took 0.05 sec\n",
            "Epoch 2300, Loss(train/val) 0.41613/0.16528. Took 0.04 sec\n",
            "Epoch 2301, Loss(train/val) 0.38829/0.16430. Took 0.07 sec\n",
            "Epoch 2302, Loss(train/val) 0.36244/0.16864. Took 0.04 sec\n",
            "Epoch 2303, Loss(train/val) 0.36578/0.17097. Took 0.05 sec\n",
            "Epoch 2304, Loss(train/val) 0.37714/0.16963. Took 0.05 sec\n",
            "Epoch 2305, Loss(train/val) 0.38109/0.17120. Took 0.05 sec\n",
            "Epoch 2306, Loss(train/val) 0.36926/0.17405. Took 0.04 sec\n",
            "Epoch 2307, Loss(train/val) 0.40500/0.16810. Took 0.05 sec\n",
            "Epoch 2308, Loss(train/val) 0.37755/0.16566. Took 0.05 sec\n",
            "Epoch 2309, Loss(train/val) 0.40790/0.16518. Took 0.05 sec\n",
            "Epoch 2310, Loss(train/val) 0.35875/0.16507. Took 0.05 sec\n",
            "Epoch 2311, Loss(train/val) 0.36315/0.16420. Took 0.05 sec\n",
            "Epoch 2312, Loss(train/val) 0.37132/0.16423. Took 0.04 sec\n",
            "Epoch 2313, Loss(train/val) 0.38594/0.16432. Took 0.05 sec\n",
            "Epoch 2314, Loss(train/val) 0.35882/0.16352. Took 0.05 sec\n",
            "Epoch 2315, Loss(train/val) 0.37553/0.16488. Took 0.04 sec\n",
            "Epoch 2316, Loss(train/val) 0.36370/0.16585. Took 0.04 sec\n",
            "Epoch 2317, Loss(train/val) 0.38103/0.16609. Took 0.05 sec\n",
            "Epoch 2318, Loss(train/val) 0.39167/0.16431. Took 0.05 sec\n",
            "Epoch 2319, Loss(train/val) 0.35271/0.16662. Took 0.04 sec\n",
            "Epoch 2320, Loss(train/val) 0.36652/0.16753. Took 0.05 sec\n",
            "Epoch 2321, Loss(train/val) 0.38796/0.16582. Took 0.05 sec\n",
            "Epoch 2322, Loss(train/val) 0.38143/0.16512. Took 0.05 sec\n",
            "Epoch 2323, Loss(train/val) 0.36722/0.16415. Took 0.05 sec\n",
            "Epoch 2324, Loss(train/val) 0.35791/0.16453. Took 0.05 sec\n",
            "Epoch 2325, Loss(train/val) 0.40398/0.16651. Took 0.05 sec\n",
            "Epoch 2326, Loss(train/val) 0.38186/0.17090. Took 0.05 sec\n",
            "Epoch 2327, Loss(train/val) 0.38012/0.16930. Took 0.05 sec\n",
            "Epoch 2328, Loss(train/val) 0.34844/0.16964. Took 0.05 sec\n",
            "Epoch 2329, Loss(train/val) 0.37721/0.17306. Took 0.04 sec\n",
            "Epoch 2330, Loss(train/val) 0.37033/0.17282. Took 0.04 sec\n",
            "Epoch 2331, Loss(train/val) 0.37223/0.17105. Took 0.04 sec\n",
            "Epoch 2332, Loss(train/val) 0.38437/0.16337. Took 0.05 sec\n",
            "Epoch 2333, Loss(train/val) 0.36263/0.16952. Took 0.05 sec\n",
            "Epoch 2334, Loss(train/val) 0.37094/0.18207. Took 0.04 sec\n",
            "Epoch 2335, Loss(train/val) 0.37305/0.18562. Took 0.05 sec\n",
            "Epoch 2336, Loss(train/val) 0.35929/0.17843. Took 0.04 sec\n",
            "Epoch 2337, Loss(train/val) 0.34031/0.17451. Took 0.04 sec\n",
            "Epoch 2338, Loss(train/val) 0.35874/0.18329. Took 0.05 sec\n",
            "Epoch 2339, Loss(train/val) 0.40808/0.19636. Took 0.05 sec\n",
            "Epoch 2340, Loss(train/val) 0.37444/0.21275. Took 0.05 sec\n",
            "Epoch 2341, Loss(train/val) 0.38246/0.20731. Took 0.04 sec\n",
            "Epoch 2342, Loss(train/val) 0.36382/0.19566. Took 0.05 sec\n",
            "Epoch 2343, Loss(train/val) 0.36304/0.17414. Took 0.06 sec\n",
            "Epoch 2344, Loss(train/val) 0.38848/0.16598. Took 0.05 sec\n",
            "Epoch 2345, Loss(train/val) 0.36897/0.16510. Took 0.05 sec\n",
            "Epoch 2346, Loss(train/val) 0.37689/0.16502. Took 0.04 sec\n",
            "Epoch 2347, Loss(train/val) 0.36690/0.16844. Took 0.04 sec\n",
            "Epoch 2348, Loss(train/val) 0.39405/0.17671. Took 0.05 sec\n",
            "Epoch 2349, Loss(train/val) 0.38287/0.17415. Took 0.05 sec\n",
            "Epoch 2350, Loss(train/val) 0.36307/0.17735. Took 0.04 sec\n",
            "Epoch 2351, Loss(train/val) 0.35901/0.17908. Took 0.04 sec\n",
            "Epoch 2352, Loss(train/val) 0.37885/0.17244. Took 0.04 sec\n",
            "Epoch 2353, Loss(train/val) 0.35738/0.17250. Took 0.05 sec\n",
            "Epoch 2354, Loss(train/val) 0.38513/0.16576. Took 0.05 sec\n",
            "Epoch 2355, Loss(train/val) 0.38555/0.16363. Took 0.04 sec\n",
            "Epoch 2356, Loss(train/val) 0.37607/0.16754. Took 0.05 sec\n",
            "Epoch 2357, Loss(train/val) 0.37733/0.17781. Took 0.04 sec\n",
            "Epoch 2358, Loss(train/val) 0.37329/0.18073. Took 0.05 sec\n",
            "Epoch 2359, Loss(train/val) 0.39077/0.18086. Took 0.05 sec\n",
            "Epoch 2360, Loss(train/val) 0.40298/0.17972. Took 0.05 sec\n",
            "Epoch 2361, Loss(train/val) 0.39900/0.18163. Took 0.04 sec\n",
            "Epoch 2362, Loss(train/val) 0.38479/0.18401. Took 0.05 sec\n",
            "Epoch 2363, Loss(train/val) 0.36956/0.17679. Took 0.05 sec\n",
            "Epoch 2364, Loss(train/val) 0.39793/0.16840. Took 0.05 sec\n",
            "Epoch 2365, Loss(train/val) 0.37182/0.16646. Took 0.05 sec\n",
            "Epoch 2366, Loss(train/val) 0.36010/0.18684. Took 0.05 sec\n",
            "Epoch 2367, Loss(train/val) 0.37314/0.20807. Took 0.04 sec\n",
            "Epoch 2368, Loss(train/val) 0.37265/0.22313. Took 0.05 sec\n",
            "Epoch 2369, Loss(train/val) 0.40199/0.22448. Took 0.05 sec\n",
            "Epoch 2370, Loss(train/val) 0.37453/0.22674. Took 0.05 sec\n",
            "Epoch 2371, Loss(train/val) 0.40064/0.20022. Took 0.05 sec\n",
            "Epoch 2372, Loss(train/val) 0.38807/0.16859. Took 0.05 sec\n",
            "Epoch 2373, Loss(train/val) 0.36565/0.16478. Took 0.05 sec\n",
            "Epoch 2374, Loss(train/val) 0.36890/0.16605. Took 0.05 sec\n",
            "Epoch 2375, Loss(train/val) 0.36338/0.16448. Took 0.05 sec\n",
            "Epoch 2376, Loss(train/val) 0.35486/0.16432. Took 0.05 sec\n",
            "Epoch 2377, Loss(train/val) 0.39588/0.16565. Took 0.05 sec\n",
            "Epoch 2378, Loss(train/val) 0.37905/0.16408. Took 0.05 sec\n",
            "Epoch 2379, Loss(train/val) 0.36695/0.16354. Took 0.05 sec\n",
            "Epoch 2380, Loss(train/val) 0.35576/0.16410. Took 0.05 sec\n",
            "Epoch 2381, Loss(train/val) 0.42247/0.16975. Took 0.05 sec\n",
            "Epoch 2382, Loss(train/val) 0.38710/0.17093. Took 0.05 sec\n",
            "Epoch 2383, Loss(train/val) 0.39817/0.17595. Took 0.06 sec\n",
            "Epoch 2384, Loss(train/val) 0.37134/0.18098. Took 0.05 sec\n",
            "Epoch 2385, Loss(train/val) 0.38828/0.18702. Took 0.04 sec\n",
            "Epoch 2386, Loss(train/val) 0.39551/0.18462. Took 0.05 sec\n",
            "Epoch 2387, Loss(train/val) 0.36904/0.19236. Took 0.05 sec\n",
            "Epoch 2388, Loss(train/val) 0.36141/0.18464. Took 0.05 sec\n",
            "Epoch 2389, Loss(train/val) 0.37189/0.17031. Took 0.05 sec\n",
            "Epoch 2390, Loss(train/val) 0.40356/0.16635. Took 0.04 sec\n",
            "Epoch 2391, Loss(train/val) 0.37288/0.16499. Took 0.05 sec\n",
            "Epoch 2392, Loss(train/val) 0.35691/0.16636. Took 0.05 sec\n",
            "Epoch 2393, Loss(train/val) 0.39451/0.16565. Took 0.05 sec\n",
            "Epoch 2394, Loss(train/val) 0.35761/0.16812. Took 0.05 sec\n",
            "Epoch 2395, Loss(train/val) 0.36581/0.16696. Took 0.04 sec\n",
            "Epoch 2396, Loss(train/val) 0.38272/0.16787. Took 0.04 sec\n",
            "Epoch 2397, Loss(train/val) 0.39914/0.16846. Took 0.04 sec\n",
            "Epoch 2398, Loss(train/val) 0.40339/0.17374. Took 0.05 sec\n",
            "Epoch 2399, Loss(train/val) 0.35414/0.17910. Took 0.05 sec\n",
            "Epoch 2400, Loss(train/val) 0.37710/0.18576. Took 0.05 sec\n",
            "Epoch 2401, Loss(train/val) 0.35984/0.18362. Took 0.05 sec\n",
            "Epoch 2402, Loss(train/val) 0.40239/0.19111. Took 0.05 sec\n",
            "Epoch 2403, Loss(train/val) 0.37171/0.19626. Took 0.05 sec\n",
            "Epoch 2404, Loss(train/val) 0.35883/0.20640. Took 0.04 sec\n",
            "Epoch 2405, Loss(train/val) 0.35513/0.19003. Took 0.04 sec\n",
            "Epoch 2406, Loss(train/val) 0.34363/0.18127. Took 0.04 sec\n",
            "Epoch 2407, Loss(train/val) 0.38576/0.17113. Took 0.04 sec\n",
            "Epoch 2408, Loss(train/val) 0.40344/0.16361. Took 0.06 sec\n",
            "Epoch 2409, Loss(train/val) 0.37784/0.16336. Took 0.05 sec\n",
            "Epoch 2410, Loss(train/val) 0.36544/0.16487. Took 0.05 sec\n",
            "Epoch 2411, Loss(train/val) 0.38720/0.16322. Took 0.05 sec\n",
            "Epoch 2412, Loss(train/val) 0.39204/0.16262. Took 0.05 sec\n",
            "Epoch 2413, Loss(train/val) 0.37499/0.16305. Took 0.05 sec\n",
            "Epoch 2414, Loss(train/val) 0.39218/0.16338. Took 0.05 sec\n",
            "Epoch 2415, Loss(train/val) 0.37186/0.16365. Took 0.04 sec\n",
            "Epoch 2416, Loss(train/val) 0.37802/0.16343. Took 0.04 sec\n",
            "Epoch 2417, Loss(train/val) 0.36150/0.16734. Took 0.05 sec\n",
            "Epoch 2418, Loss(train/val) 0.35276/0.17323. Took 0.05 sec\n",
            "Epoch 2419, Loss(train/val) 0.36143/0.17221. Took 0.05 sec\n",
            "Epoch 2420, Loss(train/val) 0.38184/0.16833. Took 0.05 sec\n",
            "Epoch 2421, Loss(train/val) 0.37325/0.17075. Took 0.04 sec\n",
            "Epoch 2422, Loss(train/val) 0.37175/0.17002. Took 0.05 sec\n",
            "Epoch 2423, Loss(train/val) 0.36915/0.16616. Took 0.05 sec\n",
            "Epoch 2424, Loss(train/val) 0.38732/0.16344. Took 0.05 sec\n",
            "Epoch 2425, Loss(train/val) 0.39026/0.16279. Took 0.05 sec\n",
            "Epoch 2426, Loss(train/val) 0.39750/0.16316. Took 0.05 sec\n",
            "Epoch 2427, Loss(train/val) 0.39486/0.16512. Took 0.04 sec\n",
            "Epoch 2428, Loss(train/val) 0.37564/0.16774. Took 0.05 sec\n",
            "Epoch 2429, Loss(train/val) 0.38589/0.17128. Took 0.06 sec\n",
            "Epoch 2430, Loss(train/val) 0.37333/0.17768. Took 0.05 sec\n",
            "Epoch 2431, Loss(train/val) 0.35558/0.17972. Took 0.05 sec\n",
            "Epoch 2432, Loss(train/val) 0.35966/0.17524. Took 0.04 sec\n",
            "Epoch 2433, Loss(train/val) 0.36870/0.17490. Took 0.06 sec\n",
            "Epoch 2434, Loss(train/val) 0.35362/0.16835. Took 0.05 sec\n",
            "Epoch 2435, Loss(train/val) 0.38244/0.16302. Took 0.04 sec\n",
            "Epoch 2436, Loss(train/val) 0.39149/0.16530. Took 0.04 sec\n",
            "Epoch 2437, Loss(train/val) 0.38479/0.17321. Took 0.05 sec\n",
            "Epoch 2438, Loss(train/val) 0.35718/0.17627. Took 0.05 sec\n",
            "Epoch 2439, Loss(train/val) 0.35107/0.18278. Took 0.04 sec\n",
            "Epoch 2440, Loss(train/val) 0.38015/0.18067. Took 0.05 sec\n",
            "Epoch 2441, Loss(train/val) 0.35718/0.18440. Took 0.04 sec\n",
            "Epoch 2442, Loss(train/val) 0.36107/0.18376. Took 0.05 sec\n",
            "Epoch 2443, Loss(train/val) 0.36619/0.18970. Took 0.05 sec\n",
            "Epoch 2444, Loss(train/val) 0.35996/0.19143. Took 0.05 sec\n",
            "Epoch 2445, Loss(train/val) 0.36877/0.19882. Took 0.05 sec\n",
            "Epoch 2446, Loss(train/val) 0.38068/0.19056. Took 0.04 sec\n",
            "Epoch 2447, Loss(train/val) 0.40008/0.18201. Took 0.05 sec\n",
            "Epoch 2448, Loss(train/val) 0.38447/0.18039. Took 0.05 sec\n",
            "Epoch 2449, Loss(train/val) 0.37582/0.17198. Took 0.04 sec\n",
            "Epoch 2450, Loss(train/val) 0.37621/0.16304. Took 0.05 sec\n",
            "Epoch 2451, Loss(train/val) 0.39460/0.16623. Took 0.04 sec\n",
            "Epoch 2452, Loss(train/val) 0.36505/0.17873. Took 0.04 sec\n",
            "Epoch 2453, Loss(train/val) 0.38584/0.19674. Took 0.05 sec\n",
            "Epoch 2454, Loss(train/val) 0.40758/0.22482. Took 0.05 sec\n",
            "Epoch 2455, Loss(train/val) 0.36613/0.21473. Took 0.04 sec\n",
            "Epoch 2456, Loss(train/val) 0.35277/0.19631. Took 0.05 sec\n",
            "Epoch 2457, Loss(train/val) 0.35494/0.17936. Took 0.04 sec\n",
            "Epoch 2458, Loss(train/val) 0.35876/0.16549. Took 0.05 sec\n",
            "Epoch 2459, Loss(train/val) 0.41314/0.16368. Took 0.05 sec\n",
            "Epoch 2460, Loss(train/val) 0.35540/0.16438. Took 0.04 sec\n",
            "Epoch 2461, Loss(train/val) 0.37064/0.16935. Took 0.05 sec\n",
            "Epoch 2462, Loss(train/val) 0.37008/0.17476. Took 0.04 sec\n",
            "Epoch 2463, Loss(train/val) 0.34304/0.17749. Took 0.05 sec\n",
            "Epoch 2464, Loss(train/val) 0.36139/0.17118. Took 0.04 sec\n",
            "Epoch 2465, Loss(train/val) 0.37793/0.16495. Took 0.05 sec\n",
            "Epoch 2466, Loss(train/val) 0.37877/0.16225. Took 0.05 sec\n",
            "Epoch 2467, Loss(train/val) 0.37850/0.16328. Took 0.04 sec\n",
            "Epoch 2468, Loss(train/val) 0.37213/0.16364. Took 0.05 sec\n",
            "Epoch 2469, Loss(train/val) 0.35413/0.16342. Took 0.05 sec\n",
            "Epoch 2470, Loss(train/val) 0.39241/0.16261. Took 0.04 sec\n",
            "Epoch 2471, Loss(train/val) 0.36840/0.16062. Took 0.05 sec\n",
            "Epoch 2472, Loss(train/val) 0.37101/0.16415. Took 0.05 sec\n",
            "Epoch 2473, Loss(train/val) 0.38344/0.17937. Took 0.05 sec\n",
            "Epoch 2474, Loss(train/val) 0.35964/0.19188. Took 0.04 sec\n",
            "Epoch 2475, Loss(train/val) 0.40212/0.18141. Took 0.05 sec\n",
            "Epoch 2476, Loss(train/val) 0.34835/0.18431. Took 0.04 sec\n",
            "Epoch 2477, Loss(train/val) 0.36918/0.18819. Took 0.04 sec\n",
            "Epoch 2478, Loss(train/val) 0.36664/0.18310. Took 0.05 sec\n",
            "Epoch 2479, Loss(train/val) 0.36132/0.18374. Took 0.04 sec\n",
            "Epoch 2480, Loss(train/val) 0.38748/0.19528. Took 0.05 sec\n",
            "Epoch 2481, Loss(train/val) 0.34872/0.18847. Took 0.05 sec\n",
            "Epoch 2482, Loss(train/val) 0.38011/0.17778. Took 0.05 sec\n",
            "Epoch 2483, Loss(train/val) 0.38520/0.17803. Took 0.05 sec\n",
            "Epoch 2484, Loss(train/val) 0.36419/0.18352. Took 0.05 sec\n",
            "Epoch 2485, Loss(train/val) 0.36246/0.20103. Took 0.05 sec\n",
            "Epoch 2486, Loss(train/val) 0.36361/0.20415. Took 0.05 sec\n",
            "Epoch 2487, Loss(train/val) 0.36870/0.17614. Took 0.05 sec\n",
            "Epoch 2488, Loss(train/val) 0.36200/0.17125. Took 0.05 sec\n",
            "Epoch 2489, Loss(train/val) 0.36242/0.16919. Took 0.05 sec\n",
            "Epoch 2490, Loss(train/val) 0.37837/0.16561. Took 0.05 sec\n",
            "Epoch 2491, Loss(train/val) 0.37947/0.16362. Took 0.05 sec\n",
            "Epoch 2492, Loss(train/val) 0.39067/0.16095. Took 0.05 sec\n",
            "Epoch 2493, Loss(train/val) 0.37623/0.16193. Took 0.06 sec\n",
            "Epoch 2494, Loss(train/val) 0.38663/0.16629. Took 0.05 sec\n",
            "Epoch 2495, Loss(train/val) 0.35150/0.17354. Took 0.04 sec\n",
            "Epoch 2496, Loss(train/val) 0.36363/0.17643. Took 0.05 sec\n",
            "Epoch 2497, Loss(train/val) 0.37018/0.16672. Took 0.04 sec\n",
            "Epoch 2498, Loss(train/val) 0.36055/0.16252. Took 0.05 sec\n",
            "Epoch 2499, Loss(train/val) 0.39136/0.16236. Took 0.04 sec\n",
            "Epoch 2500, Loss(train/val) 0.38455/0.16331. Took 0.05 sec\n",
            "Epoch 2501, Loss(train/val) 0.34739/0.16285. Took 0.04 sec\n",
            "Epoch 2502, Loss(train/val) 0.37795/0.16273. Took 0.04 sec\n",
            "Epoch 2503, Loss(train/val) 0.38416/0.16407. Took 0.05 sec\n",
            "Epoch 2504, Loss(train/val) 0.37994/0.16994. Took 0.05 sec\n",
            "Epoch 2505, Loss(train/val) 0.36469/0.17252. Took 0.04 sec\n",
            "Epoch 2506, Loss(train/val) 0.36007/0.16969. Took 0.04 sec\n",
            "Epoch 2507, Loss(train/val) 0.35356/0.16263. Took 0.05 sec\n",
            "Epoch 2508, Loss(train/val) 0.35062/0.16329. Took 0.05 sec\n",
            "Epoch 2509, Loss(train/val) 0.36634/0.16829. Took 0.05 sec\n",
            "Epoch 2510, Loss(train/val) 0.38084/0.16901. Took 0.05 sec\n",
            "Epoch 2511, Loss(train/val) 0.35796/0.16380. Took 0.04 sec\n",
            "Epoch 2512, Loss(train/val) 0.37051/0.16226. Took 0.04 sec\n",
            "Epoch 2513, Loss(train/val) 0.37667/0.16208. Took 0.05 sec\n",
            "Epoch 2514, Loss(train/val) 0.36025/0.16144. Took 0.05 sec\n",
            "Epoch 2515, Loss(train/val) 0.38736/0.16152. Took 0.05 sec\n",
            "Epoch 2516, Loss(train/val) 0.37126/0.16207. Took 0.04 sec\n",
            "Epoch 2517, Loss(train/val) 0.38371/0.16225. Took 0.04 sec\n",
            "Epoch 2518, Loss(train/val) 0.35458/0.16306. Took 0.05 sec\n",
            "Epoch 2519, Loss(train/val) 0.40359/0.16566. Took 0.05 sec\n",
            "Epoch 2520, Loss(train/val) 0.36611/0.16199. Took 0.04 sec\n",
            "Epoch 2521, Loss(train/val) 0.37622/0.16136. Took 0.04 sec\n",
            "Epoch 2522, Loss(train/val) 0.39615/0.16413. Took 0.04 sec\n",
            "Epoch 2523, Loss(train/val) 0.38317/0.16861. Took 0.05 sec\n",
            "Epoch 2524, Loss(train/val) 0.39351/0.16445. Took 0.04 sec\n",
            "Epoch 2525, Loss(train/val) 0.37209/0.16163. Took 0.04 sec\n",
            "Epoch 2526, Loss(train/val) 0.39562/0.17528. Took 0.04 sec\n",
            "Epoch 2527, Loss(train/val) 0.35305/0.18660. Took 0.04 sec\n",
            "Epoch 2528, Loss(train/val) 0.37019/0.18098. Took 0.05 sec\n",
            "Epoch 2529, Loss(train/val) 0.37078/0.16885. Took 0.04 sec\n",
            "Epoch 2530, Loss(train/val) 0.35822/0.16481. Took 0.05 sec\n",
            "Epoch 2531, Loss(train/val) 0.35254/0.16385. Took 0.05 sec\n",
            "Epoch 2532, Loss(train/val) 0.36867/0.16223. Took 0.04 sec\n",
            "Epoch 2533, Loss(train/val) 0.35774/0.16180. Took 0.05 sec\n",
            "Epoch 2534, Loss(train/val) 0.36615/0.16177. Took 0.04 sec\n",
            "Epoch 2535, Loss(train/val) 0.35176/0.16077. Took 0.04 sec\n",
            "Epoch 2536, Loss(train/val) 0.37215/0.16233. Took 0.05 sec\n",
            "Epoch 2537, Loss(train/val) 0.37909/0.16159. Took 0.05 sec\n",
            "Epoch 2538, Loss(train/val) 0.37312/0.16553. Took 0.05 sec\n",
            "Epoch 2539, Loss(train/val) 0.39205/0.17246. Took 0.05 sec\n",
            "Epoch 2540, Loss(train/val) 0.37527/0.17296. Took 0.04 sec\n",
            "Epoch 2541, Loss(train/val) 0.35073/0.18077. Took 0.04 sec\n",
            "Epoch 2542, Loss(train/val) 0.39496/0.18970. Took 0.04 sec\n",
            "Epoch 2543, Loss(train/val) 0.35871/0.18676. Took 0.05 sec\n",
            "Epoch 2544, Loss(train/val) 0.35007/0.17735. Took 0.04 sec\n",
            "Epoch 2545, Loss(train/val) 0.34770/0.17176. Took 0.04 sec\n",
            "Epoch 2546, Loss(train/val) 0.37846/0.16948. Took 0.05 sec\n",
            "Epoch 2547, Loss(train/val) 0.34190/0.16952. Took 0.04 sec\n",
            "Epoch 2548, Loss(train/val) 0.35888/0.18110. Took 0.05 sec\n",
            "Epoch 2549, Loss(train/val) 0.35128/0.17393. Took 0.05 sec\n",
            "Epoch 2550, Loss(train/val) 0.39036/0.18011. Took 0.04 sec\n",
            "Epoch 2551, Loss(train/val) 0.35951/0.17295. Took 0.05 sec\n",
            "Epoch 2552, Loss(train/val) 0.37179/0.16965. Took 0.04 sec\n",
            "Epoch 2553, Loss(train/val) 0.37523/0.16097. Took 0.05 sec\n",
            "Epoch 2554, Loss(train/val) 0.38097/0.16550. Took 0.05 sec\n",
            "Epoch 2555, Loss(train/val) 0.40024/0.17048. Took 0.05 sec\n",
            "Epoch 2556, Loss(train/val) 0.38154/0.17011. Took 0.05 sec\n",
            "Epoch 2557, Loss(train/val) 0.36815/0.17023. Took 0.04 sec\n",
            "Epoch 2558, Loss(train/val) 0.35654/0.16614. Took 0.06 sec\n",
            "Epoch 2559, Loss(train/val) 0.38914/0.16126. Took 0.05 sec\n",
            "Epoch 2560, Loss(train/val) 0.35108/0.16106. Took 0.05 sec\n",
            "Epoch 2561, Loss(train/val) 0.36908/0.16209. Took 0.04 sec\n",
            "Epoch 2562, Loss(train/val) 0.36291/0.16226. Took 0.04 sec\n",
            "Epoch 2563, Loss(train/val) 0.37204/0.16051. Took 0.05 sec\n",
            "Epoch 2564, Loss(train/val) 0.37465/0.16069. Took 0.04 sec\n",
            "Epoch 2565, Loss(train/val) 0.36500/0.16275. Took 0.05 sec\n",
            "Epoch 2566, Loss(train/val) 0.38574/0.16653. Took 0.04 sec\n",
            "Epoch 2567, Loss(train/val) 0.35225/0.17262. Took 0.05 sec\n",
            "Epoch 2568, Loss(train/val) 0.36939/0.19198. Took 0.06 sec\n",
            "Epoch 2569, Loss(train/val) 0.37753/0.19617. Took 0.05 sec\n",
            "Epoch 2570, Loss(train/val) 0.38422/0.19928. Took 0.05 sec\n",
            "Epoch 2571, Loss(train/val) 0.35876/0.21019. Took 0.05 sec\n",
            "Epoch 2572, Loss(train/val) 0.36803/0.21127. Took 0.05 sec\n",
            "Epoch 2573, Loss(train/val) 0.36619/0.20018. Took 0.05 sec\n",
            "Epoch 2574, Loss(train/val) 0.37006/0.21189. Took 0.04 sec\n",
            "Epoch 2575, Loss(train/val) 0.37067/0.19768. Took 0.05 sec\n",
            "Epoch 2576, Loss(train/val) 0.34839/0.18984. Took 0.05 sec\n",
            "Epoch 2577, Loss(train/val) 0.36580/0.17222. Took 0.05 sec\n",
            "Epoch 2578, Loss(train/val) 0.38191/0.16139. Took 0.05 sec\n",
            "Epoch 2579, Loss(train/val) 0.38383/0.16816. Took 0.05 sec\n",
            "Epoch 2580, Loss(train/val) 0.33922/0.17984. Took 0.05 sec\n",
            "Epoch 2581, Loss(train/val) 0.36313/0.18256. Took 0.05 sec\n",
            "Epoch 2582, Loss(train/val) 0.36657/0.17966. Took 0.04 sec\n",
            "Epoch 2583, Loss(train/val) 0.37685/0.16937. Took 0.05 sec\n",
            "Epoch 2584, Loss(train/val) 0.34431/0.16548. Took 0.04 sec\n",
            "Epoch 2585, Loss(train/val) 0.35755/0.16163. Took 0.04 sec\n",
            "Epoch 2586, Loss(train/val) 0.36006/0.16187. Took 0.05 sec\n",
            "Epoch 2587, Loss(train/val) 0.37015/0.16155. Took 0.05 sec\n",
            "Epoch 2588, Loss(train/val) 0.36789/0.16024. Took 0.05 sec\n",
            "Epoch 2589, Loss(train/val) 0.38166/0.16039. Took 0.04 sec\n",
            "Epoch 2590, Loss(train/val) 0.37418/0.16632. Took 0.04 sec\n",
            "Epoch 2591, Loss(train/val) 0.35950/0.17923. Took 0.05 sec\n",
            "Epoch 2592, Loss(train/val) 0.34439/0.18424. Took 0.05 sec\n",
            "Epoch 2593, Loss(train/val) 0.36993/0.17451. Took 0.05 sec\n",
            "Epoch 2594, Loss(train/val) 0.34161/0.16987. Took 0.05 sec\n",
            "Epoch 2595, Loss(train/val) 0.36255/0.16692. Took 0.05 sec\n",
            "Epoch 2596, Loss(train/val) 0.35539/0.16289. Took 0.05 sec\n",
            "Epoch 2597, Loss(train/val) 0.37179/0.16071. Took 0.04 sec\n",
            "Epoch 2598, Loss(train/val) 0.35881/0.16104. Took 0.05 sec\n",
            "Epoch 2599, Loss(train/val) 0.38595/0.15944. Took 0.05 sec\n",
            "Epoch 2600, Loss(train/val) 0.35404/0.15962. Took 0.05 sec\n",
            "Epoch 2601, Loss(train/val) 0.34720/0.15998. Took 0.05 sec\n",
            "Epoch 2602, Loss(train/val) 0.37312/0.16229. Took 0.05 sec\n",
            "Epoch 2603, Loss(train/val) 0.35469/0.16848. Took 0.05 sec\n",
            "Epoch 2604, Loss(train/val) 0.37802/0.16174. Took 0.05 sec\n",
            "Epoch 2605, Loss(train/val) 0.33303/0.16020. Took 0.05 sec\n",
            "Epoch 2606, Loss(train/val) 0.38203/0.16144. Took 0.05 sec\n",
            "Epoch 2607, Loss(train/val) 0.38390/0.16327. Took 0.04 sec\n",
            "Epoch 2608, Loss(train/val) 0.36001/0.16153. Took 0.05 sec\n",
            "Epoch 2609, Loss(train/val) 0.36788/0.16210. Took 0.04 sec\n",
            "Epoch 2610, Loss(train/val) 0.36611/0.16235. Took 0.05 sec\n",
            "Epoch 2611, Loss(train/val) 0.35401/0.16155. Took 0.04 sec\n",
            "Epoch 2612, Loss(train/val) 0.34743/0.16098. Took 0.05 sec\n",
            "Epoch 2613, Loss(train/val) 0.36180/0.16091. Took 0.05 sec\n",
            "Epoch 2614, Loss(train/val) 0.36770/0.16153. Took 0.05 sec\n",
            "Epoch 2615, Loss(train/val) 0.36826/0.16112. Took 0.04 sec\n",
            "Epoch 2616, Loss(train/val) 0.35800/0.16798. Took 0.04 sec\n",
            "Epoch 2617, Loss(train/val) 0.33738/0.17512. Took 0.04 sec\n",
            "Epoch 2618, Loss(train/val) 0.33962/0.17714. Took 0.05 sec\n",
            "Epoch 2619, Loss(train/val) 0.36113/0.17564. Took 0.05 sec\n",
            "Epoch 2620, Loss(train/val) 0.40571/0.18681. Took 0.05 sec\n",
            "Epoch 2621, Loss(train/val) 0.37526/0.19683. Took 0.04 sec\n",
            "Epoch 2622, Loss(train/val) 0.36589/0.20974. Took 0.04 sec\n",
            "Epoch 2623, Loss(train/val) 0.37369/0.21692. Took 0.06 sec\n",
            "Epoch 2624, Loss(train/val) 0.39406/0.20231. Took 0.04 sec\n",
            "Epoch 2625, Loss(train/val) 0.35028/0.18893. Took 0.04 sec\n",
            "Epoch 2626, Loss(train/val) 0.38016/0.16237. Took 0.04 sec\n",
            "Epoch 2627, Loss(train/val) 0.38121/0.16064. Took 0.04 sec\n",
            "Epoch 2628, Loss(train/val) 0.37630/0.16184. Took 0.05 sec\n",
            "Epoch 2629, Loss(train/val) 0.34856/0.16018. Took 0.04 sec\n",
            "Epoch 2630, Loss(train/val) 0.34440/0.16059. Took 0.04 sec\n",
            "Epoch 2631, Loss(train/val) 0.37739/0.16042. Took 0.04 sec\n",
            "Epoch 2632, Loss(train/val) 0.35027/0.16058. Took 0.04 sec\n",
            "Epoch 2633, Loss(train/val) 0.34951/0.16142. Took 0.05 sec\n",
            "Epoch 2634, Loss(train/val) 0.35012/0.16834. Took 0.05 sec\n",
            "Epoch 2635, Loss(train/val) 0.38195/0.16671. Took 0.04 sec\n",
            "Epoch 2636, Loss(train/val) 0.36758/0.15935. Took 0.05 sec\n",
            "Epoch 2637, Loss(train/val) 0.38919/0.16172. Took 0.04 sec\n",
            "Epoch 2638, Loss(train/val) 0.36036/0.16627. Took 0.05 sec\n",
            "Epoch 2639, Loss(train/val) 0.34287/0.16997. Took 0.05 sec\n",
            "Epoch 2640, Loss(train/val) 0.38582/0.17430. Took 0.05 sec\n",
            "Epoch 2641, Loss(train/val) 0.36986/0.17805. Took 0.05 sec\n",
            "Epoch 2642, Loss(train/val) 0.36861/0.17737. Took 0.05 sec\n",
            "Epoch 2643, Loss(train/val) 0.37169/0.17275. Took 0.05 sec\n",
            "Epoch 2644, Loss(train/val) 0.36543/0.16131. Took 0.05 sec\n",
            "Epoch 2645, Loss(train/val) 0.33783/0.16443. Took 0.05 sec\n",
            "Epoch 2646, Loss(train/val) 0.37462/0.16289. Took 0.05 sec\n",
            "Epoch 2647, Loss(train/val) 0.38896/0.15987. Took 0.04 sec\n",
            "Epoch 2648, Loss(train/val) 0.35998/0.16484. Took 0.05 sec\n",
            "Epoch 2649, Loss(train/val) 0.33246/0.17935. Took 0.05 sec\n",
            "Epoch 2650, Loss(train/val) 0.35092/0.20339. Took 0.04 sec\n",
            "Epoch 2651, Loss(train/val) 0.34551/0.20198. Took 0.04 sec\n",
            "Epoch 2652, Loss(train/val) 0.35128/0.21021. Took 0.05 sec\n",
            "Epoch 2653, Loss(train/val) 0.38864/0.21525. Took 0.05 sec\n",
            "Epoch 2654, Loss(train/val) 0.36725/0.20646. Took 0.04 sec\n",
            "Epoch 2655, Loss(train/val) 0.34570/0.18583. Took 0.05 sec\n",
            "Epoch 2656, Loss(train/val) 0.34853/0.18189. Took 0.04 sec\n",
            "Epoch 2657, Loss(train/val) 0.36284/0.16940. Took 0.05 sec\n",
            "Epoch 2658, Loss(train/val) 0.35986/0.16525. Took 0.06 sec\n",
            "Epoch 2659, Loss(train/val) 0.36631/0.16695. Took 0.05 sec\n",
            "Epoch 2660, Loss(train/val) 0.36324/0.17315. Took 0.05 sec\n",
            "Epoch 2661, Loss(train/val) 0.36041/0.17515. Took 0.05 sec\n",
            "Epoch 2662, Loss(train/val) 0.36571/0.16672. Took 0.05 sec\n",
            "Epoch 2663, Loss(train/val) 0.36270/0.16563. Took 0.05 sec\n",
            "Epoch 2664, Loss(train/val) 0.35906/0.16703. Took 0.05 sec\n",
            "Epoch 2665, Loss(train/val) 0.39061/0.16791. Took 0.05 sec\n",
            "Epoch 2666, Loss(train/val) 0.34491/0.16143. Took 0.06 sec\n",
            "Epoch 2667, Loss(train/val) 0.35222/0.16110. Took 0.04 sec\n",
            "Epoch 2668, Loss(train/val) 0.36178/0.16182. Took 0.05 sec\n",
            "Epoch 2669, Loss(train/val) 0.36279/0.16138. Took 0.04 sec\n",
            "Epoch 2670, Loss(train/val) 0.37022/0.15993. Took 0.05 sec\n",
            "Epoch 2671, Loss(train/val) 0.37397/0.16179. Took 0.04 sec\n",
            "Epoch 2672, Loss(train/val) 0.36859/0.15953. Took 0.04 sec\n",
            "Epoch 2673, Loss(train/val) 0.38404/0.16017. Took 0.05 sec\n",
            "Epoch 2674, Loss(train/val) 0.39839/0.16119. Took 0.04 sec\n",
            "Epoch 2675, Loss(train/val) 0.39840/0.16045. Took 0.04 sec\n",
            "Epoch 2676, Loss(train/val) 0.36253/0.16163. Took 0.04 sec\n",
            "Epoch 2677, Loss(train/val) 0.37475/0.16415. Took 0.05 sec\n",
            "Epoch 2678, Loss(train/val) 0.37428/0.17977. Took 0.05 sec\n",
            "Epoch 2679, Loss(train/val) 0.36147/0.19198. Took 0.05 sec\n",
            "Epoch 2680, Loss(train/val) 0.35826/0.20064. Took 0.05 sec\n",
            "Epoch 2681, Loss(train/val) 0.37153/0.19252. Took 0.05 sec\n",
            "Epoch 2682, Loss(train/val) 0.33826/0.17982. Took 0.05 sec\n",
            "Epoch 2683, Loss(train/val) 0.36746/0.16712. Took 0.06 sec\n",
            "Epoch 2684, Loss(train/val) 0.35737/0.15938. Took 0.05 sec\n",
            "Epoch 2685, Loss(train/val) 0.37492/0.15895. Took 0.05 sec\n",
            "Epoch 2686, Loss(train/val) 0.35837/0.15908. Took 0.05 sec\n",
            "Epoch 2687, Loss(train/val) 0.37487/0.15986. Took 0.05 sec\n",
            "Epoch 2688, Loss(train/val) 0.35860/0.15916. Took 0.05 sec\n",
            "Epoch 2689, Loss(train/val) 0.36736/0.16109. Took 0.05 sec\n",
            "Epoch 2690, Loss(train/val) 0.36597/0.16118. Took 0.05 sec\n",
            "Epoch 2691, Loss(train/val) 0.37994/0.16071. Took 0.05 sec\n",
            "Epoch 2692, Loss(train/val) 0.35020/0.16855. Took 0.05 sec\n",
            "Epoch 2693, Loss(train/val) 0.35280/0.18136. Took 0.05 sec\n",
            "Epoch 2694, Loss(train/val) 0.36411/0.18440. Took 0.05 sec\n",
            "Epoch 2695, Loss(train/val) 0.37118/0.18832. Took 0.05 sec\n",
            "Epoch 2696, Loss(train/val) 0.34587/0.18077. Took 0.05 sec\n",
            "Epoch 2697, Loss(train/val) 0.36103/0.17076. Took 0.05 sec\n",
            "Epoch 2698, Loss(train/val) 0.37835/0.16211. Took 0.05 sec\n",
            "Epoch 2699, Loss(train/val) 0.37144/0.16165. Took 0.04 sec\n",
            "Epoch 2700, Loss(train/val) 0.37367/0.16294. Took 0.04 sec\n",
            "Epoch 2701, Loss(train/val) 0.35640/0.16551. Took 0.04 sec\n",
            "Epoch 2702, Loss(train/val) 0.36291/0.16197. Took 0.05 sec\n",
            "Epoch 2703, Loss(train/val) 0.33311/0.16265. Took 0.06 sec\n",
            "Epoch 2704, Loss(train/val) 0.35034/0.16686. Took 0.05 sec\n",
            "Epoch 2705, Loss(train/val) 0.38009/0.16397. Took 0.05 sec\n",
            "Epoch 2706, Loss(train/val) 0.35818/0.16039. Took 0.05 sec\n",
            "Epoch 2707, Loss(train/val) 0.35834/0.16103. Took 0.05 sec\n",
            "Epoch 2708, Loss(train/val) 0.34846/0.15976. Took 0.07 sec\n",
            "Epoch 2709, Loss(train/val) 0.36670/0.15909. Took 0.05 sec\n",
            "Epoch 2710, Loss(train/val) 0.36368/0.15848. Took 0.05 sec\n",
            "Epoch 2711, Loss(train/val) 0.37624/0.16032. Took 0.04 sec\n",
            "Epoch 2712, Loss(train/val) 0.36596/0.16485. Took 0.05 sec\n",
            "Epoch 2713, Loss(train/val) 0.36274/0.17410. Took 0.06 sec\n",
            "Epoch 2714, Loss(train/val) 0.36718/0.18443. Took 0.04 sec\n",
            "Epoch 2715, Loss(train/val) 0.37364/0.19808. Took 0.05 sec\n",
            "Epoch 2716, Loss(train/val) 0.36103/0.19912. Took 0.04 sec\n",
            "Epoch 2717, Loss(train/val) 0.35799/0.19397. Took 0.04 sec\n",
            "Epoch 2718, Loss(train/val) 0.35015/0.18962. Took 0.05 sec\n",
            "Epoch 2719, Loss(train/val) 0.35638/0.18495. Took 0.05 sec\n",
            "Epoch 2720, Loss(train/val) 0.34740/0.18355. Took 0.05 sec\n",
            "Epoch 2721, Loss(train/val) 0.38477/0.16558. Took 0.05 sec\n",
            "Epoch 2722, Loss(train/val) 0.37427/0.15910. Took 0.05 sec\n",
            "Epoch 2723, Loss(train/val) 0.37141/0.15967. Took 0.05 sec\n",
            "Epoch 2724, Loss(train/val) 0.35952/0.16650. Took 0.05 sec\n",
            "Epoch 2725, Loss(train/val) 0.37741/0.16239. Took 0.05 sec\n",
            "Epoch 2726, Loss(train/val) 0.34186/0.16379. Took 0.05 sec\n",
            "Epoch 2727, Loss(train/val) 0.36727/0.16300. Took 0.05 sec\n",
            "Epoch 2728, Loss(train/val) 0.34613/0.16196. Took 0.06 sec\n",
            "Epoch 2729, Loss(train/val) 0.35794/0.16363. Took 0.05 sec\n",
            "Epoch 2730, Loss(train/val) 0.34146/0.15974. Took 0.05 sec\n",
            "Epoch 2731, Loss(train/val) 0.39822/0.16082. Took 0.04 sec\n",
            "Epoch 2732, Loss(train/val) 0.39617/0.16516. Took 0.04 sec\n",
            "Epoch 2733, Loss(train/val) 0.32776/0.16813. Took 0.05 sec\n",
            "Epoch 2734, Loss(train/val) 0.37841/0.17649. Took 0.05 sec\n",
            "Epoch 2735, Loss(train/val) 0.36729/0.17733. Took 0.04 sec\n",
            "Epoch 2736, Loss(train/val) 0.35681/0.16550. Took 0.05 sec\n",
            "Epoch 2737, Loss(train/val) 0.35799/0.16105. Took 0.06 sec\n",
            "Epoch 2738, Loss(train/val) 0.37162/0.15971. Took 0.05 sec\n",
            "Epoch 2739, Loss(train/val) 0.34674/0.16341. Took 0.05 sec\n",
            "Epoch 2740, Loss(train/val) 0.35783/0.16916. Took 0.05 sec\n",
            "Epoch 2741, Loss(train/val) 0.36913/0.16952. Took 0.04 sec\n",
            "Epoch 2742, Loss(train/val) 0.38232/0.16677. Took 0.05 sec\n",
            "Epoch 2743, Loss(train/val) 0.37029/0.16322. Took 0.05 sec\n",
            "Epoch 2744, Loss(train/val) 0.35273/0.16688. Took 0.05 sec\n",
            "Epoch 2745, Loss(train/val) 0.34549/0.16757. Took 0.05 sec\n",
            "Epoch 2746, Loss(train/val) 0.36011/0.17498. Took 0.05 sec\n",
            "Epoch 2747, Loss(train/val) 0.37766/0.17934. Took 0.04 sec\n",
            "Epoch 2748, Loss(train/val) 0.36726/0.16208. Took 0.06 sec\n",
            "Epoch 2749, Loss(train/val) 0.37285/0.15915. Took 0.05 sec\n",
            "Epoch 2750, Loss(train/val) 0.38097/0.16327. Took 0.05 sec\n",
            "Epoch 2751, Loss(train/val) 0.33709/0.17061. Took 0.05 sec\n",
            "Epoch 2752, Loss(train/val) 0.37853/0.17345. Took 0.04 sec\n",
            "Epoch 2753, Loss(train/val) 0.37580/0.16951. Took 0.05 sec\n",
            "Epoch 2754, Loss(train/val) 0.36932/0.17417. Took 0.04 sec\n",
            "Epoch 2755, Loss(train/val) 0.37080/0.19225. Took 0.04 sec\n",
            "Epoch 2756, Loss(train/val) 0.35623/0.19711. Took 0.05 sec\n",
            "Epoch 2757, Loss(train/val) 0.38988/0.19267. Took 0.05 sec\n",
            "Epoch 2758, Loss(train/val) 0.34741/0.17082. Took 0.05 sec\n",
            "Epoch 2759, Loss(train/val) 0.33651/0.16478. Took 0.05 sec\n",
            "Epoch 2760, Loss(train/val) 0.36824/0.15927. Took 0.04 sec\n",
            "Epoch 2761, Loss(train/val) 0.38134/0.15929. Took 0.05 sec\n",
            "Epoch 2762, Loss(train/val) 0.36383/0.15897. Took 0.04 sec\n",
            "Epoch 2763, Loss(train/val) 0.36178/0.16479. Took 0.05 sec\n",
            "Epoch 2764, Loss(train/val) 0.34577/0.16298. Took 0.05 sec\n",
            "Epoch 2765, Loss(train/val) 0.39348/0.16096. Took 0.05 sec\n",
            "Epoch 2766, Loss(train/val) 0.33570/0.16155. Took 0.05 sec\n",
            "Epoch 2767, Loss(train/val) 0.35466/0.18107. Took 0.05 sec\n",
            "Epoch 2768, Loss(train/val) 0.37697/0.19322. Took 0.06 sec\n",
            "Epoch 2769, Loss(train/val) 0.32785/0.18955. Took 0.05 sec\n",
            "Epoch 2770, Loss(train/val) 0.36688/0.18812. Took 0.05 sec\n",
            "Epoch 2771, Loss(train/val) 0.35770/0.18402. Took 0.05 sec\n",
            "Epoch 2772, Loss(train/val) 0.35885/0.18007. Took 0.05 sec\n",
            "Epoch 2773, Loss(train/val) 0.35464/0.19203. Took 0.05 sec\n",
            "Epoch 2774, Loss(train/val) 0.36496/0.19064. Took 0.05 sec\n",
            "Epoch 2775, Loss(train/val) 0.38890/0.18216. Took 0.06 sec\n",
            "Epoch 2776, Loss(train/val) 0.38230/0.16378. Took 0.04 sec\n",
            "Epoch 2777, Loss(train/val) 0.35935/0.16013. Took 0.05 sec\n",
            "Epoch 2778, Loss(train/val) 0.35151/0.16028. Took 0.06 sec\n",
            "Epoch 2779, Loss(train/val) 0.37462/0.16104. Took 0.05 sec\n",
            "Epoch 2780, Loss(train/val) 0.34113/0.16112. Took 0.05 sec\n",
            "Epoch 2781, Loss(train/val) 0.39765/0.16078. Took 0.05 sec\n",
            "Epoch 2782, Loss(train/val) 0.34581/0.16110. Took 0.05 sec\n",
            "Epoch 2783, Loss(train/val) 0.36480/0.15964. Took 0.06 sec\n",
            "Epoch 2784, Loss(train/val) 0.35995/0.15857. Took 0.05 sec\n",
            "Epoch 2785, Loss(train/val) 0.37430/0.15877. Took 0.05 sec\n",
            "Epoch 2786, Loss(train/val) 0.37589/0.15804. Took 0.05 sec\n",
            "Epoch 2787, Loss(train/val) 0.35575/0.15854. Took 0.06 sec\n",
            "Epoch 2788, Loss(train/val) 0.36170/0.16760. Took 0.05 sec\n",
            "Epoch 2789, Loss(train/val) 0.38498/0.17943. Took 0.05 sec\n",
            "Epoch 2790, Loss(train/val) 0.35937/0.20231. Took 0.05 sec\n",
            "Epoch 2791, Loss(train/val) 0.38528/0.22803. Took 0.06 sec\n",
            "Epoch 2792, Loss(train/val) 0.37125/0.25323. Took 0.07 sec\n",
            "Epoch 2793, Loss(train/val) 0.37376/0.23959. Took 0.05 sec\n",
            "Epoch 2794, Loss(train/val) 0.35656/0.22731. Took 0.05 sec\n",
            "Epoch 2795, Loss(train/val) 0.36756/0.19012. Took 0.06 sec\n",
            "Epoch 2796, Loss(train/val) 0.35822/0.17970. Took 0.06 sec\n",
            "Epoch 2797, Loss(train/val) 0.41678/0.17634. Took 0.05 sec\n",
            "Epoch 2798, Loss(train/val) 0.36282/0.17025. Took 0.05 sec\n",
            "Epoch 2799, Loss(train/val) 0.37943/0.16816. Took 0.06 sec\n",
            "Namespace(batch_size=193, device='cuda', dropout=0.8, epoch=2800, exp_name='exp5_dropout1', hid_dim=16, input_dim=1, l2=1e-05, lr=5e-05, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.13364/0.43487. Took 0.06 sec\n",
            "Epoch 1, Loss(train/val) 1.12395/0.43441. Took 0.05 sec\n",
            "Epoch 2, Loss(train/val) 1.14635/0.43399. Took 0.05 sec\n",
            "Epoch 3, Loss(train/val) 1.13881/0.43362. Took 0.05 sec\n",
            "Epoch 4, Loss(train/val) 1.14309/0.43327. Took 0.05 sec\n",
            "Epoch 5, Loss(train/val) 1.14253/0.43295. Took 0.05 sec\n",
            "Epoch 6, Loss(train/val) 1.14301/0.43266. Took 0.05 sec\n",
            "Epoch 7, Loss(train/val) 1.13383/0.43237. Took 0.05 sec\n",
            "Epoch 8, Loss(train/val) 1.14417/0.43211. Took 0.06 sec\n",
            "Epoch 9, Loss(train/val) 1.13740/0.43185. Took 0.05 sec\n",
            "Epoch 10, Loss(train/val) 1.14171/0.43160. Took 0.05 sec\n",
            "Epoch 11, Loss(train/val) 1.14361/0.43135. Took 0.05 sec\n",
            "Epoch 12, Loss(train/val) 1.13775/0.43112. Took 0.05 sec\n",
            "Epoch 13, Loss(train/val) 1.13602/0.43088. Took 0.05 sec\n",
            "Epoch 14, Loss(train/val) 1.14729/0.43066. Took 0.04 sec\n",
            "Epoch 15, Loss(train/val) 1.14515/0.43043. Took 0.05 sec\n",
            "Epoch 16, Loss(train/val) 1.14659/0.43020. Took 0.04 sec\n",
            "Epoch 17, Loss(train/val) 1.13605/0.42997. Took 0.05 sec\n",
            "Epoch 18, Loss(train/val) 1.14855/0.42974. Took 0.06 sec\n",
            "Epoch 19, Loss(train/val) 1.14523/0.42964. Took 0.05 sec\n",
            "Epoch 20, Loss(train/val) 1.14457/0.42952. Took 0.05 sec\n",
            "Epoch 21, Loss(train/val) 1.14430/0.42940. Took 0.04 sec\n",
            "Epoch 22, Loss(train/val) 1.14474/0.42929. Took 0.04 sec\n",
            "Epoch 23, Loss(train/val) 1.14550/0.42920. Took 0.05 sec\n",
            "Epoch 24, Loss(train/val) 1.14144/0.42912. Took 0.05 sec\n",
            "Epoch 25, Loss(train/val) 1.13974/0.42905. Took 0.05 sec\n",
            "Epoch 26, Loss(train/val) 1.14610/0.42905. Took 0.04 sec\n",
            "Epoch 27, Loss(train/val) 1.13317/0.42907. Took 0.05 sec\n",
            "Epoch 28, Loss(train/val) 1.13437/0.42920. Took 0.05 sec\n",
            "Epoch 29, Loss(train/val) 1.14476/0.42933. Took 0.05 sec\n",
            "Epoch 30, Loss(train/val) 1.13920/0.42941. Took 0.05 sec\n",
            "Epoch 31, Loss(train/val) 1.14276/0.42947. Took 0.05 sec\n",
            "Epoch 32, Loss(train/val) 1.13438/0.42949. Took 0.04 sec\n",
            "Epoch 33, Loss(train/val) 1.13537/0.42945. Took 0.06 sec\n",
            "Epoch 34, Loss(train/val) 1.13686/0.42940. Took 0.05 sec\n",
            "Epoch 35, Loss(train/val) 1.13352/0.42935. Took 0.05 sec\n",
            "Epoch 36, Loss(train/val) 1.14075/0.42948. Took 0.05 sec\n",
            "Epoch 37, Loss(train/val) 1.13951/0.42980. Took 0.05 sec\n",
            "Epoch 38, Loss(train/val) 1.13638/0.43002. Took 0.05 sec\n",
            "Epoch 39, Loss(train/val) 1.14266/0.43016. Took 0.05 sec\n",
            "Epoch 40, Loss(train/val) 1.13980/0.43123. Took 0.05 sec\n",
            "Epoch 41, Loss(train/val) 1.14621/0.43218. Took 0.05 sec\n",
            "Epoch 42, Loss(train/val) 1.14569/0.43308. Took 0.05 sec\n",
            "Epoch 43, Loss(train/val) 1.13402/0.43334. Took 0.05 sec\n",
            "Epoch 44, Loss(train/val) 1.14118/0.43352. Took 0.05 sec\n",
            "Epoch 45, Loss(train/val) 1.13291/0.43406. Took 0.04 sec\n",
            "Epoch 46, Loss(train/val) 1.13644/0.43415. Took 0.05 sec\n",
            "Epoch 47, Loss(train/val) 1.13580/0.43432. Took 0.05 sec\n",
            "Epoch 48, Loss(train/val) 1.13967/0.43428. Took 0.05 sec\n",
            "Epoch 49, Loss(train/val) 1.14006/0.43438. Took 0.05 sec\n",
            "Epoch 50, Loss(train/val) 1.13381/0.43445. Took 0.05 sec\n",
            "Epoch 51, Loss(train/val) 1.13584/0.43442. Took 0.05 sec\n",
            "Epoch 52, Loss(train/val) 1.14180/0.43442. Took 0.04 sec\n",
            "Epoch 53, Loss(train/val) 1.13855/0.43460. Took 0.05 sec\n",
            "Epoch 54, Loss(train/val) 1.13475/0.43502. Took 0.06 sec\n",
            "Epoch 55, Loss(train/val) 1.13713/0.43542. Took 0.04 sec\n",
            "Epoch 56, Loss(train/val) 1.13497/0.43594. Took 0.04 sec\n",
            "Epoch 57, Loss(train/val) 1.13159/0.43592. Took 0.05 sec\n",
            "Epoch 58, Loss(train/val) 1.12833/0.43617. Took 0.05 sec\n",
            "Epoch 59, Loss(train/val) 1.13609/0.43613. Took 0.04 sec\n",
            "Epoch 60, Loss(train/val) 1.13900/0.43636. Took 0.05 sec\n",
            "Epoch 61, Loss(train/val) 1.13966/0.43711. Took 0.05 sec\n",
            "Epoch 62, Loss(train/val) 1.13175/0.43746. Took 0.05 sec\n",
            "Epoch 63, Loss(train/val) 1.14321/0.43763. Took 0.06 sec\n",
            "Epoch 64, Loss(train/val) 1.13212/0.43878. Took 0.05 sec\n",
            "Epoch 65, Loss(train/val) 1.13630/0.43951. Took 0.05 sec\n",
            "Epoch 66, Loss(train/val) 1.13637/0.43985. Took 0.05 sec\n",
            "Epoch 67, Loss(train/val) 1.13349/0.43988. Took 0.05 sec\n",
            "Epoch 68, Loss(train/val) 1.13539/0.43982. Took 0.05 sec\n",
            "Epoch 69, Loss(train/val) 1.12398/0.43969. Took 0.05 sec\n",
            "Epoch 70, Loss(train/val) 1.12802/0.43929. Took 0.05 sec\n",
            "Epoch 71, Loss(train/val) 1.12959/0.44013. Took 0.05 sec\n",
            "Epoch 72, Loss(train/val) 1.13067/0.44086. Took 0.05 sec\n",
            "Epoch 73, Loss(train/val) 1.12680/0.44078. Took 0.05 sec\n",
            "Epoch 74, Loss(train/val) 1.12856/0.44028. Took 0.05 sec\n",
            "Epoch 75, Loss(train/val) 1.12877/0.43960. Took 0.05 sec\n",
            "Epoch 76, Loss(train/val) 1.13079/0.43932. Took 0.04 sec\n",
            "Epoch 77, Loss(train/val) 1.12888/0.43928. Took 0.04 sec\n",
            "Epoch 78, Loss(train/val) 1.12794/0.44014. Took 0.05 sec\n",
            "Epoch 79, Loss(train/val) 1.13344/0.44044. Took 0.05 sec\n",
            "Epoch 80, Loss(train/val) 1.12796/0.43971. Took 0.05 sec\n",
            "Epoch 81, Loss(train/val) 1.13152/0.43863. Took 0.05 sec\n",
            "Epoch 82, Loss(train/val) 1.13031/0.43767. Took 0.05 sec\n",
            "Epoch 83, Loss(train/val) 1.12742/0.43776. Took 0.05 sec\n",
            "Epoch 84, Loss(train/val) 1.11278/0.43760. Took 0.05 sec\n",
            "Epoch 85, Loss(train/val) 1.12671/0.43677. Took 0.05 sec\n",
            "Epoch 86, Loss(train/val) 1.10979/0.43642. Took 0.04 sec\n",
            "Epoch 87, Loss(train/val) 1.12458/0.43568. Took 0.05 sec\n",
            "Epoch 88, Loss(train/val) 1.12274/0.43434. Took 0.06 sec\n",
            "Epoch 89, Loss(train/val) 1.11968/0.43203. Took 0.05 sec\n",
            "Epoch 90, Loss(train/val) 1.11577/0.42933. Took 0.05 sec\n",
            "Epoch 91, Loss(train/val) 1.12132/0.42740. Took 0.05 sec\n",
            "Epoch 92, Loss(train/val) 1.11523/0.42567. Took 0.05 sec\n",
            "Epoch 93, Loss(train/val) 1.12340/0.42455. Took 0.05 sec\n",
            "Epoch 94, Loss(train/val) 1.12240/0.42351. Took 0.05 sec\n",
            "Epoch 95, Loss(train/val) 1.11751/0.42169. Took 0.05 sec\n",
            "Epoch 96, Loss(train/val) 1.11534/0.41983. Took 0.05 sec\n",
            "Epoch 97, Loss(train/val) 1.11235/0.41722. Took 0.05 sec\n",
            "Epoch 98, Loss(train/val) 1.11367/0.41546. Took 0.05 sec\n",
            "Epoch 99, Loss(train/val) 1.11492/0.41426. Took 0.04 sec\n",
            "Epoch 100, Loss(train/val) 1.10600/0.41283. Took 0.04 sec\n",
            "Epoch 101, Loss(train/val) 1.10534/0.41035. Took 0.05 sec\n",
            "Epoch 102, Loss(train/val) 1.09880/0.40775. Took 0.05 sec\n",
            "Epoch 103, Loss(train/val) 1.10856/0.40468. Took 0.05 sec\n",
            "Epoch 104, Loss(train/val) 1.10144/0.40157. Took 0.04 sec\n",
            "Epoch 105, Loss(train/val) 1.10286/0.39808. Took 0.05 sec\n",
            "Epoch 106, Loss(train/val) 1.10310/0.39510. Took 0.05 sec\n",
            "Epoch 107, Loss(train/val) 1.09482/0.39169. Took 0.05 sec\n",
            "Epoch 108, Loss(train/val) 1.09167/0.38821. Took 0.05 sec\n",
            "Epoch 109, Loss(train/val) 1.08998/0.38510. Took 0.05 sec\n",
            "Epoch 110, Loss(train/val) 1.09251/0.38254. Took 0.04 sec\n",
            "Epoch 111, Loss(train/val) 1.09141/0.38008. Took 0.05 sec\n",
            "Epoch 112, Loss(train/val) 1.09375/0.37726. Took 0.06 sec\n",
            "Epoch 113, Loss(train/val) 1.08960/0.37393. Took 0.05 sec\n",
            "Epoch 114, Loss(train/val) 1.08302/0.37006. Took 0.05 sec\n",
            "Epoch 115, Loss(train/val) 1.07655/0.36651. Took 0.04 sec\n",
            "Epoch 116, Loss(train/val) 1.08149/0.36323. Took 0.04 sec\n",
            "Epoch 117, Loss(train/val) 1.07316/0.36039. Took 0.06 sec\n",
            "Epoch 118, Loss(train/val) 1.06615/0.35761. Took 0.04 sec\n",
            "Epoch 119, Loss(train/val) 1.06182/0.35510. Took 0.05 sec\n",
            "Epoch 120, Loss(train/val) 1.05237/0.35244. Took 0.05 sec\n",
            "Epoch 121, Loss(train/val) 1.06035/0.35021. Took 0.04 sec\n",
            "Epoch 122, Loss(train/val) 1.05206/0.34789. Took 0.05 sec\n",
            "Epoch 123, Loss(train/val) 1.04562/0.34512. Took 0.05 sec\n",
            "Epoch 124, Loss(train/val) 1.03603/0.34200. Took 0.05 sec\n",
            "Epoch 125, Loss(train/val) 1.04824/0.33791. Took 0.05 sec\n",
            "Epoch 126, Loss(train/val) 1.03368/0.33495. Took 0.05 sec\n",
            "Epoch 127, Loss(train/val) 1.03189/0.33257. Took 0.05 sec\n",
            "Epoch 128, Loss(train/val) 1.02863/0.33105. Took 0.05 sec\n",
            "Epoch 129, Loss(train/val) 1.03315/0.33022. Took 0.04 sec\n",
            "Epoch 130, Loss(train/val) 1.02143/0.32866. Took 0.04 sec\n",
            "Epoch 131, Loss(train/val) 1.00866/0.32604. Took 0.05 sec\n",
            "Epoch 132, Loss(train/val) 1.02302/0.32312. Took 0.05 sec\n",
            "Epoch 133, Loss(train/val) 0.99700/0.32009. Took 0.05 sec\n",
            "Epoch 134, Loss(train/val) 1.00917/0.31706. Took 0.04 sec\n",
            "Epoch 135, Loss(train/val) 1.01079/0.31546. Took 0.04 sec\n",
            "Epoch 136, Loss(train/val) 0.98799/0.31499. Took 0.04 sec\n",
            "Epoch 137, Loss(train/val) 0.98924/0.31459. Took 0.05 sec\n",
            "Epoch 138, Loss(train/val) 0.97515/0.31439. Took 0.05 sec\n",
            "Epoch 139, Loss(train/val) 0.97310/0.31581. Took 0.05 sec\n",
            "Epoch 140, Loss(train/val) 0.98890/0.31756. Took 0.04 sec\n",
            "Epoch 141, Loss(train/val) 0.98086/0.32208. Took 0.05 sec\n",
            "Epoch 142, Loss(train/val) 0.96818/0.32463. Took 0.05 sec\n",
            "Epoch 143, Loss(train/val) 0.95859/0.32458. Took 0.05 sec\n",
            "Epoch 144, Loss(train/val) 0.96360/0.32177. Took 0.05 sec\n",
            "Epoch 145, Loss(train/val) 0.96474/0.32079. Took 0.06 sec\n",
            "Epoch 146, Loss(train/val) 0.95609/0.32028. Took 0.05 sec\n",
            "Epoch 147, Loss(train/val) 0.95260/0.32100. Took 0.05 sec\n",
            "Epoch 148, Loss(train/val) 0.95349/0.31924. Took 0.05 sec\n",
            "Epoch 149, Loss(train/val) 0.95258/0.31592. Took 0.05 sec\n",
            "Epoch 150, Loss(train/val) 0.95026/0.31545. Took 0.05 sec\n",
            "Epoch 151, Loss(train/val) 0.95261/0.31420. Took 0.05 sec\n",
            "Epoch 152, Loss(train/val) 0.93502/0.31253. Took 0.04 sec\n",
            "Epoch 153, Loss(train/val) 0.94038/0.31351. Took 0.05 sec\n",
            "Epoch 154, Loss(train/val) 0.94635/0.31457. Took 0.05 sec\n",
            "Epoch 155, Loss(train/val) 0.94445/0.31682. Took 0.05 sec\n",
            "Epoch 156, Loss(train/val) 0.94959/0.31892. Took 0.05 sec\n",
            "Epoch 157, Loss(train/val) 0.95081/0.32263. Took 0.05 sec\n",
            "Epoch 158, Loss(train/val) 0.93132/0.32492. Took 0.05 sec\n",
            "Epoch 159, Loss(train/val) 0.91895/0.32289. Took 0.05 sec\n",
            "Epoch 160, Loss(train/val) 0.92747/0.32067. Took 0.04 sec\n",
            "Epoch 161, Loss(train/val) 0.92214/0.32106. Took 0.05 sec\n",
            "Epoch 162, Loss(train/val) 0.93413/0.32591. Took 0.04 sec\n",
            "Epoch 163, Loss(train/val) 0.91601/0.32958. Took 0.04 sec\n",
            "Epoch 164, Loss(train/val) 0.91908/0.33014. Took 0.05 sec\n",
            "Epoch 165, Loss(train/val) 0.88712/0.32982. Took 0.05 sec\n",
            "Epoch 166, Loss(train/val) 0.90899/0.32699. Took 0.05 sec\n",
            "Epoch 167, Loss(train/val) 0.92021/0.32384. Took 0.04 sec\n",
            "Epoch 168, Loss(train/val) 0.91052/0.31687. Took 0.05 sec\n",
            "Epoch 169, Loss(train/val) 0.92352/0.31115. Took 0.04 sec\n",
            "Epoch 170, Loss(train/val) 0.91286/0.31127. Took 0.05 sec\n",
            "Epoch 171, Loss(train/val) 0.91898/0.30997. Took 0.06 sec\n",
            "Epoch 172, Loss(train/val) 0.90256/0.31035. Took 0.05 sec\n",
            "Epoch 173, Loss(train/val) 0.90067/0.31212. Took 0.05 sec\n",
            "Epoch 174, Loss(train/val) 0.89513/0.30626. Took 0.05 sec\n",
            "Epoch 175, Loss(train/val) 0.90296/0.30326. Took 0.04 sec\n",
            "Epoch 176, Loss(train/val) 0.89480/0.30321. Took 0.05 sec\n",
            "Epoch 177, Loss(train/val) 0.90301/0.30484. Took 0.05 sec\n",
            "Epoch 178, Loss(train/val) 0.89975/0.30899. Took 0.04 sec\n",
            "Epoch 179, Loss(train/val) 0.90188/0.31374. Took 0.05 sec\n",
            "Epoch 180, Loss(train/val) 0.89975/0.31480. Took 0.05 sec\n",
            "Epoch 181, Loss(train/val) 0.89381/0.31548. Took 0.06 sec\n",
            "Epoch 182, Loss(train/val) 0.89569/0.31549. Took 0.05 sec\n",
            "Epoch 183, Loss(train/val) 0.88535/0.31418. Took 0.05 sec\n",
            "Epoch 184, Loss(train/val) 0.86275/0.31046. Took 0.05 sec\n",
            "Epoch 185, Loss(train/val) 0.88748/0.30817. Took 0.04 sec\n",
            "Epoch 186, Loss(train/val) 0.89037/0.30583. Took 0.05 sec\n",
            "Epoch 187, Loss(train/val) 0.87441/0.30784. Took 0.05 sec\n",
            "Epoch 188, Loss(train/val) 0.88335/0.30822. Took 0.05 sec\n",
            "Epoch 189, Loss(train/val) 0.88210/0.30335. Took 0.04 sec\n",
            "Epoch 190, Loss(train/val) 0.87497/0.29499. Took 0.06 sec\n",
            "Epoch 191, Loss(train/val) 0.86912/0.28978. Took 0.05 sec\n",
            "Epoch 192, Loss(train/val) 0.88258/0.28761. Took 0.04 sec\n",
            "Epoch 193, Loss(train/val) 0.88093/0.28740. Took 0.04 sec\n",
            "Epoch 194, Loss(train/val) 0.88195/0.28816. Took 0.05 sec\n",
            "Epoch 195, Loss(train/val) 0.87172/0.29083. Took 0.06 sec\n",
            "Epoch 196, Loss(train/val) 0.86310/0.29345. Took 0.05 sec\n",
            "Epoch 197, Loss(train/val) 0.88885/0.29972. Took 0.04 sec\n",
            "Epoch 198, Loss(train/val) 0.86677/0.30420. Took 0.04 sec\n",
            "Epoch 199, Loss(train/val) 0.86900/0.30858. Took 0.05 sec\n",
            "Epoch 200, Loss(train/val) 0.85232/0.31311. Took 0.05 sec\n",
            "Epoch 201, Loss(train/val) 0.83919/0.31384. Took 0.05 sec\n",
            "Epoch 202, Loss(train/val) 0.84839/0.31463. Took 0.05 sec\n",
            "Epoch 203, Loss(train/val) 0.87770/0.31618. Took 0.04 sec\n",
            "Epoch 204, Loss(train/val) 0.87105/0.32125. Took 0.05 sec\n",
            "Epoch 205, Loss(train/val) 0.85939/0.32460. Took 0.05 sec\n",
            "Epoch 206, Loss(train/val) 0.86524/0.32622. Took 0.05 sec\n",
            "Epoch 207, Loss(train/val) 0.84655/0.32619. Took 0.05 sec\n",
            "Epoch 208, Loss(train/val) 0.85918/0.32250. Took 0.04 sec\n",
            "Epoch 209, Loss(train/val) 0.86104/0.31984. Took 0.05 sec\n",
            "Epoch 210, Loss(train/val) 0.83353/0.31575. Took 0.05 sec\n",
            "Epoch 211, Loss(train/val) 0.86494/0.30909. Took 0.04 sec\n",
            "Epoch 212, Loss(train/val) 0.85815/0.30553. Took 0.04 sec\n",
            "Epoch 213, Loss(train/val) 0.85523/0.30158. Took 0.04 sec\n",
            "Epoch 214, Loss(train/val) 0.83524/0.29708. Took 0.05 sec\n",
            "Epoch 215, Loss(train/val) 0.83400/0.29684. Took 0.06 sec\n",
            "Epoch 216, Loss(train/val) 0.84414/0.29759. Took 0.04 sec\n",
            "Epoch 217, Loss(train/val) 0.82503/0.29467. Took 0.04 sec\n",
            "Epoch 218, Loss(train/val) 0.84395/0.29232. Took 0.05 sec\n",
            "Epoch 219, Loss(train/val) 0.81542/0.28845. Took 0.05 sec\n",
            "Epoch 220, Loss(train/val) 0.84052/0.27975. Took 0.05 sec\n",
            "Epoch 221, Loss(train/val) 0.83906/0.27309. Took 0.05 sec\n",
            "Epoch 222, Loss(train/val) 0.83988/0.27149. Took 0.04 sec\n",
            "Epoch 223, Loss(train/val) 0.82343/0.27117. Took 0.05 sec\n",
            "Epoch 224, Loss(train/val) 0.82016/0.27084. Took 0.04 sec\n",
            "Epoch 225, Loss(train/val) 0.83059/0.27121. Took 0.05 sec\n",
            "Epoch 226, Loss(train/val) 0.83309/0.26376. Took 0.05 sec\n",
            "Epoch 227, Loss(train/val) 0.84756/0.26117. Took 0.05 sec\n",
            "Epoch 228, Loss(train/val) 0.83199/0.26137. Took 0.04 sec\n",
            "Epoch 229, Loss(train/val) 0.83538/0.26272. Took 0.05 sec\n",
            "Epoch 230, Loss(train/val) 0.83694/0.26484. Took 0.05 sec\n",
            "Epoch 231, Loss(train/val) 0.81079/0.26781. Took 0.05 sec\n",
            "Epoch 232, Loss(train/val) 0.82192/0.27188. Took 0.04 sec\n",
            "Epoch 233, Loss(train/val) 0.81913/0.27341. Took 0.04 sec\n",
            "Epoch 234, Loss(train/val) 0.82410/0.27795. Took 0.04 sec\n",
            "Epoch 235, Loss(train/val) 0.82687/0.28373. Took 0.05 sec\n",
            "Epoch 236, Loss(train/val) 0.83249/0.29018. Took 0.05 sec\n",
            "Epoch 237, Loss(train/val) 0.82256/0.29421. Took 0.05 sec\n",
            "Epoch 238, Loss(train/val) 0.81827/0.29267. Took 0.05 sec\n",
            "Epoch 239, Loss(train/val) 0.80711/0.29257. Took 0.05 sec\n",
            "Epoch 240, Loss(train/val) 0.81151/0.29163. Took 0.05 sec\n",
            "Epoch 241, Loss(train/val) 0.79397/0.28887. Took 0.05 sec\n",
            "Epoch 242, Loss(train/val) 0.83378/0.28632. Took 0.05 sec\n",
            "Epoch 243, Loss(train/val) 0.81240/0.28596. Took 0.05 sec\n",
            "Epoch 244, Loss(train/val) 0.82084/0.28722. Took 0.05 sec\n",
            "Epoch 245, Loss(train/val) 0.81502/0.28251. Took 0.05 sec\n",
            "Epoch 246, Loss(train/val) 0.83867/0.28128. Took 0.04 sec\n",
            "Epoch 247, Loss(train/val) 0.80108/0.28234. Took 0.05 sec\n",
            "Epoch 248, Loss(train/val) 0.79375/0.28732. Took 0.04 sec\n",
            "Epoch 249, Loss(train/val) 0.83785/0.29382. Took 0.04 sec\n",
            "Epoch 250, Loss(train/val) 0.80286/0.29720. Took 0.05 sec\n",
            "Epoch 251, Loss(train/val) 0.81072/0.30121. Took 0.05 sec\n",
            "Epoch 252, Loss(train/val) 0.82259/0.30290. Took 0.05 sec\n",
            "Epoch 253, Loss(train/val) 0.80693/0.29753. Took 0.04 sec\n",
            "Epoch 254, Loss(train/val) 0.79481/0.29111. Took 0.05 sec\n",
            "Epoch 255, Loss(train/val) 0.79701/0.29307. Took 0.05 sec\n",
            "Epoch 256, Loss(train/val) 0.80294/0.29228. Took 0.04 sec\n",
            "Epoch 257, Loss(train/val) 0.78999/0.29341. Took 0.04 sec\n",
            "Epoch 258, Loss(train/val) 0.80459/0.29271. Took 0.05 sec\n",
            "Epoch 259, Loss(train/val) 0.78704/0.29933. Took 0.04 sec\n",
            "Epoch 260, Loss(train/val) 0.78477/0.30545. Took 0.06 sec\n",
            "Epoch 261, Loss(train/val) 0.80969/0.31740. Took 0.05 sec\n",
            "Epoch 262, Loss(train/val) 0.80438/0.32481. Took 0.04 sec\n",
            "Epoch 263, Loss(train/val) 0.79592/0.31838. Took 0.04 sec\n",
            "Epoch 264, Loss(train/val) 0.80903/0.31000. Took 0.04 sec\n",
            "Epoch 265, Loss(train/val) 0.79265/0.30099. Took 0.05 sec\n",
            "Epoch 266, Loss(train/val) 0.79462/0.29824. Took 0.05 sec\n",
            "Epoch 267, Loss(train/val) 0.82437/0.29610. Took 0.05 sec\n",
            "Epoch 268, Loss(train/val) 0.77662/0.29003. Took 0.04 sec\n",
            "Epoch 269, Loss(train/val) 0.80937/0.28731. Took 0.04 sec\n",
            "Epoch 270, Loss(train/val) 0.79082/0.27972. Took 0.05 sec\n",
            "Epoch 271, Loss(train/val) 0.77068/0.27581. Took 0.05 sec\n",
            "Epoch 272, Loss(train/val) 0.78443/0.27204. Took 0.05 sec\n",
            "Epoch 273, Loss(train/val) 0.77973/0.26964. Took 0.04 sec\n",
            "Epoch 274, Loss(train/val) 0.79197/0.26908. Took 0.04 sec\n",
            "Epoch 275, Loss(train/val) 0.77689/0.26721. Took 0.05 sec\n",
            "Epoch 276, Loss(train/val) 0.77484/0.26839. Took 0.04 sec\n",
            "Epoch 277, Loss(train/val) 0.77135/0.26830. Took 0.04 sec\n",
            "Epoch 278, Loss(train/val) 0.79347/0.26981. Took 0.04 sec\n",
            "Epoch 279, Loss(train/val) 0.78289/0.26754. Took 0.05 sec\n",
            "Epoch 280, Loss(train/val) 0.77053/0.26722. Took 0.05 sec\n",
            "Epoch 281, Loss(train/val) 0.79210/0.26496. Took 0.05 sec\n",
            "Epoch 282, Loss(train/val) 0.77584/0.26160. Took 0.05 sec\n",
            "Epoch 283, Loss(train/val) 0.78767/0.26235. Took 0.05 sec\n",
            "Epoch 284, Loss(train/val) 0.80179/0.26042. Took 0.04 sec\n",
            "Epoch 285, Loss(train/val) 0.78043/0.26019. Took 0.05 sec\n",
            "Epoch 286, Loss(train/val) 0.77233/0.26110. Took 0.04 sec\n",
            "Epoch 287, Loss(train/val) 0.76379/0.26109. Took 0.05 sec\n",
            "Epoch 288, Loss(train/val) 0.78479/0.26372. Took 0.05 sec\n",
            "Epoch 289, Loss(train/val) 0.78587/0.26516. Took 0.05 sec\n",
            "Epoch 290, Loss(train/val) 0.77410/0.26107. Took 0.05 sec\n",
            "Epoch 291, Loss(train/val) 0.76672/0.25815. Took 0.05 sec\n",
            "Epoch 292, Loss(train/val) 0.77179/0.25650. Took 0.04 sec\n",
            "Epoch 293, Loss(train/val) 0.78341/0.25539. Took 0.06 sec\n",
            "Epoch 294, Loss(train/val) 0.75802/0.25281. Took 0.04 sec\n",
            "Epoch 295, Loss(train/val) 0.76032/0.24929. Took 0.05 sec\n",
            "Epoch 296, Loss(train/val) 0.76243/0.24782. Took 0.05 sec\n",
            "Epoch 297, Loss(train/val) 0.76288/0.24787. Took 0.04 sec\n",
            "Epoch 298, Loss(train/val) 0.79097/0.25309. Took 0.05 sec\n",
            "Epoch 299, Loss(train/val) 0.76109/0.25507. Took 0.04 sec\n",
            "Epoch 300, Loss(train/val) 0.75203/0.26269. Took 0.05 sec\n",
            "Epoch 301, Loss(train/val) 0.74852/0.26660. Took 0.05 sec\n",
            "Epoch 302, Loss(train/val) 0.76927/0.26478. Took 0.04 sec\n",
            "Epoch 303, Loss(train/val) 0.75433/0.26368. Took 0.04 sec\n",
            "Epoch 304, Loss(train/val) 0.75029/0.25561. Took 0.04 sec\n",
            "Epoch 305, Loss(train/val) 0.76078/0.25299. Took 0.05 sec\n",
            "Epoch 306, Loss(train/val) 0.76926/0.25269. Took 0.04 sec\n",
            "Epoch 307, Loss(train/val) 0.75479/0.25494. Took 0.05 sec\n",
            "Epoch 308, Loss(train/val) 0.77765/0.25672. Took 0.05 sec\n",
            "Epoch 309, Loss(train/val) 0.77702/0.25757. Took 0.05 sec\n",
            "Epoch 310, Loss(train/val) 0.77153/0.25935. Took 0.05 sec\n",
            "Epoch 311, Loss(train/val) 0.74566/0.26139. Took 0.05 sec\n",
            "Epoch 312, Loss(train/val) 0.73575/0.25290. Took 0.05 sec\n",
            "Epoch 313, Loss(train/val) 0.76546/0.24805. Took 0.05 sec\n",
            "Epoch 314, Loss(train/val) 0.77215/0.24779. Took 0.04 sec\n",
            "Epoch 315, Loss(train/val) 0.76474/0.25471. Took 0.05 sec\n",
            "Epoch 316, Loss(train/val) 0.74527/0.26209. Took 0.05 sec\n",
            "Epoch 317, Loss(train/val) 0.75437/0.26312. Took 0.05 sec\n",
            "Epoch 318, Loss(train/val) 0.76334/0.26477. Took 0.05 sec\n",
            "Epoch 319, Loss(train/val) 0.74889/0.26480. Took 0.05 sec\n",
            "Epoch 320, Loss(train/val) 0.74890/0.26125. Took 0.05 sec\n",
            "Epoch 321, Loss(train/val) 0.78221/0.25578. Took 0.05 sec\n",
            "Epoch 322, Loss(train/val) 0.75521/0.25365. Took 0.04 sec\n",
            "Epoch 323, Loss(train/val) 0.73855/0.25379. Took 0.04 sec\n",
            "Epoch 324, Loss(train/val) 0.74687/0.25935. Took 0.05 sec\n",
            "Epoch 325, Loss(train/val) 0.76625/0.26237. Took 0.05 sec\n",
            "Epoch 326, Loss(train/val) 0.73332/0.26554. Took 0.04 sec\n",
            "Epoch 327, Loss(train/val) 0.75893/0.26349. Took 0.04 sec\n",
            "Epoch 328, Loss(train/val) 0.75680/0.26176. Took 0.04 sec\n",
            "Epoch 329, Loss(train/val) 0.75374/0.26078. Took 0.05 sec\n",
            "Epoch 330, Loss(train/val) 0.75084/0.25797. Took 0.06 sec\n",
            "Epoch 331, Loss(train/val) 0.74914/0.25833. Took 0.05 sec\n",
            "Epoch 332, Loss(train/val) 0.75736/0.25238. Took 0.04 sec\n",
            "Epoch 333, Loss(train/val) 0.74072/0.25112. Took 0.05 sec\n",
            "Epoch 334, Loss(train/val) 0.75011/0.24668. Took 0.04 sec\n",
            "Epoch 335, Loss(train/val) 0.75231/0.24645. Took 0.06 sec\n",
            "Epoch 336, Loss(train/val) 0.74326/0.25008. Took 0.05 sec\n",
            "Epoch 337, Loss(train/val) 0.73648/0.25315. Took 0.05 sec\n",
            "Epoch 338, Loss(train/val) 0.71651/0.25552. Took 0.05 sec\n",
            "Epoch 339, Loss(train/val) 0.73558/0.25410. Took 0.05 sec\n",
            "Epoch 340, Loss(train/val) 0.76555/0.25459. Took 0.06 sec\n",
            "Epoch 341, Loss(train/val) 0.74237/0.25677. Took 0.05 sec\n",
            "Epoch 342, Loss(train/val) 0.74409/0.25934. Took 0.05 sec\n",
            "Epoch 343, Loss(train/val) 0.73342/0.26379. Took 0.04 sec\n",
            "Epoch 344, Loss(train/val) 0.72570/0.26846. Took 0.04 sec\n",
            "Epoch 345, Loss(train/val) 0.74766/0.26939. Took 0.05 sec\n",
            "Epoch 346, Loss(train/val) 0.76752/0.27080. Took 0.04 sec\n",
            "Epoch 347, Loss(train/val) 0.73335/0.27439. Took 0.05 sec\n",
            "Epoch 348, Loss(train/val) 0.71203/0.27537. Took 0.04 sec\n",
            "Epoch 349, Loss(train/val) 0.74458/0.27045. Took 0.04 sec\n",
            "Epoch 350, Loss(train/val) 0.72543/0.25771. Took 0.05 sec\n",
            "Epoch 351, Loss(train/val) 0.72867/0.24924. Took 0.05 sec\n",
            "Epoch 352, Loss(train/val) 0.73238/0.24467. Took 0.05 sec\n",
            "Epoch 353, Loss(train/val) 0.73415/0.24060. Took 0.05 sec\n",
            "Epoch 354, Loss(train/val) 0.74714/0.23828. Took 0.05 sec\n",
            "Epoch 355, Loss(train/val) 0.74327/0.23744. Took 0.05 sec\n",
            "Epoch 356, Loss(train/val) 0.71789/0.23677. Took 0.05 sec\n",
            "Epoch 357, Loss(train/val) 0.74108/0.23926. Took 0.05 sec\n",
            "Epoch 358, Loss(train/val) 0.73637/0.23962. Took 0.05 sec\n",
            "Epoch 359, Loss(train/val) 0.73707/0.23912. Took 0.05 sec\n",
            "Epoch 360, Loss(train/val) 0.71935/0.23910. Took 0.05 sec\n",
            "Epoch 361, Loss(train/val) 0.75666/0.23895. Took 0.04 sec\n",
            "Epoch 362, Loss(train/val) 0.72894/0.23753. Took 0.04 sec\n",
            "Epoch 363, Loss(train/val) 0.69393/0.23915. Took 0.05 sec\n",
            "Epoch 364, Loss(train/val) 0.74817/0.23983. Took 0.04 sec\n",
            "Epoch 365, Loss(train/val) 0.73589/0.24259. Took 0.05 sec\n",
            "Epoch 366, Loss(train/val) 0.72037/0.24274. Took 0.05 sec\n",
            "Epoch 367, Loss(train/val) 0.73674/0.24611. Took 0.04 sec\n",
            "Epoch 368, Loss(train/val) 0.71442/0.24758. Took 0.04 sec\n",
            "Epoch 369, Loss(train/val) 0.73622/0.25097. Took 0.04 sec\n",
            "Epoch 370, Loss(train/val) 0.70323/0.25297. Took 0.05 sec\n",
            "Epoch 371, Loss(train/val) 0.71906/0.25174. Took 0.05 sec\n",
            "Epoch 372, Loss(train/val) 0.72908/0.24577. Took 0.05 sec\n",
            "Epoch 373, Loss(train/val) 0.72492/0.24166. Took 0.05 sec\n",
            "Epoch 374, Loss(train/val) 0.70500/0.24048. Took 0.04 sec\n",
            "Epoch 375, Loss(train/val) 0.72930/0.24043. Took 0.06 sec\n",
            "Epoch 376, Loss(train/val) 0.72528/0.24405. Took 0.05 sec\n",
            "Epoch 377, Loss(train/val) 0.72732/0.24813. Took 0.05 sec\n",
            "Epoch 378, Loss(train/val) 0.70560/0.25009. Took 0.05 sec\n",
            "Epoch 379, Loss(train/val) 0.70557/0.25131. Took 0.06 sec\n",
            "Epoch 380, Loss(train/val) 0.70452/0.25206. Took 0.05 sec\n",
            "Epoch 381, Loss(train/val) 0.71713/0.25245. Took 0.05 sec\n",
            "Epoch 382, Loss(train/val) 0.73059/0.25395. Took 0.05 sec\n",
            "Epoch 383, Loss(train/val) 0.72994/0.25900. Took 0.05 sec\n",
            "Epoch 384, Loss(train/val) 0.69382/0.25502. Took 0.05 sec\n",
            "Epoch 385, Loss(train/val) 0.72168/0.25622. Took 0.05 sec\n",
            "Epoch 386, Loss(train/val) 0.69986/0.25686. Took 0.05 sec\n",
            "Epoch 387, Loss(train/val) 0.69788/0.25004. Took 0.05 sec\n",
            "Epoch 388, Loss(train/val) 0.71226/0.24490. Took 0.05 sec\n",
            "Epoch 389, Loss(train/val) 0.69555/0.23979. Took 0.05 sec\n",
            "Epoch 390, Loss(train/val) 0.72428/0.23582. Took 0.05 sec\n",
            "Epoch 391, Loss(train/val) 0.74200/0.23484. Took 0.05 sec\n",
            "Epoch 392, Loss(train/val) 0.69402/0.23539. Took 0.05 sec\n",
            "Epoch 393, Loss(train/val) 0.73268/0.23540. Took 0.06 sec\n",
            "Epoch 394, Loss(train/val) 0.72359/0.23608. Took 0.05 sec\n",
            "Epoch 395, Loss(train/val) 0.71995/0.23950. Took 0.05 sec\n",
            "Epoch 396, Loss(train/val) 0.71319/0.24261. Took 0.04 sec\n",
            "Epoch 397, Loss(train/val) 0.70693/0.24351. Took 0.05 sec\n",
            "Epoch 398, Loss(train/val) 0.70820/0.24357. Took 0.04 sec\n",
            "Epoch 399, Loss(train/val) 0.70934/0.24989. Took 0.06 sec\n",
            "Epoch 400, Loss(train/val) 0.71443/0.25861. Took 0.05 sec\n",
            "Epoch 401, Loss(train/val) 0.70924/0.25835. Took 0.05 sec\n",
            "Epoch 402, Loss(train/val) 0.71955/0.25485. Took 0.04 sec\n",
            "Epoch 403, Loss(train/val) 0.71067/0.24879. Took 0.05 sec\n",
            "Epoch 404, Loss(train/val) 0.68734/0.24252. Took 0.05 sec\n",
            "Epoch 405, Loss(train/val) 0.71185/0.23882. Took 0.05 sec\n",
            "Epoch 406, Loss(train/val) 0.72009/0.24101. Took 0.04 sec\n",
            "Epoch 407, Loss(train/val) 0.72275/0.24539. Took 0.04 sec\n",
            "Epoch 408, Loss(train/val) 0.69216/0.24435. Took 0.05 sec\n",
            "Epoch 409, Loss(train/val) 0.72181/0.24503. Took 0.05 sec\n",
            "Epoch 410, Loss(train/val) 0.73557/0.25063. Took 0.04 sec\n",
            "Epoch 411, Loss(train/val) 0.69236/0.25032. Took 0.04 sec\n",
            "Epoch 412, Loss(train/val) 0.69489/0.25300. Took 0.06 sec\n",
            "Epoch 413, Loss(train/val) 0.69953/0.25200. Took 0.05 sec\n",
            "Epoch 414, Loss(train/val) 0.69917/0.25112. Took 0.06 sec\n",
            "Epoch 415, Loss(train/val) 0.68019/0.24689. Took 0.04 sec\n",
            "Epoch 416, Loss(train/val) 0.70414/0.24553. Took 0.05 sec\n",
            "Epoch 417, Loss(train/val) 0.71446/0.24802. Took 0.05 sec\n",
            "Epoch 418, Loss(train/val) 0.70014/0.24647. Took 0.05 sec\n",
            "Epoch 419, Loss(train/val) 0.69187/0.25031. Took 0.05 sec\n",
            "Epoch 420, Loss(train/val) 0.71186/0.25017. Took 0.06 sec\n",
            "Epoch 421, Loss(train/val) 0.73100/0.24917. Took 0.05 sec\n",
            "Epoch 422, Loss(train/val) 0.71369/0.25070. Took 0.05 sec\n",
            "Epoch 423, Loss(train/val) 0.69144/0.25632. Took 0.06 sec\n",
            "Epoch 424, Loss(train/val) 0.68512/0.26402. Took 0.05 sec\n",
            "Epoch 425, Loss(train/val) 0.68438/0.26216. Took 0.04 sec\n",
            "Epoch 426, Loss(train/val) 0.70438/0.25912. Took 0.04 sec\n",
            "Epoch 427, Loss(train/val) 0.67898/0.25680. Took 0.04 sec\n",
            "Epoch 428, Loss(train/val) 0.70940/0.25399. Took 0.05 sec\n",
            "Epoch 429, Loss(train/val) 0.66781/0.25357. Took 0.05 sec\n",
            "Epoch 430, Loss(train/val) 0.71628/0.25218. Took 0.04 sec\n",
            "Epoch 431, Loss(train/val) 0.69649/0.25307. Took 0.04 sec\n",
            "Epoch 432, Loss(train/val) 0.69740/0.25137. Took 0.05 sec\n",
            "Epoch 433, Loss(train/val) 0.70103/0.24849. Took 0.05 sec\n",
            "Epoch 434, Loss(train/val) 0.68189/0.24585. Took 0.05 sec\n",
            "Epoch 435, Loss(train/val) 0.71071/0.24262. Took 0.06 sec\n",
            "Epoch 436, Loss(train/val) 0.67656/0.23874. Took 0.05 sec\n",
            "Epoch 437, Loss(train/val) 0.69372/0.23626. Took 0.05 sec\n",
            "Epoch 438, Loss(train/val) 0.71032/0.23849. Took 0.05 sec\n",
            "Epoch 439, Loss(train/val) 0.70651/0.24577. Took 0.05 sec\n",
            "Epoch 440, Loss(train/val) 0.70050/0.25392. Took 0.05 sec\n",
            "Epoch 441, Loss(train/val) 0.70290/0.25465. Took 0.05 sec\n",
            "Epoch 442, Loss(train/val) 0.70917/0.25669. Took 0.04 sec\n",
            "Epoch 443, Loss(train/val) 0.67421/0.25722. Took 0.05 sec\n",
            "Epoch 444, Loss(train/val) 0.70232/0.25787. Took 0.05 sec\n",
            "Epoch 445, Loss(train/val) 0.67807/0.25925. Took 0.05 sec\n",
            "Epoch 446, Loss(train/val) 0.71717/0.25972. Took 0.04 sec\n",
            "Epoch 447, Loss(train/val) 0.68672/0.25619. Took 0.04 sec\n",
            "Epoch 448, Loss(train/val) 0.67384/0.24984. Took 0.05 sec\n",
            "Epoch 449, Loss(train/val) 0.67973/0.24294. Took 0.04 sec\n",
            "Epoch 450, Loss(train/val) 0.67463/0.23735. Took 0.05 sec\n",
            "Epoch 451, Loss(train/val) 0.69055/0.23471. Took 0.04 sec\n",
            "Epoch 452, Loss(train/val) 0.68794/0.23517. Took 0.04 sec\n",
            "Epoch 453, Loss(train/val) 0.69797/0.23380. Took 0.05 sec\n",
            "Epoch 454, Loss(train/val) 0.68562/0.23292. Took 0.05 sec\n",
            "Epoch 455, Loss(train/val) 0.69163/0.23527. Took 0.05 sec\n",
            "Epoch 456, Loss(train/val) 0.68739/0.23812. Took 0.04 sec\n",
            "Epoch 457, Loss(train/val) 0.68655/0.24050. Took 0.05 sec\n",
            "Epoch 458, Loss(train/val) 0.70923/0.24628. Took 0.05 sec\n",
            "Epoch 459, Loss(train/val) 0.69427/0.24597. Took 0.04 sec\n",
            "Epoch 460, Loss(train/val) 0.68912/0.24224. Took 0.05 sec\n",
            "Epoch 461, Loss(train/val) 0.69848/0.24049. Took 0.05 sec\n",
            "Epoch 462, Loss(train/val) 0.67170/0.24413. Took 0.05 sec\n",
            "Epoch 463, Loss(train/val) 0.67853/0.24769. Took 0.05 sec\n",
            "Epoch 464, Loss(train/val) 0.67081/0.24711. Took 0.05 sec\n",
            "Epoch 465, Loss(train/val) 0.68600/0.24632. Took 0.04 sec\n",
            "Epoch 466, Loss(train/val) 0.70589/0.23941. Took 0.05 sec\n",
            "Epoch 467, Loss(train/val) 0.67928/0.23292. Took 0.04 sec\n",
            "Epoch 468, Loss(train/val) 0.68284/0.22948. Took 0.05 sec\n",
            "Epoch 469, Loss(train/val) 0.69721/0.22945. Took 0.04 sec\n",
            "Epoch 470, Loss(train/val) 0.69459/0.22944. Took 0.04 sec\n",
            "Epoch 471, Loss(train/val) 0.65813/0.22981. Took 0.05 sec\n",
            "Epoch 472, Loss(train/val) 0.71809/0.23149. Took 0.05 sec\n",
            "Epoch 473, Loss(train/val) 0.67083/0.23509. Took 0.05 sec\n",
            "Epoch 474, Loss(train/val) 0.68917/0.23737. Took 0.05 sec\n",
            "Epoch 475, Loss(train/val) 0.69270/0.23704. Took 0.04 sec\n",
            "Epoch 476, Loss(train/val) 0.69887/0.23792. Took 0.05 sec\n",
            "Epoch 477, Loss(train/val) 0.69127/0.24186. Took 0.04 sec\n",
            "Epoch 478, Loss(train/val) 0.70327/0.24127. Took 0.06 sec\n",
            "Epoch 479, Loss(train/val) 0.64559/0.24031. Took 0.05 sec\n",
            "Epoch 480, Loss(train/val) 0.70504/0.24170. Took 0.04 sec\n",
            "Epoch 481, Loss(train/val) 0.70199/0.24063. Took 0.05 sec\n",
            "Epoch 482, Loss(train/val) 0.67118/0.23833. Took 0.05 sec\n",
            "Epoch 483, Loss(train/val) 0.67244/0.24096. Took 0.05 sec\n",
            "Epoch 484, Loss(train/val) 0.66156/0.24272. Took 0.04 sec\n",
            "Epoch 485, Loss(train/val) 0.70075/0.24495. Took 0.04 sec\n",
            "Epoch 486, Loss(train/val) 0.67893/0.24530. Took 0.05 sec\n",
            "Epoch 487, Loss(train/val) 0.68814/0.24310. Took 0.05 sec\n",
            "Epoch 488, Loss(train/val) 0.67526/0.24017. Took 0.05 sec\n",
            "Epoch 489, Loss(train/val) 0.68733/0.23641. Took 0.05 sec\n",
            "Epoch 490, Loss(train/val) 0.71290/0.23656. Took 0.05 sec\n",
            "Epoch 491, Loss(train/val) 0.67808/0.23624. Took 0.04 sec\n",
            "Epoch 492, Loss(train/val) 0.67834/0.23693. Took 0.04 sec\n",
            "Epoch 493, Loss(train/val) 0.67686/0.23631. Took 0.05 sec\n",
            "Epoch 494, Loss(train/val) 0.68597/0.23476. Took 0.04 sec\n",
            "Epoch 495, Loss(train/val) 0.67752/0.23446. Took 0.04 sec\n",
            "Epoch 496, Loss(train/val) 0.66410/0.23679. Took 0.04 sec\n",
            "Epoch 497, Loss(train/val) 0.66393/0.23781. Took 0.04 sec\n",
            "Epoch 498, Loss(train/val) 0.67632/0.23747. Took 0.05 sec\n",
            "Epoch 499, Loss(train/val) 0.67057/0.23661. Took 0.05 sec\n",
            "Epoch 500, Loss(train/val) 0.67307/0.23371. Took 0.06 sec\n",
            "Epoch 501, Loss(train/val) 0.69105/0.23534. Took 0.05 sec\n",
            "Epoch 502, Loss(train/val) 0.67923/0.23815. Took 0.04 sec\n",
            "Epoch 503, Loss(train/val) 0.69096/0.23900. Took 0.05 sec\n",
            "Epoch 504, Loss(train/val) 0.68622/0.23817. Took 0.05 sec\n",
            "Epoch 505, Loss(train/val) 0.69100/0.24060. Took 0.05 sec\n",
            "Epoch 506, Loss(train/val) 0.66766/0.24456. Took 0.05 sec\n",
            "Epoch 507, Loss(train/val) 0.67731/0.24474. Took 0.05 sec\n",
            "Epoch 508, Loss(train/val) 0.67622/0.24453. Took 0.05 sec\n",
            "Epoch 509, Loss(train/val) 0.67537/0.24161. Took 0.05 sec\n",
            "Epoch 510, Loss(train/val) 0.67263/0.24555. Took 0.04 sec\n",
            "Epoch 511, Loss(train/val) 0.65609/0.24802. Took 0.05 sec\n",
            "Epoch 512, Loss(train/val) 0.65444/0.24588. Took 0.05 sec\n",
            "Epoch 513, Loss(train/val) 0.66803/0.24051. Took 0.05 sec\n",
            "Epoch 514, Loss(train/val) 0.69213/0.23921. Took 0.05 sec\n",
            "Epoch 515, Loss(train/val) 0.65530/0.23493. Took 0.05 sec\n",
            "Epoch 516, Loss(train/val) 0.65683/0.23556. Took 0.05 sec\n",
            "Epoch 517, Loss(train/val) 0.67314/0.23823. Took 0.05 sec\n",
            "Epoch 518, Loss(train/val) 0.68372/0.24156. Took 0.06 sec\n",
            "Epoch 519, Loss(train/val) 0.65112/0.24112. Took 0.05 sec\n",
            "Epoch 520, Loss(train/val) 0.66475/0.23873. Took 0.05 sec\n",
            "Epoch 521, Loss(train/val) 0.67605/0.23488. Took 0.05 sec\n",
            "Epoch 522, Loss(train/val) 0.65157/0.23528. Took 0.05 sec\n",
            "Epoch 523, Loss(train/val) 0.66077/0.23386. Took 0.06 sec\n",
            "Epoch 524, Loss(train/val) 0.66123/0.23256. Took 0.05 sec\n",
            "Epoch 525, Loss(train/val) 0.64784/0.23254. Took 0.05 sec\n",
            "Epoch 526, Loss(train/val) 0.64180/0.23555. Took 0.05 sec\n",
            "Epoch 527, Loss(train/val) 0.63959/0.24141. Took 0.04 sec\n",
            "Epoch 528, Loss(train/val) 0.65952/0.24361. Took 0.05 sec\n",
            "Epoch 529, Loss(train/val) 0.68960/0.24157. Took 0.05 sec\n",
            "Epoch 530, Loss(train/val) 0.67095/0.23928. Took 0.05 sec\n",
            "Epoch 531, Loss(train/val) 0.66533/0.23806. Took 0.05 sec\n",
            "Epoch 532, Loss(train/val) 0.65655/0.24027. Took 0.04 sec\n",
            "Epoch 533, Loss(train/val) 0.66284/0.24003. Took 0.05 sec\n",
            "Epoch 534, Loss(train/val) 0.63376/0.23560. Took 0.05 sec\n",
            "Epoch 535, Loss(train/val) 0.68451/0.23111. Took 0.04 sec\n",
            "Epoch 536, Loss(train/val) 0.66589/0.22895. Took 0.04 sec\n",
            "Epoch 537, Loss(train/val) 0.66090/0.22849. Took 0.04 sec\n",
            "Epoch 538, Loss(train/val) 0.63835/0.22978. Took 0.05 sec\n",
            "Epoch 539, Loss(train/val) 0.67692/0.23065. Took 0.05 sec\n",
            "Epoch 540, Loss(train/val) 0.67046/0.23133. Took 0.05 sec\n",
            "Epoch 541, Loss(train/val) 0.62857/0.23298. Took 0.04 sec\n",
            "Epoch 542, Loss(train/val) 0.65692/0.23541. Took 0.06 sec\n",
            "Epoch 543, Loss(train/val) 0.66958/0.23434. Took 0.05 sec\n",
            "Epoch 544, Loss(train/val) 0.63297/0.23238. Took 0.05 sec\n",
            "Epoch 545, Loss(train/val) 0.66073/0.23081. Took 0.04 sec\n",
            "Epoch 546, Loss(train/val) 0.66209/0.23024. Took 0.04 sec\n",
            "Epoch 547, Loss(train/val) 0.64432/0.23011. Took 0.04 sec\n",
            "Epoch 548, Loss(train/val) 0.64801/0.22943. Took 0.05 sec\n",
            "Epoch 549, Loss(train/val) 0.63767/0.22877. Took 0.05 sec\n",
            "Epoch 550, Loss(train/val) 0.68579/0.22858. Took 0.04 sec\n",
            "Epoch 551, Loss(train/val) 0.67992/0.22837. Took 0.05 sec\n",
            "Epoch 552, Loss(train/val) 0.65330/0.22888. Took 0.04 sec\n",
            "Epoch 553, Loss(train/val) 0.64621/0.23055. Took 0.05 sec\n",
            "Epoch 554, Loss(train/val) 0.64831/0.23206. Took 0.04 sec\n",
            "Epoch 555, Loss(train/val) 0.65466/0.23668. Took 0.05 sec\n",
            "Epoch 556, Loss(train/val) 0.65731/0.24158. Took 0.05 sec\n",
            "Epoch 557, Loss(train/val) 0.64538/0.24881. Took 0.04 sec\n",
            "Epoch 558, Loss(train/val) 0.66496/0.24543. Took 0.05 sec\n",
            "Epoch 559, Loss(train/val) 0.65382/0.23925. Took 0.05 sec\n",
            "Epoch 560, Loss(train/val) 0.67490/0.23698. Took 0.04 sec\n",
            "Epoch 561, Loss(train/val) 0.61606/0.24011. Took 0.05 sec\n",
            "Epoch 562, Loss(train/val) 0.66638/0.24193. Took 0.05 sec\n",
            "Epoch 563, Loss(train/val) 0.65448/0.24049. Took 0.05 sec\n",
            "Epoch 564, Loss(train/val) 0.67876/0.23804. Took 0.05 sec\n",
            "Epoch 565, Loss(train/val) 0.63944/0.23184. Took 0.05 sec\n",
            "Epoch 566, Loss(train/val) 0.67076/0.22961. Took 0.05 sec\n",
            "Epoch 567, Loss(train/val) 0.61503/0.22944. Took 0.04 sec\n",
            "Epoch 568, Loss(train/val) 0.64975/0.22949. Took 0.05 sec\n",
            "Epoch 569, Loss(train/val) 0.63179/0.22849. Took 0.05 sec\n",
            "Epoch 570, Loss(train/val) 0.63100/0.22693. Took 0.05 sec\n",
            "Epoch 571, Loss(train/val) 0.67749/0.22636. Took 0.05 sec\n",
            "Epoch 572, Loss(train/val) 0.64350/0.22610. Took 0.05 sec\n",
            "Epoch 573, Loss(train/val) 0.66782/0.22629. Took 0.05 sec\n",
            "Epoch 574, Loss(train/val) 0.66088/0.22785. Took 0.05 sec\n",
            "Epoch 575, Loss(train/val) 0.64737/0.22809. Took 0.05 sec\n",
            "Epoch 576, Loss(train/val) 0.63968/0.22989. Took 0.05 sec\n",
            "Epoch 577, Loss(train/val) 0.68315/0.23341. Took 0.05 sec\n",
            "Epoch 578, Loss(train/val) 0.64220/0.24003. Took 0.05 sec\n",
            "Epoch 579, Loss(train/val) 0.64762/0.24752. Took 0.05 sec\n",
            "Epoch 580, Loss(train/val) 0.62995/0.25563. Took 0.05 sec\n",
            "Epoch 581, Loss(train/val) 0.64593/0.25769. Took 0.05 sec\n",
            "Epoch 582, Loss(train/val) 0.64954/0.25816. Took 0.05 sec\n",
            "Epoch 583, Loss(train/val) 0.63267/0.26047. Took 0.05 sec\n",
            "Epoch 584, Loss(train/val) 0.65916/0.25870. Took 0.05 sec\n",
            "Epoch 585, Loss(train/val) 0.63575/0.25448. Took 0.05 sec\n",
            "Epoch 586, Loss(train/val) 0.67259/0.25116. Took 0.05 sec\n",
            "Epoch 587, Loss(train/val) 0.64894/0.24321. Took 0.05 sec\n",
            "Epoch 588, Loss(train/val) 0.61743/0.23610. Took 0.05 sec\n",
            "Epoch 589, Loss(train/val) 0.64470/0.23394. Took 0.05 sec\n",
            "Epoch 590, Loss(train/val) 0.63120/0.23006. Took 0.05 sec\n",
            "Epoch 591, Loss(train/val) 0.67416/0.22811. Took 0.05 sec\n",
            "Epoch 592, Loss(train/val) 0.64279/0.22719. Took 0.05 sec\n",
            "Epoch 593, Loss(train/val) 0.64746/0.22704. Took 0.04 sec\n",
            "Epoch 594, Loss(train/val) 0.67348/0.22689. Took 0.04 sec\n",
            "Epoch 595, Loss(train/val) 0.64335/0.22690. Took 0.05 sec\n",
            "Epoch 596, Loss(train/val) 0.65595/0.22688. Took 0.04 sec\n",
            "Epoch 597, Loss(train/val) 0.65654/0.22697. Took 0.05 sec\n",
            "Epoch 598, Loss(train/val) 0.62581/0.22745. Took 0.04 sec\n",
            "Epoch 599, Loss(train/val) 0.64948/0.22793. Took 0.04 sec\n",
            "Epoch 600, Loss(train/val) 0.63257/0.22799. Took 0.05 sec\n",
            "Epoch 601, Loss(train/val) 0.65624/0.22792. Took 0.05 sec\n",
            "Epoch 602, Loss(train/val) 0.64914/0.22803. Took 0.06 sec\n",
            "Epoch 603, Loss(train/val) 0.64959/0.22867. Took 0.04 sec\n",
            "Epoch 604, Loss(train/val) 0.67273/0.23176. Took 0.04 sec\n",
            "Epoch 605, Loss(train/val) 0.64302/0.23524. Took 0.05 sec\n",
            "Epoch 606, Loss(train/val) 0.60322/0.23599. Took 0.06 sec\n",
            "Epoch 607, Loss(train/val) 0.62718/0.23519. Took 0.05 sec\n",
            "Epoch 608, Loss(train/val) 0.63082/0.23209. Took 0.05 sec\n",
            "Epoch 609, Loss(train/val) 0.63650/0.23081. Took 0.04 sec\n",
            "Epoch 610, Loss(train/val) 0.60954/0.22959. Took 0.05 sec\n",
            "Epoch 611, Loss(train/val) 0.62547/0.22915. Took 0.04 sec\n",
            "Epoch 612, Loss(train/val) 0.63377/0.22913. Took 0.05 sec\n",
            "Epoch 613, Loss(train/val) 0.63390/0.22734. Took 0.05 sec\n",
            "Epoch 614, Loss(train/val) 0.65492/0.22668. Took 0.05 sec\n",
            "Epoch 615, Loss(train/val) 0.62938/0.22677. Took 0.04 sec\n",
            "Epoch 616, Loss(train/val) 0.66510/0.22840. Took 0.04 sec\n",
            "Epoch 617, Loss(train/val) 0.62198/0.22985. Took 0.05 sec\n",
            "Epoch 618, Loss(train/val) 0.63735/0.23321. Took 0.05 sec\n",
            "Epoch 619, Loss(train/val) 0.65323/0.23696. Took 0.05 sec\n",
            "Epoch 620, Loss(train/val) 0.64434/0.23815. Took 0.05 sec\n",
            "Epoch 621, Loss(train/val) 0.63342/0.23706. Took 0.05 sec\n",
            "Epoch 622, Loss(train/val) 0.60537/0.23205. Took 0.05 sec\n",
            "Epoch 623, Loss(train/val) 0.63305/0.23182. Took 0.05 sec\n",
            "Epoch 624, Loss(train/val) 0.63925/0.22943. Took 0.05 sec\n",
            "Epoch 625, Loss(train/val) 0.60999/0.22963. Took 0.04 sec\n",
            "Epoch 626, Loss(train/val) 0.66449/0.23008. Took 0.05 sec\n",
            "Epoch 627, Loss(train/val) 0.64448/0.23067. Took 0.06 sec\n",
            "Epoch 628, Loss(train/val) 0.63000/0.22957. Took 0.05 sec\n",
            "Epoch 629, Loss(train/val) 0.64358/0.22950. Took 0.04 sec\n",
            "Epoch 630, Loss(train/val) 0.69351/0.22852. Took 0.06 sec\n",
            "Epoch 631, Loss(train/val) 0.63741/0.22781. Took 0.05 sec\n",
            "Epoch 632, Loss(train/val) 0.64742/0.22762. Took 0.05 sec\n",
            "Epoch 633, Loss(train/val) 0.63239/0.22714. Took 0.05 sec\n",
            "Epoch 634, Loss(train/val) 0.63378/0.22710. Took 0.05 sec\n",
            "Epoch 635, Loss(train/val) 0.64733/0.22731. Took 0.04 sec\n",
            "Epoch 636, Loss(train/val) 0.64188/0.22867. Took 0.04 sec\n",
            "Epoch 637, Loss(train/val) 0.65674/0.22684. Took 0.05 sec\n",
            "Epoch 638, Loss(train/val) 0.63089/0.22564. Took 0.05 sec\n",
            "Epoch 639, Loss(train/val) 0.65363/0.22515. Took 0.04 sec\n",
            "Epoch 640, Loss(train/val) 0.66945/0.22483. Took 0.04 sec\n",
            "Epoch 641, Loss(train/val) 0.65604/0.22485. Took 0.04 sec\n",
            "Epoch 642, Loss(train/val) 0.66663/0.22590. Took 0.05 sec\n",
            "Epoch 643, Loss(train/val) 0.66142/0.22718. Took 0.05 sec\n",
            "Epoch 644, Loss(train/val) 0.63481/0.22725. Took 0.04 sec\n",
            "Epoch 645, Loss(train/val) 0.64866/0.22659. Took 0.05 sec\n",
            "Epoch 646, Loss(train/val) 0.63682/0.22630. Took 0.04 sec\n",
            "Epoch 647, Loss(train/val) 0.64844/0.22605. Took 0.05 sec\n",
            "Epoch 648, Loss(train/val) 0.64086/0.22605. Took 0.05 sec\n",
            "Epoch 649, Loss(train/val) 0.62889/0.22652. Took 0.06 sec\n",
            "Epoch 650, Loss(train/val) 0.65066/0.22774. Took 0.05 sec\n",
            "Epoch 651, Loss(train/val) 0.56254/0.22963. Took 0.05 sec\n",
            "Epoch 652, Loss(train/val) 0.63531/0.23334. Took 0.05 sec\n",
            "Epoch 653, Loss(train/val) 0.62377/0.23505. Took 0.05 sec\n",
            "Epoch 654, Loss(train/val) 0.60482/0.23971. Took 0.05 sec\n",
            "Epoch 655, Loss(train/val) 0.62577/0.24549. Took 0.04 sec\n",
            "Epoch 656, Loss(train/val) 0.65575/0.24942. Took 0.05 sec\n",
            "Epoch 657, Loss(train/val) 0.61052/0.25213. Took 0.05 sec\n",
            "Epoch 658, Loss(train/val) 0.62879/0.25486. Took 0.05 sec\n",
            "Epoch 659, Loss(train/val) 0.66522/0.25554. Took 0.04 sec\n",
            "Epoch 660, Loss(train/val) 0.60345/0.24727. Took 0.04 sec\n",
            "Epoch 661, Loss(train/val) 0.60809/0.23997. Took 0.04 sec\n",
            "Epoch 662, Loss(train/val) 0.63389/0.23645. Took 0.06 sec\n",
            "Epoch 663, Loss(train/val) 0.64029/0.23418. Took 0.05 sec\n",
            "Epoch 664, Loss(train/val) 0.64344/0.23648. Took 0.05 sec\n",
            "Epoch 665, Loss(train/val) 0.60445/0.23556. Took 0.04 sec\n",
            "Epoch 666, Loss(train/val) 0.63996/0.23752. Took 0.05 sec\n",
            "Epoch 667, Loss(train/val) 0.64865/0.24175. Took 0.05 sec\n",
            "Epoch 668, Loss(train/val) 0.61053/0.24370. Took 0.05 sec\n",
            "Epoch 669, Loss(train/val) 0.61833/0.24078. Took 0.05 sec\n",
            "Epoch 670, Loss(train/val) 0.64940/0.23818. Took 0.05 sec\n",
            "Epoch 671, Loss(train/val) 0.64638/0.23583. Took 0.04 sec\n",
            "Epoch 672, Loss(train/val) 0.63887/0.23645. Took 0.05 sec\n",
            "Epoch 673, Loss(train/val) 0.60667/0.23707. Took 0.04 sec\n",
            "Epoch 674, Loss(train/val) 0.61703/0.23402. Took 0.04 sec\n",
            "Epoch 675, Loss(train/val) 0.62617/0.22963. Took 0.05 sec\n",
            "Epoch 676, Loss(train/val) 0.61919/0.22786. Took 0.05 sec\n",
            "Epoch 677, Loss(train/val) 0.62708/0.22674. Took 0.05 sec\n",
            "Epoch 678, Loss(train/val) 0.65966/0.22531. Took 0.04 sec\n",
            "Epoch 679, Loss(train/val) 0.66604/0.22598. Took 0.04 sec\n",
            "Epoch 680, Loss(train/val) 0.61957/0.22941. Took 0.05 sec\n",
            "Epoch 681, Loss(train/val) 0.63364/0.23417. Took 0.04 sec\n",
            "Epoch 682, Loss(train/val) 0.62520/0.23865. Took 0.05 sec\n",
            "Epoch 683, Loss(train/val) 0.63391/0.24295. Took 0.05 sec\n",
            "Epoch 684, Loss(train/val) 0.63226/0.23905. Took 0.04 sec\n",
            "Epoch 685, Loss(train/val) 0.61459/0.23350. Took 0.04 sec\n",
            "Epoch 686, Loss(train/val) 0.63894/0.22842. Took 0.04 sec\n",
            "Epoch 687, Loss(train/val) 0.64191/0.22806. Took 0.05 sec\n",
            "Epoch 688, Loss(train/val) 0.63963/0.22728. Took 0.05 sec\n",
            "Epoch 689, Loss(train/val) 0.65280/0.22652. Took 0.04 sec\n",
            "Epoch 690, Loss(train/val) 0.62713/0.22743. Took 0.05 sec\n",
            "Epoch 691, Loss(train/val) 0.64073/0.22840. Took 0.05 sec\n",
            "Epoch 692, Loss(train/val) 0.61940/0.22879. Took 0.06 sec\n",
            "Epoch 693, Loss(train/val) 0.62930/0.22844. Took 0.05 sec\n",
            "Epoch 694, Loss(train/val) 0.62444/0.22899. Took 0.04 sec\n",
            "Epoch 695, Loss(train/val) 0.62232/0.22730. Took 0.04 sec\n",
            "Epoch 696, Loss(train/val) 0.63107/0.22732. Took 0.05 sec\n",
            "Epoch 697, Loss(train/val) 0.62349/0.22752. Took 0.05 sec\n",
            "Epoch 698, Loss(train/val) 0.60786/0.22617. Took 0.05 sec\n",
            "Epoch 699, Loss(train/val) 0.61823/0.22691. Took 0.05 sec\n",
            "Epoch 700, Loss(train/val) 0.61206/0.22786. Took 0.05 sec\n",
            "Epoch 701, Loss(train/val) 0.63728/0.22940. Took 0.05 sec\n",
            "Epoch 702, Loss(train/val) 0.64640/0.22950. Took 0.05 sec\n",
            "Epoch 703, Loss(train/val) 0.63219/0.22888. Took 0.05 sec\n",
            "Epoch 704, Loss(train/val) 0.60865/0.22904. Took 0.05 sec\n",
            "Epoch 705, Loss(train/val) 0.62937/0.22805. Took 0.05 sec\n",
            "Epoch 706, Loss(train/val) 0.62174/0.22655. Took 0.04 sec\n",
            "Epoch 707, Loss(train/val) 0.61566/0.22619. Took 0.06 sec\n",
            "Epoch 708, Loss(train/val) 0.63061/0.22722. Took 0.04 sec\n",
            "Epoch 709, Loss(train/val) 0.62308/0.22775. Took 0.04 sec\n",
            "Epoch 710, Loss(train/val) 0.61617/0.22749. Took 0.05 sec\n",
            "Epoch 711, Loss(train/val) 0.63669/0.22745. Took 0.04 sec\n",
            "Epoch 712, Loss(train/val) 0.64032/0.22868. Took 0.05 sec\n",
            "Epoch 713, Loss(train/val) 0.62800/0.23078. Took 0.05 sec\n",
            "Epoch 714, Loss(train/val) 0.62324/0.23077. Took 0.05 sec\n",
            "Epoch 715, Loss(train/val) 0.63083/0.22923. Took 0.04 sec\n",
            "Epoch 716, Loss(train/val) 0.65110/0.22670. Took 0.04 sec\n",
            "Epoch 717, Loss(train/val) 0.61220/0.22506. Took 0.05 sec\n",
            "Epoch 718, Loss(train/val) 0.61660/0.22610. Took 0.05 sec\n",
            "Epoch 719, Loss(train/val) 0.62198/0.22602. Took 0.05 sec\n",
            "Epoch 720, Loss(train/val) 0.64356/0.22684. Took 0.05 sec\n",
            "Epoch 721, Loss(train/val) 0.61394/0.22738. Took 0.04 sec\n",
            "Epoch 722, Loss(train/val) 0.62932/0.22731. Took 0.05 sec\n",
            "Epoch 723, Loss(train/val) 0.61468/0.22759. Took 0.05 sec\n",
            "Epoch 724, Loss(train/val) 0.62672/0.22763. Took 0.05 sec\n",
            "Epoch 725, Loss(train/val) 0.65040/0.22841. Took 0.05 sec\n",
            "Epoch 726, Loss(train/val) 0.61053/0.22786. Took 0.05 sec\n",
            "Epoch 727, Loss(train/val) 0.64124/0.23107. Took 0.06 sec\n",
            "Epoch 728, Loss(train/val) 0.64502/0.23201. Took 0.05 sec\n",
            "Epoch 729, Loss(train/val) 0.63639/0.23610. Took 0.05 sec\n",
            "Epoch 730, Loss(train/val) 0.63926/0.24091. Took 0.04 sec\n",
            "Epoch 731, Loss(train/val) 0.63085/0.24406. Took 0.05 sec\n",
            "Epoch 732, Loss(train/val) 0.63904/0.24790. Took 0.05 sec\n",
            "Epoch 733, Loss(train/val) 0.63676/0.24613. Took 0.05 sec\n",
            "Epoch 734, Loss(train/val) 0.62723/0.24643. Took 0.05 sec\n",
            "Epoch 735, Loss(train/val) 0.63501/0.24606. Took 0.05 sec\n",
            "Epoch 736, Loss(train/val) 0.62571/0.24285. Took 0.05 sec\n",
            "Epoch 737, Loss(train/val) 0.65527/0.24421. Took 0.05 sec\n",
            "Epoch 738, Loss(train/val) 0.61430/0.24718. Took 0.05 sec\n",
            "Epoch 739, Loss(train/val) 0.61595/0.25119. Took 0.05 sec\n",
            "Epoch 740, Loss(train/val) 0.60524/0.24721. Took 0.05 sec\n",
            "Epoch 741, Loss(train/val) 0.59711/0.23911. Took 0.05 sec\n",
            "Epoch 742, Loss(train/val) 0.64329/0.23770. Took 0.05 sec\n",
            "Epoch 743, Loss(train/val) 0.59022/0.23466. Took 0.05 sec\n",
            "Epoch 744, Loss(train/val) 0.62256/0.23170. Took 0.04 sec\n",
            "Epoch 745, Loss(train/val) 0.61464/0.23139. Took 0.04 sec\n",
            "Epoch 746, Loss(train/val) 0.59394/0.23127. Took 0.04 sec\n",
            "Epoch 747, Loss(train/val) 0.60041/0.23059. Took 0.05 sec\n",
            "Epoch 748, Loss(train/val) 0.59116/0.22930. Took 0.05 sec\n",
            "Epoch 749, Loss(train/val) 0.62025/0.22756. Took 0.05 sec\n",
            "Epoch 750, Loss(train/val) 0.63097/0.22630. Took 0.05 sec\n",
            "Epoch 751, Loss(train/val) 0.63467/0.22502. Took 0.06 sec\n",
            "Epoch 752, Loss(train/val) 0.65003/0.22632. Took 0.05 sec\n",
            "Epoch 753, Loss(train/val) 0.62669/0.22553. Took 0.05 sec\n",
            "Epoch 754, Loss(train/val) 0.59479/0.22426. Took 0.05 sec\n",
            "Epoch 755, Loss(train/val) 0.62768/0.22413. Took 0.05 sec\n",
            "Epoch 756, Loss(train/val) 0.62778/0.22440. Took 0.05 sec\n",
            "Epoch 757, Loss(train/val) 0.62630/0.22419. Took 0.05 sec\n",
            "Epoch 758, Loss(train/val) 0.60224/0.22596. Took 0.05 sec\n",
            "Epoch 759, Loss(train/val) 0.60831/0.22847. Took 0.05 sec\n",
            "Epoch 760, Loss(train/val) 0.61189/0.22850. Took 0.04 sec\n",
            "Epoch 761, Loss(train/val) 0.60780/0.23003. Took 0.04 sec\n",
            "Epoch 762, Loss(train/val) 0.63339/0.22962. Took 0.05 sec\n",
            "Epoch 763, Loss(train/val) 0.62441/0.22787. Took 0.05 sec\n",
            "Epoch 764, Loss(train/val) 0.61241/0.22791. Took 0.05 sec\n",
            "Epoch 765, Loss(train/val) 0.61826/0.23017. Took 0.04 sec\n",
            "Epoch 766, Loss(train/val) 0.60485/0.22934. Took 0.05 sec\n",
            "Epoch 767, Loss(train/val) 0.65240/0.22745. Took 0.05 sec\n",
            "Epoch 768, Loss(train/val) 0.61804/0.22661. Took 0.05 sec\n",
            "Epoch 769, Loss(train/val) 0.58508/0.22617. Took 0.05 sec\n",
            "Epoch 770, Loss(train/val) 0.59560/0.22595. Took 0.05 sec\n",
            "Epoch 771, Loss(train/val) 0.59562/0.22528. Took 0.05 sec\n",
            "Epoch 772, Loss(train/val) 0.62844/0.22635. Took 0.05 sec\n",
            "Epoch 773, Loss(train/val) 0.60077/0.22662. Took 0.05 sec\n",
            "Epoch 774, Loss(train/val) 0.64066/0.22684. Took 0.05 sec\n",
            "Epoch 775, Loss(train/val) 0.58585/0.22881. Took 0.04 sec\n",
            "Epoch 776, Loss(train/val) 0.62095/0.22822. Took 0.05 sec\n",
            "Epoch 777, Loss(train/val) 0.63285/0.22817. Took 0.06 sec\n",
            "Epoch 778, Loss(train/val) 0.63047/0.22509. Took 0.05 sec\n",
            "Epoch 779, Loss(train/val) 0.62927/0.22372. Took 0.05 sec\n",
            "Epoch 780, Loss(train/val) 0.62341/0.22455. Took 0.05 sec\n",
            "Epoch 781, Loss(train/val) 0.61311/0.22769. Took 0.04 sec\n",
            "Epoch 782, Loss(train/val) 0.59070/0.23020. Took 0.05 sec\n",
            "Epoch 783, Loss(train/val) 0.58585/0.23348. Took 0.05 sec\n",
            "Epoch 784, Loss(train/val) 0.64186/0.23603. Took 0.05 sec\n",
            "Epoch 785, Loss(train/val) 0.64991/0.23963. Took 0.04 sec\n",
            "Epoch 786, Loss(train/val) 0.62802/0.24319. Took 0.04 sec\n",
            "Epoch 787, Loss(train/val) 0.64687/0.24290. Took 0.05 sec\n",
            "Epoch 788, Loss(train/val) 0.64115/0.24240. Took 0.04 sec\n",
            "Epoch 789, Loss(train/val) 0.59564/0.23943. Took 0.05 sec\n",
            "Epoch 790, Loss(train/val) 0.60958/0.23772. Took 0.04 sec\n",
            "Epoch 791, Loss(train/val) 0.64103/0.23317. Took 0.05 sec\n",
            "Epoch 792, Loss(train/val) 0.58958/0.22728. Took 0.05 sec\n",
            "Epoch 793, Loss(train/val) 0.60702/0.22612. Took 0.05 sec\n",
            "Epoch 794, Loss(train/val) 0.60231/0.22767. Took 0.05 sec\n",
            "Epoch 795, Loss(train/val) 0.63344/0.22872. Took 0.05 sec\n",
            "Epoch 796, Loss(train/val) 0.62216/0.22760. Took 0.05 sec\n",
            "Epoch 797, Loss(train/val) 0.60155/0.22821. Took 0.06 sec\n",
            "Epoch 798, Loss(train/val) 0.60839/0.22760. Took 0.06 sec\n",
            "Epoch 799, Loss(train/val) 0.62799/0.23108. Took 0.05 sec\n",
            "Epoch 800, Loss(train/val) 0.60242/0.23069. Took 0.05 sec\n",
            "Epoch 801, Loss(train/val) 0.62110/0.23273. Took 0.05 sec\n",
            "Epoch 802, Loss(train/val) 0.60299/0.22995. Took 0.05 sec\n",
            "Epoch 803, Loss(train/val) 0.62281/0.22864. Took 0.05 sec\n",
            "Epoch 804, Loss(train/val) 0.60605/0.22717. Took 0.04 sec\n",
            "Epoch 805, Loss(train/val) 0.59773/0.22620. Took 0.04 sec\n",
            "Epoch 806, Loss(train/val) 0.64043/0.22780. Took 0.05 sec\n",
            "Epoch 807, Loss(train/val) 0.63907/0.23061. Took 0.05 sec\n",
            "Epoch 808, Loss(train/val) 0.63594/0.23376. Took 0.04 sec\n",
            "Epoch 809, Loss(train/val) 0.58531/0.23753. Took 0.04 sec\n",
            "Epoch 810, Loss(train/val) 0.60143/0.23970. Took 0.05 sec\n",
            "Epoch 811, Loss(train/val) 0.59194/0.24099. Took 0.05 sec\n",
            "Epoch 812, Loss(train/val) 0.60603/0.23536. Took 0.05 sec\n",
            "Epoch 813, Loss(train/val) 0.63841/0.22684. Took 0.04 sec\n",
            "Epoch 814, Loss(train/val) 0.63494/0.22445. Took 0.04 sec\n",
            "Epoch 815, Loss(train/val) 0.60245/0.22431. Took 0.05 sec\n",
            "Epoch 816, Loss(train/val) 0.62141/0.22422. Took 0.05 sec\n",
            "Epoch 817, Loss(train/val) 0.61003/0.22365. Took 0.05 sec\n",
            "Epoch 818, Loss(train/val) 0.62083/0.22364. Took 0.05 sec\n",
            "Epoch 819, Loss(train/val) 0.58731/0.22451. Took 0.04 sec\n",
            "Epoch 820, Loss(train/val) 0.63576/0.22578. Took 0.05 sec\n",
            "Epoch 821, Loss(train/val) 0.63708/0.22512. Took 0.04 sec\n",
            "Epoch 822, Loss(train/val) 0.61191/0.22509. Took 0.05 sec\n",
            "Epoch 823, Loss(train/val) 0.59194/0.22346. Took 0.04 sec\n",
            "Epoch 824, Loss(train/val) 0.62105/0.22297. Took 0.04 sec\n",
            "Epoch 825, Loss(train/val) 0.59888/0.22364. Took 0.04 sec\n",
            "Epoch 826, Loss(train/val) 0.59465/0.22584. Took 0.05 sec\n",
            "Epoch 827, Loss(train/val) 0.60378/0.22660. Took 0.05 sec\n",
            "Epoch 828, Loss(train/val) 0.64821/0.22627. Took 0.04 sec\n",
            "Epoch 829, Loss(train/val) 0.59577/0.22469. Took 0.04 sec\n",
            "Epoch 830, Loss(train/val) 0.63299/0.22314. Took 0.04 sec\n",
            "Epoch 831, Loss(train/val) 0.60956/0.22281. Took 0.04 sec\n",
            "Epoch 832, Loss(train/val) 0.61913/0.22337. Took 0.05 sec\n",
            "Epoch 833, Loss(train/val) 0.57991/0.22410. Took 0.04 sec\n",
            "Epoch 834, Loss(train/val) 0.58153/0.22343. Took 0.05 sec\n",
            "Epoch 835, Loss(train/val) 0.59535/0.22297. Took 0.04 sec\n",
            "Epoch 836, Loss(train/val) 0.62289/0.22310. Took 0.05 sec\n",
            "Epoch 837, Loss(train/val) 0.62344/0.22327. Took 0.05 sec\n",
            "Epoch 838, Loss(train/val) 0.57360/0.22365. Took 0.04 sec\n",
            "Epoch 839, Loss(train/val) 0.60613/0.22531. Took 0.04 sec\n",
            "Epoch 840, Loss(train/val) 0.61516/0.22656. Took 0.04 sec\n",
            "Epoch 841, Loss(train/val) 0.61163/0.22659. Took 0.05 sec\n",
            "Epoch 842, Loss(train/val) 0.61142/0.22695. Took 0.06 sec\n",
            "Epoch 843, Loss(train/val) 0.62792/0.22798. Took 0.05 sec\n",
            "Epoch 844, Loss(train/val) 0.60055/0.22907. Took 0.04 sec\n",
            "Epoch 845, Loss(train/val) 0.60238/0.22982. Took 0.05 sec\n",
            "Epoch 846, Loss(train/val) 0.58403/0.23020. Took 0.04 sec\n",
            "Epoch 847, Loss(train/val) 0.58968/0.23148. Took 0.05 sec\n",
            "Epoch 848, Loss(train/val) 0.61109/0.23161. Took 0.05 sec\n",
            "Epoch 849, Loss(train/val) 0.59986/0.23069. Took 0.04 sec\n",
            "Epoch 850, Loss(train/val) 0.58255/0.23260. Took 0.05 sec\n",
            "Epoch 851, Loss(train/val) 0.59608/0.23101. Took 0.05 sec\n",
            "Epoch 852, Loss(train/val) 0.63858/0.22881. Took 0.06 sec\n",
            "Epoch 853, Loss(train/val) 0.58502/0.22615. Took 0.05 sec\n",
            "Epoch 854, Loss(train/val) 0.60277/0.22475. Took 0.05 sec\n",
            "Epoch 855, Loss(train/val) 0.61677/0.22617. Took 0.05 sec\n",
            "Epoch 856, Loss(train/val) 0.59192/0.22985. Took 0.05 sec\n",
            "Epoch 857, Loss(train/val) 0.58943/0.23547. Took 0.05 sec\n",
            "Epoch 858, Loss(train/val) 0.62262/0.23782. Took 0.05 sec\n",
            "Epoch 859, Loss(train/val) 0.63130/0.24417. Took 0.06 sec\n",
            "Epoch 860, Loss(train/val) 0.58498/0.24926. Took 0.07 sec\n",
            "Epoch 861, Loss(train/val) 0.62068/0.25153. Took 0.06 sec\n",
            "Epoch 862, Loss(train/val) 0.60719/0.25644. Took 0.06 sec\n",
            "Epoch 863, Loss(train/val) 0.61905/0.25180. Took 0.05 sec\n",
            "Epoch 864, Loss(train/val) 0.59830/0.24710. Took 0.05 sec\n",
            "Epoch 865, Loss(train/val) 0.57851/0.24597. Took 0.06 sec\n",
            "Epoch 866, Loss(train/val) 0.56308/0.24669. Took 0.05 sec\n",
            "Epoch 867, Loss(train/val) 0.60091/0.24410. Took 0.05 sec\n",
            "Epoch 868, Loss(train/val) 0.59691/0.23903. Took 0.05 sec\n",
            "Epoch 869, Loss(train/val) 0.61969/0.23208. Took 0.05 sec\n",
            "Epoch 870, Loss(train/val) 0.60834/0.22560. Took 0.05 sec\n",
            "Epoch 871, Loss(train/val) 0.60279/0.22269. Took 0.05 sec\n",
            "Epoch 872, Loss(train/val) 0.60131/0.22297. Took 0.04 sec\n",
            "Epoch 873, Loss(train/val) 0.58815/0.22362. Took 0.04 sec\n",
            "Epoch 874, Loss(train/val) 0.61276/0.22396. Took 0.04 sec\n",
            "Epoch 875, Loss(train/val) 0.61699/0.22492. Took 0.05 sec\n",
            "Epoch 876, Loss(train/val) 0.59600/0.22569. Took 0.05 sec\n",
            "Epoch 877, Loss(train/val) 0.61994/0.22474. Took 0.05 sec\n",
            "Epoch 878, Loss(train/val) 0.59879/0.22399. Took 0.05 sec\n",
            "Epoch 879, Loss(train/val) 0.60139/0.22327. Took 0.05 sec\n",
            "Epoch 880, Loss(train/val) 0.60564/0.22285. Took 0.06 sec\n",
            "Epoch 881, Loss(train/val) 0.61324/0.22251. Took 0.05 sec\n",
            "Epoch 882, Loss(train/val) 0.59733/0.22286. Took 0.06 sec\n",
            "Epoch 883, Loss(train/val) 0.58695/0.22353. Took 0.05 sec\n",
            "Epoch 884, Loss(train/val) 0.62013/0.22357. Took 0.04 sec\n",
            "Epoch 885, Loss(train/val) 0.61179/0.22346. Took 0.05 sec\n",
            "Epoch 886, Loss(train/val) 0.59772/0.22313. Took 0.04 sec\n",
            "Epoch 887, Loss(train/val) 0.61927/0.22373. Took 0.04 sec\n",
            "Epoch 888, Loss(train/val) 0.60417/0.22560. Took 0.04 sec\n",
            "Epoch 889, Loss(train/val) 0.60159/0.22541. Took 0.04 sec\n",
            "Epoch 890, Loss(train/val) 0.57677/0.22668. Took 0.05 sec\n",
            "Epoch 891, Loss(train/val) 0.60803/0.22662. Took 0.05 sec\n",
            "Epoch 892, Loss(train/val) 0.62622/0.22783. Took 0.05 sec\n",
            "Epoch 893, Loss(train/val) 0.59355/0.22961. Took 0.05 sec\n",
            "Epoch 894, Loss(train/val) 0.59266/0.22803. Took 0.04 sec\n",
            "Epoch 895, Loss(train/val) 0.61647/0.22595. Took 0.05 sec\n",
            "Epoch 896, Loss(train/val) 0.58580/0.22449. Took 0.04 sec\n",
            "Epoch 897, Loss(train/val) 0.61196/0.22382. Took 0.05 sec\n",
            "Epoch 898, Loss(train/val) 0.60409/0.22333. Took 0.05 sec\n",
            "Epoch 899, Loss(train/val) 0.59084/0.22283. Took 0.05 sec\n",
            "Epoch 900, Loss(train/val) 0.58839/0.22282. Took 0.06 sec\n",
            "Epoch 901, Loss(train/val) 0.60299/0.22344. Took 0.05 sec\n",
            "Epoch 902, Loss(train/val) 0.59360/0.22394. Took 0.05 sec\n",
            "Epoch 903, Loss(train/val) 0.59725/0.22326. Took 0.06 sec\n",
            "Epoch 904, Loss(train/val) 0.62213/0.22372. Took 0.06 sec\n",
            "Epoch 905, Loss(train/val) 0.60866/0.22514. Took 0.05 sec\n",
            "Epoch 906, Loss(train/val) 0.60336/0.22714. Took 0.04 sec\n",
            "Epoch 907, Loss(train/val) 0.64132/0.22656. Took 0.05 sec\n",
            "Epoch 908, Loss(train/val) 0.60650/0.22854. Took 0.05 sec\n",
            "Epoch 909, Loss(train/val) 0.59168/0.22826. Took 0.05 sec\n",
            "Epoch 910, Loss(train/val) 0.60611/0.23028. Took 0.05 sec\n",
            "Epoch 911, Loss(train/val) 0.62332/0.22758. Took 0.05 sec\n",
            "Epoch 912, Loss(train/val) 0.61291/0.22890. Took 0.05 sec\n",
            "Epoch 913, Loss(train/val) 0.61161/0.22721. Took 0.05 sec\n",
            "Epoch 914, Loss(train/val) 0.61782/0.22529. Took 0.05 sec\n",
            "Epoch 915, Loss(train/val) 0.59996/0.22448. Took 0.04 sec\n",
            "Epoch 916, Loss(train/val) 0.63782/0.22413. Took 0.05 sec\n",
            "Epoch 917, Loss(train/val) 0.61240/0.22398. Took 0.04 sec\n",
            "Epoch 918, Loss(train/val) 0.62216/0.22513. Took 0.05 sec\n",
            "Epoch 919, Loss(train/val) 0.59040/0.22748. Took 0.06 sec\n",
            "Epoch 920, Loss(train/val) 0.61886/0.22834. Took 0.05 sec\n",
            "Epoch 921, Loss(train/val) 0.58692/0.22656. Took 0.05 sec\n",
            "Epoch 922, Loss(train/val) 0.58676/0.22574. Took 0.05 sec\n",
            "Epoch 923, Loss(train/val) 0.60795/0.22733. Took 0.04 sec\n",
            "Epoch 924, Loss(train/val) 0.61978/0.22910. Took 0.06 sec\n",
            "Epoch 925, Loss(train/val) 0.59521/0.22754. Took 0.05 sec\n",
            "Epoch 926, Loss(train/val) 0.60871/0.22702. Took 0.04 sec\n",
            "Epoch 927, Loss(train/val) 0.60883/0.22684. Took 0.05 sec\n",
            "Epoch 928, Loss(train/val) 0.57904/0.22938. Took 0.05 sec\n",
            "Epoch 929, Loss(train/val) 0.61242/0.22904. Took 0.05 sec\n",
            "Epoch 930, Loss(train/val) 0.61678/0.22772. Took 0.05 sec\n",
            "Epoch 931, Loss(train/val) 0.57865/0.22687. Took 0.04 sec\n",
            "Epoch 932, Loss(train/val) 0.59691/0.22461. Took 0.05 sec\n",
            "Epoch 933, Loss(train/val) 0.62570/0.22380. Took 0.05 sec\n",
            "Epoch 934, Loss(train/val) 0.57175/0.22293. Took 0.05 sec\n",
            "Epoch 935, Loss(train/val) 0.60551/0.22260. Took 0.04 sec\n",
            "Epoch 936, Loss(train/val) 0.59362/0.22436. Took 0.05 sec\n",
            "Epoch 937, Loss(train/val) 0.62060/0.22480. Took 0.04 sec\n",
            "Epoch 938, Loss(train/val) 0.60135/0.22326. Took 0.05 sec\n",
            "Epoch 939, Loss(train/val) 0.59274/0.22265. Took 0.06 sec\n",
            "Epoch 940, Loss(train/val) 0.61238/0.22318. Took 0.05 sec\n",
            "Epoch 941, Loss(train/val) 0.61653/0.22312. Took 0.05 sec\n",
            "Epoch 942, Loss(train/val) 0.58707/0.22232. Took 0.05 sec\n",
            "Epoch 943, Loss(train/val) 0.61763/0.22215. Took 0.05 sec\n",
            "Epoch 944, Loss(train/val) 0.57878/0.22182. Took 0.05 sec\n",
            "Epoch 945, Loss(train/val) 0.58870/0.22195. Took 0.05 sec\n",
            "Epoch 946, Loss(train/val) 0.59899/0.22216. Took 0.05 sec\n",
            "Epoch 947, Loss(train/val) 0.59718/0.22209. Took 0.05 sec\n",
            "Epoch 948, Loss(train/val) 0.60353/0.22193. Took 0.04 sec\n",
            "Epoch 949, Loss(train/val) 0.61964/0.22220. Took 0.05 sec\n",
            "Epoch 950, Loss(train/val) 0.60001/0.22188. Took 0.05 sec\n",
            "Epoch 951, Loss(train/val) 0.61106/0.22175. Took 0.04 sec\n",
            "Epoch 952, Loss(train/val) 0.61212/0.22198. Took 0.04 sec\n",
            "Epoch 953, Loss(train/val) 0.61778/0.22193. Took 0.04 sec\n",
            "Epoch 954, Loss(train/val) 0.57354/0.22218. Took 0.05 sec\n",
            "Epoch 955, Loss(train/val) 0.61081/0.22093. Took 0.05 sec\n",
            "Epoch 956, Loss(train/val) 0.59571/0.22096. Took 0.04 sec\n",
            "Epoch 957, Loss(train/val) 0.61258/0.22104. Took 0.05 sec\n",
            "Epoch 958, Loss(train/val) 0.61175/0.22197. Took 0.05 sec\n",
            "Epoch 959, Loss(train/val) 0.56991/0.22349. Took 0.05 sec\n",
            "Epoch 960, Loss(train/val) 0.57708/0.22511. Took 0.04 sec\n",
            "Epoch 961, Loss(train/val) 0.59488/0.22616. Took 0.04 sec\n",
            "Epoch 962, Loss(train/val) 0.59895/0.22482. Took 0.04 sec\n",
            "Epoch 963, Loss(train/val) 0.58531/0.22504. Took 0.04 sec\n",
            "Epoch 964, Loss(train/val) 0.62042/0.22755. Took 0.05 sec\n",
            "Epoch 965, Loss(train/val) 0.58802/0.22881. Took 0.04 sec\n",
            "Epoch 966, Loss(train/val) 0.58694/0.22844. Took 0.05 sec\n",
            "Epoch 967, Loss(train/val) 0.61962/0.22703. Took 0.05 sec\n",
            "Epoch 968, Loss(train/val) 0.57519/0.22644. Took 0.04 sec\n",
            "Epoch 969, Loss(train/val) 0.59535/0.22712. Took 0.05 sec\n",
            "Epoch 970, Loss(train/val) 0.61100/0.23096. Took 0.05 sec\n",
            "Epoch 971, Loss(train/val) 0.59205/0.23113. Took 0.05 sec\n",
            "Epoch 972, Loss(train/val) 0.61799/0.23141. Took 0.04 sec\n",
            "Epoch 973, Loss(train/val) 0.55989/0.23183. Took 0.04 sec\n",
            "Epoch 974, Loss(train/val) 0.57479/0.23192. Took 0.05 sec\n",
            "Epoch 975, Loss(train/val) 0.58280/0.22995. Took 0.04 sec\n",
            "Epoch 976, Loss(train/val) 0.61815/0.22646. Took 0.05 sec\n",
            "Epoch 977, Loss(train/val) 0.57397/0.22279. Took 0.05 sec\n",
            "Epoch 978, Loss(train/val) 0.61921/0.22208. Took 0.05 sec\n",
            "Epoch 979, Loss(train/val) 0.60901/0.22171. Took 0.05 sec\n",
            "Epoch 980, Loss(train/val) 0.61716/0.22174. Took 0.04 sec\n",
            "Epoch 981, Loss(train/val) 0.61507/0.22187. Took 0.04 sec\n",
            "Epoch 982, Loss(train/val) 0.61117/0.22198. Took 0.04 sec\n",
            "Epoch 983, Loss(train/val) 0.57889/0.22329. Took 0.04 sec\n",
            "Epoch 984, Loss(train/val) 0.60673/0.22656. Took 0.05 sec\n",
            "Epoch 985, Loss(train/val) 0.59220/0.22902. Took 0.05 sec\n",
            "Epoch 986, Loss(train/val) 0.63452/0.23007. Took 0.04 sec\n",
            "Epoch 987, Loss(train/val) 0.59611/0.23407. Took 0.05 sec\n",
            "Epoch 988, Loss(train/val) 0.57328/0.23454. Took 0.05 sec\n",
            "Epoch 989, Loss(train/val) 0.62148/0.23497. Took 0.06 sec\n",
            "Epoch 990, Loss(train/val) 0.59353/0.23283. Took 0.04 sec\n",
            "Epoch 991, Loss(train/val) 0.58794/0.23264. Took 0.04 sec\n",
            "Epoch 992, Loss(train/val) 0.58771/0.23145. Took 0.05 sec\n",
            "Epoch 993, Loss(train/val) 0.57965/0.22875. Took 0.04 sec\n",
            "Epoch 994, Loss(train/val) 0.58786/0.22798. Took 0.05 sec\n",
            "Epoch 995, Loss(train/val) 0.58218/0.22748. Took 0.04 sec\n",
            "Epoch 996, Loss(train/val) 0.59428/0.22832. Took 0.05 sec\n",
            "Epoch 997, Loss(train/val) 0.60653/0.22854. Took 0.05 sec\n",
            "Epoch 998, Loss(train/val) 0.57581/0.22919. Took 0.05 sec\n",
            "Epoch 999, Loss(train/val) 0.57411/0.22879. Took 0.05 sec\n",
            "Epoch 1000, Loss(train/val) 0.57268/0.22766. Took 0.05 sec\n",
            "Epoch 1001, Loss(train/val) 0.58213/0.23013. Took 0.04 sec\n",
            "Epoch 1002, Loss(train/val) 0.58507/0.23128. Took 0.05 sec\n",
            "Epoch 1003, Loss(train/val) 0.60100/0.23164. Took 0.05 sec\n",
            "Epoch 1004, Loss(train/val) 0.56425/0.22998. Took 0.06 sec\n",
            "Epoch 1005, Loss(train/val) 0.55711/0.22778. Took 0.05 sec\n",
            "Epoch 1006, Loss(train/val) 0.59073/0.22494. Took 0.05 sec\n",
            "Epoch 1007, Loss(train/val) 0.59349/0.22393. Took 0.05 sec\n",
            "Epoch 1008, Loss(train/val) 0.59274/0.22325. Took 0.05 sec\n",
            "Epoch 1009, Loss(train/val) 0.58251/0.22328. Took 0.05 sec\n",
            "Epoch 1010, Loss(train/val) 0.60421/0.22413. Took 0.06 sec\n",
            "Epoch 1011, Loss(train/val) 0.59594/0.22386. Took 0.04 sec\n",
            "Epoch 1012, Loss(train/val) 0.60316/0.22366. Took 0.04 sec\n",
            "Epoch 1013, Loss(train/val) 0.59376/0.22203. Took 0.04 sec\n",
            "Epoch 1014, Loss(train/val) 0.60571/0.22172. Took 0.05 sec\n",
            "Epoch 1015, Loss(train/val) 0.63743/0.22188. Took 0.04 sec\n",
            "Epoch 1016, Loss(train/val) 0.63692/0.22337. Took 0.05 sec\n",
            "Epoch 1017, Loss(train/val) 0.58087/0.22525. Took 0.05 sec\n",
            "Epoch 1018, Loss(train/val) 0.59481/0.22545. Took 0.05 sec\n",
            "Epoch 1019, Loss(train/val) 0.58181/0.22460. Took 0.06 sec\n",
            "Epoch 1020, Loss(train/val) 0.58553/0.22589. Took 0.05 sec\n",
            "Epoch 1021, Loss(train/val) 0.55552/0.22607. Took 0.05 sec\n",
            "Epoch 1022, Loss(train/val) 0.58211/0.22464. Took 0.04 sec\n",
            "Epoch 1023, Loss(train/val) 0.57664/0.22270. Took 0.05 sec\n",
            "Epoch 1024, Loss(train/val) 0.61709/0.22161. Took 0.05 sec\n",
            "Epoch 1025, Loss(train/val) 0.56912/0.22162. Took 0.05 sec\n",
            "Epoch 1026, Loss(train/val) 0.57274/0.22206. Took 0.04 sec\n",
            "Epoch 1027, Loss(train/val) 0.61457/0.22296. Took 0.04 sec\n",
            "Epoch 1028, Loss(train/val) 0.60611/0.22362. Took 0.05 sec\n",
            "Epoch 1029, Loss(train/val) 0.58222/0.22526. Took 0.05 sec\n",
            "Epoch 1030, Loss(train/val) 0.57315/0.22700. Took 0.04 sec\n",
            "Epoch 1031, Loss(train/val) 0.57664/0.22629. Took 0.06 sec\n",
            "Epoch 1032, Loss(train/val) 0.60442/0.22624. Took 0.04 sec\n",
            "Epoch 1033, Loss(train/val) 0.61059/0.22516. Took 0.05 sec\n",
            "Epoch 1034, Loss(train/val) 0.57074/0.22477. Took 0.05 sec\n",
            "Epoch 1035, Loss(train/val) 0.56797/0.22487. Took 0.04 sec\n",
            "Epoch 1036, Loss(train/val) 0.55828/0.22364. Took 0.04 sec\n",
            "Epoch 1037, Loss(train/val) 0.56836/0.22372. Took 0.04 sec\n",
            "Epoch 1038, Loss(train/val) 0.57708/0.22246. Took 0.04 sec\n",
            "Epoch 1039, Loss(train/val) 0.55322/0.22246. Took 0.05 sec\n",
            "Epoch 1040, Loss(train/val) 0.60525/0.22241. Took 0.05 sec\n",
            "Epoch 1041, Loss(train/val) 0.57898/0.22150. Took 0.04 sec\n",
            "Epoch 1042, Loss(train/val) 0.59391/0.22140. Took 0.04 sec\n",
            "Epoch 1043, Loss(train/val) 0.56921/0.22134. Took 0.05 sec\n",
            "Epoch 1044, Loss(train/val) 0.56456/0.22232. Took 0.05 sec\n",
            "Epoch 1045, Loss(train/val) 0.58168/0.22335. Took 0.04 sec\n",
            "Epoch 1046, Loss(train/val) 0.60601/0.22355. Took 0.05 sec\n",
            "Epoch 1047, Loss(train/val) 0.58316/0.22314. Took 0.04 sec\n",
            "Epoch 1048, Loss(train/val) 0.60168/0.22385. Took 0.05 sec\n",
            "Epoch 1049, Loss(train/val) 0.57732/0.22331. Took 0.05 sec\n",
            "Epoch 1050, Loss(train/val) 0.58564/0.22305. Took 0.05 sec\n",
            "Epoch 1051, Loss(train/val) 0.58597/0.22275. Took 0.04 sec\n",
            "Epoch 1052, Loss(train/val) 0.60161/0.22330. Took 0.04 sec\n",
            "Epoch 1053, Loss(train/val) 0.58361/0.22290. Took 0.05 sec\n",
            "Epoch 1054, Loss(train/val) 0.60528/0.22274. Took 0.05 sec\n",
            "Epoch 1055, Loss(train/val) 0.59693/0.22595. Took 0.04 sec\n",
            "Epoch 1056, Loss(train/val) 0.61602/0.23340. Took 0.04 sec\n",
            "Epoch 1057, Loss(train/val) 0.56140/0.23957. Took 0.04 sec\n",
            "Epoch 1058, Loss(train/val) 0.58273/0.24361. Took 0.04 sec\n",
            "Epoch 1059, Loss(train/val) 0.60472/0.24086. Took 0.05 sec\n",
            "Epoch 1060, Loss(train/val) 0.59133/0.23634. Took 0.05 sec\n",
            "Epoch 1061, Loss(train/val) 0.58104/0.23501. Took 0.05 sec\n",
            "Epoch 1062, Loss(train/val) 0.57417/0.23272. Took 0.05 sec\n",
            "Epoch 1063, Loss(train/val) 0.60098/0.23123. Took 0.05 sec\n",
            "Epoch 1064, Loss(train/val) 0.56803/0.23252. Took 0.05 sec\n",
            "Epoch 1065, Loss(train/val) 0.59589/0.23150. Took 0.04 sec\n",
            "Epoch 1066, Loss(train/val) 0.58668/0.23122. Took 0.05 sec\n",
            "Epoch 1067, Loss(train/val) 0.60289/0.23134. Took 0.04 sec\n",
            "Epoch 1068, Loss(train/val) 0.58640/0.23027. Took 0.05 sec\n",
            "Epoch 1069, Loss(train/val) 0.61244/0.22727. Took 0.05 sec\n",
            "Epoch 1070, Loss(train/val) 0.56855/0.22379. Took 0.05 sec\n",
            "Epoch 1071, Loss(train/val) 0.58883/0.22301. Took 0.05 sec\n",
            "Epoch 1072, Loss(train/val) 0.59210/0.22330. Took 0.05 sec\n",
            "Epoch 1073, Loss(train/val) 0.59638/0.22359. Took 0.04 sec\n",
            "Epoch 1074, Loss(train/val) 0.58725/0.22287. Took 0.06 sec\n",
            "Epoch 1075, Loss(train/val) 0.61000/0.22190. Took 0.05 sec\n",
            "Epoch 1076, Loss(train/val) 0.57753/0.22106. Took 0.05 sec\n",
            "Epoch 1077, Loss(train/val) 0.61604/0.22065. Took 0.06 sec\n",
            "Epoch 1078, Loss(train/val) 0.59097/0.22091. Took 0.04 sec\n",
            "Epoch 1079, Loss(train/val) 0.58043/0.22170. Took 0.05 sec\n",
            "Epoch 1080, Loss(train/val) 0.55355/0.22454. Took 0.05 sec\n",
            "Epoch 1081, Loss(train/val) 0.56540/0.22309. Took 0.05 sec\n",
            "Epoch 1082, Loss(train/val) 0.54274/0.22375. Took 0.04 sec\n",
            "Epoch 1083, Loss(train/val) 0.57121/0.22524. Took 0.05 sec\n",
            "Epoch 1084, Loss(train/val) 0.56548/0.22664. Took 0.06 sec\n",
            "Epoch 1085, Loss(train/val) 0.60063/0.22788. Took 0.04 sec\n",
            "Epoch 1086, Loss(train/val) 0.59650/0.22600. Took 0.04 sec\n",
            "Epoch 1087, Loss(train/val) 0.56741/0.22424. Took 0.04 sec\n",
            "Epoch 1088, Loss(train/val) 0.58971/0.22196. Took 0.05 sec\n",
            "Epoch 1089, Loss(train/val) 0.60714/0.22112. Took 0.05 sec\n",
            "Epoch 1090, Loss(train/val) 0.60580/0.22094. Took 0.04 sec\n",
            "Epoch 1091, Loss(train/val) 0.57820/0.21996. Took 0.04 sec\n",
            "Epoch 1092, Loss(train/val) 0.60306/0.21956. Took 0.05 sec\n",
            "Epoch 1093, Loss(train/val) 0.59972/0.21945. Took 0.04 sec\n",
            "Epoch 1094, Loss(train/val) 0.58797/0.22086. Took 0.05 sec\n",
            "Epoch 1095, Loss(train/val) 0.57332/0.22413. Took 0.04 sec\n",
            "Epoch 1096, Loss(train/val) 0.59757/0.22865. Took 0.05 sec\n",
            "Epoch 1097, Loss(train/val) 0.57208/0.23081. Took 0.04 sec\n",
            "Epoch 1098, Loss(train/val) 0.54840/0.23197. Took 0.04 sec\n",
            "Epoch 1099, Loss(train/val) 0.56312/0.23601. Took 0.05 sec\n",
            "Epoch 1100, Loss(train/val) 0.56267/0.23917. Took 0.05 sec\n",
            "Epoch 1101, Loss(train/val) 0.58341/0.23840. Took 0.04 sec\n",
            "Epoch 1102, Loss(train/val) 0.61226/0.23460. Took 0.05 sec\n",
            "Epoch 1103, Loss(train/val) 0.58226/0.23487. Took 0.04 sec\n",
            "Epoch 1104, Loss(train/val) 0.57092/0.23455. Took 0.05 sec\n",
            "Epoch 1105, Loss(train/val) 0.59658/0.23168. Took 0.04 sec\n",
            "Epoch 1106, Loss(train/val) 0.59664/0.22693. Took 0.04 sec\n",
            "Epoch 1107, Loss(train/val) 0.59880/0.22334. Took 0.04 sec\n",
            "Epoch 1108, Loss(train/val) 0.63776/0.22424. Took 0.04 sec\n",
            "Epoch 1109, Loss(train/val) 0.59019/0.22464. Took 0.05 sec\n",
            "Epoch 1110, Loss(train/val) 0.56904/0.22440. Took 0.05 sec\n",
            "Epoch 1111, Loss(train/val) 0.60162/0.22409. Took 0.05 sec\n",
            "Epoch 1112, Loss(train/val) 0.55935/0.22545. Took 0.04 sec\n",
            "Epoch 1113, Loss(train/val) 0.60176/0.22519. Took 0.05 sec\n",
            "Epoch 1114, Loss(train/val) 0.58982/0.22342. Took 0.05 sec\n",
            "Epoch 1115, Loss(train/val) 0.56181/0.22237. Took 0.04 sec\n",
            "Epoch 1116, Loss(train/val) 0.56809/0.22109. Took 0.04 sec\n",
            "Epoch 1117, Loss(train/val) 0.58331/0.22088. Took 0.05 sec\n",
            "Epoch 1118, Loss(train/val) 0.60717/0.22111. Took 0.05 sec\n",
            "Epoch 1119, Loss(train/val) 0.56250/0.22086. Took 0.05 sec\n",
            "Epoch 1120, Loss(train/val) 0.61029/0.22038. Took 0.05 sec\n",
            "Epoch 1121, Loss(train/val) 0.59717/0.22027. Took 0.04 sec\n",
            "Epoch 1122, Loss(train/val) 0.57043/0.21990. Took 0.04 sec\n",
            "Epoch 1123, Loss(train/val) 0.58646/0.21987. Took 0.05 sec\n",
            "Epoch 1124, Loss(train/val) 0.55110/0.22006. Took 0.06 sec\n",
            "Epoch 1125, Loss(train/val) 0.57528/0.21978. Took 0.05 sec\n",
            "Epoch 1126, Loss(train/val) 0.57071/0.21961. Took 0.05 sec\n",
            "Epoch 1127, Loss(train/val) 0.59408/0.22020. Took 0.04 sec\n",
            "Epoch 1128, Loss(train/val) 0.58107/0.22163. Took 0.04 sec\n",
            "Epoch 1129, Loss(train/val) 0.58616/0.22186. Took 0.05 sec\n",
            "Epoch 1130, Loss(train/val) 0.54943/0.22184. Took 0.05 sec\n",
            "Epoch 1131, Loss(train/val) 0.58139/0.22117. Took 0.05 sec\n",
            "Epoch 1132, Loss(train/val) 0.55539/0.22000. Took 0.04 sec\n",
            "Epoch 1133, Loss(train/val) 0.56354/0.21903. Took 0.05 sec\n",
            "Epoch 1134, Loss(train/val) 0.60000/0.21910. Took 0.06 sec\n",
            "Epoch 1135, Loss(train/val) 0.59472/0.21938. Took 0.04 sec\n",
            "Epoch 1136, Loss(train/val) 0.58547/0.21980. Took 0.04 sec\n",
            "Epoch 1137, Loss(train/val) 0.57582/0.21992. Took 0.05 sec\n",
            "Epoch 1138, Loss(train/val) 0.59398/0.21982. Took 0.05 sec\n",
            "Epoch 1139, Loss(train/val) 0.56745/0.21964. Took 0.06 sec\n",
            "Epoch 1140, Loss(train/val) 0.55624/0.21928. Took 0.04 sec\n",
            "Epoch 1141, Loss(train/val) 0.58286/0.21962. Took 0.04 sec\n",
            "Epoch 1142, Loss(train/val) 0.55434/0.21980. Took 0.05 sec\n",
            "Epoch 1143, Loss(train/val) 0.55301/0.22059. Took 0.05 sec\n",
            "Epoch 1144, Loss(train/val) 0.55529/0.22113. Took 0.05 sec\n",
            "Epoch 1145, Loss(train/val) 0.56640/0.22069. Took 0.05 sec\n",
            "Epoch 1146, Loss(train/val) 0.57116/0.21914. Took 0.04 sec\n",
            "Epoch 1147, Loss(train/val) 0.59199/0.21860. Took 0.04 sec\n",
            "Epoch 1148, Loss(train/val) 0.55737/0.21836. Took 0.05 sec\n",
            "Epoch 1149, Loss(train/val) 0.58286/0.21833. Took 0.05 sec\n",
            "Epoch 1150, Loss(train/val) 0.57503/0.21887. Took 0.04 sec\n",
            "Epoch 1151, Loss(train/val) 0.58437/0.21983. Took 0.05 sec\n",
            "Epoch 1152, Loss(train/val) 0.56372/0.22204. Took 0.05 sec\n",
            "Epoch 1153, Loss(train/val) 0.57788/0.22294. Took 0.05 sec\n",
            "Epoch 1154, Loss(train/val) 0.57472/0.22248. Took 0.06 sec\n",
            "Epoch 1155, Loss(train/val) 0.60030/0.22129. Took 0.05 sec\n",
            "Epoch 1156, Loss(train/val) 0.57628/0.21941. Took 0.05 sec\n",
            "Epoch 1157, Loss(train/val) 0.59592/0.21791. Took 0.05 sec\n",
            "Epoch 1158, Loss(train/val) 0.58652/0.21796. Took 0.05 sec\n",
            "Epoch 1159, Loss(train/val) 0.60349/0.21803. Took 0.06 sec\n",
            "Epoch 1160, Loss(train/val) 0.59890/0.21875. Took 0.05 sec\n",
            "Epoch 1161, Loss(train/val) 0.56649/0.21964. Took 0.05 sec\n",
            "Epoch 1162, Loss(train/val) 0.55878/0.22206. Took 0.05 sec\n",
            "Epoch 1163, Loss(train/val) 0.58959/0.22479. Took 0.05 sec\n",
            "Epoch 1164, Loss(train/val) 0.57517/0.22590. Took 0.06 sec\n",
            "Epoch 1165, Loss(train/val) 0.57055/0.23152. Took 0.05 sec\n",
            "Epoch 1166, Loss(train/val) 0.56364/0.23978. Took 0.04 sec\n",
            "Epoch 1167, Loss(train/val) 0.58404/0.24740. Took 0.05 sec\n",
            "Epoch 1168, Loss(train/val) 0.58080/0.25086. Took 0.05 sec\n",
            "Epoch 1169, Loss(train/val) 0.57537/0.25154. Took 0.05 sec\n",
            "Epoch 1170, Loss(train/val) 0.59677/0.24804. Took 0.04 sec\n",
            "Epoch 1171, Loss(train/val) 0.59040/0.24555. Took 0.04 sec\n",
            "Epoch 1172, Loss(train/val) 0.59186/0.24812. Took 0.05 sec\n",
            "Epoch 1173, Loss(train/val) 0.60828/0.24693. Took 0.05 sec\n",
            "Epoch 1174, Loss(train/val) 0.55871/0.23742. Took 0.05 sec\n",
            "Epoch 1175, Loss(train/val) 0.56037/0.23186. Took 0.05 sec\n",
            "Epoch 1176, Loss(train/val) 0.58471/0.22876. Took 0.04 sec\n",
            "Epoch 1177, Loss(train/val) 0.57503/0.22663. Took 0.04 sec\n",
            "Epoch 1178, Loss(train/val) 0.56614/0.22617. Took 0.05 sec\n",
            "Epoch 1179, Loss(train/val) 0.55856/0.22700. Took 0.06 sec\n",
            "Epoch 1180, Loss(train/val) 0.58639/0.22765. Took 0.05 sec\n",
            "Epoch 1181, Loss(train/val) 0.59045/0.22922. Took 0.05 sec\n",
            "Epoch 1182, Loss(train/val) 0.62625/0.23391. Took 0.04 sec\n",
            "Epoch 1183, Loss(train/val) 0.60691/0.23121. Took 0.04 sec\n",
            "Epoch 1184, Loss(train/val) 0.57444/0.22757. Took 0.05 sec\n",
            "Epoch 1185, Loss(train/val) 0.56197/0.22421. Took 0.05 sec\n",
            "Epoch 1186, Loss(train/val) 0.57767/0.22255. Took 0.05 sec\n",
            "Epoch 1187, Loss(train/val) 0.59242/0.22101. Took 0.04 sec\n",
            "Epoch 1188, Loss(train/val) 0.58738/0.21970. Took 0.04 sec\n",
            "Epoch 1189, Loss(train/val) 0.57902/0.21888. Took 0.05 sec\n",
            "Epoch 1190, Loss(train/val) 0.57770/0.21797. Took 0.05 sec\n",
            "Epoch 1191, Loss(train/val) 0.59610/0.21816. Took 0.04 sec\n",
            "Epoch 1192, Loss(train/val) 0.58697/0.21859. Took 0.05 sec\n",
            "Epoch 1193, Loss(train/val) 0.58877/0.21869. Took 0.04 sec\n",
            "Epoch 1194, Loss(train/val) 0.58405/0.21903. Took 0.06 sec\n",
            "Epoch 1195, Loss(train/val) 0.56453/0.21881. Took 0.05 sec\n",
            "Epoch 1196, Loss(train/val) 0.54927/0.21862. Took 0.05 sec\n",
            "Epoch 1197, Loss(train/val) 0.57008/0.21850. Took 0.04 sec\n",
            "Epoch 1198, Loss(train/val) 0.59062/0.21835. Took 0.04 sec\n",
            "Epoch 1199, Loss(train/val) 0.56145/0.21859. Took 0.05 sec\n",
            "Epoch 1200, Loss(train/val) 0.59923/0.21963. Took 0.04 sec\n",
            "Epoch 1201, Loss(train/val) 0.58513/0.21932. Took 0.04 sec\n",
            "Epoch 1202, Loss(train/val) 0.59132/0.21830. Took 0.04 sec\n",
            "Epoch 1203, Loss(train/val) 0.59582/0.21734. Took 0.06 sec\n",
            "Epoch 1204, Loss(train/val) 0.59133/0.21750. Took 0.06 sec\n",
            "Epoch 1205, Loss(train/val) 0.57562/0.21835. Took 0.05 sec\n",
            "Epoch 1206, Loss(train/val) 0.53657/0.21884. Took 0.05 sec\n",
            "Epoch 1207, Loss(train/val) 0.55964/0.22010. Took 0.05 sec\n",
            "Epoch 1208, Loss(train/val) 0.59677/0.22380. Took 0.05 sec\n",
            "Epoch 1209, Loss(train/val) 0.56759/0.22622. Took 0.05 sec\n",
            "Epoch 1210, Loss(train/val) 0.58899/0.22949. Took 0.05 sec\n",
            "Epoch 1211, Loss(train/val) 0.56259/0.23245. Took 0.05 sec\n",
            "Epoch 1212, Loss(train/val) 0.57717/0.23393. Took 0.05 sec\n",
            "Epoch 1213, Loss(train/val) 0.59371/0.23068. Took 0.06 sec\n",
            "Epoch 1214, Loss(train/val) 0.58077/0.22492. Took 0.05 sec\n",
            "Epoch 1215, Loss(train/val) 0.56636/0.22161. Took 0.05 sec\n",
            "Epoch 1216, Loss(train/val) 0.57670/0.22302. Took 0.05 sec\n",
            "Epoch 1217, Loss(train/val) 0.56506/0.22277. Took 0.04 sec\n",
            "Epoch 1218, Loss(train/val) 0.56921/0.22262. Took 0.05 sec\n",
            "Epoch 1219, Loss(train/val) 0.61564/0.22449. Took 0.04 sec\n",
            "Epoch 1220, Loss(train/val) 0.58809/0.23092. Took 0.04 sec\n",
            "Epoch 1221, Loss(train/val) 0.57453/0.23529. Took 0.04 sec\n",
            "Epoch 1222, Loss(train/val) 0.58378/0.23680. Took 0.05 sec\n",
            "Epoch 1223, Loss(train/val) 0.60941/0.23811. Took 0.06 sec\n",
            "Epoch 1224, Loss(train/val) 0.58486/0.23725. Took 0.04 sec\n",
            "Epoch 1225, Loss(train/val) 0.60634/0.23475. Took 0.04 sec\n",
            "Epoch 1226, Loss(train/val) 0.55487/0.23315. Took 0.05 sec\n",
            "Epoch 1227, Loss(train/val) 0.56947/0.23079. Took 0.05 sec\n",
            "Epoch 1228, Loss(train/val) 0.56925/0.22792. Took 0.05 sec\n",
            "Epoch 1229, Loss(train/val) 0.58012/0.22401. Took 0.05 sec\n",
            "Epoch 1230, Loss(train/val) 0.54428/0.22123. Took 0.05 sec\n",
            "Epoch 1231, Loss(train/val) 0.58326/0.21912. Took 0.05 sec\n",
            "Epoch 1232, Loss(train/val) 0.58828/0.21841. Took 0.05 sec\n",
            "Epoch 1233, Loss(train/val) 0.57680/0.21952. Took 0.06 sec\n",
            "Epoch 1234, Loss(train/val) 0.57738/0.21919. Took 0.05 sec\n",
            "Epoch 1235, Loss(train/val) 0.58038/0.21805. Took 0.05 sec\n",
            "Epoch 1236, Loss(train/val) 0.56902/0.21726. Took 0.05 sec\n",
            "Epoch 1237, Loss(train/val) 0.59014/0.21640. Took 0.05 sec\n",
            "Epoch 1238, Loss(train/val) 0.58023/0.21624. Took 0.05 sec\n",
            "Epoch 1239, Loss(train/val) 0.57266/0.21611. Took 0.05 sec\n",
            "Epoch 1240, Loss(train/val) 0.56997/0.21745. Took 0.05 sec\n",
            "Epoch 1241, Loss(train/val) 0.57311/0.21863. Took 0.05 sec\n",
            "Epoch 1242, Loss(train/val) 0.57155/0.21935. Took 0.06 sec\n",
            "Epoch 1243, Loss(train/val) 0.58731/0.22169. Took 0.06 sec\n",
            "Epoch 1244, Loss(train/val) 0.57487/0.22192. Took 0.05 sec\n",
            "Epoch 1245, Loss(train/val) 0.57896/0.22090. Took 0.05 sec\n",
            "Epoch 1246, Loss(train/val) 0.55237/0.22011. Took 0.06 sec\n",
            "Epoch 1247, Loss(train/val) 0.58350/0.21931. Took 0.06 sec\n",
            "Epoch 1248, Loss(train/val) 0.57404/0.21827. Took 0.05 sec\n",
            "Epoch 1249, Loss(train/val) 0.58845/0.21753. Took 0.06 sec\n",
            "Epoch 1250, Loss(train/val) 0.58988/0.21733. Took 0.06 sec\n",
            "Epoch 1251, Loss(train/val) 0.58874/0.21760. Took 0.05 sec\n",
            "Epoch 1252, Loss(train/val) 0.57754/0.21753. Took 0.05 sec\n",
            "Epoch 1253, Loss(train/val) 0.57951/0.21742. Took 0.05 sec\n",
            "Epoch 1254, Loss(train/val) 0.59185/0.21729. Took 0.05 sec\n",
            "Epoch 1255, Loss(train/val) 0.57972/0.21724. Took 0.05 sec\n",
            "Epoch 1256, Loss(train/val) 0.56656/0.21739. Took 0.05 sec\n",
            "Epoch 1257, Loss(train/val) 0.59178/0.21792. Took 0.04 sec\n",
            "Epoch 1258, Loss(train/val) 0.57521/0.21784. Took 0.05 sec\n",
            "Epoch 1259, Loss(train/val) 0.56977/0.21727. Took 0.05 sec\n",
            "Epoch 1260, Loss(train/val) 0.56009/0.21719. Took 0.05 sec\n",
            "Epoch 1261, Loss(train/val) 0.56999/0.21727. Took 0.05 sec\n",
            "Epoch 1262, Loss(train/val) 0.55872/0.21749. Took 0.04 sec\n",
            "Epoch 1263, Loss(train/val) 0.59088/0.21761. Took 0.05 sec\n",
            "Epoch 1264, Loss(train/val) 0.58833/0.21805. Took 0.05 sec\n",
            "Epoch 1265, Loss(train/val) 0.58558/0.21886. Took 0.06 sec\n",
            "Epoch 1266, Loss(train/val) 0.55674/0.22024. Took 0.05 sec\n",
            "Epoch 1267, Loss(train/val) 0.56012/0.22018. Took 0.05 sec\n",
            "Epoch 1268, Loss(train/val) 0.55542/0.21978. Took 0.05 sec\n",
            "Epoch 1269, Loss(train/val) 0.57870/0.21920. Took 0.05 sec\n",
            "Epoch 1270, Loss(train/val) 0.59348/0.22121. Took 0.05 sec\n",
            "Epoch 1271, Loss(train/val) 0.55573/0.22240. Took 0.05 sec\n",
            "Epoch 1272, Loss(train/val) 0.56006/0.22178. Took 0.04 sec\n",
            "Epoch 1273, Loss(train/val) 0.61082/0.22319. Took 0.05 sec\n",
            "Epoch 1274, Loss(train/val) 0.59674/0.22574. Took 0.04 sec\n",
            "Epoch 1275, Loss(train/val) 0.57676/0.22753. Took 0.06 sec\n",
            "Epoch 1276, Loss(train/val) 0.57429/0.22772. Took 0.05 sec\n",
            "Epoch 1277, Loss(train/val) 0.57778/0.22677. Took 0.04 sec\n",
            "Epoch 1278, Loss(train/val) 0.58076/0.22767. Took 0.05 sec\n",
            "Epoch 1279, Loss(train/val) 0.59125/0.22392. Took 0.05 sec\n",
            "Epoch 1280, Loss(train/val) 0.53997/0.21849. Took 0.05 sec\n",
            "Epoch 1281, Loss(train/val) 0.58912/0.21648. Took 0.05 sec\n",
            "Epoch 1282, Loss(train/val) 0.57855/0.21608. Took 0.05 sec\n",
            "Epoch 1283, Loss(train/val) 0.58297/0.21592. Took 0.05 sec\n",
            "Epoch 1284, Loss(train/val) 0.55692/0.21626. Took 0.08 sec\n",
            "Epoch 1285, Loss(train/val) 0.57902/0.21711. Took 0.05 sec\n",
            "Epoch 1286, Loss(train/val) 0.57898/0.21619. Took 0.06 sec\n",
            "Epoch 1287, Loss(train/val) 0.56492/0.21598. Took 0.05 sec\n",
            "Epoch 1288, Loss(train/val) 0.58689/0.21602. Took 0.06 sec\n",
            "Epoch 1289, Loss(train/val) 0.55263/0.21622. Took 0.08 sec\n",
            "Epoch 1290, Loss(train/val) 0.57012/0.21655. Took 0.05 sec\n",
            "Epoch 1291, Loss(train/val) 0.58256/0.21705. Took 0.05 sec\n",
            "Epoch 1292, Loss(train/val) 0.55719/0.21906. Took 0.06 sec\n",
            "Epoch 1293, Loss(train/val) 0.57231/0.22169. Took 0.05 sec\n",
            "Epoch 1294, Loss(train/val) 0.54554/0.22411. Took 0.05 sec\n",
            "Epoch 1295, Loss(train/val) 0.57724/0.22539. Took 0.05 sec\n",
            "Epoch 1296, Loss(train/val) 0.58634/0.22680. Took 0.04 sec\n",
            "Epoch 1297, Loss(train/val) 0.55770/0.22604. Took 0.05 sec\n",
            "Epoch 1298, Loss(train/val) 0.56425/0.22256. Took 0.05 sec\n",
            "Epoch 1299, Loss(train/val) 0.58712/0.21893. Took 0.05 sec\n",
            "Epoch 1300, Loss(train/val) 0.56373/0.21665. Took 0.05 sec\n",
            "Epoch 1301, Loss(train/val) 0.57175/0.21594. Took 0.05 sec\n",
            "Epoch 1302, Loss(train/val) 0.57939/0.21579. Took 0.05 sec\n",
            "Epoch 1303, Loss(train/val) 0.54122/0.21592. Took 0.05 sec\n",
            "Epoch 1304, Loss(train/val) 0.58981/0.21583. Took 0.05 sec\n",
            "Epoch 1305, Loss(train/val) 0.60055/0.21568. Took 0.05 sec\n",
            "Epoch 1306, Loss(train/val) 0.56589/0.21581. Took 0.05 sec\n",
            "Epoch 1307, Loss(train/val) 0.56696/0.21589. Took 0.05 sec\n",
            "Epoch 1308, Loss(train/val) 0.60235/0.21590. Took 0.05 sec\n",
            "Epoch 1309, Loss(train/val) 0.56114/0.21596. Took 0.05 sec\n",
            "Epoch 1310, Loss(train/val) 0.56047/0.21591. Took 0.05 sec\n",
            "Epoch 1311, Loss(train/val) 0.57233/0.21611. Took 0.05 sec\n",
            "Epoch 1312, Loss(train/val) 0.57443/0.21742. Took 0.05 sec\n",
            "Epoch 1313, Loss(train/val) 0.56282/0.21808. Took 0.05 sec\n",
            "Epoch 1314, Loss(train/val) 0.53715/0.22054. Took 0.04 sec\n",
            "Epoch 1315, Loss(train/val) 0.60954/0.22097. Took 0.05 sec\n",
            "Epoch 1316, Loss(train/val) 0.55066/0.21958. Took 0.05 sec\n",
            "Epoch 1317, Loss(train/val) 0.59150/0.21827. Took 0.05 sec\n",
            "Epoch 1318, Loss(train/val) 0.53364/0.21859. Took 0.05 sec\n",
            "Epoch 1319, Loss(train/val) 0.57325/0.21842. Took 0.05 sec\n",
            "Epoch 1320, Loss(train/val) 0.55838/0.21772. Took 0.04 sec\n",
            "Epoch 1321, Loss(train/val) 0.56420/0.21701. Took 0.06 sec\n",
            "Epoch 1322, Loss(train/val) 0.56247/0.21643. Took 0.05 sec\n",
            "Epoch 1323, Loss(train/val) 0.58299/0.21688. Took 0.05 sec\n",
            "Epoch 1324, Loss(train/val) 0.58767/0.21619. Took 0.05 sec\n",
            "Epoch 1325, Loss(train/val) 0.55880/0.21519. Took 0.05 sec\n",
            "Epoch 1326, Loss(train/val) 0.56108/0.21416. Took 0.05 sec\n",
            "Epoch 1327, Loss(train/val) 0.59827/0.21424. Took 0.05 sec\n",
            "Epoch 1328, Loss(train/val) 0.60452/0.21463. Took 0.05 sec\n",
            "Epoch 1329, Loss(train/val) 0.54617/0.21504. Took 0.05 sec\n",
            "Epoch 1330, Loss(train/val) 0.59948/0.21564. Took 0.05 sec\n",
            "Epoch 1331, Loss(train/val) 0.53882/0.21526. Took 0.06 sec\n",
            "Epoch 1332, Loss(train/val) 0.60284/0.21528. Took 0.05 sec\n",
            "Epoch 1333, Loss(train/val) 0.52858/0.21520. Took 0.05 sec\n",
            "Epoch 1334, Loss(train/val) 0.58415/0.21524. Took 0.05 sec\n",
            "Epoch 1335, Loss(train/val) 0.57430/0.21607. Took 0.05 sec\n",
            "Epoch 1336, Loss(train/val) 0.60593/0.21678. Took 0.06 sec\n",
            "Epoch 1337, Loss(train/val) 0.55718/0.21639. Took 0.05 sec\n",
            "Epoch 1338, Loss(train/val) 0.55546/0.21551. Took 0.05 sec\n",
            "Epoch 1339, Loss(train/val) 0.57152/0.21510. Took 0.05 sec\n",
            "Epoch 1340, Loss(train/val) 0.52090/0.21655. Took 0.05 sec\n",
            "Epoch 1341, Loss(train/val) 0.56808/0.21942. Took 0.05 sec\n",
            "Epoch 1342, Loss(train/val) 0.58621/0.22162. Took 0.05 sec\n",
            "Epoch 1343, Loss(train/val) 0.57151/0.21944. Took 0.05 sec\n",
            "Epoch 1344, Loss(train/val) 0.57338/0.21747. Took 0.05 sec\n",
            "Epoch 1345, Loss(train/val) 0.59444/0.21674. Took 0.05 sec\n",
            "Epoch 1346, Loss(train/val) 0.56040/0.21774. Took 0.06 sec\n",
            "Epoch 1347, Loss(train/val) 0.56108/0.21568. Took 0.05 sec\n",
            "Epoch 1348, Loss(train/val) 0.58709/0.21543. Took 0.04 sec\n",
            "Epoch 1349, Loss(train/val) 0.57995/0.22083. Took 0.04 sec\n",
            "Epoch 1350, Loss(train/val) 0.58281/0.22637. Took 0.05 sec\n",
            "Epoch 1351, Loss(train/val) 0.54276/0.22644. Took 0.05 sec\n",
            "Epoch 1352, Loss(train/val) 0.57611/0.22464. Took 0.05 sec\n",
            "Epoch 1353, Loss(train/val) 0.58069/0.22232. Took 0.05 sec\n",
            "Epoch 1354, Loss(train/val) 0.54228/0.22111. Took 0.05 sec\n",
            "Epoch 1355, Loss(train/val) 0.54211/0.21927. Took 0.05 sec\n",
            "Epoch 1356, Loss(train/val) 0.54651/0.21712. Took 0.05 sec\n",
            "Epoch 1357, Loss(train/val) 0.55043/0.21630. Took 0.05 sec\n",
            "Epoch 1358, Loss(train/val) 0.56845/0.21609. Took 0.05 sec\n",
            "Epoch 1359, Loss(train/val) 0.58241/0.21615. Took 0.05 sec\n",
            "Epoch 1360, Loss(train/val) 0.55623/0.21630. Took 0.05 sec\n",
            "Epoch 1361, Loss(train/val) 0.56814/0.21705. Took 0.05 sec\n",
            "Epoch 1362, Loss(train/val) 0.56033/0.21845. Took 0.05 sec\n",
            "Epoch 1363, Loss(train/val) 0.53974/0.21850. Took 0.05 sec\n",
            "Epoch 1364, Loss(train/val) 0.57998/0.21759. Took 0.05 sec\n",
            "Epoch 1365, Loss(train/val) 0.58189/0.21797. Took 0.05 sec\n",
            "Epoch 1366, Loss(train/val) 0.55854/0.21754. Took 0.06 sec\n",
            "Epoch 1367, Loss(train/val) 0.56382/0.21681. Took 0.05 sec\n",
            "Epoch 1368, Loss(train/val) 0.57462/0.21652. Took 0.05 sec\n",
            "Epoch 1369, Loss(train/val) 0.54844/0.21713. Took 0.05 sec\n",
            "Epoch 1370, Loss(train/val) 0.55720/0.21677. Took 0.05 sec\n",
            "Epoch 1371, Loss(train/val) 0.57236/0.21582. Took 0.05 sec\n",
            "Epoch 1372, Loss(train/val) 0.55755/0.21696. Took 0.04 sec\n",
            "Epoch 1373, Loss(train/val) 0.58311/0.21771. Took 0.05 sec\n",
            "Epoch 1374, Loss(train/val) 0.56307/0.21710. Took 0.05 sec\n",
            "Epoch 1375, Loss(train/val) 0.55024/0.21640. Took 0.04 sec\n",
            "Epoch 1376, Loss(train/val) 0.55755/0.21563. Took 0.05 sec\n",
            "Epoch 1377, Loss(train/val) 0.52723/0.21503. Took 0.05 sec\n",
            "Epoch 1378, Loss(train/val) 0.57101/0.21490. Took 0.04 sec\n",
            "Epoch 1379, Loss(train/val) 0.56989/0.21461. Took 0.04 sec\n",
            "Epoch 1380, Loss(train/val) 0.53673/0.21429. Took 0.05 sec\n",
            "Epoch 1381, Loss(train/val) 0.53364/0.21509. Took 0.05 sec\n",
            "Epoch 1382, Loss(train/val) 0.56341/0.21467. Took 0.05 sec\n",
            "Epoch 1383, Loss(train/val) 0.58046/0.21443. Took 0.04 sec\n",
            "Epoch 1384, Loss(train/val) 0.58383/0.21515. Took 0.04 sec\n",
            "Epoch 1385, Loss(train/val) 0.58183/0.21813. Took 0.05 sec\n",
            "Epoch 1386, Loss(train/val) 0.57716/0.22189. Took 0.05 sec\n",
            "Epoch 1387, Loss(train/val) 0.56105/0.22649. Took 0.04 sec\n",
            "Epoch 1388, Loss(train/val) 0.52986/0.22660. Took 0.05 sec\n",
            "Epoch 1389, Loss(train/val) 0.56729/0.22860. Took 0.04 sec\n",
            "Epoch 1390, Loss(train/val) 0.56077/0.22556. Took 0.04 sec\n",
            "Epoch 1391, Loss(train/val) 0.52522/0.22214. Took 0.05 sec\n",
            "Epoch 1392, Loss(train/val) 0.55613/0.22065. Took 0.05 sec\n",
            "Epoch 1393, Loss(train/val) 0.52488/0.22051. Took 0.05 sec\n",
            "Epoch 1394, Loss(train/val) 0.56868/0.22047. Took 0.04 sec\n",
            "Epoch 1395, Loss(train/val) 0.55092/0.21944. Took 0.05 sec\n",
            "Epoch 1396, Loss(train/val) 0.56040/0.21662. Took 0.05 sec\n",
            "Epoch 1397, Loss(train/val) 0.57675/0.21513. Took 0.05 sec\n",
            "Epoch 1398, Loss(train/val) 0.59191/0.21473. Took 0.05 sec\n",
            "Epoch 1399, Loss(train/val) 0.53534/0.21449. Took 0.05 sec\n",
            "Epoch 1400, Loss(train/val) 0.55619/0.21424. Took 0.05 sec\n",
            "Epoch 1401, Loss(train/val) 0.56009/0.21449. Took 0.05 sec\n",
            "Epoch 1402, Loss(train/val) 0.54979/0.21532. Took 0.05 sec\n",
            "Epoch 1403, Loss(train/val) 0.57237/0.21464. Took 0.05 sec\n",
            "Epoch 1404, Loss(train/val) 0.52592/0.21446. Took 0.04 sec\n",
            "Epoch 1405, Loss(train/val) 0.57229/0.21459. Took 0.04 sec\n",
            "Epoch 1406, Loss(train/val) 0.55751/0.21538. Took 0.05 sec\n",
            "Epoch 1407, Loss(train/val) 0.53175/0.21497. Took 0.05 sec\n",
            "Epoch 1408, Loss(train/val) 0.56439/0.21516. Took 0.04 sec\n",
            "Epoch 1409, Loss(train/val) 0.55004/0.21456. Took 0.05 sec\n",
            "Epoch 1410, Loss(train/val) 0.54599/0.21389. Took 0.05 sec\n",
            "Epoch 1411, Loss(train/val) 0.56186/0.21356. Took 0.06 sec\n",
            "Epoch 1412, Loss(train/val) 0.55685/0.21334. Took 0.05 sec\n",
            "Epoch 1413, Loss(train/val) 0.55586/0.21333. Took 0.05 sec\n",
            "Epoch 1414, Loss(train/val) 0.52381/0.21441. Took 0.05 sec\n",
            "Epoch 1415, Loss(train/val) 0.58720/0.21368. Took 0.05 sec\n",
            "Epoch 1416, Loss(train/val) 0.57905/0.21331. Took 0.06 sec\n",
            "Epoch 1417, Loss(train/val) 0.58086/0.21351. Took 0.05 sec\n",
            "Epoch 1418, Loss(train/val) 0.54617/0.21352. Took 0.05 sec\n",
            "Epoch 1419, Loss(train/val) 0.55729/0.21438. Took 0.05 sec\n",
            "Epoch 1420, Loss(train/val) 0.55940/0.21593. Took 0.05 sec\n",
            "Epoch 1421, Loss(train/val) 0.53937/0.21650. Took 0.05 sec\n",
            "Epoch 1422, Loss(train/val) 0.56930/0.21645. Took 0.05 sec\n",
            "Epoch 1423, Loss(train/val) 0.60501/0.21678. Took 0.04 sec\n",
            "Epoch 1424, Loss(train/val) 0.59856/0.21777. Took 0.05 sec\n",
            "Epoch 1425, Loss(train/val) 0.55399/0.21595. Took 0.05 sec\n",
            "Epoch 1426, Loss(train/val) 0.58205/0.21437. Took 0.05 sec\n",
            "Epoch 1427, Loss(train/val) 0.55041/0.21347. Took 0.05 sec\n",
            "Epoch 1428, Loss(train/val) 0.56563/0.21379. Took 0.04 sec\n",
            "Epoch 1429, Loss(train/val) 0.60225/0.21416. Took 0.05 sec\n",
            "Epoch 1430, Loss(train/val) 0.53329/0.21412. Took 0.05 sec\n",
            "Epoch 1431, Loss(train/val) 0.53456/0.21410. Took 0.05 sec\n",
            "Epoch 1432, Loss(train/val) 0.54735/0.21370. Took 0.05 sec\n",
            "Epoch 1433, Loss(train/val) 0.56791/0.21332. Took 0.05 sec\n",
            "Epoch 1434, Loss(train/val) 0.56090/0.21319. Took 0.05 sec\n",
            "Epoch 1435, Loss(train/val) 0.53721/0.21362. Took 0.05 sec\n",
            "Epoch 1436, Loss(train/val) 0.56306/0.21341. Took 0.05 sec\n",
            "Epoch 1437, Loss(train/val) 0.54436/0.21349. Took 0.05 sec\n",
            "Epoch 1438, Loss(train/val) 0.54856/0.21450. Took 0.05 sec\n",
            "Epoch 1439, Loss(train/val) 0.55487/0.21646. Took 0.04 sec\n",
            "Epoch 1440, Loss(train/val) 0.58395/0.21678. Took 0.05 sec\n",
            "Epoch 1441, Loss(train/val) 0.55232/0.21798. Took 0.06 sec\n",
            "Epoch 1442, Loss(train/val) 0.54974/0.21853. Took 0.05 sec\n",
            "Epoch 1443, Loss(train/val) 0.56411/0.22029. Took 0.05 sec\n",
            "Epoch 1444, Loss(train/val) 0.55685/0.21983. Took 0.05 sec\n",
            "Epoch 1445, Loss(train/val) 0.55474/0.21752. Took 0.05 sec\n",
            "Epoch 1446, Loss(train/val) 0.56916/0.21860. Took 0.05 sec\n",
            "Epoch 1447, Loss(train/val) 0.53829/0.21714. Took 0.05 sec\n",
            "Epoch 1448, Loss(train/val) 0.54254/0.21659. Took 0.04 sec\n",
            "Epoch 1449, Loss(train/val) 0.57248/0.21533. Took 0.05 sec\n",
            "Epoch 1450, Loss(train/val) 0.52759/0.21686. Took 0.05 sec\n",
            "Epoch 1451, Loss(train/val) 0.54034/0.21830. Took 0.06 sec\n",
            "Epoch 1452, Loss(train/val) 0.57468/0.21650. Took 0.05 sec\n",
            "Epoch 1453, Loss(train/val) 0.59232/0.21450. Took 0.05 sec\n",
            "Epoch 1454, Loss(train/val) 0.58082/0.21274. Took 0.05 sec\n",
            "Epoch 1455, Loss(train/val) 0.55609/0.21395. Took 0.05 sec\n",
            "Epoch 1456, Loss(train/val) 0.56638/0.21659. Took 0.05 sec\n",
            "Epoch 1457, Loss(train/val) 0.54998/0.21804. Took 0.05 sec\n",
            "Epoch 1458, Loss(train/val) 0.54479/0.22093. Took 0.05 sec\n",
            "Epoch 1459, Loss(train/val) 0.59671/0.21799. Took 0.05 sec\n",
            "Epoch 1460, Loss(train/val) 0.52894/0.21516. Took 0.04 sec\n",
            "Epoch 1461, Loss(train/val) 0.55174/0.21525. Took 0.05 sec\n",
            "Epoch 1462, Loss(train/val) 0.56553/0.21385. Took 0.05 sec\n",
            "Epoch 1463, Loss(train/val) 0.55044/0.21321. Took 0.05 sec\n",
            "Epoch 1464, Loss(train/val) 0.54774/0.21341. Took 0.05 sec\n",
            "Epoch 1465, Loss(train/val) 0.55216/0.21272. Took 0.05 sec\n",
            "Epoch 1466, Loss(train/val) 0.54848/0.21228. Took 0.05 sec\n",
            "Epoch 1467, Loss(train/val) 0.57185/0.21237. Took 0.04 sec\n",
            "Epoch 1468, Loss(train/val) 0.57652/0.21249. Took 0.05 sec\n",
            "Epoch 1469, Loss(train/val) 0.55096/0.21216. Took 0.05 sec\n",
            "Epoch 1470, Loss(train/val) 0.55662/0.21224. Took 0.05 sec\n",
            "Epoch 1471, Loss(train/val) 0.55906/0.21305. Took 0.05 sec\n",
            "Epoch 1472, Loss(train/val) 0.56849/0.21372. Took 0.05 sec\n",
            "Epoch 1473, Loss(train/val) 0.55097/0.21406. Took 0.05 sec\n",
            "Epoch 1474, Loss(train/val) 0.55692/0.21303. Took 0.05 sec\n",
            "Epoch 1475, Loss(train/val) 0.58191/0.21218. Took 0.05 sec\n",
            "Epoch 1476, Loss(train/val) 0.54274/0.21209. Took 0.05 sec\n",
            "Epoch 1477, Loss(train/val) 0.55355/0.21268. Took 0.05 sec\n",
            "Epoch 1478, Loss(train/val) 0.55293/0.21297. Took 0.05 sec\n",
            "Epoch 1479, Loss(train/val) 0.56043/0.21254. Took 0.05 sec\n",
            "Epoch 1480, Loss(train/val) 0.55634/0.21251. Took 0.04 sec\n",
            "Epoch 1481, Loss(train/val) 0.58056/0.21262. Took 0.05 sec\n",
            "Epoch 1482, Loss(train/val) 0.55408/0.21287. Took 0.05 sec\n",
            "Epoch 1483, Loss(train/val) 0.54348/0.21279. Took 0.05 sec\n",
            "Epoch 1484, Loss(train/val) 0.54501/0.21266. Took 0.05 sec\n",
            "Epoch 1485, Loss(train/val) 0.56811/0.21416. Took 0.05 sec\n",
            "Epoch 1486, Loss(train/val) 0.54645/0.21575. Took 0.05 sec\n",
            "Epoch 1487, Loss(train/val) 0.54330/0.21476. Took 0.05 sec\n",
            "Epoch 1488, Loss(train/val) 0.55932/0.21404. Took 0.05 sec\n",
            "Epoch 1489, Loss(train/val) 0.55385/0.21372. Took 0.04 sec\n",
            "Epoch 1490, Loss(train/val) 0.55311/0.21368. Took 0.05 sec\n",
            "Epoch 1491, Loss(train/val) 0.57830/0.21277. Took 0.05 sec\n",
            "Epoch 1492, Loss(train/val) 0.58433/0.21191. Took 0.04 sec\n",
            "Epoch 1493, Loss(train/val) 0.56203/0.21171. Took 0.05 sec\n",
            "Epoch 1494, Loss(train/val) 0.58200/0.21194. Took 0.05 sec\n",
            "Epoch 1495, Loss(train/val) 0.55830/0.21178. Took 0.05 sec\n",
            "Epoch 1496, Loss(train/val) 0.55973/0.21175. Took 0.05 sec\n",
            "Epoch 1497, Loss(train/val) 0.57316/0.21203. Took 0.05 sec\n",
            "Epoch 1498, Loss(train/val) 0.56398/0.21178. Took 0.05 sec\n",
            "Epoch 1499, Loss(train/val) 0.57113/0.21180. Took 0.05 sec\n",
            "Epoch 1500, Loss(train/val) 0.54179/0.21234. Took 0.04 sec\n",
            "Epoch 1501, Loss(train/val) 0.55942/0.21264. Took 0.05 sec\n",
            "Epoch 1502, Loss(train/val) 0.54127/0.21315. Took 0.05 sec\n",
            "Epoch 1503, Loss(train/val) 0.53857/0.21223. Took 0.05 sec\n",
            "Epoch 1504, Loss(train/val) 0.55580/0.21181. Took 0.05 sec\n",
            "Epoch 1505, Loss(train/val) 0.58129/0.21312. Took 0.04 sec\n",
            "Epoch 1506, Loss(train/val) 0.56789/0.21741. Took 0.05 sec\n",
            "Epoch 1507, Loss(train/val) 0.57517/0.21908. Took 0.06 sec\n",
            "Epoch 1508, Loss(train/val) 0.55160/0.22533. Took 0.05 sec\n",
            "Epoch 1509, Loss(train/val) 0.54524/0.22554. Took 0.05 sec\n",
            "Epoch 1510, Loss(train/val) 0.54393/0.22220. Took 0.05 sec\n",
            "Epoch 1511, Loss(train/val) 0.56842/0.21785. Took 0.05 sec\n",
            "Epoch 1512, Loss(train/val) 0.53711/0.21614. Took 0.05 sec\n",
            "Epoch 1513, Loss(train/val) 0.55872/0.21487. Took 0.05 sec\n",
            "Epoch 1514, Loss(train/val) 0.54921/0.21407. Took 0.05 sec\n",
            "Epoch 1515, Loss(train/val) 0.55811/0.21341. Took 0.05 sec\n",
            "Epoch 1516, Loss(train/val) 0.58558/0.21266. Took 0.05 sec\n",
            "Epoch 1517, Loss(train/val) 0.58447/0.21351. Took 0.05 sec\n",
            "Epoch 1518, Loss(train/val) 0.54151/0.21415. Took 0.05 sec\n",
            "Epoch 1519, Loss(train/val) 0.54113/0.21314. Took 0.05 sec\n",
            "Epoch 1520, Loss(train/val) 0.54484/0.21243. Took 0.05 sec\n",
            "Epoch 1521, Loss(train/val) 0.54736/0.21164. Took 0.05 sec\n",
            "Epoch 1522, Loss(train/val) 0.55708/0.21152. Took 0.05 sec\n",
            "Epoch 1523, Loss(train/val) 0.51045/0.21202. Took 0.05 sec\n",
            "Epoch 1524, Loss(train/val) 0.54444/0.21265. Took 0.04 sec\n",
            "Epoch 1525, Loss(train/val) 0.54508/0.21198. Took 0.05 sec\n",
            "Epoch 1526, Loss(train/val) 0.57738/0.21235. Took 0.05 sec\n",
            "Epoch 1527, Loss(train/val) 0.54457/0.21249. Took 0.05 sec\n",
            "Epoch 1528, Loss(train/val) 0.56861/0.21194. Took 0.05 sec\n",
            "Epoch 1529, Loss(train/val) 0.55612/0.21092. Took 0.05 sec\n",
            "Epoch 1530, Loss(train/val) 0.56017/0.21006. Took 0.05 sec\n",
            "Epoch 1531, Loss(train/val) 0.55463/0.20996. Took 0.05 sec\n",
            "Epoch 1532, Loss(train/val) 0.56895/0.21009. Took 0.05 sec\n",
            "Epoch 1533, Loss(train/val) 0.52516/0.21008. Took 0.05 sec\n",
            "Epoch 1534, Loss(train/val) 0.52885/0.21001. Took 0.05 sec\n",
            "Epoch 1535, Loss(train/val) 0.56123/0.21088. Took 0.06 sec\n",
            "Epoch 1536, Loss(train/val) 0.57014/0.21244. Took 0.05 sec\n",
            "Epoch 1537, Loss(train/val) 0.55420/0.21178. Took 0.06 sec\n",
            "Epoch 1538, Loss(train/val) 0.54289/0.21164. Took 0.05 sec\n",
            "Epoch 1539, Loss(train/val) 0.54993/0.21161. Took 0.05 sec\n",
            "Epoch 1540, Loss(train/val) 0.56611/0.21155. Took 0.05 sec\n",
            "Epoch 1541, Loss(train/val) 0.53361/0.21216. Took 0.05 sec\n",
            "Epoch 1542, Loss(train/val) 0.58121/0.21339. Took 0.05 sec\n",
            "Epoch 1543, Loss(train/val) 0.60159/0.21367. Took 0.04 sec\n",
            "Epoch 1544, Loss(train/val) 0.53827/0.21492. Took 0.05 sec\n",
            "Epoch 1545, Loss(train/val) 0.52979/0.21624. Took 0.05 sec\n",
            "Epoch 1546, Loss(train/val) 0.53651/0.21743. Took 0.05 sec\n",
            "Epoch 1547, Loss(train/val) 0.57511/0.21377. Took 0.04 sec\n",
            "Epoch 1548, Loss(train/val) 0.59900/0.21101. Took 0.04 sec\n",
            "Epoch 1549, Loss(train/val) 0.55482/0.21165. Took 0.05 sec\n",
            "Epoch 1550, Loss(train/val) 0.53698/0.21152. Took 0.05 sec\n",
            "Epoch 1551, Loss(train/val) 0.54367/0.21165. Took 0.05 sec\n",
            "Epoch 1552, Loss(train/val) 0.56355/0.21171. Took 0.05 sec\n",
            "Epoch 1553, Loss(train/val) 0.58558/0.21218. Took 0.05 sec\n",
            "Epoch 1554, Loss(train/val) 0.54641/0.21280. Took 0.05 sec\n",
            "Epoch 1555, Loss(train/val) 0.56828/0.21234. Took 0.05 sec\n",
            "Epoch 1556, Loss(train/val) 0.54874/0.21244. Took 0.06 sec\n",
            "Epoch 1557, Loss(train/val) 0.57234/0.21270. Took 0.05 sec\n",
            "Epoch 1558, Loss(train/val) 0.55119/0.21353. Took 0.05 sec\n",
            "Epoch 1559, Loss(train/val) 0.56207/0.21420. Took 0.05 sec\n",
            "Epoch 1560, Loss(train/val) 0.57379/0.21340. Took 0.05 sec\n",
            "Epoch 1561, Loss(train/val) 0.53344/0.21407. Took 0.05 sec\n",
            "Epoch 1562, Loss(train/val) 0.56661/0.21249. Took 0.05 sec\n",
            "Epoch 1563, Loss(train/val) 0.53526/0.21200. Took 0.05 sec\n",
            "Epoch 1564, Loss(train/val) 0.56372/0.21066. Took 0.05 sec\n",
            "Epoch 1565, Loss(train/val) 0.59488/0.21075. Took 0.05 sec\n",
            "Epoch 1566, Loss(train/val) 0.56234/0.21059. Took 0.04 sec\n",
            "Epoch 1567, Loss(train/val) 0.57354/0.21044. Took 0.05 sec\n",
            "Epoch 1568, Loss(train/val) 0.55615/0.21028. Took 0.05 sec\n",
            "Epoch 1569, Loss(train/val) 0.57910/0.21016. Took 0.05 sec\n",
            "Epoch 1570, Loss(train/val) 0.52667/0.21031. Took 0.05 sec\n",
            "Epoch 1571, Loss(train/val) 0.57591/0.21039. Took 0.05 sec\n",
            "Epoch 1572, Loss(train/val) 0.57453/0.21189. Took 0.05 sec\n",
            "Epoch 1573, Loss(train/val) 0.58041/0.21347. Took 0.05 sec\n",
            "Epoch 1574, Loss(train/val) 0.55382/0.21393. Took 0.05 sec\n",
            "Epoch 1575, Loss(train/val) 0.54844/0.21439. Took 0.06 sec\n",
            "Epoch 1576, Loss(train/val) 0.54078/0.21360. Took 0.05 sec\n",
            "Epoch 1577, Loss(train/val) 0.54892/0.21177. Took 0.05 sec\n",
            "Epoch 1578, Loss(train/val) 0.55097/0.21014. Took 0.05 sec\n",
            "Epoch 1579, Loss(train/val) 0.55043/0.20971. Took 0.05 sec\n",
            "Epoch 1580, Loss(train/val) 0.55644/0.20944. Took 0.06 sec\n",
            "Epoch 1581, Loss(train/val) 0.54805/0.20952. Took 0.05 sec\n",
            "Epoch 1582, Loss(train/val) 0.52748/0.21003. Took 0.04 sec\n",
            "Epoch 1583, Loss(train/val) 0.55938/0.21065. Took 0.05 sec\n",
            "Epoch 1584, Loss(train/val) 0.56236/0.21000. Took 0.05 sec\n",
            "Epoch 1585, Loss(train/val) 0.56033/0.21022. Took 0.05 sec\n",
            "Epoch 1586, Loss(train/val) 0.57759/0.21026. Took 0.04 sec\n",
            "Epoch 1587, Loss(train/val) 0.58825/0.21072. Took 0.05 sec\n",
            "Epoch 1588, Loss(train/val) 0.57611/0.21055. Took 0.05 sec\n",
            "Epoch 1589, Loss(train/val) 0.57608/0.21034. Took 0.05 sec\n",
            "Epoch 1590, Loss(train/val) 0.56544/0.21041. Took 0.05 sec\n",
            "Epoch 1591, Loss(train/val) 0.55933/0.21058. Took 0.05 sec\n",
            "Epoch 1592, Loss(train/val) 0.52681/0.21050. Took 0.05 sec\n",
            "Epoch 1593, Loss(train/val) 0.56601/0.21086. Took 0.05 sec\n",
            "Epoch 1594, Loss(train/val) 0.56076/0.21042. Took 0.05 sec\n",
            "Epoch 1595, Loss(train/val) 0.56381/0.21064. Took 0.05 sec\n",
            "Epoch 1596, Loss(train/val) 0.56281/0.21065. Took 0.05 sec\n",
            "Epoch 1597, Loss(train/val) 0.56421/0.21073. Took 0.05 sec\n",
            "Epoch 1598, Loss(train/val) 0.55610/0.21086. Took 0.06 sec\n",
            "Epoch 1599, Loss(train/val) 0.53055/0.21143. Took 0.05 sec\n",
            "Epoch 1600, Loss(train/val) 0.54337/0.21155. Took 0.06 sec\n",
            "Epoch 1601, Loss(train/val) 0.56593/0.21120. Took 0.05 sec\n",
            "Epoch 1602, Loss(train/val) 0.53519/0.21048. Took 0.05 sec\n",
            "Epoch 1603, Loss(train/val) 0.51184/0.21023. Took 0.05 sec\n",
            "Epoch 1604, Loss(train/val) 0.55052/0.20950. Took 0.05 sec\n",
            "Epoch 1605, Loss(train/val) 0.55290/0.20975. Took 0.05 sec\n",
            "Epoch 1606, Loss(train/val) 0.56258/0.20976. Took 0.04 sec\n",
            "Epoch 1607, Loss(train/val) 0.54102/0.21048. Took 0.04 sec\n",
            "Epoch 1608, Loss(train/val) 0.55951/0.21291. Took 0.04 sec\n",
            "Epoch 1609, Loss(train/val) 0.53810/0.21549. Took 0.05 sec\n",
            "Epoch 1610, Loss(train/val) 0.53873/0.21394. Took 0.05 sec\n",
            "Epoch 1611, Loss(train/val) 0.58395/0.21234. Took 0.05 sec\n",
            "Epoch 1612, Loss(train/val) 0.53905/0.21015. Took 0.05 sec\n",
            "Epoch 1613, Loss(train/val) 0.57247/0.20996. Took 0.05 sec\n",
            "Epoch 1614, Loss(train/val) 0.52108/0.20964. Took 0.05 sec\n",
            "Epoch 1615, Loss(train/val) 0.57353/0.20854. Took 0.04 sec\n",
            "Epoch 1616, Loss(train/val) 0.54096/0.20818. Took 0.05 sec\n",
            "Epoch 1617, Loss(train/val) 0.55352/0.20887. Took 0.05 sec\n",
            "Epoch 1618, Loss(train/val) 0.60269/0.20865. Took 0.05 sec\n",
            "Epoch 1619, Loss(train/val) 0.57848/0.20844. Took 0.06 sec\n",
            "Epoch 1620, Loss(train/val) 0.56014/0.20879. Took 0.05 sec\n",
            "Epoch 1621, Loss(train/val) 0.58412/0.20866. Took 0.05 sec\n",
            "Epoch 1622, Loss(train/val) 0.54521/0.20847. Took 0.05 sec\n",
            "Epoch 1623, Loss(train/val) 0.51702/0.20770. Took 0.05 sec\n",
            "Epoch 1624, Loss(train/val) 0.54439/0.20890. Took 0.05 sec\n",
            "Epoch 1625, Loss(train/val) 0.52500/0.21182. Took 0.05 sec\n",
            "Epoch 1626, Loss(train/val) 0.56039/0.21168. Took 0.05 sec\n",
            "Epoch 1627, Loss(train/val) 0.57555/0.21032. Took 0.05 sec\n",
            "Epoch 1628, Loss(train/val) 0.52678/0.21156. Took 0.05 sec\n",
            "Epoch 1629, Loss(train/val) 0.55779/0.21144. Took 0.05 sec\n",
            "Epoch 1630, Loss(train/val) 0.53882/0.21089. Took 0.05 sec\n",
            "Epoch 1631, Loss(train/val) 0.53710/0.21125. Took 0.05 sec\n",
            "Epoch 1632, Loss(train/val) 0.52847/0.21323. Took 0.04 sec\n",
            "Epoch 1633, Loss(train/val) 0.53264/0.21360. Took 0.05 sec\n",
            "Epoch 1634, Loss(train/val) 0.53614/0.21266. Took 0.05 sec\n",
            "Epoch 1635, Loss(train/val) 0.56100/0.21204. Took 0.05 sec\n",
            "Epoch 1636, Loss(train/val) 0.52744/0.21311. Took 0.04 sec\n",
            "Epoch 1637, Loss(train/val) 0.60611/0.20968. Took 0.05 sec\n",
            "Epoch 1638, Loss(train/val) 0.55237/0.20902. Took 0.05 sec\n",
            "Epoch 1639, Loss(train/val) 0.55271/0.21026. Took 0.05 sec\n",
            "Epoch 1640, Loss(train/val) 0.53715/0.21123. Took 0.05 sec\n",
            "Epoch 1641, Loss(train/val) 0.54480/0.21030. Took 0.05 sec\n",
            "Epoch 1642, Loss(train/val) 0.53309/0.21009. Took 0.05 sec\n",
            "Epoch 1643, Loss(train/val) 0.53112/0.21047. Took 0.05 sec\n",
            "Epoch 1644, Loss(train/val) 0.55322/0.21231. Took 0.05 sec\n",
            "Epoch 1645, Loss(train/val) 0.55098/0.21319. Took 0.05 sec\n",
            "Epoch 1646, Loss(train/val) 0.53946/0.21431. Took 0.05 sec\n",
            "Epoch 1647, Loss(train/val) 0.55150/0.21375. Took 0.05 sec\n",
            "Epoch 1648, Loss(train/val) 0.55049/0.21179. Took 0.05 sec\n",
            "Epoch 1649, Loss(train/val) 0.55649/0.20914. Took 0.05 sec\n",
            "Epoch 1650, Loss(train/val) 0.56290/0.20846. Took 0.05 sec\n",
            "Epoch 1651, Loss(train/val) 0.54894/0.20805. Took 0.05 sec\n",
            "Epoch 1652, Loss(train/val) 0.53129/0.20713. Took 0.05 sec\n",
            "Epoch 1653, Loss(train/val) 0.54725/0.20727. Took 0.04 sec\n",
            "Epoch 1654, Loss(train/val) 0.55090/0.20755. Took 0.05 sec\n",
            "Epoch 1655, Loss(train/val) 0.54617/0.20805. Took 0.05 sec\n",
            "Epoch 1656, Loss(train/val) 0.55215/0.20733. Took 0.05 sec\n",
            "Epoch 1657, Loss(train/val) 0.54263/0.20831. Took 0.04 sec\n",
            "Epoch 1658, Loss(train/val) 0.56814/0.21081. Took 0.05 sec\n",
            "Epoch 1659, Loss(train/val) 0.53659/0.21147. Took 0.05 sec\n",
            "Epoch 1660, Loss(train/val) 0.54354/0.21118. Took 0.05 sec\n",
            "Epoch 1661, Loss(train/val) 0.56247/0.21208. Took 0.06 sec\n",
            "Epoch 1662, Loss(train/val) 0.58001/0.21012. Took 0.05 sec\n",
            "Epoch 1663, Loss(train/val) 0.54897/0.20818. Took 0.05 sec\n",
            "Epoch 1664, Loss(train/val) 0.55399/0.20720. Took 0.05 sec\n",
            "Epoch 1665, Loss(train/val) 0.54208/0.20835. Took 0.05 sec\n",
            "Epoch 1666, Loss(train/val) 0.54553/0.21185. Took 0.05 sec\n",
            "Epoch 1667, Loss(train/val) 0.53481/0.21455. Took 0.05 sec\n",
            "Epoch 1668, Loss(train/val) 0.60082/0.21633. Took 0.05 sec\n",
            "Epoch 1669, Loss(train/val) 0.56846/0.21307. Took 0.05 sec\n",
            "Epoch 1670, Loss(train/val) 0.56436/0.21028. Took 0.05 sec\n",
            "Epoch 1671, Loss(train/val) 0.55147/0.20977. Took 0.05 sec\n",
            "Epoch 1672, Loss(train/val) 0.52254/0.20811. Took 0.05 sec\n",
            "Epoch 1673, Loss(train/val) 0.55257/0.20723. Took 0.05 sec\n",
            "Epoch 1674, Loss(train/val) 0.50613/0.20707. Took 0.05 sec\n",
            "Epoch 1675, Loss(train/val) 0.54918/0.20774. Took 0.05 sec\n",
            "Epoch 1676, Loss(train/val) 0.54685/0.20817. Took 0.05 sec\n",
            "Epoch 1677, Loss(train/val) 0.55520/0.20802. Took 0.05 sec\n",
            "Epoch 1678, Loss(train/val) 0.55295/0.20766. Took 0.05 sec\n",
            "Epoch 1679, Loss(train/val) 0.53496/0.20767. Took 0.05 sec\n",
            "Epoch 1680, Loss(train/val) 0.53280/0.20763. Took 0.05 sec\n",
            "Epoch 1681, Loss(train/val) 0.53161/0.20953. Took 0.06 sec\n",
            "Epoch 1682, Loss(train/val) 0.55705/0.21238. Took 0.06 sec\n",
            "Epoch 1683, Loss(train/val) 0.54057/0.21259. Took 0.05 sec\n",
            "Epoch 1684, Loss(train/val) 0.53680/0.21266. Took 0.05 sec\n",
            "Epoch 1685, Loss(train/val) 0.54480/0.21105. Took 0.04 sec\n",
            "Epoch 1686, Loss(train/val) 0.55552/0.21070. Took 0.05 sec\n",
            "Epoch 1687, Loss(train/val) 0.55991/0.20759. Took 0.06 sec\n",
            "Epoch 1688, Loss(train/val) 0.56918/0.20608. Took 0.05 sec\n",
            "Epoch 1689, Loss(train/val) 0.58733/0.20617. Took 0.05 sec\n",
            "Epoch 1690, Loss(train/val) 0.52955/0.20647. Took 0.05 sec\n",
            "Epoch 1691, Loss(train/val) 0.54653/0.20730. Took 0.05 sec\n",
            "Epoch 1692, Loss(train/val) 0.51571/0.20633. Took 0.05 sec\n",
            "Epoch 1693, Loss(train/val) 0.54186/0.20542. Took 0.05 sec\n",
            "Epoch 1694, Loss(train/val) 0.56780/0.20603. Took 0.05 sec\n",
            "Epoch 1695, Loss(train/val) 0.50911/0.20570. Took 0.05 sec\n",
            "Epoch 1696, Loss(train/val) 0.52693/0.20588. Took 0.05 sec\n",
            "Epoch 1697, Loss(train/val) 0.53219/0.20629. Took 0.06 sec\n",
            "Epoch 1698, Loss(train/val) 0.55282/0.20744. Took 0.05 sec\n",
            "Epoch 1699, Loss(train/val) 0.54696/0.20811. Took 0.05 sec\n",
            "Epoch 1700, Loss(train/val) 0.55519/0.20552. Took 0.04 sec\n",
            "Epoch 1701, Loss(train/val) 0.54204/0.20558. Took 0.04 sec\n",
            "Epoch 1702, Loss(train/val) 0.56966/0.20619. Took 0.07 sec\n",
            "Epoch 1703, Loss(train/val) 0.55220/0.20603. Took 0.05 sec\n",
            "Epoch 1704, Loss(train/val) 0.56406/0.20576. Took 0.05 sec\n",
            "Epoch 1705, Loss(train/val) 0.53720/0.20647. Took 0.05 sec\n",
            "Epoch 1706, Loss(train/val) 0.57066/0.20771. Took 0.05 sec\n",
            "Epoch 1707, Loss(train/val) 0.56582/0.20627. Took 0.05 sec\n",
            "Epoch 1708, Loss(train/val) 0.55543/0.20585. Took 0.05 sec\n",
            "Epoch 1709, Loss(train/val) 0.51877/0.20584. Took 0.05 sec\n",
            "Epoch 1710, Loss(train/val) 0.54634/0.20590. Took 0.05 sec\n",
            "Epoch 1711, Loss(train/val) 0.55959/0.20587. Took 0.05 sec\n",
            "Epoch 1712, Loss(train/val) 0.52519/0.20572. Took 0.06 sec\n",
            "Epoch 1713, Loss(train/val) 0.53682/0.20568. Took 0.05 sec\n",
            "Epoch 1714, Loss(train/val) 0.56943/0.20629. Took 0.05 sec\n",
            "Epoch 1715, Loss(train/val) 0.57570/0.20831. Took 0.05 sec\n",
            "Epoch 1716, Loss(train/val) 0.54717/0.20876. Took 0.05 sec\n",
            "Epoch 1717, Loss(train/val) 0.52540/0.20829. Took 0.06 sec\n",
            "Epoch 1718, Loss(train/val) 0.52440/0.20782. Took 0.05 sec\n",
            "Epoch 1719, Loss(train/val) 0.56843/0.20695. Took 0.05 sec\n",
            "Epoch 1720, Loss(train/val) 0.50373/0.20563. Took 0.05 sec\n",
            "Epoch 1721, Loss(train/val) 0.52346/0.20585. Took 0.05 sec\n",
            "Epoch 1722, Loss(train/val) 0.51598/0.20609. Took 0.06 sec\n",
            "Epoch 1723, Loss(train/val) 0.53697/0.20602. Took 0.05 sec\n",
            "Epoch 1724, Loss(train/val) 0.52029/0.20611. Took 0.05 sec\n",
            "Epoch 1725, Loss(train/val) 0.53985/0.20610. Took 0.05 sec\n",
            "Epoch 1726, Loss(train/val) 0.51572/0.20600. Took 0.05 sec\n",
            "Epoch 1727, Loss(train/val) 0.55342/0.20732. Took 0.05 sec\n",
            "Epoch 1728, Loss(train/val) 0.57233/0.20723. Took 0.05 sec\n",
            "Epoch 1729, Loss(train/val) 0.53218/0.20733. Took 0.06 sec\n",
            "Epoch 1730, Loss(train/val) 0.54067/0.20647. Took 0.05 sec\n",
            "Epoch 1731, Loss(train/val) 0.53923/0.20566. Took 0.05 sec\n",
            "Epoch 1732, Loss(train/val) 0.51271/0.20567. Took 0.05 sec\n",
            "Epoch 1733, Loss(train/val) 0.51381/0.20695. Took 0.05 sec\n",
            "Epoch 1734, Loss(train/val) 0.51569/0.20923. Took 0.05 sec\n",
            "Epoch 1735, Loss(train/val) 0.54314/0.20974. Took 0.05 sec\n",
            "Epoch 1736, Loss(train/val) 0.52356/0.21141. Took 0.06 sec\n",
            "Epoch 1737, Loss(train/val) 0.55980/0.21245. Took 0.05 sec\n",
            "Epoch 1738, Loss(train/val) 0.56173/0.21341. Took 0.05 sec\n",
            "Epoch 1739, Loss(train/val) 0.55104/0.21383. Took 0.05 sec\n",
            "Epoch 1740, Loss(train/val) 0.53667/0.21850. Took 0.05 sec\n",
            "Epoch 1741, Loss(train/val) 0.52431/0.22117. Took 0.06 sec\n",
            "Epoch 1742, Loss(train/val) 0.51729/0.21568. Took 0.05 sec\n",
            "Epoch 1743, Loss(train/val) 0.52763/0.21089. Took 0.05 sec\n",
            "Epoch 1744, Loss(train/val) 0.55545/0.20683. Took 0.05 sec\n",
            "Epoch 1745, Loss(train/val) 0.50071/0.20458. Took 0.05 sec\n",
            "Epoch 1746, Loss(train/val) 0.52342/0.20477. Took 0.06 sec\n",
            "Epoch 1747, Loss(train/val) 0.53589/0.20518. Took 0.05 sec\n",
            "Epoch 1748, Loss(train/val) 0.57331/0.20703. Took 0.05 sec\n",
            "Epoch 1749, Loss(train/val) 0.52351/0.21122. Took 0.05 sec\n",
            "Epoch 1750, Loss(train/val) 0.55044/0.21334. Took 0.05 sec\n",
            "Epoch 1751, Loss(train/val) 0.56379/0.20946. Took 0.06 sec\n",
            "Epoch 1752, Loss(train/val) 0.51505/0.20970. Took 0.05 sec\n",
            "Epoch 1753, Loss(train/val) 0.53083/0.20823. Took 0.05 sec\n",
            "Epoch 1754, Loss(train/val) 0.51035/0.20936. Took 0.05 sec\n",
            "Epoch 1755, Loss(train/val) 0.54577/0.20803. Took 0.05 sec\n",
            "Epoch 1756, Loss(train/val) 0.50780/0.20777. Took 0.05 sec\n",
            "Epoch 1757, Loss(train/val) 0.53658/0.20535. Took 0.05 sec\n",
            "Epoch 1758, Loss(train/val) 0.54322/0.20473. Took 0.05 sec\n",
            "Epoch 1759, Loss(train/val) 0.55331/0.20373. Took 0.06 sec\n",
            "Epoch 1760, Loss(train/val) 0.58136/0.20632. Took 0.05 sec\n",
            "Epoch 1761, Loss(train/val) 0.55383/0.20982. Took 0.05 sec\n",
            "Epoch 1762, Loss(train/val) 0.55404/0.20861. Took 0.05 sec\n",
            "Epoch 1763, Loss(train/val) 0.56758/0.20759. Took 0.05 sec\n",
            "Epoch 1764, Loss(train/val) 0.51690/0.20905. Took 0.05 sec\n",
            "Epoch 1765, Loss(train/val) 0.52952/0.21247. Took 0.05 sec\n",
            "Epoch 1766, Loss(train/val) 0.50995/0.21509. Took 0.05 sec\n",
            "Epoch 1767, Loss(train/val) 0.52618/0.21679. Took 0.05 sec\n",
            "Epoch 1768, Loss(train/val) 0.54607/0.21665. Took 0.05 sec\n",
            "Epoch 1769, Loss(train/val) 0.56541/0.21206. Took 0.05 sec\n",
            "Epoch 1770, Loss(train/val) 0.55003/0.20652. Took 0.06 sec\n",
            "Epoch 1771, Loss(train/val) 0.54927/0.20347. Took 0.05 sec\n",
            "Epoch 1772, Loss(train/val) 0.55850/0.20402. Took 0.05 sec\n",
            "Epoch 1773, Loss(train/val) 0.54143/0.20427. Took 0.05 sec\n",
            "Epoch 1774, Loss(train/val) 0.55419/0.20429. Took 0.05 sec\n",
            "Epoch 1775, Loss(train/val) 0.52698/0.20336. Took 0.06 sec\n",
            "Epoch 1776, Loss(train/val) 0.55521/0.20398. Took 0.05 sec\n",
            "Epoch 1777, Loss(train/val) 0.53162/0.20597. Took 0.05 sec\n",
            "Epoch 1778, Loss(train/val) 0.56265/0.20667. Took 0.05 sec\n",
            "Epoch 1779, Loss(train/val) 0.56378/0.20422. Took 0.05 sec\n",
            "Epoch 1780, Loss(train/val) 0.53261/0.20334. Took 0.05 sec\n",
            "Epoch 1781, Loss(train/val) 0.52625/0.20383. Took 0.05 sec\n",
            "Epoch 1782, Loss(train/val) 0.55785/0.20515. Took 0.05 sec\n",
            "Epoch 1783, Loss(train/val) 0.54966/0.20541. Took 0.05 sec\n",
            "Epoch 1784, Loss(train/val) 0.52462/0.20462. Took 0.06 sec\n",
            "Epoch 1785, Loss(train/val) 0.54065/0.20322. Took 0.05 sec\n",
            "Epoch 1786, Loss(train/val) 0.53492/0.20318. Took 0.05 sec\n",
            "Epoch 1787, Loss(train/val) 0.50945/0.20353. Took 0.05 sec\n",
            "Epoch 1788, Loss(train/val) 0.54962/0.20357. Took 0.05 sec\n",
            "Epoch 1789, Loss(train/val) 0.56067/0.20683. Took 0.05 sec\n",
            "Epoch 1790, Loss(train/val) 0.53793/0.21376. Took 0.05 sec\n",
            "Epoch 1791, Loss(train/val) 0.54847/0.21862. Took 0.05 sec\n",
            "Epoch 1792, Loss(train/val) 0.54282/0.21612. Took 0.05 sec\n",
            "Epoch 1793, Loss(train/val) 0.52385/0.21054. Took 0.04 sec\n",
            "Epoch 1794, Loss(train/val) 0.50695/0.20506. Took 0.06 sec\n",
            "Epoch 1795, Loss(train/val) 0.54138/0.20209. Took 0.05 sec\n",
            "Epoch 1796, Loss(train/val) 0.53600/0.20253. Took 0.05 sec\n",
            "Epoch 1797, Loss(train/val) 0.57124/0.20795. Took 0.05 sec\n",
            "Epoch 1798, Loss(train/val) 0.55189/0.21004. Took 0.05 sec\n",
            "Epoch 1799, Loss(train/val) 0.53636/0.20557. Took 0.05 sec\n",
            "Epoch 1800, Loss(train/val) 0.54912/0.20394. Took 0.05 sec\n",
            "Epoch 1801, Loss(train/val) 0.56218/0.20677. Took 0.04 sec\n",
            "Epoch 1802, Loss(train/val) 0.56091/0.20586. Took 0.05 sec\n",
            "Epoch 1803, Loss(train/val) 0.53212/0.20341. Took 0.05 sec\n",
            "Epoch 1804, Loss(train/val) 0.52379/0.20366. Took 0.06 sec\n",
            "Epoch 1805, Loss(train/val) 0.53922/0.20292. Took 0.05 sec\n",
            "Epoch 1806, Loss(train/val) 0.52806/0.20184. Took 0.05 sec\n",
            "Epoch 1807, Loss(train/val) 0.52673/0.20258. Took 0.05 sec\n",
            "Epoch 1808, Loss(train/val) 0.54874/0.20492. Took 0.05 sec\n",
            "Epoch 1809, Loss(train/val) 0.55066/0.20605. Took 0.05 sec\n",
            "Epoch 1810, Loss(train/val) 0.55318/0.20706. Took 0.05 sec\n",
            "Epoch 1811, Loss(train/val) 0.50651/0.20860. Took 0.05 sec\n",
            "Epoch 1812, Loss(train/val) 0.53994/0.20872. Took 0.05 sec\n",
            "Epoch 1813, Loss(train/val) 0.57601/0.20445. Took 0.05 sec\n",
            "Epoch 1814, Loss(train/val) 0.55964/0.20180. Took 0.05 sec\n",
            "Epoch 1815, Loss(train/val) 0.51938/0.20157. Took 0.05 sec\n",
            "Epoch 1816, Loss(train/val) 0.51231/0.20170. Took 0.05 sec\n",
            "Epoch 1817, Loss(train/val) 0.56898/0.20178. Took 0.05 sec\n",
            "Epoch 1818, Loss(train/val) 0.49533/0.20105. Took 0.04 sec\n",
            "Epoch 1819, Loss(train/val) 0.52955/0.20113. Took 0.05 sec\n",
            "Epoch 1820, Loss(train/val) 0.51699/0.20193. Took 0.06 sec\n",
            "Epoch 1821, Loss(train/val) 0.54924/0.20462. Took 0.05 sec\n",
            "Epoch 1822, Loss(train/val) 0.56163/0.20452. Took 0.05 sec\n",
            "Epoch 1823, Loss(train/val) 0.53194/0.20471. Took 0.05 sec\n",
            "Epoch 1824, Loss(train/val) 0.51470/0.20396. Took 0.05 sec\n",
            "Epoch 1825, Loss(train/val) 0.52903/0.20484. Took 0.09 sec\n",
            "Epoch 1826, Loss(train/val) 0.53220/0.20562. Took 0.08 sec\n",
            "Epoch 1827, Loss(train/val) 0.57402/0.20407. Took 0.07 sec\n",
            "Epoch 1828, Loss(train/val) 0.53626/0.20384. Took 0.07 sec\n",
            "Epoch 1829, Loss(train/val) 0.51762/0.20309. Took 0.08 sec\n",
            "Epoch 1830, Loss(train/val) 0.55444/0.20155. Took 0.07 sec\n",
            "Epoch 1831, Loss(train/val) 0.54605/0.20099. Took 0.07 sec\n",
            "Epoch 1832, Loss(train/val) 0.56725/0.20196. Took 0.08 sec\n",
            "Epoch 1833, Loss(train/val) 0.55075/0.20313. Took 0.08 sec\n",
            "Epoch 1834, Loss(train/val) 0.52879/0.20397. Took 0.07 sec\n",
            "Epoch 1835, Loss(train/val) 0.51964/0.20236. Took 0.08 sec\n",
            "Epoch 1836, Loss(train/val) 0.56181/0.20153. Took 0.07 sec\n",
            "Epoch 1837, Loss(train/val) 0.52427/0.20115. Took 0.07 sec\n",
            "Epoch 1838, Loss(train/val) 0.51994/0.20009. Took 0.09 sec\n",
            "Epoch 1839, Loss(train/val) 0.52811/0.20004. Took 0.08 sec\n",
            "Epoch 1840, Loss(train/val) 0.52742/0.20041. Took 0.07 sec\n",
            "Epoch 1841, Loss(train/val) 0.55010/0.20038. Took 0.08 sec\n",
            "Epoch 1842, Loss(train/val) 0.53376/0.20098. Took 0.07 sec\n",
            "Epoch 1843, Loss(train/val) 0.54249/0.20172. Took 0.07 sec\n",
            "Epoch 1844, Loss(train/val) 0.55141/0.20105. Took 0.08 sec\n",
            "Epoch 1845, Loss(train/val) 0.52244/0.20104. Took 0.07 sec\n",
            "Epoch 1846, Loss(train/val) 0.52380/0.20216. Took 0.07 sec\n",
            "Epoch 1847, Loss(train/val) 0.54057/0.20326. Took 0.07 sec\n",
            "Epoch 1848, Loss(train/val) 0.51428/0.20250. Took 0.07 sec\n",
            "Epoch 1849, Loss(train/val) 0.56487/0.20451. Took 0.07 sec\n",
            "Epoch 1850, Loss(train/val) 0.54781/0.20531. Took 0.08 sec\n",
            "Epoch 1851, Loss(train/val) 0.52866/0.20257. Took 0.08 sec\n",
            "Epoch 1852, Loss(train/val) 0.55362/0.19980. Took 0.08 sec\n",
            "Epoch 1853, Loss(train/val) 0.52398/0.19947. Took 0.09 sec\n",
            "Epoch 1854, Loss(train/val) 0.55349/0.19941. Took 0.07 sec\n",
            "Epoch 1855, Loss(train/val) 0.51194/0.19949. Took 0.05 sec\n",
            "Epoch 1856, Loss(train/val) 0.53250/0.20036. Took 0.05 sec\n",
            "Epoch 1857, Loss(train/val) 0.53967/0.20226. Took 0.05 sec\n",
            "Epoch 1858, Loss(train/val) 0.54056/0.20684. Took 0.05 sec\n",
            "Epoch 1859, Loss(train/val) 0.55189/0.21586. Took 0.05 sec\n",
            "Epoch 1860, Loss(train/val) 0.54080/0.22133. Took 0.05 sec\n",
            "Epoch 1861, Loss(train/val) 0.57949/0.22458. Took 0.05 sec\n",
            "Epoch 1862, Loss(train/val) 0.54121/0.21797. Took 0.05 sec\n",
            "Epoch 1863, Loss(train/val) 0.54267/0.21355. Took 0.05 sec\n",
            "Epoch 1864, Loss(train/val) 0.54231/0.20939. Took 0.05 sec\n",
            "Epoch 1865, Loss(train/val) 0.56143/0.20457. Took 0.05 sec\n",
            "Epoch 1866, Loss(train/val) 0.52361/0.20319. Took 0.05 sec\n",
            "Epoch 1867, Loss(train/val) 0.51696/0.20461. Took 0.05 sec\n",
            "Epoch 1868, Loss(train/val) 0.53939/0.20549. Took 0.05 sec\n",
            "Epoch 1869, Loss(train/val) 0.53536/0.20396. Took 0.05 sec\n",
            "Epoch 1870, Loss(train/val) 0.53735/0.20189. Took 0.05 sec\n",
            "Epoch 1871, Loss(train/val) 0.55853/0.20085. Took 0.05 sec\n",
            "Epoch 1872, Loss(train/val) 0.51408/0.20062. Took 0.06 sec\n",
            "Epoch 1873, Loss(train/val) 0.53818/0.19996. Took 0.05 sec\n",
            "Epoch 1874, Loss(train/val) 0.53127/0.20066. Took 0.05 sec\n",
            "Epoch 1875, Loss(train/val) 0.53498/0.20048. Took 0.05 sec\n",
            "Epoch 1876, Loss(train/val) 0.52278/0.20057. Took 0.05 sec\n",
            "Epoch 1877, Loss(train/val) 0.52874/0.20032. Took 0.06 sec\n",
            "Epoch 1878, Loss(train/val) 0.51088/0.20175. Took 0.05 sec\n",
            "Epoch 1879, Loss(train/val) 0.51213/0.20716. Took 0.05 sec\n",
            "Epoch 1880, Loss(train/val) 0.55022/0.21168. Took 0.05 sec\n",
            "Epoch 1881, Loss(train/val) 0.55092/0.21491. Took 0.05 sec\n",
            "Epoch 1882, Loss(train/val) 0.53818/0.21844. Took 0.06 sec\n",
            "Epoch 1883, Loss(train/val) 0.54680/0.21209. Took 0.05 sec\n",
            "Epoch 1884, Loss(train/val) 0.52680/0.20344. Took 0.05 sec\n",
            "Epoch 1885, Loss(train/val) 0.53914/0.19952. Took 0.05 sec\n",
            "Epoch 1886, Loss(train/val) 0.50595/0.19885. Took 0.05 sec\n",
            "Epoch 1887, Loss(train/val) 0.51312/0.19908. Took 0.06 sec\n",
            "Epoch 1888, Loss(train/val) 0.55014/0.20226. Took 0.05 sec\n",
            "Epoch 1889, Loss(train/val) 0.53872/0.20479. Took 0.05 sec\n",
            "Epoch 1890, Loss(train/val) 0.52773/0.20616. Took 0.05 sec\n",
            "Epoch 1891, Loss(train/val) 0.52977/0.20336. Took 0.05 sec\n",
            "Epoch 1892, Loss(train/val) 0.52172/0.20027. Took 0.05 sec\n",
            "Epoch 1893, Loss(train/val) 0.54754/0.19885. Took 0.05 sec\n",
            "Epoch 1894, Loss(train/val) 0.52686/0.19857. Took 0.05 sec\n",
            "Epoch 1895, Loss(train/val) 0.53018/0.19973. Took 0.05 sec\n",
            "Epoch 1896, Loss(train/val) 0.53121/0.19898. Took 0.04 sec\n",
            "Epoch 1897, Loss(train/val) 0.51420/0.19754. Took 0.05 sec\n",
            "Epoch 1898, Loss(train/val) 0.56188/0.19772. Took 0.05 sec\n",
            "Epoch 1899, Loss(train/val) 0.52089/0.19794. Took 0.05 sec\n",
            "Epoch 1900, Loss(train/val) 0.52491/0.19788. Took 0.05 sec\n",
            "Epoch 1901, Loss(train/val) 0.53426/0.19801. Took 0.05 sec\n",
            "Epoch 1902, Loss(train/val) 0.54681/0.20060. Took 0.06 sec\n",
            "Epoch 1903, Loss(train/val) 0.51926/0.20127. Took 0.05 sec\n",
            "Epoch 1904, Loss(train/val) 0.52840/0.20104. Took 0.05 sec\n",
            "Epoch 1905, Loss(train/val) 0.52206/0.19750. Took 0.05 sec\n",
            "Epoch 1906, Loss(train/val) 0.51484/0.19703. Took 0.05 sec\n",
            "Epoch 1907, Loss(train/val) 0.51180/0.19691. Took 0.06 sec\n",
            "Epoch 1908, Loss(train/val) 0.54570/0.19692. Took 0.05 sec\n",
            "Epoch 1909, Loss(train/val) 0.53588/0.19779. Took 0.05 sec\n",
            "Epoch 1910, Loss(train/val) 0.52371/0.20029. Took 0.05 sec\n",
            "Epoch 1911, Loss(train/val) 0.56365/0.19910. Took 0.05 sec\n",
            "Epoch 1912, Loss(train/val) 0.54541/0.19914. Took 0.06 sec\n",
            "Epoch 1913, Loss(train/val) 0.55844/0.19746. Took 0.05 sec\n",
            "Epoch 1914, Loss(train/val) 0.56474/0.19663. Took 0.05 sec\n",
            "Epoch 1915, Loss(train/val) 0.53364/0.19909. Took 0.05 sec\n",
            "Epoch 1916, Loss(train/val) 0.48334/0.19852. Took 0.05 sec\n",
            "Epoch 1917, Loss(train/val) 0.53102/0.19761. Took 0.06 sec\n",
            "Epoch 1918, Loss(train/val) 0.48980/0.19711. Took 0.05 sec\n",
            "Epoch 1919, Loss(train/val) 0.54172/0.19711. Took 0.05 sec\n",
            "Epoch 1920, Loss(train/val) 0.53253/0.19840. Took 0.05 sec\n",
            "Epoch 1921, Loss(train/val) 0.52529/0.19929. Took 0.05 sec\n",
            "Epoch 1922, Loss(train/val) 0.48642/0.19792. Took 0.05 sec\n",
            "Epoch 1923, Loss(train/val) 0.54348/0.19689. Took 0.05 sec\n",
            "Epoch 1924, Loss(train/val) 0.55326/0.19698. Took 0.04 sec\n",
            "Epoch 1925, Loss(train/val) 0.53995/0.19739. Took 0.05 sec\n",
            "Epoch 1926, Loss(train/val) 0.55902/0.19893. Took 0.06 sec\n",
            "Epoch 1927, Loss(train/val) 0.55679/0.19907. Took 0.05 sec\n",
            "Epoch 1928, Loss(train/val) 0.53768/0.19979. Took 0.05 sec\n",
            "Epoch 1929, Loss(train/val) 0.50443/0.20064. Took 0.05 sec\n",
            "Epoch 1930, Loss(train/val) 0.52299/0.20056. Took 0.05 sec\n",
            "Epoch 1931, Loss(train/val) 0.53110/0.19895. Took 0.05 sec\n",
            "Epoch 1932, Loss(train/val) 0.51836/0.19642. Took 0.06 sec\n",
            "Epoch 1933, Loss(train/val) 0.51870/0.19526. Took 0.05 sec\n",
            "Epoch 1934, Loss(train/val) 0.53302/0.19985. Took 0.06 sec\n",
            "Epoch 1935, Loss(train/val) 0.55156/0.20621. Took 0.05 sec\n",
            "Epoch 1936, Loss(train/val) 0.50092/0.21686. Took 0.05 sec\n",
            "Epoch 1937, Loss(train/val) 0.52411/0.22607. Took 0.05 sec\n",
            "Epoch 1938, Loss(train/val) 0.54399/0.23689. Took 0.05 sec\n",
            "Epoch 1939, Loss(train/val) 0.54741/0.22601. Took 0.05 sec\n",
            "Epoch 1940, Loss(train/val) 0.51693/0.21033. Took 0.05 sec\n",
            "Epoch 1941, Loss(train/val) 0.53891/0.20186. Took 0.05 sec\n",
            "Epoch 1942, Loss(train/val) 0.53088/0.19848. Took 0.05 sec\n",
            "Epoch 1943, Loss(train/val) 0.52209/0.19641. Took 0.05 sec\n",
            "Epoch 1944, Loss(train/val) 0.52418/0.19629. Took 0.05 sec\n",
            "Epoch 1945, Loss(train/val) 0.54855/0.19443. Took 0.05 sec\n",
            "Epoch 1946, Loss(train/val) 0.51025/0.19391. Took 0.05 sec\n",
            "Epoch 1947, Loss(train/val) 0.51434/0.19432. Took 0.05 sec\n",
            "Epoch 1948, Loss(train/val) 0.53158/0.19455. Took 0.05 sec\n",
            "Epoch 1949, Loss(train/val) 0.52645/0.19534. Took 0.05 sec\n",
            "Epoch 1950, Loss(train/val) 0.50528/0.19398. Took 0.05 sec\n",
            "Epoch 1951, Loss(train/val) 0.52075/0.19365. Took 0.05 sec\n",
            "Epoch 1952, Loss(train/val) 0.53525/0.19369. Took 0.05 sec\n",
            "Epoch 1953, Loss(train/val) 0.53818/0.19374. Took 0.05 sec\n",
            "Epoch 1954, Loss(train/val) 0.55383/0.19419. Took 0.05 sec\n",
            "Epoch 1955, Loss(train/val) 0.52002/0.19991. Took 0.05 sec\n",
            "Epoch 1956, Loss(train/val) 0.53469/0.20678. Took 0.05 sec\n",
            "Epoch 1957, Loss(train/val) 0.55177/0.20905. Took 0.05 sec\n",
            "Epoch 1958, Loss(train/val) 0.54188/0.20714. Took 0.05 sec\n",
            "Epoch 1959, Loss(train/val) 0.55822/0.20541. Took 0.05 sec\n",
            "Epoch 1960, Loss(train/val) 0.52509/0.19796. Took 0.05 sec\n",
            "Epoch 1961, Loss(train/val) 0.52976/0.19421. Took 0.05 sec\n",
            "Epoch 1962, Loss(train/val) 0.51378/0.19356. Took 0.05 sec\n",
            "Epoch 1963, Loss(train/val) 0.53817/0.19514. Took 0.05 sec\n",
            "Epoch 1964, Loss(train/val) 0.50687/0.19445. Took 0.05 sec\n",
            "Epoch 1965, Loss(train/val) 0.53896/0.19393. Took 0.05 sec\n",
            "Epoch 1966, Loss(train/val) 0.51055/0.19396. Took 0.05 sec\n",
            "Epoch 1967, Loss(train/val) 0.55959/0.19606. Took 0.05 sec\n",
            "Epoch 1968, Loss(train/val) 0.52846/0.19380. Took 0.05 sec\n",
            "Epoch 1969, Loss(train/val) 0.53616/0.19313. Took 0.05 sec\n",
            "Epoch 1970, Loss(train/val) 0.54345/0.19240. Took 0.05 sec\n",
            "Epoch 1971, Loss(train/val) 0.54936/0.19258. Took 0.05 sec\n",
            "Epoch 1972, Loss(train/val) 0.53488/0.19240. Took 0.05 sec\n",
            "Epoch 1973, Loss(train/val) 0.50971/0.19292. Took 0.05 sec\n",
            "Epoch 1974, Loss(train/val) 0.54325/0.19534. Took 0.05 sec\n",
            "Epoch 1975, Loss(train/val) 0.53811/0.20012. Took 0.05 sec\n",
            "Epoch 1976, Loss(train/val) 0.51904/0.20037. Took 0.07 sec\n",
            "Epoch 1977, Loss(train/val) 0.53463/0.20322. Took 0.05 sec\n",
            "Epoch 1978, Loss(train/val) 0.52145/0.21216. Took 0.05 sec\n",
            "Epoch 1979, Loss(train/val) 0.54585/0.22225. Took 0.04 sec\n",
            "Epoch 1980, Loss(train/val) 0.51674/0.22175. Took 0.05 sec\n",
            "Epoch 1981, Loss(train/val) 0.54299/0.21763. Took 0.05 sec\n",
            "Epoch 1982, Loss(train/val) 0.54783/0.21140. Took 0.04 sec\n",
            "Epoch 1983, Loss(train/val) 0.50632/0.21048. Took 0.05 sec\n",
            "Epoch 1984, Loss(train/val) 0.52646/0.20504. Took 0.05 sec\n",
            "Epoch 1985, Loss(train/val) 0.51995/0.19913. Took 0.05 sec\n",
            "Epoch 1986, Loss(train/val) 0.51704/0.19330. Took 0.05 sec\n",
            "Epoch 1987, Loss(train/val) 0.49889/0.19145. Took 0.05 sec\n",
            "Epoch 1988, Loss(train/val) 0.52497/0.19147. Took 0.05 sec\n",
            "Epoch 1989, Loss(train/val) 0.53112/0.19409. Took 0.05 sec\n",
            "Epoch 1990, Loss(train/val) 0.54156/0.19390. Took 0.04 sec\n",
            "Epoch 1991, Loss(train/val) 0.51639/0.19300. Took 0.05 sec\n",
            "Epoch 1992, Loss(train/val) 0.54208/0.19176. Took 0.05 sec\n",
            "Epoch 1993, Loss(train/val) 0.53570/0.19201. Took 0.05 sec\n",
            "Epoch 1994, Loss(train/val) 0.50694/0.19245. Took 0.05 sec\n",
            "Epoch 1995, Loss(train/val) 0.48842/0.19439. Took 0.04 sec\n",
            "Epoch 1996, Loss(train/val) 0.54856/0.19538. Took 0.05 sec\n",
            "Epoch 1997, Loss(train/val) 0.50381/0.19873. Took 0.05 sec\n",
            "Epoch 1998, Loss(train/val) 0.52179/0.20067. Took 0.05 sec\n",
            "Epoch 1999, Loss(train/val) 0.51531/0.20250. Took 0.05 sec\n",
            "Epoch 2000, Loss(train/val) 0.53345/0.20261. Took 0.05 sec\n",
            "Epoch 2001, Loss(train/val) 0.55502/0.19629. Took 0.05 sec\n",
            "Epoch 2002, Loss(train/val) 0.56427/0.19520. Took 0.04 sec\n",
            "Epoch 2003, Loss(train/val) 0.53141/0.19277. Took 0.05 sec\n",
            "Epoch 2004, Loss(train/val) 0.52467/0.19433. Took 0.05 sec\n",
            "Epoch 2005, Loss(train/val) 0.52506/0.19517. Took 0.04 sec\n",
            "Epoch 2006, Loss(train/val) 0.53405/0.19463. Took 0.05 sec\n",
            "Epoch 2007, Loss(train/val) 0.55238/0.19353. Took 0.05 sec\n",
            "Epoch 2008, Loss(train/val) 0.51135/0.19150. Took 0.05 sec\n",
            "Epoch 2009, Loss(train/val) 0.50753/0.19266. Took 0.05 sec\n",
            "Epoch 2010, Loss(train/val) 0.56426/0.19207. Took 0.05 sec\n",
            "Epoch 2011, Loss(train/val) 0.47503/0.19481. Took 0.05 sec\n",
            "Epoch 2012, Loss(train/val) 0.53027/0.19593. Took 0.05 sec\n",
            "Epoch 2013, Loss(train/val) 0.54014/0.19570. Took 0.04 sec\n",
            "Epoch 2014, Loss(train/val) 0.52665/0.19875. Took 0.05 sec\n",
            "Epoch 2015, Loss(train/val) 0.57269/0.19871. Took 0.05 sec\n",
            "Epoch 2016, Loss(train/val) 0.52473/0.19582. Took 0.05 sec\n",
            "Epoch 2017, Loss(train/val) 0.50182/0.19302. Took 0.04 sec\n",
            "Epoch 2018, Loss(train/val) 0.52654/0.18939. Took 0.05 sec\n",
            "Epoch 2019, Loss(train/val) 0.50940/0.18965. Took 0.05 sec\n",
            "Epoch 2020, Loss(train/val) 0.49912/0.19111. Took 0.04 sec\n",
            "Epoch 2021, Loss(train/val) 0.53144/0.19112. Took 0.06 sec\n",
            "Epoch 2022, Loss(train/val) 0.50563/0.19016. Took 0.04 sec\n",
            "Epoch 2023, Loss(train/val) 0.50555/0.18947. Took 0.04 sec\n",
            "Epoch 2024, Loss(train/val) 0.53210/0.18903. Took 0.05 sec\n",
            "Epoch 2025, Loss(train/val) 0.50819/0.18863. Took 0.05 sec\n",
            "Epoch 2026, Loss(train/val) 0.50972/0.18860. Took 0.06 sec\n",
            "Epoch 2027, Loss(train/val) 0.55291/0.18852. Took 0.05 sec\n",
            "Epoch 2028, Loss(train/val) 0.51337/0.18874. Took 0.04 sec\n",
            "Epoch 2029, Loss(train/val) 0.53000/0.19137. Took 0.05 sec\n",
            "Epoch 2030, Loss(train/val) 0.50167/0.19164. Took 0.05 sec\n",
            "Epoch 2031, Loss(train/val) 0.55983/0.19226. Took 0.05 sec\n",
            "Epoch 2032, Loss(train/val) 0.50541/0.19123. Took 0.05 sec\n",
            "Epoch 2033, Loss(train/val) 0.51376/0.19250. Took 0.05 sec\n",
            "Epoch 2034, Loss(train/val) 0.51385/0.19091. Took 0.04 sec\n",
            "Epoch 2035, Loss(train/val) 0.50609/0.19017. Took 0.05 sec\n",
            "Epoch 2036, Loss(train/val) 0.49001/0.18786. Took 0.05 sec\n",
            "Epoch 2037, Loss(train/val) 0.52514/0.19002. Took 0.04 sec\n",
            "Epoch 2038, Loss(train/val) 0.53197/0.19110. Took 0.04 sec\n",
            "Epoch 2039, Loss(train/val) 0.50835/0.19308. Took 0.05 sec\n",
            "Epoch 2040, Loss(train/val) 0.49240/0.19120. Took 0.05 sec\n",
            "Epoch 2041, Loss(train/val) 0.53384/0.18748. Took 0.05 sec\n",
            "Epoch 2042, Loss(train/val) 0.53528/0.18717. Took 0.05 sec\n",
            "Epoch 2043, Loss(train/val) 0.52961/0.18709. Took 0.04 sec\n",
            "Epoch 2044, Loss(train/val) 0.49932/0.18710. Took 0.04 sec\n",
            "Epoch 2045, Loss(train/val) 0.50998/0.18786. Took 0.05 sec\n",
            "Epoch 2046, Loss(train/val) 0.52094/0.19024. Took 0.05 sec\n",
            "Epoch 2047, Loss(train/val) 0.51197/0.19356. Took 0.05 sec\n",
            "Epoch 2048, Loss(train/val) 0.51572/0.19843. Took 0.05 sec\n",
            "Epoch 2049, Loss(train/val) 0.54435/0.20047. Took 0.05 sec\n",
            "Epoch 2050, Loss(train/val) 0.50695/0.19848. Took 0.05 sec\n",
            "Epoch 2051, Loss(train/val) 0.53419/0.19994. Took 0.05 sec\n",
            "Epoch 2052, Loss(train/val) 0.51825/0.19553. Took 0.04 sec\n",
            "Epoch 2053, Loss(train/val) 0.52944/0.19230. Took 0.05 sec\n",
            "Epoch 2054, Loss(train/val) 0.53309/0.18909. Took 0.04 sec\n",
            "Epoch 2055, Loss(train/val) 0.52073/0.18977. Took 0.04 sec\n",
            "Epoch 2056, Loss(train/val) 0.54909/0.18712. Took 0.05 sec\n",
            "Epoch 2057, Loss(train/val) 0.53757/0.18821. Took 0.05 sec\n",
            "Epoch 2058, Loss(train/val) 0.51361/0.19476. Took 0.04 sec\n",
            "Epoch 2059, Loss(train/val) 0.53781/0.20330. Took 0.04 sec\n",
            "Epoch 2060, Loss(train/val) 0.54159/0.20646. Took 0.04 sec\n",
            "Epoch 2061, Loss(train/val) 0.56298/0.19602. Took 0.06 sec\n",
            "Epoch 2062, Loss(train/val) 0.55047/0.19884. Took 0.04 sec\n",
            "Epoch 2063, Loss(train/val) 0.53069/0.20440. Took 0.04 sec\n",
            "Epoch 2064, Loss(train/val) 0.50184/0.20347. Took 0.04 sec\n",
            "Epoch 2065, Loss(train/val) 0.56027/0.19600. Took 0.05 sec\n",
            "Epoch 2066, Loss(train/val) 0.49608/0.19683. Took 0.05 sec\n",
            "Epoch 2067, Loss(train/val) 0.52758/0.19780. Took 0.05 sec\n",
            "Epoch 2068, Loss(train/val) 0.54612/0.20004. Took 0.05 sec\n",
            "Epoch 2069, Loss(train/val) 0.54390/0.19576. Took 0.05 sec\n",
            "Epoch 2070, Loss(train/val) 0.50670/0.18984. Took 0.05 sec\n",
            "Epoch 2071, Loss(train/val) 0.52139/0.18691. Took 0.05 sec\n",
            "Epoch 2072, Loss(train/val) 0.50714/0.18500. Took 0.05 sec\n",
            "Epoch 2073, Loss(train/val) 0.53243/0.18905. Took 0.04 sec\n",
            "Epoch 2074, Loss(train/val) 0.52337/0.20015. Took 0.05 sec\n",
            "Epoch 2075, Loss(train/val) 0.52529/0.20459. Took 0.05 sec\n",
            "Epoch 2076, Loss(train/val) 0.50903/0.20734. Took 0.05 sec\n",
            "Epoch 2077, Loss(train/val) 0.56119/0.22020. Took 0.05 sec\n",
            "Epoch 2078, Loss(train/val) 0.51028/0.21480. Took 0.05 sec\n",
            "Epoch 2079, Loss(train/val) 0.51376/0.20879. Took 0.05 sec\n",
            "Epoch 2080, Loss(train/val) 0.55178/0.20091. Took 0.04 sec\n",
            "Epoch 2081, Loss(train/val) 0.52413/0.19251. Took 0.05 sec\n",
            "Epoch 2082, Loss(train/val) 0.51480/0.18579. Took 0.05 sec\n",
            "Epoch 2083, Loss(train/val) 0.53778/0.18587. Took 0.05 sec\n",
            "Epoch 2084, Loss(train/val) 0.50358/0.18669. Took 0.05 sec\n",
            "Epoch 2085, Loss(train/val) 0.52459/0.18465. Took 0.05 sec\n",
            "Epoch 2086, Loss(train/val) 0.52692/0.18440. Took 0.05 sec\n",
            "Epoch 2087, Loss(train/val) 0.55037/0.18452. Took 0.05 sec\n",
            "Epoch 2088, Loss(train/val) 0.53523/0.18436. Took 0.05 sec\n",
            "Epoch 2089, Loss(train/val) 0.49550/0.18397. Took 0.05 sec\n",
            "Epoch 2090, Loss(train/val) 0.51330/0.18479. Took 0.04 sec\n",
            "Epoch 2091, Loss(train/val) 0.54091/0.18440. Took 0.05 sec\n",
            "Epoch 2092, Loss(train/val) 0.48425/0.18347. Took 0.04 sec\n",
            "Epoch 2093, Loss(train/val) 0.50340/0.18344. Took 0.05 sec\n",
            "Epoch 2094, Loss(train/val) 0.52492/0.18321. Took 0.04 sec\n",
            "Epoch 2095, Loss(train/val) 0.50753/0.18321. Took 0.04 sec\n",
            "Epoch 2096, Loss(train/val) 0.51087/0.18357. Took 0.05 sec\n",
            "Epoch 2097, Loss(train/val) 0.50660/0.18332. Took 0.05 sec\n",
            "Epoch 2098, Loss(train/val) 0.57308/0.18411. Took 0.05 sec\n",
            "Epoch 2099, Loss(train/val) 0.50663/0.18532. Took 0.04 sec\n",
            "Epoch 2100, Loss(train/val) 0.50367/0.18463. Took 0.04 sec\n",
            "Epoch 2101, Loss(train/val) 0.50295/0.18447. Took 0.05 sec\n",
            "Epoch 2102, Loss(train/val) 0.55010/0.18371. Took 0.04 sec\n",
            "Epoch 2103, Loss(train/val) 0.51982/0.18293. Took 0.04 sec\n",
            "Epoch 2104, Loss(train/val) 0.50066/0.18276. Took 0.05 sec\n",
            "Epoch 2105, Loss(train/val) 0.53685/0.18254. Took 0.05 sec\n",
            "Epoch 2106, Loss(train/val) 0.53099/0.18248. Took 0.05 sec\n",
            "Epoch 2107, Loss(train/val) 0.53425/0.18500. Took 0.05 sec\n",
            "Epoch 2108, Loss(train/val) 0.52154/0.18536. Took 0.05 sec\n",
            "Epoch 2109, Loss(train/val) 0.47966/0.18308. Took 0.05 sec\n",
            "Epoch 2110, Loss(train/val) 0.49458/0.18335. Took 0.05 sec\n",
            "Epoch 2111, Loss(train/val) 0.50799/0.18808. Took 0.05 sec\n",
            "Epoch 2112, Loss(train/val) 0.51342/0.19298. Took 0.04 sec\n",
            "Epoch 2113, Loss(train/val) 0.56882/0.18351. Took 0.04 sec\n",
            "Epoch 2114, Loss(train/val) 0.50832/0.18461. Took 0.05 sec\n",
            "Epoch 2115, Loss(train/val) 0.50225/0.19244. Took 0.05 sec\n",
            "Epoch 2116, Loss(train/val) 0.53453/0.21587. Took 0.05 sec\n",
            "Epoch 2117, Loss(train/val) 0.52534/0.22545. Took 0.05 sec\n",
            "Epoch 2118, Loss(train/val) 0.52158/0.23154. Took 0.05 sec\n",
            "Epoch 2119, Loss(train/val) 0.50730/0.23934. Took 0.05 sec\n",
            "Epoch 2120, Loss(train/val) 0.53665/0.24109. Took 0.04 sec\n",
            "Epoch 2121, Loss(train/val) 0.49468/0.24029. Took 0.05 sec\n",
            "Epoch 2122, Loss(train/val) 0.52403/0.22515. Took 0.05 sec\n",
            "Epoch 2123, Loss(train/val) 0.53442/0.20077. Took 0.05 sec\n",
            "Epoch 2124, Loss(train/val) 0.53744/0.18345. Took 0.04 sec\n",
            "Epoch 2125, Loss(train/val) 0.52362/0.18106. Took 0.04 sec\n",
            "Epoch 2126, Loss(train/val) 0.51647/0.18151. Took 0.06 sec\n",
            "Epoch 2127, Loss(train/val) 0.53562/0.18248. Took 0.05 sec\n",
            "Epoch 2128, Loss(train/val) 0.51230/0.18374. Took 0.05 sec\n",
            "Epoch 2129, Loss(train/val) 0.51740/0.18855. Took 0.05 sec\n",
            "Epoch 2130, Loss(train/val) 0.47057/0.19500. Took 0.04 sec\n",
            "Epoch 2131, Loss(train/val) 0.52274/0.18818. Took 0.05 sec\n",
            "Epoch 2132, Loss(train/val) 0.54110/0.18652. Took 0.04 sec\n",
            "Epoch 2133, Loss(train/val) 0.52143/0.18158. Took 0.05 sec\n",
            "Epoch 2134, Loss(train/val) 0.49563/0.18082. Took 0.04 sec\n",
            "Epoch 2135, Loss(train/val) 0.51238/0.18804. Took 0.05 sec\n",
            "Epoch 2136, Loss(train/val) 0.51349/0.19799. Took 0.05 sec\n",
            "Epoch 2137, Loss(train/val) 0.51108/0.19821. Took 0.05 sec\n",
            "Epoch 2138, Loss(train/val) 0.53248/0.19731. Took 0.05 sec\n",
            "Epoch 2139, Loss(train/val) 0.49520/0.19156. Took 0.04 sec\n",
            "Epoch 2140, Loss(train/val) 0.50899/0.18262. Took 0.05 sec\n",
            "Epoch 2141, Loss(train/val) 0.51032/0.18047. Took 0.05 sec\n",
            "Epoch 2142, Loss(train/val) 0.50039/0.18449. Took 0.04 sec\n",
            "Epoch 2143, Loss(train/val) 0.54043/0.18692. Took 0.04 sec\n",
            "Epoch 2144, Loss(train/val) 0.52669/0.18501. Took 0.04 sec\n",
            "Epoch 2145, Loss(train/val) 0.48485/0.18812. Took 0.04 sec\n",
            "Epoch 2146, Loss(train/val) 0.50893/0.19044. Took 0.05 sec\n",
            "Epoch 2147, Loss(train/val) 0.52704/0.19016. Took 0.06 sec\n",
            "Epoch 2148, Loss(train/val) 0.54495/0.19013. Took 0.05 sec\n",
            "Epoch 2149, Loss(train/val) 0.50810/0.18954. Took 0.05 sec\n",
            "Epoch 2150, Loss(train/val) 0.46981/0.18488. Took 0.05 sec\n",
            "Epoch 2151, Loss(train/val) 0.48533/0.18431. Took 0.05 sec\n",
            "Epoch 2152, Loss(train/val) 0.49879/0.18626. Took 0.05 sec\n",
            "Epoch 2153, Loss(train/val) 0.51447/0.18808. Took 0.05 sec\n",
            "Epoch 2154, Loss(train/val) 0.49312/0.19270. Took 0.05 sec\n",
            "Epoch 2155, Loss(train/val) 0.49727/0.19286. Took 0.05 sec\n",
            "Epoch 2156, Loss(train/val) 0.50594/0.19346. Took 0.04 sec\n",
            "Epoch 2157, Loss(train/val) 0.50814/0.18989. Took 0.04 sec\n",
            "Epoch 2158, Loss(train/val) 0.49916/0.18463. Took 0.05 sec\n",
            "Epoch 2159, Loss(train/val) 0.54336/0.18219. Took 0.04 sec\n",
            "Epoch 2160, Loss(train/val) 0.52106/0.17927. Took 0.06 sec\n",
            "Epoch 2161, Loss(train/val) 0.51555/0.18115. Took 0.05 sec\n",
            "Epoch 2162, Loss(train/val) 0.51848/0.18177. Took 0.04 sec\n",
            "Epoch 2163, Loss(train/val) 0.49731/0.17927. Took 0.04 sec\n",
            "Epoch 2164, Loss(train/val) 0.51377/0.17865. Took 0.04 sec\n",
            "Epoch 2165, Loss(train/val) 0.51227/0.18240. Took 0.05 sec\n",
            "Epoch 2166, Loss(train/val) 0.50049/0.18297. Took 0.04 sec\n",
            "Epoch 2167, Loss(train/val) 0.51286/0.17916. Took 0.05 sec\n",
            "Epoch 2168, Loss(train/val) 0.50034/0.17875. Took 0.05 sec\n",
            "Epoch 2169, Loss(train/val) 0.53446/0.17830. Took 0.04 sec\n",
            "Epoch 2170, Loss(train/val) 0.52480/0.17914. Took 0.05 sec\n",
            "Epoch 2171, Loss(train/val) 0.53285/0.18782. Took 0.05 sec\n",
            "Epoch 2172, Loss(train/val) 0.53925/0.19148. Took 0.04 sec\n",
            "Epoch 2173, Loss(train/val) 0.53042/0.18613. Took 0.04 sec\n",
            "Epoch 2174, Loss(train/val) 0.51211/0.18138. Took 0.04 sec\n",
            "Epoch 2175, Loss(train/val) 0.50512/0.17982. Took 0.05 sec\n",
            "Epoch 2176, Loss(train/val) 0.50241/0.17844. Took 0.04 sec\n",
            "Epoch 2177, Loss(train/val) 0.51917/0.17786. Took 0.05 sec\n",
            "Epoch 2178, Loss(train/val) 0.52734/0.17951. Took 0.05 sec\n",
            "Epoch 2179, Loss(train/val) 0.56009/0.18739. Took 0.05 sec\n",
            "Epoch 2180, Loss(train/val) 0.52354/0.19570. Took 0.05 sec\n",
            "Epoch 2181, Loss(train/val) 0.52146/0.18961. Took 0.04 sec\n",
            "Epoch 2182, Loss(train/val) 0.53603/0.18872. Took 0.05 sec\n",
            "Epoch 2183, Loss(train/val) 0.48331/0.18900. Took 0.04 sec\n",
            "Epoch 2184, Loss(train/val) 0.52970/0.18229. Took 0.04 sec\n",
            "Epoch 2185, Loss(train/val) 0.51842/0.18482. Took 0.05 sec\n",
            "Epoch 2186, Loss(train/val) 0.50880/0.18255. Took 0.04 sec\n",
            "Epoch 2187, Loss(train/val) 0.49329/0.18288. Took 0.04 sec\n",
            "Epoch 2188, Loss(train/val) 0.50296/0.18062. Took 0.04 sec\n",
            "Epoch 2189, Loss(train/val) 0.51889/0.17861. Took 0.05 sec\n",
            "Epoch 2190, Loss(train/val) 0.51746/0.17928. Took 0.06 sec\n",
            "Epoch 2191, Loss(train/val) 0.52540/0.18442. Took 0.04 sec\n",
            "Epoch 2192, Loss(train/val) 0.48262/0.18348. Took 0.05 sec\n",
            "Epoch 2193, Loss(train/val) 0.52187/0.17751. Took 0.05 sec\n",
            "Epoch 2194, Loss(train/val) 0.50663/0.17942. Took 0.05 sec\n",
            "Epoch 2195, Loss(train/val) 0.54690/0.18799. Took 0.05 sec\n",
            "Epoch 2196, Loss(train/val) 0.50460/0.19350. Took 0.04 sec\n",
            "Epoch 2197, Loss(train/val) 0.48516/0.19670. Took 0.05 sec\n",
            "Epoch 2198, Loss(train/val) 0.52464/0.19075. Took 0.05 sec\n",
            "Epoch 2199, Loss(train/val) 0.51407/0.19379. Took 0.05 sec\n",
            "Epoch 2200, Loss(train/val) 0.48912/0.19590. Took 0.05 sec\n",
            "Epoch 2201, Loss(train/val) 0.49522/0.20585. Took 0.04 sec\n",
            "Epoch 2202, Loss(train/val) 0.51912/0.22147. Took 0.04 sec\n",
            "Epoch 2203, Loss(train/val) 0.51413/0.20870. Took 0.05 sec\n",
            "Epoch 2204, Loss(train/val) 0.51581/0.19949. Took 0.05 sec\n",
            "Epoch 2205, Loss(train/val) 0.51563/0.19663. Took 0.05 sec\n",
            "Epoch 2206, Loss(train/val) 0.48717/0.19930. Took 0.04 sec\n",
            "Epoch 2207, Loss(train/val) 0.49744/0.20059. Took 0.04 sec\n",
            "Epoch 2208, Loss(train/val) 0.49619/0.19637. Took 0.05 sec\n",
            "Epoch 2209, Loss(train/val) 0.52304/0.20704. Took 0.04 sec\n",
            "Epoch 2210, Loss(train/val) 0.52492/0.19762. Took 0.05 sec\n",
            "Epoch 2211, Loss(train/val) 0.52480/0.19462. Took 0.05 sec\n",
            "Epoch 2212, Loss(train/val) 0.50082/0.19576. Took 0.04 sec\n",
            "Epoch 2213, Loss(train/val) 0.54394/0.18333. Took 0.04 sec\n",
            "Epoch 2214, Loss(train/val) 0.52958/0.17743. Took 0.04 sec\n",
            "Epoch 2215, Loss(train/val) 0.51401/0.17585. Took 0.05 sec\n",
            "Epoch 2216, Loss(train/val) 0.52661/0.17801. Took 0.05 sec\n",
            "Epoch 2217, Loss(train/val) 0.45840/0.18049. Took 0.04 sec\n",
            "Epoch 2218, Loss(train/val) 0.49105/0.18104. Took 0.05 sec\n",
            "Epoch 2219, Loss(train/val) 0.51018/0.18071. Took 0.05 sec\n",
            "Epoch 2220, Loss(train/val) 0.49795/0.18110. Took 0.05 sec\n",
            "Epoch 2221, Loss(train/val) 0.51815/0.18012. Took 0.05 sec\n",
            "Epoch 2222, Loss(train/val) 0.52213/0.17951. Took 0.05 sec\n",
            "Epoch 2223, Loss(train/val) 0.49996/0.17627. Took 0.04 sec\n",
            "Epoch 2224, Loss(train/val) 0.51391/0.17499. Took 0.05 sec\n",
            "Epoch 2225, Loss(train/val) 0.53941/0.17578. Took 0.05 sec\n",
            "Epoch 2226, Loss(train/val) 0.50619/0.17779. Took 0.04 sec\n",
            "Epoch 2227, Loss(train/val) 0.50009/0.17799. Took 0.05 sec\n",
            "Epoch 2228, Loss(train/val) 0.52191/0.18521. Took 0.04 sec\n",
            "Epoch 2229, Loss(train/val) 0.49532/0.19078. Took 0.05 sec\n",
            "Epoch 2230, Loss(train/val) 0.51276/0.19191. Took 0.05 sec\n",
            "Epoch 2231, Loss(train/val) 0.54484/0.19288. Took 0.05 sec\n",
            "Epoch 2232, Loss(train/val) 0.50449/0.18578. Took 0.04 sec\n",
            "Epoch 2233, Loss(train/val) 0.52688/0.17812. Took 0.05 sec\n",
            "Epoch 2234, Loss(train/val) 0.49183/0.17523. Took 0.04 sec\n",
            "Epoch 2235, Loss(train/val) 0.54177/0.17501. Took 0.05 sec\n",
            "Epoch 2236, Loss(train/val) 0.49194/0.17490. Took 0.05 sec\n",
            "Epoch 2237, Loss(train/val) 0.53225/0.17507. Took 0.05 sec\n",
            "Epoch 2238, Loss(train/val) 0.50904/0.17476. Took 0.04 sec\n",
            "Epoch 2239, Loss(train/val) 0.50291/0.17701. Took 0.04 sec\n",
            "Epoch 2240, Loss(train/val) 0.50603/0.17874. Took 0.06 sec\n",
            "Epoch 2241, Loss(train/val) 0.51140/0.17818. Took 0.05 sec\n",
            "Epoch 2242, Loss(train/val) 0.51986/0.17772. Took 0.04 sec\n",
            "Epoch 2243, Loss(train/val) 0.49684/0.17838. Took 0.04 sec\n",
            "Epoch 2244, Loss(train/val) 0.51060/0.18376. Took 0.05 sec\n",
            "Epoch 2245, Loss(train/val) 0.53526/0.17785. Took 0.05 sec\n",
            "Epoch 2246, Loss(train/val) 0.50357/0.17459. Took 0.05 sec\n",
            "Epoch 2247, Loss(train/val) 0.50090/0.17493. Took 0.04 sec\n",
            "Epoch 2248, Loss(train/val) 0.49836/0.17821. Took 0.04 sec\n",
            "Epoch 2249, Loss(train/val) 0.48210/0.17515. Took 0.04 sec\n",
            "Epoch 2250, Loss(train/val) 0.51735/0.17508. Took 0.05 sec\n",
            "Epoch 2251, Loss(train/val) 0.52028/0.17847. Took 0.05 sec\n",
            "Epoch 2252, Loss(train/val) 0.52502/0.17977. Took 0.04 sec\n",
            "Epoch 2253, Loss(train/val) 0.47261/0.18213. Took 0.04 sec\n",
            "Epoch 2254, Loss(train/val) 0.50444/0.18037. Took 0.06 sec\n",
            "Epoch 2255, Loss(train/val) 0.50389/0.17730. Took 0.06 sec\n",
            "Epoch 2256, Loss(train/val) 0.50804/0.17832. Took 0.04 sec\n",
            "Epoch 2257, Loss(train/val) 0.51128/0.18267. Took 0.05 sec\n",
            "Epoch 2258, Loss(train/val) 0.52688/0.18772. Took 0.04 sec\n",
            "Epoch 2259, Loss(train/val) 0.50747/0.18898. Took 0.04 sec\n",
            "Epoch 2260, Loss(train/val) 0.48922/0.19012. Took 0.05 sec\n",
            "Epoch 2261, Loss(train/val) 0.57163/0.18160. Took 0.04 sec\n",
            "Epoch 2262, Loss(train/val) 0.49919/0.17752. Took 0.04 sec\n",
            "Epoch 2263, Loss(train/val) 0.51697/0.17936. Took 0.05 sec\n",
            "Epoch 2264, Loss(train/val) 0.47769/0.19018. Took 0.04 sec\n",
            "Epoch 2265, Loss(train/val) 0.51543/0.20108. Took 0.05 sec\n",
            "Epoch 2266, Loss(train/val) 0.52747/0.18674. Took 0.05 sec\n",
            "Epoch 2267, Loss(train/val) 0.53300/0.17343. Took 0.04 sec\n",
            "Epoch 2268, Loss(train/val) 0.48481/0.17384. Took 0.04 sec\n",
            "Epoch 2269, Loss(train/val) 0.49786/0.17484. Took 0.05 sec\n",
            "Epoch 2270, Loss(train/val) 0.49435/0.17457. Took 0.05 sec\n",
            "Epoch 2271, Loss(train/val) 0.48585/0.17309. Took 0.05 sec\n",
            "Epoch 2272, Loss(train/val) 0.48230/0.17341. Took 0.04 sec\n",
            "Epoch 2273, Loss(train/val) 0.50044/0.17307. Took 0.04 sec\n",
            "Epoch 2274, Loss(train/val) 0.52777/0.17391. Took 0.04 sec\n",
            "Epoch 2275, Loss(train/val) 0.48795/0.17331. Took 0.05 sec\n",
            "Epoch 2276, Loss(train/val) 0.45612/0.17647. Took 0.05 sec\n",
            "Epoch 2277, Loss(train/val) 0.51318/0.19071. Took 0.05 sec\n",
            "Epoch 2278, Loss(train/val) 0.52699/0.20312. Took 0.05 sec\n",
            "Epoch 2279, Loss(train/val) 0.53345/0.18585. Took 0.05 sec\n",
            "Epoch 2280, Loss(train/val) 0.51547/0.17466. Took 0.05 sec\n",
            "Epoch 2281, Loss(train/val) 0.51653/0.17277. Took 0.05 sec\n",
            "Epoch 2282, Loss(train/val) 0.51570/0.17447. Took 0.05 sec\n",
            "Epoch 2283, Loss(train/val) 0.50438/0.17707. Took 0.04 sec\n",
            "Epoch 2284, Loss(train/val) 0.51131/0.17440. Took 0.05 sec\n",
            "Epoch 2285, Loss(train/val) 0.50957/0.17233. Took 0.05 sec\n",
            "Epoch 2286, Loss(train/val) 0.47906/0.17634. Took 0.05 sec\n",
            "Epoch 2287, Loss(train/val) 0.49179/0.18149. Took 0.05 sec\n",
            "Epoch 2288, Loss(train/val) 0.52696/0.17391. Took 0.05 sec\n",
            "Epoch 2289, Loss(train/val) 0.56207/0.17247. Took 0.05 sec\n",
            "Epoch 2290, Loss(train/val) 0.51736/0.17322. Took 0.06 sec\n",
            "Epoch 2291, Loss(train/val) 0.51326/0.17294. Took 0.05 sec\n",
            "Epoch 2292, Loss(train/val) 0.51587/0.17330. Took 0.05 sec\n",
            "Epoch 2293, Loss(train/val) 0.49314/0.17252. Took 0.04 sec\n",
            "Epoch 2294, Loss(train/val) 0.49219/0.17299. Took 0.05 sec\n",
            "Epoch 2295, Loss(train/val) 0.54373/0.17316. Took 0.05 sec\n",
            "Epoch 2296, Loss(train/val) 0.52120/0.17285. Took 0.05 sec\n",
            "Epoch 2297, Loss(train/val) 0.51081/0.17270. Took 0.05 sec\n",
            "Epoch 2298, Loss(train/val) 0.46351/0.17273. Took 0.05 sec\n",
            "Epoch 2299, Loss(train/val) 0.54145/0.17263. Took 0.05 sec\n",
            "Epoch 2300, Loss(train/val) 0.53316/0.17393. Took 0.05 sec\n",
            "Epoch 2301, Loss(train/val) 0.48152/0.17797. Took 0.05 sec\n",
            "Epoch 2302, Loss(train/val) 0.54235/0.18140. Took 0.04 sec\n",
            "Epoch 2303, Loss(train/val) 0.52489/0.18127. Took 0.04 sec\n",
            "Epoch 2304, Loss(train/val) 0.52094/0.17455. Took 0.05 sec\n",
            "Epoch 2305, Loss(train/val) 0.49900/0.17374. Took 0.05 sec\n",
            "Epoch 2306, Loss(train/val) 0.50970/0.17207. Took 0.05 sec\n",
            "Epoch 2307, Loss(train/val) 0.51529/0.17567. Took 0.04 sec\n",
            "Epoch 2308, Loss(train/val) 0.48169/0.17590. Took 0.05 sec\n",
            "Epoch 2309, Loss(train/val) 0.48304/0.17311. Took 0.04 sec\n",
            "Epoch 2310, Loss(train/val) 0.49009/0.17196. Took 0.05 sec\n",
            "Epoch 2311, Loss(train/val) 0.48064/0.17397. Took 0.04 sec\n",
            "Epoch 2312, Loss(train/val) 0.51666/0.18204. Took 0.05 sec\n",
            "Epoch 2313, Loss(train/val) 0.50072/0.18803. Took 0.05 sec\n",
            "Epoch 2314, Loss(train/val) 0.55033/0.17665. Took 0.04 sec\n",
            "Epoch 2315, Loss(train/val) 0.51003/0.17243. Took 0.05 sec\n",
            "Epoch 2316, Loss(train/val) 0.50231/0.17213. Took 0.04 sec\n",
            "Epoch 2317, Loss(train/val) 0.49241/0.17496. Took 0.04 sec\n",
            "Epoch 2318, Loss(train/val) 0.52060/0.17763. Took 0.05 sec\n",
            "Epoch 2319, Loss(train/val) 0.49443/0.17997. Took 0.05 sec\n",
            "Epoch 2320, Loss(train/val) 0.51605/0.18004. Took 0.05 sec\n",
            "Epoch 2321, Loss(train/val) 0.51190/0.17669. Took 0.04 sec\n",
            "Epoch 2322, Loss(train/val) 0.52813/0.17450. Took 0.04 sec\n",
            "Epoch 2323, Loss(train/val) 0.49005/0.17245. Took 0.04 sec\n",
            "Epoch 2324, Loss(train/val) 0.49629/0.17251. Took 0.05 sec\n",
            "Epoch 2325, Loss(train/val) 0.47628/0.17464. Took 0.05 sec\n",
            "Epoch 2326, Loss(train/val) 0.49099/0.17488. Took 0.05 sec\n",
            "Epoch 2327, Loss(train/val) 0.47011/0.17180. Took 0.05 sec\n",
            "Epoch 2328, Loss(train/val) 0.50643/0.17282. Took 0.05 sec\n",
            "Epoch 2329, Loss(train/val) 0.49039/0.17385. Took 0.05 sec\n",
            "Epoch 2330, Loss(train/val) 0.50477/0.17249. Took 0.05 sec\n",
            "Epoch 2331, Loss(train/val) 0.50785/0.17211. Took 0.05 sec\n",
            "Epoch 2332, Loss(train/val) 0.49968/0.17273. Took 0.05 sec\n",
            "Epoch 2333, Loss(train/val) 0.52660/0.17216. Took 0.05 sec\n",
            "Epoch 2334, Loss(train/val) 0.52853/0.17208. Took 0.05 sec\n",
            "Epoch 2335, Loss(train/val) 0.51402/0.17282. Took 0.05 sec\n",
            "Epoch 2336, Loss(train/val) 0.54955/0.17188. Took 0.04 sec\n",
            "Epoch 2337, Loss(train/val) 0.52579/0.17263. Took 0.04 sec\n",
            "Epoch 2338, Loss(train/val) 0.49215/0.17454. Took 0.05 sec\n",
            "Epoch 2339, Loss(train/val) 0.49179/0.17574. Took 0.04 sec\n",
            "Epoch 2340, Loss(train/val) 0.50004/0.17591. Took 0.06 sec\n",
            "Epoch 2341, Loss(train/val) 0.50131/0.19003. Took 0.05 sec\n",
            "Epoch 2342, Loss(train/val) 0.53504/0.19280. Took 0.05 sec\n",
            "Epoch 2343, Loss(train/val) 0.53562/0.20814. Took 0.05 sec\n",
            "Epoch 2344, Loss(train/val) 0.51501/0.19596. Took 0.05 sec\n",
            "Epoch 2345, Loss(train/val) 0.52111/0.18059. Took 0.05 sec\n",
            "Epoch 2346, Loss(train/val) 0.49899/0.17598. Took 0.04 sec\n",
            "Epoch 2347, Loss(train/val) 0.51923/0.17106. Took 0.05 sec\n",
            "Epoch 2348, Loss(train/val) 0.47818/0.17449. Took 0.05 sec\n",
            "Epoch 2349, Loss(train/val) 0.50804/0.18600. Took 0.05 sec\n",
            "Epoch 2350, Loss(train/val) 0.51129/0.20882. Took 0.05 sec\n",
            "Epoch 2351, Loss(train/val) 0.51364/0.20964. Took 0.05 sec\n",
            "Epoch 2352, Loss(train/val) 0.46445/0.20475. Took 0.04 sec\n",
            "Epoch 2353, Loss(train/val) 0.53407/0.17848. Took 0.04 sec\n",
            "Epoch 2354, Loss(train/val) 0.50650/0.17201. Took 0.04 sec\n",
            "Epoch 2355, Loss(train/val) 0.47308/0.17342. Took 0.05 sec\n",
            "Epoch 2356, Loss(train/val) 0.50279/0.17238. Took 0.04 sec\n",
            "Epoch 2357, Loss(train/val) 0.51296/0.17173. Took 0.04 sec\n",
            "Epoch 2358, Loss(train/val) 0.49828/0.17156. Took 0.04 sec\n",
            "Epoch 2359, Loss(train/val) 0.46846/0.17592. Took 0.04 sec\n",
            "Epoch 2360, Loss(train/val) 0.49716/0.17559. Took 0.05 sec\n",
            "Epoch 2361, Loss(train/val) 0.51983/0.17115. Took 0.05 sec\n",
            "Epoch 2362, Loss(train/val) 0.48957/0.17544. Took 0.06 sec\n",
            "Epoch 2363, Loss(train/val) 0.51009/0.17191. Took 0.05 sec\n",
            "Epoch 2364, Loss(train/val) 0.49362/0.17177. Took 0.04 sec\n",
            "Epoch 2365, Loss(train/val) 0.51005/0.17733. Took 0.06 sec\n",
            "Epoch 2366, Loss(train/val) 0.47730/0.18669. Took 0.05 sec\n",
            "Epoch 2367, Loss(train/val) 0.48212/0.20247. Took 0.05 sec\n",
            "Epoch 2368, Loss(train/val) 0.50110/0.21070. Took 0.05 sec\n",
            "Epoch 2369, Loss(train/val) 0.48046/0.21061. Took 0.05 sec\n",
            "Epoch 2370, Loss(train/val) 0.51912/0.19708. Took 0.05 sec\n",
            "Epoch 2371, Loss(train/val) 0.47719/0.19726. Took 0.05 sec\n",
            "Epoch 2372, Loss(train/val) 0.48303/0.20137. Took 0.04 sec\n",
            "Epoch 2373, Loss(train/val) 0.50787/0.21488. Took 0.05 sec\n",
            "Epoch 2374, Loss(train/val) 0.51630/0.23078. Took 0.04 sec\n",
            "Epoch 2375, Loss(train/val) 0.51243/0.22341. Took 0.05 sec\n",
            "Epoch 2376, Loss(train/val) 0.51622/0.21822. Took 0.05 sec\n",
            "Epoch 2377, Loss(train/val) 0.52936/0.20447. Took 0.04 sec\n",
            "Epoch 2378, Loss(train/val) 0.53083/0.19839. Took 0.04 sec\n",
            "Epoch 2379, Loss(train/val) 0.49219/0.20103. Took 0.04 sec\n",
            "Epoch 2380, Loss(train/val) 0.51905/0.19875. Took 0.05 sec\n",
            "Epoch 2381, Loss(train/val) 0.52314/0.19770. Took 0.05 sec\n",
            "Epoch 2382, Loss(train/val) 0.48254/0.20428. Took 0.05 sec\n",
            "Epoch 2383, Loss(train/val) 0.46903/0.19419. Took 0.05 sec\n",
            "Epoch 2384, Loss(train/val) 0.50142/0.18107. Took 0.04 sec\n",
            "Epoch 2385, Loss(train/val) 0.46644/0.18045. Took 0.05 sec\n",
            "Epoch 2386, Loss(train/val) 0.47773/0.17939. Took 0.05 sec\n",
            "Epoch 2387, Loss(train/val) 0.51973/0.17450. Took 0.05 sec\n",
            "Epoch 2388, Loss(train/val) 0.50239/0.17041. Took 0.05 sec\n",
            "Epoch 2389, Loss(train/val) 0.47970/0.17049. Took 0.05 sec\n",
            "Epoch 2390, Loss(train/val) 0.46951/0.17075. Took 0.06 sec\n",
            "Epoch 2391, Loss(train/val) 0.45890/0.17173. Took 0.05 sec\n",
            "Epoch 2392, Loss(train/val) 0.49279/0.17827. Took 0.05 sec\n",
            "Epoch 2393, Loss(train/val) 0.50131/0.18321. Took 0.05 sec\n",
            "Epoch 2394, Loss(train/val) 0.51927/0.17671. Took 0.05 sec\n",
            "Epoch 2395, Loss(train/val) 0.48562/0.17311. Took 0.06 sec\n",
            "Epoch 2396, Loss(train/val) 0.51592/0.17065. Took 0.05 sec\n",
            "Epoch 2397, Loss(train/val) 0.51410/0.17109. Took 0.05 sec\n",
            "Epoch 2398, Loss(train/val) 0.48506/0.17632. Took 0.04 sec\n",
            "Epoch 2399, Loss(train/val) 0.53319/0.18403. Took 0.04 sec\n",
            "Epoch 2400, Loss(train/val) 0.50021/0.18464. Took 0.05 sec\n",
            "Epoch 2401, Loss(train/val) 0.51979/0.17734. Took 0.05 sec\n",
            "Epoch 2402, Loss(train/val) 0.50768/0.17299. Took 0.05 sec\n",
            "Epoch 2403, Loss(train/val) 0.50633/0.17886. Took 0.05 sec\n",
            "Epoch 2404, Loss(train/val) 0.53709/0.18108. Took 0.05 sec\n",
            "Epoch 2405, Loss(train/val) 0.47805/0.18316. Took 0.05 sec\n",
            "Epoch 2406, Loss(train/val) 0.47670/0.18073. Took 0.05 sec\n",
            "Epoch 2407, Loss(train/val) 0.52391/0.18131. Took 0.05 sec\n",
            "Epoch 2408, Loss(train/val) 0.51674/0.19898. Took 0.04 sec\n",
            "Epoch 2409, Loss(train/val) 0.45402/0.20025. Took 0.04 sec\n",
            "Epoch 2410, Loss(train/val) 0.47436/0.19382. Took 0.05 sec\n",
            "Epoch 2411, Loss(train/val) 0.49920/0.17741. Took 0.05 sec\n",
            "Epoch 2412, Loss(train/val) 0.51974/0.17211. Took 0.05 sec\n",
            "Epoch 2413, Loss(train/val) 0.46736/0.17060. Took 0.05 sec\n",
            "Epoch 2414, Loss(train/val) 0.50546/0.17051. Took 0.05 sec\n",
            "Epoch 2415, Loss(train/val) 0.50798/0.17056. Took 0.06 sec\n",
            "Epoch 2416, Loss(train/val) 0.49862/0.17059. Took 0.05 sec\n",
            "Epoch 2417, Loss(train/val) 0.50555/0.18289. Took 0.05 sec\n",
            "Epoch 2418, Loss(train/val) 0.49343/0.19677. Took 0.04 sec\n",
            "Epoch 2419, Loss(train/val) 0.47903/0.20677. Took 0.05 sec\n",
            "Epoch 2420, Loss(train/val) 0.50647/0.20589. Took 0.05 sec\n",
            "Epoch 2421, Loss(train/val) 0.50326/0.20309. Took 0.05 sec\n",
            "Epoch 2422, Loss(train/val) 0.48021/0.19521. Took 0.05 sec\n",
            "Epoch 2423, Loss(train/val) 0.48695/0.18246. Took 0.05 sec\n",
            "Epoch 2424, Loss(train/val) 0.47931/0.17381. Took 0.05 sec\n",
            "Epoch 2425, Loss(train/val) 0.50016/0.17059. Took 0.06 sec\n",
            "Epoch 2426, Loss(train/val) 0.49021/0.17300. Took 0.05 sec\n",
            "Epoch 2427, Loss(train/val) 0.48491/0.17713. Took 0.04 sec\n",
            "Epoch 2428, Loss(train/val) 0.48509/0.17881. Took 0.05 sec\n",
            "Epoch 2429, Loss(train/val) 0.49970/0.17050. Took 0.05 sec\n",
            "Epoch 2430, Loss(train/val) 0.51988/0.17036. Took 0.05 sec\n",
            "Epoch 2431, Loss(train/val) 0.49609/0.17112. Took 0.05 sec\n",
            "Epoch 2432, Loss(train/val) 0.50736/0.17041. Took 0.05 sec\n",
            "Epoch 2433, Loss(train/val) 0.47467/0.17842. Took 0.05 sec\n",
            "Epoch 2434, Loss(train/val) 0.49278/0.18856. Took 0.04 sec\n",
            "Epoch 2435, Loss(train/val) 0.49365/0.19129. Took 0.05 sec\n",
            "Epoch 2436, Loss(train/val) 0.50337/0.20719. Took 0.04 sec\n",
            "Epoch 2437, Loss(train/val) 0.51659/0.19895. Took 0.04 sec\n",
            "Epoch 2438, Loss(train/val) 0.48994/0.18815. Took 0.05 sec\n",
            "Epoch 2439, Loss(train/val) 0.48494/0.18047. Took 0.04 sec\n",
            "Epoch 2440, Loss(train/val) 0.49182/0.17651. Took 0.05 sec\n",
            "Epoch 2441, Loss(train/val) 0.49460/0.18205. Took 0.04 sec\n",
            "Epoch 2442, Loss(train/val) 0.48789/0.17684. Took 0.04 sec\n",
            "Epoch 2443, Loss(train/val) 0.50561/0.17548. Took 0.04 sec\n",
            "Epoch 2444, Loss(train/val) 0.50274/0.17055. Took 0.05 sec\n",
            "Epoch 2445, Loss(train/val) 0.48020/0.17239. Took 0.05 sec\n",
            "Epoch 2446, Loss(train/val) 0.48953/0.17137. Took 0.05 sec\n",
            "Epoch 2447, Loss(train/val) 0.47167/0.17210. Took 0.04 sec\n",
            "Epoch 2448, Loss(train/val) 0.51753/0.17260. Took 0.04 sec\n",
            "Epoch 2449, Loss(train/val) 0.49340/0.17384. Took 0.04 sec\n",
            "Epoch 2450, Loss(train/val) 0.48792/0.17585. Took 0.06 sec\n",
            "Epoch 2451, Loss(train/val) 0.51283/0.18039. Took 0.05 sec\n",
            "Epoch 2452, Loss(train/val) 0.50091/0.19148. Took 0.05 sec\n",
            "Epoch 2453, Loss(train/val) 0.45907/0.20450. Took 0.05 sec\n",
            "Epoch 2454, Loss(train/val) 0.48303/0.21867. Took 0.05 sec\n",
            "Epoch 2455, Loss(train/val) 0.48925/0.22352. Took 0.06 sec\n",
            "Epoch 2456, Loss(train/val) 0.51945/0.20629. Took 0.05 sec\n",
            "Epoch 2457, Loss(train/val) 0.48512/0.20491. Took 0.05 sec\n",
            "Epoch 2458, Loss(train/val) 0.51491/0.20277. Took 0.05 sec\n",
            "Epoch 2459, Loss(train/val) 0.50460/0.19369. Took 0.05 sec\n",
            "Epoch 2460, Loss(train/val) 0.48058/0.17946. Took 0.06 sec\n",
            "Epoch 2461, Loss(train/val) 0.52930/0.17022. Took 0.05 sec\n",
            "Epoch 2462, Loss(train/val) 0.46523/0.17305. Took 0.05 sec\n",
            "Epoch 2463, Loss(train/val) 0.52249/0.17644. Took 0.05 sec\n",
            "Epoch 2464, Loss(train/val) 0.49961/0.17574. Took 0.05 sec\n",
            "Epoch 2465, Loss(train/val) 0.50944/0.17144. Took 0.05 sec\n",
            "Epoch 2466, Loss(train/val) 0.52988/0.17058. Took 0.06 sec\n",
            "Epoch 2467, Loss(train/val) 0.50522/0.17161. Took 0.05 sec\n",
            "Epoch 2468, Loss(train/val) 0.52649/0.17030. Took 0.06 sec\n",
            "Epoch 2469, Loss(train/val) 0.49719/0.17061. Took 0.05 sec\n",
            "Epoch 2470, Loss(train/val) 0.51140/0.17845. Took 0.05 sec\n",
            "Epoch 2471, Loss(train/val) 0.51456/0.20655. Took 0.05 sec\n",
            "Epoch 2472, Loss(train/val) 0.49641/0.23409. Took 0.05 sec\n",
            "Epoch 2473, Loss(train/val) 0.51167/0.23118. Took 0.06 sec\n",
            "Epoch 2474, Loss(train/val) 0.48616/0.21659. Took 0.05 sec\n",
            "Epoch 2475, Loss(train/val) 0.51835/0.19170. Took 0.05 sec\n",
            "Epoch 2476, Loss(train/val) 0.52661/0.18500. Took 0.04 sec\n",
            "Epoch 2477, Loss(train/val) 0.49557/0.17777. Took 0.05 sec\n",
            "Epoch 2478, Loss(train/val) 0.48635/0.17123. Took 0.04 sec\n",
            "Epoch 2479, Loss(train/val) 0.51203/0.17159. Took 0.04 sec\n",
            "Epoch 2480, Loss(train/val) 0.47888/0.17718. Took 0.05 sec\n",
            "Epoch 2481, Loss(train/val) 0.46408/0.17912. Took 0.05 sec\n",
            "Epoch 2482, Loss(train/val) 0.54407/0.17728. Took 0.05 sec\n",
            "Epoch 2483, Loss(train/val) 0.54062/0.17585. Took 0.05 sec\n",
            "Epoch 2484, Loss(train/val) 0.49771/0.18258. Took 0.05 sec\n",
            "Epoch 2485, Loss(train/val) 0.49064/0.17907. Took 0.05 sec\n",
            "Epoch 2486, Loss(train/val) 0.50133/0.17170. Took 0.06 sec\n",
            "Epoch 2487, Loss(train/val) 0.48346/0.17552. Took 0.05 sec\n",
            "Epoch 2488, Loss(train/val) 0.49069/0.18494. Took 0.05 sec\n",
            "Epoch 2489, Loss(train/val) 0.51512/0.18285. Took 0.05 sec\n",
            "Epoch 2490, Loss(train/val) 0.48945/0.18006. Took 0.05 sec\n",
            "Epoch 2491, Loss(train/val) 0.49079/0.17229. Took 0.05 sec\n",
            "Epoch 2492, Loss(train/val) 0.54083/0.17019. Took 0.06 sec\n",
            "Epoch 2493, Loss(train/val) 0.48570/0.17182. Took 0.04 sec\n",
            "Epoch 2494, Loss(train/val) 0.47742/0.17004. Took 0.05 sec\n",
            "Epoch 2495, Loss(train/val) 0.51180/0.16999. Took 0.05 sec\n",
            "Epoch 2496, Loss(train/val) 0.51082/0.17008. Took 0.05 sec\n",
            "Epoch 2497, Loss(train/val) 0.49008/0.16997. Took 0.05 sec\n",
            "Epoch 2498, Loss(train/val) 0.50707/0.18075. Took 0.05 sec\n",
            "Epoch 2499, Loss(train/val) 0.52176/0.19662. Took 0.05 sec\n",
            "Epoch 2500, Loss(train/val) 0.51825/0.20720. Took 0.04 sec\n",
            "Epoch 2501, Loss(train/val) 0.50545/0.21283. Took 0.05 sec\n",
            "Epoch 2502, Loss(train/val) 0.48285/0.20355. Took 0.05 sec\n",
            "Epoch 2503, Loss(train/val) 0.49140/0.18975. Took 0.05 sec\n",
            "Epoch 2504, Loss(train/val) 0.50470/0.17114. Took 0.05 sec\n",
            "Epoch 2505, Loss(train/val) 0.46331/0.16954. Took 0.05 sec\n",
            "Epoch 2506, Loss(train/val) 0.50333/0.16967. Took 0.05 sec\n",
            "Epoch 2507, Loss(train/val) 0.49853/0.17030. Took 0.06 sec\n",
            "Epoch 2508, Loss(train/val) 0.48294/0.16990. Took 0.05 sec\n",
            "Epoch 2509, Loss(train/val) 0.52366/0.16969. Took 0.05 sec\n",
            "Epoch 2510, Loss(train/val) 0.51599/0.17159. Took 0.04 sec\n",
            "Epoch 2511, Loss(train/val) 0.53595/0.17344. Took 0.05 sec\n",
            "Epoch 2512, Loss(train/val) 0.49508/0.17194. Took 0.06 sec\n",
            "Epoch 2513, Loss(train/val) 0.47008/0.16982. Took 0.05 sec\n",
            "Epoch 2514, Loss(train/val) 0.49572/0.17622. Took 0.04 sec\n",
            "Epoch 2515, Loss(train/val) 0.50621/0.19245. Took 0.05 sec\n",
            "Epoch 2516, Loss(train/val) 0.50579/0.20310. Took 0.04 sec\n",
            "Epoch 2517, Loss(train/val) 0.51121/0.20016. Took 0.05 sec\n",
            "Epoch 2518, Loss(train/val) 0.52111/0.19847. Took 0.05 sec\n",
            "Epoch 2519, Loss(train/val) 0.45944/0.19794. Took 0.04 sec\n",
            "Epoch 2520, Loss(train/val) 0.50494/0.20615. Took 0.05 sec\n",
            "Epoch 2521, Loss(train/val) 0.49549/0.21203. Took 0.05 sec\n",
            "Epoch 2522, Loss(train/val) 0.47879/0.20828. Took 0.05 sec\n",
            "Epoch 2523, Loss(train/val) 0.54393/0.18037. Took 0.05 sec\n",
            "Epoch 2524, Loss(train/val) 0.50742/0.17597. Took 0.05 sec\n",
            "Epoch 2525, Loss(train/val) 0.51679/0.16967. Took 0.05 sec\n",
            "Epoch 2526, Loss(train/val) 0.50275/0.17751. Took 0.05 sec\n",
            "Epoch 2527, Loss(train/val) 0.52636/0.20055. Took 0.05 sec\n",
            "Epoch 2528, Loss(train/val) 0.46557/0.20356. Took 0.04 sec\n",
            "Epoch 2529, Loss(train/val) 0.52079/0.19489. Took 0.05 sec\n",
            "Epoch 2530, Loss(train/val) 0.46189/0.17323. Took 0.05 sec\n",
            "Epoch 2531, Loss(train/val) 0.48741/0.17185. Took 0.05 sec\n",
            "Epoch 2532, Loss(train/val) 0.50161/0.18451. Took 0.05 sec\n",
            "Epoch 2533, Loss(train/val) 0.51415/0.18781. Took 0.04 sec\n",
            "Epoch 2534, Loss(train/val) 0.46303/0.19540. Took 0.04 sec\n",
            "Epoch 2535, Loss(train/val) 0.52166/0.21213. Took 0.04 sec\n",
            "Epoch 2536, Loss(train/val) 0.45658/0.22489. Took 0.05 sec\n",
            "Epoch 2537, Loss(train/val) 0.49859/0.22326. Took 0.05 sec\n",
            "Epoch 2538, Loss(train/val) 0.51173/0.21520. Took 0.04 sec\n",
            "Epoch 2539, Loss(train/val) 0.48392/0.19470. Took 0.04 sec\n",
            "Epoch 2540, Loss(train/val) 0.45204/0.18293. Took 0.04 sec\n",
            "Epoch 2541, Loss(train/val) 0.53239/0.17309. Took 0.04 sec\n",
            "Epoch 2542, Loss(train/val) 0.48470/0.17097. Took 0.05 sec\n",
            "Epoch 2543, Loss(train/val) 0.48282/0.18458. Took 0.05 sec\n",
            "Epoch 2544, Loss(train/val) 0.49564/0.18364. Took 0.05 sec\n",
            "Epoch 2545, Loss(train/val) 0.51372/0.17909. Took 0.04 sec\n",
            "Epoch 2546, Loss(train/val) 0.47814/0.17351. Took 0.05 sec\n",
            "Epoch 2547, Loss(train/val) 0.48986/0.17003. Took 0.05 sec\n",
            "Epoch 2548, Loss(train/val) 0.51072/0.17324. Took 0.05 sec\n",
            "Epoch 2549, Loss(train/val) 0.48817/0.17587. Took 0.05 sec\n",
            "Epoch 2550, Loss(train/val) 0.49705/0.18055. Took 0.05 sec\n",
            "Epoch 2551, Loss(train/val) 0.46872/0.18984. Took 0.04 sec\n",
            "Epoch 2552, Loss(train/val) 0.52831/0.18955. Took 0.05 sec\n",
            "Epoch 2553, Loss(train/val) 0.49434/0.18555. Took 0.04 sec\n",
            "Epoch 2554, Loss(train/val) 0.52186/0.17425. Took 0.05 sec\n",
            "Epoch 2555, Loss(train/val) 0.46767/0.16953. Took 0.04 sec\n",
            "Epoch 2556, Loss(train/val) 0.48699/0.16940. Took 0.04 sec\n",
            "Epoch 2557, Loss(train/val) 0.47963/0.17277. Took 0.05 sec\n",
            "Epoch 2558, Loss(train/val) 0.51470/0.18598. Took 0.05 sec\n",
            "Epoch 2559, Loss(train/val) 0.51313/0.18161. Took 0.04 sec\n",
            "Epoch 2560, Loss(train/val) 0.53699/0.17736. Took 0.05 sec\n",
            "Epoch 2561, Loss(train/val) 0.49177/0.17698. Took 0.04 sec\n",
            "Epoch 2562, Loss(train/val) 0.48907/0.17855. Took 0.05 sec\n",
            "Epoch 2563, Loss(train/val) 0.49635/0.17667. Took 0.05 sec\n",
            "Epoch 2564, Loss(train/val) 0.48302/0.17277. Took 0.04 sec\n",
            "Epoch 2565, Loss(train/val) 0.49541/0.16951. Took 0.04 sec\n",
            "Epoch 2566, Loss(train/val) 0.53709/0.16959. Took 0.05 sec\n",
            "Epoch 2567, Loss(train/val) 0.50637/0.16979. Took 0.05 sec\n",
            "Epoch 2568, Loss(train/val) 0.50406/0.17301. Took 0.04 sec\n",
            "Epoch 2569, Loss(train/val) 0.50724/0.17152. Took 0.05 sec\n",
            "Epoch 2570, Loss(train/val) 0.47657/0.17232. Took 0.05 sec\n",
            "Epoch 2571, Loss(train/val) 0.51447/0.18131. Took 0.04 sec\n",
            "Epoch 2572, Loss(train/val) 0.51238/0.19986. Took 0.06 sec\n",
            "Epoch 2573, Loss(train/val) 0.53800/0.21418. Took 0.04 sec\n",
            "Epoch 2574, Loss(train/val) 0.50515/0.22364. Took 0.05 sec\n",
            "Epoch 2575, Loss(train/val) 0.46737/0.21380. Took 0.05 sec\n",
            "Epoch 2576, Loss(train/val) 0.50003/0.21110. Took 0.04 sec\n",
            "Epoch 2577, Loss(train/val) 0.52061/0.20912. Took 0.05 sec\n",
            "Epoch 2578, Loss(train/val) 0.51484/0.19788. Took 0.05 sec\n",
            "Epoch 2579, Loss(train/val) 0.51126/0.18925. Took 0.04 sec\n",
            "Epoch 2580, Loss(train/val) 0.49853/0.18462. Took 0.05 sec\n",
            "Epoch 2581, Loss(train/val) 0.45547/0.17247. Took 0.04 sec\n",
            "Epoch 2582, Loss(train/val) 0.49164/0.17005. Took 0.06 sec\n",
            "Epoch 2583, Loss(train/val) 0.49815/0.17142. Took 0.05 sec\n",
            "Epoch 2584, Loss(train/val) 0.50645/0.17784. Took 0.04 sec\n",
            "Epoch 2585, Loss(train/val) 0.50643/0.19418. Took 0.06 sec\n",
            "Epoch 2586, Loss(train/val) 0.52406/0.21019. Took 0.04 sec\n",
            "Epoch 2587, Loss(train/val) 0.49032/0.21530. Took 0.05 sec\n",
            "Epoch 2588, Loss(train/val) 0.53063/0.20202. Took 0.05 sec\n",
            "Epoch 2589, Loss(train/val) 0.53559/0.20666. Took 0.05 sec\n",
            "Epoch 2590, Loss(train/val) 0.50135/0.20990. Took 0.05 sec\n",
            "Epoch 2591, Loss(train/val) 0.51065/0.22328. Took 0.04 sec\n",
            "Epoch 2592, Loss(train/val) 0.49612/0.21681. Took 0.05 sec\n",
            "Epoch 2593, Loss(train/val) 0.47286/0.19408. Took 0.05 sec\n",
            "Epoch 2594, Loss(train/val) 0.49299/0.17434. Took 0.05 sec\n",
            "Epoch 2595, Loss(train/val) 0.45775/0.16972. Took 0.04 sec\n",
            "Epoch 2596, Loss(train/val) 0.47467/0.17858. Took 0.05 sec\n",
            "Epoch 2597, Loss(train/val) 0.48564/0.17975. Took 0.05 sec\n",
            "Epoch 2598, Loss(train/val) 0.48969/0.17414. Took 0.04 sec\n",
            "Epoch 2599, Loss(train/val) 0.48256/0.17423. Took 0.05 sec\n",
            "Epoch 2600, Loss(train/val) 0.50939/0.16931. Took 0.04 sec\n",
            "Epoch 2601, Loss(train/val) 0.50842/0.16911. Took 0.04 sec\n",
            "Epoch 2602, Loss(train/val) 0.49310/0.16950. Took 0.06 sec\n",
            "Epoch 2603, Loss(train/val) 0.49841/0.17117. Took 0.04 sec\n",
            "Epoch 2604, Loss(train/val) 0.51052/0.17185. Took 0.04 sec\n",
            "Epoch 2605, Loss(train/val) 0.48173/0.17556. Took 0.04 sec\n",
            "Epoch 2606, Loss(train/val) 0.48806/0.17653. Took 0.04 sec\n",
            "Epoch 2607, Loss(train/val) 0.47974/0.17658. Took 0.05 sec\n",
            "Epoch 2608, Loss(train/val) 0.50135/0.17578. Took 0.04 sec\n",
            "Epoch 2609, Loss(train/val) 0.48940/0.18047. Took 0.05 sec\n",
            "Epoch 2610, Loss(train/val) 0.48756/0.18053. Took 0.05 sec\n",
            "Epoch 2611, Loss(train/val) 0.49472/0.18600. Took 0.04 sec\n",
            "Epoch 2612, Loss(train/val) 0.50559/0.18429. Took 0.05 sec\n",
            "Epoch 2613, Loss(train/val) 0.47722/0.18821. Took 0.05 sec\n",
            "Epoch 2614, Loss(train/val) 0.49673/0.18691. Took 0.05 sec\n",
            "Epoch 2615, Loss(train/val) 0.52220/0.18201. Took 0.05 sec\n",
            "Epoch 2616, Loss(train/val) 0.51748/0.17972. Took 0.04 sec\n",
            "Epoch 2617, Loss(train/val) 0.46202/0.19084. Took 0.06 sec\n",
            "Epoch 2618, Loss(train/val) 0.50028/0.20989. Took 0.04 sec\n",
            "Epoch 2619, Loss(train/val) 0.50249/0.22097. Took 0.05 sec\n",
            "Epoch 2620, Loss(train/val) 0.50381/0.21506. Took 0.05 sec\n",
            "Epoch 2621, Loss(train/val) 0.45996/0.22058. Took 0.04 sec\n",
            "Epoch 2622, Loss(train/val) 0.47430/0.22929. Took 0.05 sec\n",
            "Epoch 2623, Loss(train/val) 0.46876/0.21808. Took 0.05 sec\n",
            "Epoch 2624, Loss(train/val) 0.48763/0.18434. Took 0.05 sec\n",
            "Epoch 2625, Loss(train/val) 0.49752/0.17032. Took 0.04 sec\n",
            "Epoch 2626, Loss(train/val) 0.51239/0.17040. Took 0.04 sec\n",
            "Epoch 2627, Loss(train/val) 0.49226/0.17321. Took 0.05 sec\n",
            "Epoch 2628, Loss(train/val) 0.49603/0.18114. Took 0.04 sec\n",
            "Epoch 2629, Loss(train/val) 0.48884/0.18780. Took 0.04 sec\n",
            "Epoch 2630, Loss(train/val) 0.45657/0.18013. Took 0.04 sec\n",
            "Epoch 2631, Loss(train/val) 0.45449/0.17587. Took 0.05 sec\n",
            "Epoch 2632, Loss(train/val) 0.45711/0.17009. Took 0.06 sec\n",
            "Epoch 2633, Loss(train/val) 0.46114/0.17107. Took 0.05 sec\n",
            "Epoch 2634, Loss(train/val) 0.50548/0.17844. Took 0.04 sec\n",
            "Epoch 2635, Loss(train/val) 0.48068/0.19307. Took 0.04 sec\n",
            "Epoch 2636, Loss(train/val) 0.49272/0.20562. Took 0.05 sec\n",
            "Epoch 2637, Loss(train/val) 0.51363/0.19377. Took 0.05 sec\n",
            "Epoch 2638, Loss(train/val) 0.48127/0.18537. Took 0.05 sec\n",
            "Epoch 2639, Loss(train/val) 0.45399/0.18451. Took 0.05 sec\n",
            "Epoch 2640, Loss(train/val) 0.47660/0.18390. Took 0.05 sec\n",
            "Epoch 2641, Loss(train/val) 0.50744/0.18133. Took 0.05 sec\n",
            "Epoch 2642, Loss(train/val) 0.48270/0.18252. Took 0.05 sec\n",
            "Epoch 2643, Loss(train/val) 0.47509/0.18789. Took 0.05 sec\n",
            "Epoch 2644, Loss(train/val) 0.47476/0.18972. Took 0.05 sec\n",
            "Epoch 2645, Loss(train/val) 0.47985/0.18981. Took 0.04 sec\n",
            "Epoch 2646, Loss(train/val) 0.48668/0.19611. Took 0.04 sec\n",
            "Epoch 2647, Loss(train/val) 0.50266/0.20487. Took 0.05 sec\n",
            "Epoch 2648, Loss(train/val) 0.48440/0.21592. Took 0.05 sec\n",
            "Epoch 2649, Loss(train/val) 0.45369/0.23473. Took 0.04 sec\n",
            "Epoch 2650, Loss(train/val) 0.47698/0.24270. Took 0.04 sec\n",
            "Epoch 2651, Loss(train/val) 0.50221/0.22802. Took 0.05 sec\n",
            "Epoch 2652, Loss(train/val) 0.49735/0.20321. Took 0.05 sec\n",
            "Epoch 2653, Loss(train/val) 0.49005/0.18103. Took 0.04 sec\n",
            "Epoch 2654, Loss(train/val) 0.49929/0.17772. Took 0.05 sec\n",
            "Epoch 2655, Loss(train/val) 0.45130/0.17248. Took 0.04 sec\n",
            "Epoch 2656, Loss(train/val) 0.48083/0.17036. Took 0.04 sec\n",
            "Epoch 2657, Loss(train/val) 0.49231/0.17073. Took 0.05 sec\n",
            "Epoch 2658, Loss(train/val) 0.48438/0.17095. Took 0.06 sec\n",
            "Epoch 2659, Loss(train/val) 0.46818/0.17356. Took 0.04 sec\n",
            "Epoch 2660, Loss(train/val) 0.50559/0.17653. Took 0.05 sec\n",
            "Epoch 2661, Loss(train/val) 0.47653/0.17696. Took 0.05 sec\n",
            "Epoch 2662, Loss(train/val) 0.50019/0.17340. Took 0.05 sec\n",
            "Epoch 2663, Loss(train/val) 0.46710/0.17037. Took 0.04 sec\n",
            "Epoch 2664, Loss(train/val) 0.50406/0.17106. Took 0.04 sec\n",
            "Epoch 2665, Loss(train/val) 0.51146/0.17573. Took 0.05 sec\n",
            "Epoch 2666, Loss(train/val) 0.48172/0.17531. Took 0.05 sec\n",
            "Epoch 2667, Loss(train/val) 0.50340/0.17358. Took 0.06 sec\n",
            "Epoch 2668, Loss(train/val) 0.49813/0.17340. Took 0.05 sec\n",
            "Epoch 2669, Loss(train/val) 0.50162/0.17176. Took 0.04 sec\n",
            "Epoch 2670, Loss(train/val) 0.50855/0.17008. Took 0.04 sec\n",
            "Epoch 2671, Loss(train/val) 0.49164/0.17081. Took 0.05 sec\n",
            "Epoch 2672, Loss(train/val) 0.49237/0.17553. Took 0.05 sec\n",
            "Epoch 2673, Loss(train/val) 0.45833/0.17767. Took 0.05 sec\n",
            "Epoch 2674, Loss(train/val) 0.45665/0.17637. Took 0.04 sec\n",
            "Epoch 2675, Loss(train/val) 0.49040/0.17997. Took 0.05 sec\n",
            "Epoch 2676, Loss(train/val) 0.52536/0.17856. Took 0.04 sec\n",
            "Epoch 2677, Loss(train/val) 0.51689/0.17617. Took 0.05 sec\n",
            "Epoch 2678, Loss(train/val) 0.50202/0.17704. Took 0.05 sec\n",
            "Epoch 2679, Loss(train/val) 0.53449/0.17427. Took 0.05 sec\n",
            "Epoch 2680, Loss(train/val) 0.48729/0.17622. Took 0.05 sec\n",
            "Epoch 2681, Loss(train/val) 0.46341/0.17799. Took 0.04 sec\n",
            "Epoch 2682, Loss(train/val) 0.48385/0.18116. Took 0.05 sec\n",
            "Epoch 2683, Loss(train/val) 0.49869/0.17972. Took 0.05 sec\n",
            "Epoch 2684, Loss(train/val) 0.51289/0.17021. Took 0.04 sec\n",
            "Epoch 2685, Loss(train/val) 0.48733/0.17159. Took 0.04 sec\n",
            "Epoch 2686, Loss(train/val) 0.51930/0.18565. Took 0.05 sec\n",
            "Epoch 2687, Loss(train/val) 0.48470/0.18430. Took 0.05 sec\n",
            "Epoch 2688, Loss(train/val) 0.50653/0.17285. Took 0.05 sec\n",
            "Epoch 2689, Loss(train/val) 0.47669/0.17043. Took 0.05 sec\n",
            "Epoch 2690, Loss(train/val) 0.50783/0.17672. Took 0.04 sec\n",
            "Epoch 2691, Loss(train/val) 0.48698/0.18294. Took 0.04 sec\n",
            "Epoch 2692, Loss(train/val) 0.47466/0.17698. Took 0.05 sec\n",
            "Epoch 2693, Loss(train/val) 0.51860/0.17077. Took 0.04 sec\n",
            "Epoch 2694, Loss(train/val) 0.46662/0.17183. Took 0.05 sec\n",
            "Epoch 2695, Loss(train/val) 0.49438/0.17042. Took 0.04 sec\n",
            "Epoch 2696, Loss(train/val) 0.50123/0.18078. Took 0.04 sec\n",
            "Epoch 2697, Loss(train/val) 0.47221/0.18593. Took 0.06 sec\n",
            "Epoch 2698, Loss(train/val) 0.47546/0.18579. Took 0.04 sec\n",
            "Epoch 2699, Loss(train/val) 0.49233/0.18107. Took 0.04 sec\n",
            "Epoch 2700, Loss(train/val) 0.47648/0.18119. Took 0.04 sec\n",
            "Epoch 2701, Loss(train/val) 0.44708/0.17993. Took 0.05 sec\n",
            "Epoch 2702, Loss(train/val) 0.48784/0.17176. Took 0.05 sec\n",
            "Epoch 2703, Loss(train/val) 0.52493/0.17034. Took 0.04 sec\n",
            "Epoch 2704, Loss(train/val) 0.48737/0.18123. Took 0.04 sec\n",
            "Epoch 2705, Loss(train/val) 0.48912/0.20085. Took 0.05 sec\n",
            "Epoch 2706, Loss(train/val) 0.47997/0.21374. Took 0.04 sec\n",
            "Epoch 2707, Loss(train/val) 0.49182/0.20326. Took 0.05 sec\n",
            "Epoch 2708, Loss(train/val) 0.48659/0.19613. Took 0.05 sec\n",
            "Epoch 2709, Loss(train/val) 0.50751/0.19503. Took 0.05 sec\n",
            "Epoch 2710, Loss(train/val) 0.47167/0.20107. Took 0.04 sec\n",
            "Epoch 2711, Loss(train/val) 0.50378/0.18448. Took 0.05 sec\n",
            "Epoch 2712, Loss(train/val) 0.47717/0.17387. Took 0.05 sec\n",
            "Epoch 2713, Loss(train/val) 0.46656/0.17314. Took 0.04 sec\n",
            "Epoch 2714, Loss(train/val) 0.51476/0.17232. Took 0.04 sec\n",
            "Epoch 2715, Loss(train/val) 0.43220/0.17316. Took 0.05 sec\n",
            "Epoch 2716, Loss(train/val) 0.47447/0.17414. Took 0.05 sec\n",
            "Epoch 2717, Loss(train/val) 0.47391/0.18338. Took 0.05 sec\n",
            "Epoch 2718, Loss(train/val) 0.47210/0.19555. Took 0.04 sec\n",
            "Epoch 2719, Loss(train/val) 0.49844/0.20525. Took 0.05 sec\n",
            "Epoch 2720, Loss(train/val) 0.55787/0.20881. Took 0.04 sec\n",
            "Epoch 2721, Loss(train/val) 0.48508/0.21153. Took 0.04 sec\n",
            "Epoch 2722, Loss(train/val) 0.49720/0.20356. Took 0.05 sec\n",
            "Epoch 2723, Loss(train/val) 0.50526/0.19811. Took 0.06 sec\n",
            "Epoch 2724, Loss(train/val) 0.48475/0.19874. Took 0.04 sec\n",
            "Epoch 2725, Loss(train/val) 0.48222/0.20619. Took 0.04 sec\n",
            "Epoch 2726, Loss(train/val) 0.52274/0.19164. Took 0.05 sec\n",
            "Epoch 2727, Loss(train/val) 0.47335/0.19456. Took 0.05 sec\n",
            "Epoch 2728, Loss(train/val) 0.50042/0.19352. Took 0.04 sec\n",
            "Epoch 2729, Loss(train/val) 0.49759/0.19450. Took 0.04 sec\n",
            "Epoch 2730, Loss(train/val) 0.50203/0.18856. Took 0.04 sec\n",
            "Epoch 2731, Loss(train/val) 0.46985/0.18181. Took 0.04 sec\n",
            "Epoch 2732, Loss(train/val) 0.53098/0.18180. Took 0.05 sec\n",
            "Epoch 2733, Loss(train/val) 0.50023/0.18533. Took 0.04 sec\n",
            "Epoch 2734, Loss(train/val) 0.51381/0.18837. Took 0.04 sec\n",
            "Epoch 2735, Loss(train/val) 0.46164/0.18717. Took 0.04 sec\n",
            "Epoch 2736, Loss(train/val) 0.51344/0.18069. Took 0.05 sec\n",
            "Epoch 2737, Loss(train/val) 0.50938/0.17290. Took 0.06 sec\n",
            "Epoch 2738, Loss(train/val) 0.51347/0.17146. Took 0.05 sec\n",
            "Epoch 2739, Loss(train/val) 0.50571/0.17943. Took 0.05 sec\n",
            "Epoch 2740, Loss(train/val) 0.49252/0.18899. Took 0.05 sec\n",
            "Epoch 2741, Loss(train/val) 0.48265/0.18977. Took 0.05 sec\n",
            "Epoch 2742, Loss(train/val) 0.46527/0.18320. Took 0.05 sec\n",
            "Epoch 2743, Loss(train/val) 0.45517/0.18112. Took 0.05 sec\n",
            "Epoch 2744, Loss(train/val) 0.50916/0.17010. Took 0.05 sec\n",
            "Epoch 2745, Loss(train/val) 0.45751/0.17950. Took 0.05 sec\n",
            "Epoch 2746, Loss(train/val) 0.45392/0.18511. Took 0.04 sec\n",
            "Epoch 2747, Loss(train/val) 0.49444/0.18752. Took 0.05 sec\n",
            "Epoch 2748, Loss(train/val) 0.50157/0.18245. Took 0.04 sec\n",
            "Epoch 2749, Loss(train/val) 0.48742/0.17408. Took 0.04 sec\n",
            "Epoch 2750, Loss(train/val) 0.52109/0.17135. Took 0.04 sec\n",
            "Epoch 2751, Loss(train/val) 0.48249/0.17018. Took 0.05 sec\n",
            "Epoch 2752, Loss(train/val) 0.50564/0.17020. Took 0.05 sec\n",
            "Epoch 2753, Loss(train/val) 0.50447/0.18251. Took 0.04 sec\n",
            "Epoch 2754, Loss(train/val) 0.46857/0.19090. Took 0.05 sec\n",
            "Epoch 2755, Loss(train/val) 0.48455/0.18990. Took 0.04 sec\n",
            "Epoch 2756, Loss(train/val) 0.49646/0.18585. Took 0.05 sec\n",
            "Epoch 2757, Loss(train/val) 0.48948/0.17738. Took 0.05 sec\n",
            "Epoch 2758, Loss(train/val) 0.49575/0.17150. Took 0.05 sec\n",
            "Epoch 2759, Loss(train/val) 0.48669/0.17261. Took 0.04 sec\n",
            "Epoch 2760, Loss(train/val) 0.47210/0.17079. Took 0.05 sec\n",
            "Epoch 2761, Loss(train/val) 0.49192/0.17135. Took 0.05 sec\n",
            "Epoch 2762, Loss(train/val) 0.46889/0.17128. Took 0.05 sec\n",
            "Epoch 2763, Loss(train/val) 0.49396/0.17175. Took 0.06 sec\n",
            "Epoch 2764, Loss(train/val) 0.49654/0.17393. Took 0.04 sec\n",
            "Epoch 2765, Loss(train/val) 0.51000/0.18210. Took 0.05 sec\n",
            "Epoch 2766, Loss(train/val) 0.51262/0.17871. Took 0.05 sec\n",
            "Epoch 2767, Loss(train/val) 0.48029/0.17512. Took 0.05 sec\n",
            "Epoch 2768, Loss(train/val) 0.48927/0.17296. Took 0.05 sec\n",
            "Epoch 2769, Loss(train/val) 0.50069/0.16982. Took 0.04 sec\n",
            "Epoch 2770, Loss(train/val) 0.45920/0.16993. Took 0.04 sec\n",
            "Epoch 2771, Loss(train/val) 0.52796/0.17052. Took 0.04 sec\n",
            "Epoch 2772, Loss(train/val) 0.50616/0.17144. Took 0.05 sec\n",
            "Epoch 2773, Loss(train/val) 0.47117/0.17352. Took 0.05 sec\n",
            "Epoch 2774, Loss(train/val) 0.48846/0.17379. Took 0.05 sec\n",
            "Epoch 2775, Loss(train/val) 0.51277/0.17202. Took 0.05 sec\n",
            "Epoch 2776, Loss(train/val) 0.47923/0.17081. Took 0.04 sec\n",
            "Epoch 2777, Loss(train/val) 0.50524/0.17323. Took 0.05 sec\n",
            "Epoch 2778, Loss(train/val) 0.47402/0.17234. Took 0.05 sec\n",
            "Epoch 2779, Loss(train/val) 0.49901/0.16941. Took 0.05 sec\n",
            "Epoch 2780, Loss(train/val) 0.48758/0.17311. Took 0.05 sec\n",
            "Epoch 2781, Loss(train/val) 0.48901/0.18038. Took 0.05 sec\n",
            "Epoch 2782, Loss(train/val) 0.46751/0.17994. Took 0.05 sec\n",
            "Epoch 2783, Loss(train/val) 0.49726/0.17989. Took 0.05 sec\n",
            "Epoch 2784, Loss(train/val) 0.47401/0.18531. Took 0.04 sec\n",
            "Epoch 2785, Loss(train/val) 0.48556/0.19587. Took 0.05 sec\n",
            "Epoch 2786, Loss(train/val) 0.44294/0.19489. Took 0.05 sec\n",
            "Epoch 2787, Loss(train/val) 0.48045/0.18896. Took 0.06 sec\n",
            "Epoch 2788, Loss(train/val) 0.52264/0.18057. Took 0.05 sec\n",
            "Epoch 2789, Loss(train/val) 0.46460/0.18010. Took 0.04 sec\n",
            "Epoch 2790, Loss(train/val) 0.45011/0.18152. Took 0.04 sec\n",
            "Epoch 2791, Loss(train/val) 0.47345/0.18633. Took 0.05 sec\n",
            "Epoch 2792, Loss(train/val) 0.50894/0.19307. Took 0.05 sec\n",
            "Epoch 2793, Loss(train/val) 0.49517/0.19213. Took 0.05 sec\n",
            "Epoch 2794, Loss(train/val) 0.49766/0.18301. Took 0.04 sec\n",
            "Epoch 2795, Loss(train/val) 0.47556/0.18639. Took 0.05 sec\n",
            "Epoch 2796, Loss(train/val) 0.52541/0.17756. Took 0.04 sec\n",
            "Epoch 2797, Loss(train/val) 0.52159/0.17496. Took 0.05 sec\n",
            "Epoch 2798, Loss(train/val) 0.49223/0.17177. Took 0.05 sec\n",
            "Epoch 2799, Loss(train/val) 0.51813/0.17039. Took 0.05 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var1 = 'dropout'\n",
        "df = load_exp_result('exp5_dropout1')\n",
        "\n",
        "#plot_acc(var1, var2, df)\n",
        "plot_loss_variation(var1, df, sharey=False) #sharey를 True로 하면 모둔 subplot의 y축의 스케일이 같아집니다.\n",
        "#plot_acc_variation(var1, var2, df, margin_titles=True, sharey=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "Ox7zom-HEOhy",
        "outputId": "387faefa-37ad-4d94-a657-799dbcd9d9cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1139.38x216 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "### dropout = 0.2"
      ],
      "metadata": {
        "id": "uy_ficBg3uwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "9cIwJaxO1H6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 666\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp0_test\"\n",
        "args.device = 'cpu'\n",
        "\n",
        "args.batch_size = 193\n",
        "args.x_frames = 4\n",
        "args.y_frames = 4\n",
        "trainset = MyDataset(args.x_frames, args.y_frames, mydata, 0, int(0.6*len(mydata)))\n",
        "valset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.6*len(mydata)), int(0.8*len(mydata)))\n",
        "testset = MyDataset(args.x_frames, args.y_frames, mydata, int(0.8*len(mydata)), int(len(mydata)))\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
        "\n",
        "args.input_dim = 1\n",
        "args.hid_dim = 16\n",
        "args.n_layers = 8\n",
        "\n",
        "args.l2 = 0.00001\n",
        "args.dropout = 0.2\n",
        "args.use_bn = True\n",
        "\n",
        "args.optim = 'Adam'\n",
        "args.lr = 0.00005\n",
        "args.epoch = 3000\n",
        "\n",
        "name_var1 = 'hid_dim'\n",
        "list_var1 = [16]\n",
        "\n",
        "for var1 in list_var1:\n",
        "    setattr(args, name_var1, var1)\n",
        "    print(args)\n",
        "                \n",
        "    setting, result = prediction(partition, deepcopy(args))\n",
        "    save_exp_result(setting, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7ViQf2hv3IBF",
        "outputId": "f1810647-0ca1-4ba1-9803-8f286f1dd8e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=193, device='cpu', dropout=0.2, epoch=3000, exp_name='exp0_test', hid_dim=16, input_dim=1, l2=1e-05, lr=5e-05, n_layers=8, optim='Adam', use_bn=True, x_frames=4, y_frames=4)\n",
            "Epoch 0, Loss(train/val) 1.02559/0.37242. Took 0.37 sec\n",
            "Epoch 1, Loss(train/val) 1.01741/0.37350. Took 0.09 sec\n",
            "Epoch 2, Loss(train/val) 1.01566/0.37443. Took 0.09 sec\n",
            "Epoch 3, Loss(train/val) 1.01762/0.37527. Took 0.10 sec\n",
            "Epoch 4, Loss(train/val) 1.01729/0.37601. Took 0.11 sec\n",
            "Epoch 5, Loss(train/val) 1.01731/0.37666. Took 0.10 sec\n",
            "Epoch 6, Loss(train/val) 1.02222/0.37723. Took 0.10 sec\n",
            "Epoch 7, Loss(train/val) 1.01695/0.37772. Took 0.09 sec\n",
            "Epoch 8, Loss(train/val) 1.01266/0.37817. Took 0.10 sec\n",
            "Epoch 9, Loss(train/val) 1.02451/0.37858. Took 0.10 sec\n",
            "Epoch 10, Loss(train/val) 1.02597/0.37895. Took 0.11 sec\n",
            "Epoch 11, Loss(train/val) 1.02043/0.37929. Took 0.09 sec\n",
            "Epoch 12, Loss(train/val) 1.02386/0.37961. Took 0.09 sec\n",
            "Epoch 13, Loss(train/val) 1.02193/0.37990. Took 0.10 sec\n",
            "Epoch 14, Loss(train/val) 1.01697/0.38020. Took 0.10 sec\n",
            "Epoch 15, Loss(train/val) 1.01830/0.38047. Took 0.09 sec\n",
            "Epoch 16, Loss(train/val) 1.01958/0.38075. Took 0.11 sec\n",
            "Epoch 17, Loss(train/val) 1.02304/0.38104. Took 0.09 sec\n",
            "Epoch 18, Loss(train/val) 1.01906/0.38129. Took 0.10 sec\n",
            "Epoch 19, Loss(train/val) 1.01807/0.38150. Took 0.10 sec\n",
            "Epoch 20, Loss(train/val) 1.02128/0.38171. Took 0.10 sec\n",
            "Epoch 21, Loss(train/val) 1.01211/0.38196. Took 0.09 sec\n",
            "Epoch 22, Loss(train/val) 1.02424/0.38225. Took 0.10 sec\n",
            "Epoch 23, Loss(train/val) 1.01824/0.38257. Took 0.10 sec\n",
            "Epoch 24, Loss(train/val) 1.02387/0.38296. Took 0.09 sec\n",
            "Epoch 25, Loss(train/val) 1.02382/0.38339. Took 0.10 sec\n",
            "Epoch 26, Loss(train/val) 1.01707/0.38390. Took 0.09 sec\n",
            "Epoch 27, Loss(train/val) 1.01908/0.38453. Took 0.10 sec\n",
            "Epoch 28, Loss(train/val) 1.01559/0.38446. Took 0.10 sec\n",
            "Epoch 29, Loss(train/val) 1.01811/0.38413. Took 0.09 sec\n",
            "Epoch 30, Loss(train/val) 1.01832/0.38377. Took 0.10 sec\n",
            "Epoch 31, Loss(train/val) 1.01957/0.38341. Took 0.10 sec\n",
            "Epoch 32, Loss(train/val) 1.01294/0.38305. Took 0.09 sec\n",
            "Epoch 33, Loss(train/val) 1.02250/0.38275. Took 0.09 sec\n",
            "Epoch 34, Loss(train/val) 1.01910/0.38246. Took 0.10 sec\n",
            "Epoch 35, Loss(train/val) 1.02230/0.38219. Took 0.09 sec\n",
            "Epoch 36, Loss(train/val) 1.01748/0.38195. Took 0.09 sec\n",
            "Epoch 37, Loss(train/val) 1.00980/0.38178. Took 0.12 sec\n",
            "Epoch 38, Loss(train/val) 1.01309/0.38164. Took 0.09 sec\n",
            "Epoch 39, Loss(train/val) 1.01875/0.38154. Took 0.10 sec\n",
            "Epoch 40, Loss(train/val) 1.00380/0.38147. Took 0.10 sec\n",
            "Epoch 41, Loss(train/val) 1.01553/0.38127. Took 0.10 sec\n",
            "Epoch 42, Loss(train/val) 1.02169/0.38094. Took 0.09 sec\n",
            "Epoch 43, Loss(train/val) 1.02527/0.38070. Took 0.10 sec\n",
            "Epoch 44, Loss(train/val) 1.02381/0.38045. Took 0.09 sec\n",
            "Epoch 45, Loss(train/val) 1.02098/0.38016. Took 0.10 sec\n",
            "Epoch 46, Loss(train/val) 1.02339/0.37985. Took 0.10 sec\n",
            "Epoch 47, Loss(train/val) 1.01860/0.37951. Took 0.10 sec\n",
            "Epoch 48, Loss(train/val) 1.01304/0.37925. Took 0.11 sec\n",
            "Epoch 49, Loss(train/val) 1.01213/0.37897. Took 0.10 sec\n",
            "Epoch 50, Loss(train/val) 1.01956/0.37867. Took 0.10 sec\n",
            "Epoch 51, Loss(train/val) 1.02143/0.37834. Took 0.10 sec\n",
            "Epoch 52, Loss(train/val) 1.01701/0.37807. Took 0.10 sec\n",
            "Epoch 53, Loss(train/val) 1.01010/0.37773. Took 0.09 sec\n",
            "Epoch 54, Loss(train/val) 1.01664/0.37744. Took 0.10 sec\n",
            "Epoch 55, Loss(train/val) 1.01430/0.37701. Took 0.17 sec\n",
            "Epoch 56, Loss(train/val) 1.01105/0.37661. Took 0.10 sec\n",
            "Epoch 57, Loss(train/val) 1.01119/0.37619. Took 0.09 sec\n",
            "Epoch 58, Loss(train/val) 1.02055/0.37584. Took 0.12 sec\n",
            "Epoch 59, Loss(train/val) 1.02403/0.37538. Took 0.11 sec\n",
            "Epoch 60, Loss(train/val) 1.01088/0.37495. Took 0.11 sec\n",
            "Epoch 61, Loss(train/val) 1.01577/0.37466. Took 0.16 sec\n",
            "Epoch 62, Loss(train/val) 1.01854/0.37440. Took 0.10 sec\n",
            "Epoch 63, Loss(train/val) 1.01502/0.37430. Took 0.10 sec\n",
            "Epoch 64, Loss(train/val) 1.02097/0.37404. Took 0.10 sec\n",
            "Epoch 65, Loss(train/val) 1.01868/0.37383. Took 0.10 sec\n",
            "Epoch 66, Loss(train/val) 1.01990/0.37372. Took 0.10 sec\n",
            "Epoch 67, Loss(train/val) 1.01933/0.37355. Took 0.17 sec\n",
            "Epoch 68, Loss(train/val) 1.01465/0.37319. Took 0.09 sec\n",
            "Epoch 69, Loss(train/val) 1.01484/0.37264. Took 0.09 sec\n",
            "Epoch 70, Loss(train/val) 1.00950/0.37224. Took 0.10 sec\n",
            "Epoch 71, Loss(train/val) 1.01935/0.37183. Took 0.09 sec\n",
            "Epoch 72, Loss(train/val) 1.01602/0.37147. Took 0.10 sec\n",
            "Epoch 73, Loss(train/val) 1.01674/0.37104. Took 0.10 sec\n",
            "Epoch 74, Loss(train/val) 1.01688/0.37070. Took 0.10 sec\n",
            "Epoch 75, Loss(train/val) 1.02093/0.37050. Took 0.11 sec\n",
            "Epoch 76, Loss(train/val) 1.00615/0.37012. Took 0.09 sec\n",
            "Epoch 77, Loss(train/val) 1.00560/0.37013. Took 0.11 sec\n",
            "Epoch 78, Loss(train/val) 1.01679/0.37045. Took 0.10 sec\n",
            "Epoch 79, Loss(train/val) 1.01799/0.37088. Took 0.09 sec\n",
            "Epoch 80, Loss(train/val) 1.00813/0.37130. Took 0.09 sec\n",
            "Epoch 81, Loss(train/val) 1.00996/0.37182. Took 0.10 sec\n",
            "Epoch 82, Loss(train/val) 1.00899/0.37222. Took 0.10 sec\n",
            "Epoch 83, Loss(train/val) 1.00229/0.37260. Took 0.09 sec\n",
            "Epoch 84, Loss(train/val) 1.00938/0.37310. Took 0.11 sec\n",
            "Epoch 85, Loss(train/val) 1.00816/0.37349. Took 0.10 sec\n",
            "Epoch 86, Loss(train/val) 1.01324/0.37400. Took 0.09 sec\n",
            "Epoch 87, Loss(train/val) 1.01724/0.37436. Took 0.11 sec\n",
            "Epoch 88, Loss(train/val) 1.01134/0.37496. Took 0.09 sec\n",
            "Epoch 89, Loss(train/val) 0.99008/0.37560. Took 0.10 sec\n",
            "Epoch 90, Loss(train/val) 1.00824/0.37612. Took 0.10 sec\n",
            "Epoch 91, Loss(train/val) 1.00227/0.37677. Took 0.09 sec\n",
            "Epoch 92, Loss(train/val) 1.01359/0.37764. Took 0.09 sec\n",
            "Epoch 93, Loss(train/val) 1.00226/0.37880. Took 0.10 sec\n",
            "Epoch 94, Loss(train/val) 1.00281/0.37978. Took 0.09 sec\n",
            "Epoch 95, Loss(train/val) 1.01448/0.38075. Took 0.10 sec\n",
            "Epoch 96, Loss(train/val) 1.01349/0.38187. Took 0.11 sec\n",
            "Epoch 97, Loss(train/val) 1.00783/0.38306. Took 0.11 sec\n",
            "Epoch 98, Loss(train/val) 1.00740/0.38413. Took 0.10 sec\n",
            "Epoch 99, Loss(train/val) 0.98990/0.38512. Took 0.10 sec\n",
            "Epoch 100, Loss(train/val) 1.00518/0.38611. Took 0.09 sec\n",
            "Epoch 101, Loss(train/val) 0.99543/0.38716. Took 0.10 sec\n",
            "Epoch 102, Loss(train/val) 1.00599/0.38840. Took 0.10 sec\n",
            "Epoch 103, Loss(train/val) 0.99183/0.38945. Took 0.09 sec\n",
            "Epoch 104, Loss(train/val) 0.99648/0.39042. Took 0.10 sec\n",
            "Epoch 105, Loss(train/val) 0.98853/0.39146. Took 0.10 sec\n",
            "Epoch 106, Loss(train/val) 0.99722/0.39264. Took 0.10 sec\n",
            "Epoch 107, Loss(train/val) 0.98892/0.39356. Took 0.10 sec\n",
            "Epoch 108, Loss(train/val) 0.99052/0.39455. Took 0.10 sec\n",
            "Epoch 109, Loss(train/val) 0.99483/0.39541. Took 0.10 sec\n",
            "Epoch 110, Loss(train/val) 0.99207/0.39631. Took 0.10 sec\n",
            "Epoch 111, Loss(train/val) 0.99095/0.39724. Took 0.10 sec\n",
            "Epoch 112, Loss(train/val) 0.98678/0.39865. Took 0.11 sec\n",
            "Epoch 113, Loss(train/val) 0.98173/0.40071. Took 0.10 sec\n",
            "Epoch 114, Loss(train/val) 0.97949/0.40240. Took 0.10 sec\n",
            "Epoch 115, Loss(train/val) 0.97078/0.40443. Took 0.10 sec\n",
            "Epoch 116, Loss(train/val) 0.97789/0.40613. Took 0.10 sec\n",
            "Epoch 117, Loss(train/val) 0.97232/0.40793. Took 0.10 sec\n",
            "Epoch 118, Loss(train/val) 0.96110/0.40908. Took 0.11 sec\n",
            "Epoch 119, Loss(train/val) 0.96950/0.41016. Took 0.10 sec\n",
            "Epoch 120, Loss(train/val) 0.95048/0.41114. Took 0.10 sec\n",
            "Epoch 121, Loss(train/val) 0.95611/0.41241. Took 0.10 sec\n",
            "Epoch 122, Loss(train/val) 0.95395/0.41364. Took 0.09 sec\n",
            "Epoch 123, Loss(train/val) 0.94367/0.41485. Took 0.10 sec\n",
            "Epoch 124, Loss(train/val) 0.93920/0.41621. Took 0.10 sec\n",
            "Epoch 125, Loss(train/val) 0.93376/0.41789. Took 0.09 sec\n",
            "Epoch 126, Loss(train/val) 0.92705/0.41908. Took 0.09 sec\n",
            "Epoch 127, Loss(train/val) 0.92343/0.41985. Took 0.10 sec\n",
            "Epoch 128, Loss(train/val) 0.91309/0.42021. Took 0.11 sec\n",
            "Epoch 129, Loss(train/val) 0.91332/0.41990. Took 0.10 sec\n",
            "Epoch 130, Loss(train/val) 0.89879/0.42037. Took 0.10 sec\n",
            "Epoch 131, Loss(train/val) 0.89185/0.42154. Took 0.09 sec\n",
            "Epoch 132, Loss(train/val) 0.88576/0.42245. Took 0.10 sec\n",
            "Epoch 133, Loss(train/val) 0.88642/0.42343. Took 0.10 sec\n",
            "Epoch 134, Loss(train/val) 0.86676/0.42420. Took 0.10 sec\n",
            "Epoch 135, Loss(train/val) 0.87265/0.42551. Took 0.10 sec\n",
            "Epoch 136, Loss(train/val) 0.85791/0.42689. Took 0.09 sec\n",
            "Epoch 137, Loss(train/val) 0.85087/0.42825. Took 0.09 sec\n",
            "Epoch 138, Loss(train/val) 0.85095/0.42807. Took 0.10 sec\n",
            "Epoch 139, Loss(train/val) 0.83732/0.42609. Took 0.11 sec\n",
            "Epoch 140, Loss(train/val) 0.82486/0.42414. Took 0.09 sec\n",
            "Epoch 141, Loss(train/val) 0.82787/0.42290. Took 0.10 sec\n",
            "Epoch 142, Loss(train/val) 0.81190/0.42062. Took 0.09 sec\n",
            "Epoch 143, Loss(train/val) 0.80601/0.41648. Took 0.10 sec\n",
            "Epoch 144, Loss(train/val) 0.80572/0.41553. Took 0.10 sec\n",
            "Epoch 145, Loss(train/val) 0.80199/0.41232. Took 0.10 sec\n",
            "Epoch 146, Loss(train/val) 0.80002/0.40764. Took 0.09 sec\n",
            "Epoch 147, Loss(train/val) 0.78429/0.40321. Took 0.10 sec\n",
            "Epoch 148, Loss(train/val) 0.79200/0.39877. Took 0.10 sec\n",
            "Epoch 149, Loss(train/val) 0.78604/0.39139. Took 0.10 sec\n",
            "Epoch 150, Loss(train/val) 0.78225/0.38500. Took 0.10 sec\n",
            "Epoch 151, Loss(train/val) 0.77933/0.37668. Took 0.09 sec\n",
            "Epoch 152, Loss(train/val) 0.78448/0.36760. Took 0.09 sec\n",
            "Epoch 153, Loss(train/val) 0.77826/0.36057. Took 0.10 sec\n",
            "Epoch 154, Loss(train/val) 0.77890/0.35331. Took 0.10 sec\n",
            "Epoch 155, Loss(train/val) 0.77557/0.34690. Took 0.09 sec\n",
            "Epoch 156, Loss(train/val) 0.77633/0.34122. Took 0.10 sec\n",
            "Epoch 157, Loss(train/val) 0.77306/0.33607. Took 0.09 sec\n",
            "Epoch 158, Loss(train/val) 0.76435/0.32959. Took 0.10 sec\n",
            "Epoch 159, Loss(train/val) 0.76881/0.32311. Took 0.10 sec\n",
            "Epoch 160, Loss(train/val) 0.76793/0.31708. Took 0.10 sec\n",
            "Epoch 161, Loss(train/val) 0.75075/0.31015. Took 0.10 sec\n",
            "Epoch 162, Loss(train/val) 0.75622/0.30305. Took 0.11 sec\n",
            "Epoch 163, Loss(train/val) 0.75765/0.29880. Took 0.10 sec\n",
            "Epoch 164, Loss(train/val) 0.75343/0.29311. Took 0.09 sec\n",
            "Epoch 165, Loss(train/val) 0.74908/0.28645. Took 0.10 sec\n",
            "Epoch 166, Loss(train/val) 0.75637/0.27915. Took 0.10 sec\n",
            "Epoch 167, Loss(train/val) 0.74441/0.27219. Took 0.10 sec\n",
            "Epoch 168, Loss(train/val) 0.75433/0.26954. Took 0.10 sec\n",
            "Epoch 169, Loss(train/val) 0.75381/0.26598. Took 0.09 sec\n",
            "Epoch 170, Loss(train/val) 0.75054/0.26245. Took 0.11 sec\n",
            "Epoch 171, Loss(train/val) 0.75193/0.26163. Took 0.09 sec\n",
            "Epoch 172, Loss(train/val) 0.74593/0.25738. Took 0.09 sec\n",
            "Epoch 173, Loss(train/val) 0.74852/0.25308. Took 0.10 sec\n",
            "Epoch 174, Loss(train/val) 0.74217/0.24498. Took 0.09 sec\n",
            "Epoch 175, Loss(train/val) 0.74357/0.23654. Took 0.09 sec\n",
            "Epoch 176, Loss(train/val) 0.74815/0.23151. Took 0.10 sec\n",
            "Epoch 177, Loss(train/val) 0.74735/0.22902. Took 0.10 sec\n",
            "Epoch 178, Loss(train/val) 0.74706/0.22697. Took 0.09 sec\n",
            "Epoch 179, Loss(train/val) 0.74716/0.22574. Took 0.10 sec\n",
            "Epoch 180, Loss(train/val) 0.74774/0.22522. Took 0.11 sec\n",
            "Epoch 181, Loss(train/val) 0.75559/0.22626. Took 0.09 sec\n",
            "Epoch 182, Loss(train/val) 0.74508/0.22566. Took 0.10 sec\n",
            "Epoch 183, Loss(train/val) 0.74262/0.22655. Took 0.09 sec\n",
            "Epoch 184, Loss(train/val) 0.73387/0.22897. Took 0.10 sec\n",
            "Epoch 185, Loss(train/val) 0.73844/0.23062. Took 0.10 sec\n",
            "Epoch 186, Loss(train/val) 0.73665/0.23073. Took 0.10 sec\n",
            "Epoch 187, Loss(train/val) 0.73956/0.22821. Took 0.09 sec\n",
            "Epoch 188, Loss(train/val) 0.73365/0.22623. Took 0.10 sec\n",
            "Epoch 189, Loss(train/val) 0.73278/0.22416. Took 0.10 sec\n",
            "Epoch 190, Loss(train/val) 0.73029/0.22489. Took 0.09 sec\n",
            "Epoch 191, Loss(train/val) 0.73593/0.22318. Took 0.11 sec\n",
            "Epoch 192, Loss(train/val) 0.73640/0.22249. Took 0.09 sec\n",
            "Epoch 193, Loss(train/val) 0.73678/0.22287. Took 0.09 sec\n",
            "Epoch 194, Loss(train/val) 0.72540/0.22288. Took 0.10 sec\n",
            "Epoch 195, Loss(train/val) 0.73231/0.22234. Took 0.09 sec\n",
            "Epoch 196, Loss(train/val) 0.73613/0.22236. Took 0.09 sec\n",
            "Epoch 197, Loss(train/val) 0.73407/0.22322. Took 0.10 sec\n",
            "Epoch 198, Loss(train/val) 0.73071/0.22539. Took 0.09 sec\n",
            "Epoch 199, Loss(train/val) 0.72809/0.22669. Took 0.10 sec\n",
            "Epoch 200, Loss(train/val) 0.72791/0.22654. Took 0.10 sec\n",
            "Epoch 201, Loss(train/val) 0.72640/0.22471. Took 0.11 sec\n",
            "Epoch 202, Loss(train/val) 0.73121/0.22167. Took 0.10 sec\n",
            "Epoch 203, Loss(train/val) 0.72893/0.22064. Took 0.10 sec\n",
            "Epoch 204, Loss(train/val) 0.73570/0.22025. Took 0.10 sec\n",
            "Epoch 205, Loss(train/val) 0.71747/0.22014. Took 0.10 sec\n",
            "Epoch 206, Loss(train/val) 0.72629/0.22024. Took 0.10 sec\n",
            "Epoch 207, Loss(train/val) 0.72737/0.22020. Took 0.10 sec\n",
            "Epoch 208, Loss(train/val) 0.72941/0.22050. Took 0.10 sec\n",
            "Epoch 209, Loss(train/val) 0.73269/0.22110. Took 0.10 sec\n",
            "Epoch 210, Loss(train/val) 0.72158/0.22166. Took 0.10 sec\n",
            "Epoch 211, Loss(train/val) 0.72039/0.22206. Took 0.11 sec\n",
            "Epoch 212, Loss(train/val) 0.71852/0.22228. Took 0.09 sec\n",
            "Epoch 213, Loss(train/val) 0.71803/0.22181. Took 0.09 sec\n",
            "Epoch 214, Loss(train/val) 0.71986/0.22027. Took 0.10 sec\n",
            "Epoch 215, Loss(train/val) 0.71936/0.21984. Took 0.12 sec\n",
            "Epoch 216, Loss(train/val) 0.71888/0.21989. Took 0.14 sec\n",
            "Epoch 217, Loss(train/val) 0.71540/0.21995. Took 0.15 sec\n",
            "Epoch 218, Loss(train/val) 0.71747/0.22021. Took 0.17 sec\n",
            "Epoch 219, Loss(train/val) 0.71644/0.22007. Took 0.15 sec\n",
            "Epoch 220, Loss(train/val) 0.71808/0.22007. Took 0.17 sec\n",
            "Epoch 221, Loss(train/val) 0.71231/0.21997. Took 0.15 sec\n",
            "Epoch 222, Loss(train/val) 0.72622/0.22032. Took 0.15 sec\n",
            "Epoch 223, Loss(train/val) 0.72202/0.22023. Took 0.17 sec\n",
            "Epoch 224, Loss(train/val) 0.72128/0.22075. Took 0.16 sec\n",
            "Epoch 225, Loss(train/val) 0.71426/0.21973. Took 0.15 sec\n",
            "Epoch 226, Loss(train/val) 0.71975/0.21912. Took 0.16 sec\n",
            "Epoch 227, Loss(train/val) 0.71704/0.21893. Took 0.15 sec\n",
            "Epoch 228, Loss(train/val) 0.71759/0.21881. Took 0.15 sec\n",
            "Epoch 229, Loss(train/val) 0.71178/0.21885. Took 0.16 sec\n",
            "Epoch 230, Loss(train/val) 0.71254/0.21836. Took 0.16 sec\n",
            "Epoch 231, Loss(train/val) 0.71004/0.21817. Took 0.15 sec\n",
            "Epoch 232, Loss(train/val) 0.70606/0.21812. Took 0.16 sec\n",
            "Epoch 233, Loss(train/val) 0.70091/0.21837. Took 0.15 sec\n",
            "Epoch 234, Loss(train/val) 0.71492/0.21849. Took 0.10 sec\n",
            "Epoch 235, Loss(train/val) 0.70989/0.21852. Took 0.10 sec\n",
            "Epoch 236, Loss(train/val) 0.70826/0.21821. Took 0.09 sec\n",
            "Epoch 237, Loss(train/val) 0.71021/0.21816. Took 0.10 sec\n",
            "Epoch 238, Loss(train/val) 0.70764/0.21825. Took 0.09 sec\n",
            "Epoch 239, Loss(train/val) 0.70389/0.21804. Took 0.10 sec\n",
            "Epoch 240, Loss(train/val) 0.70522/0.21808. Took 0.10 sec\n",
            "Epoch 241, Loss(train/val) 0.70915/0.21813. Took 0.09 sec\n",
            "Epoch 242, Loss(train/val) 0.70287/0.21829. Took 0.09 sec\n",
            "Epoch 243, Loss(train/val) 0.70474/0.21817. Took 0.10 sec\n",
            "Epoch 244, Loss(train/val) 0.69799/0.21801. Took 0.10 sec\n",
            "Epoch 245, Loss(train/val) 0.70497/0.21814. Took 0.10 sec\n",
            "Epoch 246, Loss(train/val) 0.71134/0.21840. Took 0.10 sec\n",
            "Epoch 247, Loss(train/val) 0.70276/0.21845. Took 0.10 sec\n",
            "Epoch 248, Loss(train/val) 0.70238/0.21824. Took 0.09 sec\n",
            "Epoch 249, Loss(train/val) 0.70192/0.21817. Took 0.11 sec\n",
            "Epoch 250, Loss(train/val) 0.69938/0.21859. Took 0.10 sec\n",
            "Epoch 251, Loss(train/val) 0.70296/0.21891. Took 0.10 sec\n",
            "Epoch 252, Loss(train/val) 0.69668/0.21893. Took 0.10 sec\n",
            "Epoch 253, Loss(train/val) 0.69740/0.21887. Took 0.10 sec\n",
            "Epoch 254, Loss(train/val) 0.69527/0.21996. Took 0.10 sec\n",
            "Epoch 255, Loss(train/val) 0.69694/0.21946. Took 0.10 sec\n",
            "Epoch 256, Loss(train/val) 0.70131/0.21909. Took 0.11 sec\n",
            "Epoch 257, Loss(train/val) 0.69267/0.21938. Took 0.11 sec\n",
            "Epoch 258, Loss(train/val) 0.69132/0.21994. Took 0.09 sec\n",
            "Epoch 259, Loss(train/val) 0.69516/0.22046. Took 0.10 sec\n",
            "Epoch 260, Loss(train/val) 0.69842/0.22085. Took 0.10 sec\n",
            "Epoch 261, Loss(train/val) 0.70275/0.22307. Took 0.10 sec\n",
            "Epoch 262, Loss(train/val) 0.70466/0.22396. Took 0.10 sec\n",
            "Epoch 263, Loss(train/val) 0.68219/0.22273. Took 0.10 sec\n",
            "Epoch 264, Loss(train/val) 0.70586/0.22668. Took 0.10 sec\n",
            "Epoch 265, Loss(train/val) 0.69569/0.23040. Took 0.10 sec\n",
            "Epoch 266, Loss(train/val) 0.68005/0.23645. Took 0.10 sec\n",
            "Epoch 267, Loss(train/val) 0.69059/0.23780. Took 0.10 sec\n",
            "Epoch 268, Loss(train/val) 0.69883/0.23955. Took 0.10 sec\n",
            "Epoch 269, Loss(train/val) 0.68917/0.23486. Took 0.10 sec\n",
            "Epoch 270, Loss(train/val) 0.69488/0.23091. Took 0.10 sec\n",
            "Epoch 271, Loss(train/val) 0.69943/0.23090. Took 0.10 sec\n",
            "Epoch 272, Loss(train/val) 0.69816/0.22749. Took 0.10 sec\n",
            "Epoch 273, Loss(train/val) 0.69910/0.22452. Took 0.10 sec\n",
            "Epoch 274, Loss(train/val) 0.69798/0.22155. Took 0.12 sec\n",
            "Epoch 275, Loss(train/val) 0.68828/0.22035. Took 0.10 sec\n",
            "Epoch 276, Loss(train/val) 0.68853/0.21966. Took 0.09 sec\n",
            "Epoch 277, Loss(train/val) 0.69025/0.21972. Took 0.10 sec\n",
            "Epoch 278, Loss(train/val) 0.69490/0.21919. Took 0.10 sec\n",
            "Epoch 279, Loss(train/val) 0.68723/0.21939. Took 0.09 sec\n",
            "Epoch 280, Loss(train/val) 0.69294/0.22026. Took 0.10 sec\n",
            "Epoch 281, Loss(train/val) 0.69250/0.22324. Took 0.09 sec\n",
            "Epoch 282, Loss(train/val) 0.67797/0.22325. Took 0.09 sec\n",
            "Epoch 283, Loss(train/val) 0.70194/0.22091. Took 0.10 sec\n",
            "Epoch 284, Loss(train/val) 0.69841/0.21907. Took 0.10 sec\n",
            "Epoch 285, Loss(train/val) 0.68341/0.21797. Took 0.10 sec\n",
            "Epoch 286, Loss(train/val) 0.68536/0.21771. Took 0.10 sec\n",
            "Epoch 287, Loss(train/val) 0.69022/0.21774. Took 0.10 sec\n",
            "Epoch 288, Loss(train/val) 0.68294/0.21764. Took 0.09 sec\n",
            "Epoch 289, Loss(train/val) 0.68483/0.21787. Took 0.10 sec\n",
            "Epoch 290, Loss(train/val) 0.68035/0.21981. Took 0.10 sec\n",
            "Epoch 291, Loss(train/val) 0.67516/0.22128. Took 0.09 sec\n",
            "Epoch 292, Loss(train/val) 0.68229/0.22160. Took 0.10 sec\n",
            "Epoch 293, Loss(train/val) 0.68349/0.22180. Took 0.09 sec\n",
            "Epoch 294, Loss(train/val) 0.68545/0.22221. Took 0.10 sec\n",
            "Epoch 295, Loss(train/val) 0.67200/0.22478. Took 0.11 sec\n",
            "Epoch 296, Loss(train/val) 0.68012/0.22547. Took 0.09 sec\n",
            "Epoch 297, Loss(train/val) 0.68275/0.22264. Took 0.10 sec\n",
            "Epoch 298, Loss(train/val) 0.68242/0.22014. Took 0.10 sec\n",
            "Epoch 299, Loss(train/val) 0.67320/0.21967. Took 0.10 sec\n",
            "Epoch 300, Loss(train/val) 0.68058/0.21794. Took 0.10 sec\n",
            "Epoch 301, Loss(train/val) 0.66547/0.21740. Took 0.10 sec\n",
            "Epoch 302, Loss(train/val) 0.67567/0.21753. Took 0.09 sec\n",
            "Epoch 303, Loss(train/val) 0.67773/0.21777. Took 0.09 sec\n",
            "Epoch 304, Loss(train/val) 0.67579/0.21707. Took 0.10 sec\n",
            "Epoch 305, Loss(train/val) 0.68536/0.21713. Took 0.09 sec\n",
            "Epoch 306, Loss(train/val) 0.67627/0.21762. Took 0.11 sec\n",
            "Epoch 307, Loss(train/val) 0.67753/0.21710. Took 0.10 sec\n",
            "Epoch 308, Loss(train/val) 0.67260/0.21753. Took 0.09 sec\n",
            "Epoch 309, Loss(train/val) 0.67078/0.21883. Took 0.10 sec\n",
            "Epoch 310, Loss(train/val) 0.67184/0.22048. Took 0.10 sec\n",
            "Epoch 311, Loss(train/val) 0.67242/0.22553. Took 0.09 sec\n",
            "Epoch 312, Loss(train/val) 0.67512/0.22694. Took 0.10 sec\n",
            "Epoch 313, Loss(train/val) 0.67604/0.22222. Took 0.09 sec\n",
            "Epoch 314, Loss(train/val) 0.67273/0.22293. Took 0.10 sec\n",
            "Epoch 315, Loss(train/val) 0.67065/0.22527. Took 0.10 sec\n",
            "Epoch 316, Loss(train/val) 0.66656/0.22459. Took 0.10 sec\n",
            "Epoch 317, Loss(train/val) 0.67147/0.22564. Took 0.09 sec\n",
            "Epoch 318, Loss(train/val) 0.66863/0.22859. Took 0.10 sec\n",
            "Epoch 319, Loss(train/val) 0.66991/0.23131. Took 0.09 sec\n",
            "Epoch 320, Loss(train/val) 0.66833/0.23181. Took 0.10 sec\n",
            "Epoch 321, Loss(train/val) 0.66741/0.23114. Took 0.10 sec\n",
            "Epoch 322, Loss(train/val) 0.66411/0.23338. Took 0.09 sec\n",
            "Epoch 323, Loss(train/val) 0.66902/0.23629. Took 0.09 sec\n",
            "Epoch 324, Loss(train/val) 0.66538/0.24309. Took 0.10 sec\n",
            "Epoch 325, Loss(train/val) 0.67692/0.24624. Took 0.09 sec\n",
            "Epoch 326, Loss(train/val) 0.66890/0.24227. Took 0.10 sec\n",
            "Epoch 327, Loss(train/val) 0.66649/0.23842. Took 0.11 sec\n",
            "Epoch 328, Loss(train/val) 0.66351/0.23638. Took 0.10 sec\n",
            "Epoch 329, Loss(train/val) 0.66578/0.23287. Took 0.09 sec\n",
            "Epoch 330, Loss(train/val) 0.66701/0.22972. Took 0.10 sec\n",
            "Epoch 331, Loss(train/val) 0.66488/0.22669. Took 0.10 sec\n",
            "Epoch 332, Loss(train/val) 0.67123/0.22370. Took 0.09 sec\n",
            "Epoch 333, Loss(train/val) 0.65016/0.22319. Took 0.10 sec\n",
            "Epoch 334, Loss(train/val) 0.65226/0.21944. Took 0.09 sec\n",
            "Epoch 335, Loss(train/val) 0.66116/0.21748. Took 0.10 sec\n",
            "Epoch 336, Loss(train/val) 0.66558/0.21624. Took 0.10 sec\n",
            "Epoch 337, Loss(train/val) 0.66018/0.21694. Took 0.11 sec\n",
            "Epoch 338, Loss(train/val) 0.66510/0.21699. Took 0.10 sec\n",
            "Epoch 339, Loss(train/val) 0.65731/0.21699. Took 0.10 sec\n",
            "Epoch 340, Loss(train/val) 0.66747/0.21599. Took 0.10 sec\n",
            "Epoch 341, Loss(train/val) 0.66589/0.21622. Took 0.10 sec\n",
            "Epoch 342, Loss(train/val) 0.66488/0.21656. Took 0.10 sec\n",
            "Epoch 343, Loss(train/val) 0.66106/0.21613. Took 0.09 sec\n",
            "Epoch 344, Loss(train/val) 0.65449/0.21525. Took 0.10 sec\n",
            "Epoch 345, Loss(train/val) 0.65808/0.21508. Took 0.09 sec\n",
            "Epoch 346, Loss(train/val) 0.65602/0.21518. Took 0.10 sec\n",
            "Epoch 347, Loss(train/val) 0.65703/0.21594. Took 0.10 sec\n",
            "Epoch 348, Loss(train/val) 0.64878/0.21664. Took 0.10 sec\n",
            "Epoch 349, Loss(train/val) 0.65484/0.21764. Took 0.09 sec\n",
            "Epoch 350, Loss(train/val) 0.65454/0.21870. Took 0.10 sec\n",
            "Epoch 351, Loss(train/val) 0.65113/0.21915. Took 0.10 sec\n",
            "Epoch 352, Loss(train/val) 0.64994/0.21925. Took 0.10 sec\n",
            "Epoch 353, Loss(train/val) 0.64747/0.21866. Took 0.10 sec\n",
            "Epoch 354, Loss(train/val) 0.64904/0.21709. Took 0.09 sec\n",
            "Epoch 355, Loss(train/val) 0.66049/0.21740. Took 0.09 sec\n",
            "Epoch 356, Loss(train/val) 0.65267/0.21726. Took 0.10 sec\n",
            "Epoch 357, Loss(train/val) 0.65511/0.21744. Took 0.09 sec\n",
            "Epoch 358, Loss(train/val) 0.65602/0.21996. Took 0.11 sec\n",
            "Epoch 359, Loss(train/val) 0.65216/0.21984. Took 0.10 sec\n",
            "Epoch 360, Loss(train/val) 0.64790/0.21934. Took 0.10 sec\n",
            "Epoch 361, Loss(train/val) 0.64756/0.22150. Took 0.10 sec\n",
            "Epoch 362, Loss(train/val) 0.65778/0.22283. Took 0.10 sec\n",
            "Epoch 363, Loss(train/val) 0.64768/0.22362. Took 0.09 sec\n",
            "Epoch 364, Loss(train/val) 0.65626/0.22519. Took 0.10 sec\n",
            "Epoch 365, Loss(train/val) 0.64687/0.22665. Took 0.11 sec\n",
            "Epoch 366, Loss(train/val) 0.64834/0.22495. Took 0.11 sec\n",
            "Epoch 367, Loss(train/val) 0.64264/0.22410. Took 0.09 sec\n",
            "Epoch 368, Loss(train/val) 0.65453/0.22392. Took 0.11 sec\n",
            "Epoch 369, Loss(train/val) 0.64253/0.22795. Took 0.10 sec\n",
            "Epoch 370, Loss(train/val) 0.64291/0.23276. Took 0.09 sec\n",
            "Epoch 371, Loss(train/val) 0.63837/0.23691. Took 0.10 sec\n",
            "Epoch 372, Loss(train/val) 0.64116/0.23792. Took 0.10 sec\n",
            "Epoch 373, Loss(train/val) 0.64699/0.23521. Took 0.09 sec\n",
            "Epoch 374, Loss(train/val) 0.64896/0.23383. Took 0.10 sec\n",
            "Epoch 375, Loss(train/val) 0.65021/0.22650. Took 0.09 sec\n",
            "Epoch 376, Loss(train/val) 0.64299/0.22431. Took 0.09 sec\n",
            "Epoch 377, Loss(train/val) 0.63308/0.22189. Took 0.10 sec\n",
            "Epoch 378, Loss(train/val) 0.64181/0.22190. Took 0.10 sec\n",
            "Epoch 379, Loss(train/val) 0.63798/0.21871. Took 0.10 sec\n",
            "Epoch 380, Loss(train/val) 0.63464/0.21880. Took 0.10 sec\n",
            "Epoch 381, Loss(train/val) 0.64394/0.22112. Took 0.10 sec\n",
            "Epoch 382, Loss(train/val) 0.64689/0.22198. Took 0.09 sec\n",
            "Epoch 383, Loss(train/val) 0.64314/0.22131. Took 0.10 sec\n",
            "Epoch 384, Loss(train/val) 0.65038/0.22075. Took 0.09 sec\n",
            "Epoch 385, Loss(train/val) 0.64091/0.21692. Took 0.10 sec\n",
            "Epoch 386, Loss(train/val) 0.63270/0.21502. Took 0.10 sec\n",
            "Epoch 387, Loss(train/val) 0.63874/0.21465. Took 0.09 sec\n",
            "Epoch 388, Loss(train/val) 0.64339/0.21512. Took 0.09 sec\n",
            "Epoch 389, Loss(train/val) 0.64175/0.21549. Took 0.11 sec\n",
            "Epoch 390, Loss(train/val) 0.64552/0.21524. Took 0.10 sec\n",
            "Epoch 391, Loss(train/val) 0.63548/0.21454. Took 0.10 sec\n",
            "Epoch 392, Loss(train/val) 0.63682/0.21374. Took 0.10 sec\n",
            "Epoch 393, Loss(train/val) 0.63240/0.21375. Took 0.10 sec\n",
            "Epoch 394, Loss(train/val) 0.63213/0.21490. Took 0.09 sec\n",
            "Epoch 395, Loss(train/val) 0.63532/0.21506. Took 0.10 sec\n",
            "Epoch 396, Loss(train/val) 0.63472/0.21727. Took 0.09 sec\n",
            "Epoch 397, Loss(train/val) 0.63261/0.22038. Took 0.10 sec\n",
            "Epoch 398, Loss(train/val) 0.63967/0.22463. Took 0.10 sec\n",
            "Epoch 399, Loss(train/val) 0.63468/0.22959. Took 0.10 sec\n",
            "Epoch 400, Loss(train/val) 0.63200/0.23165. Took 0.10 sec\n",
            "Epoch 401, Loss(train/val) 0.63031/0.23099. Took 0.10 sec\n",
            "Epoch 402, Loss(train/val) 0.64042/0.22898. Took 0.09 sec\n",
            "Epoch 403, Loss(train/val) 0.62826/0.22554. Took 0.10 sec\n",
            "Epoch 404, Loss(train/val) 0.64645/0.22450. Took 0.10 sec\n",
            "Epoch 405, Loss(train/val) 0.62985/0.22613. Took 0.09 sec\n",
            "Epoch 406, Loss(train/val) 0.62427/0.22520. Took 0.09 sec\n",
            "Epoch 407, Loss(train/val) 0.63343/0.22577. Took 0.10 sec\n",
            "Epoch 408, Loss(train/val) 0.62881/0.22706. Took 0.10 sec\n",
            "Epoch 409, Loss(train/val) 0.62653/0.22659. Took 0.09 sec\n",
            "Epoch 410, Loss(train/val) 0.62398/0.22598. Took 0.11 sec\n",
            "Epoch 411, Loss(train/val) 0.62252/0.22668. Took 0.10 sec\n",
            "Epoch 412, Loss(train/val) 0.63447/0.22654. Took 0.10 sec\n",
            "Epoch 413, Loss(train/val) 0.63148/0.22669. Took 0.10 sec\n",
            "Epoch 414, Loss(train/val) 0.62341/0.22671. Took 0.10 sec\n",
            "Epoch 415, Loss(train/val) 0.62035/0.22564. Took 0.09 sec\n",
            "Epoch 416, Loss(train/val) 0.63114/0.22531. Took 0.10 sec\n",
            "Epoch 417, Loss(train/val) 0.61670/0.22456. Took 0.09 sec\n",
            "Epoch 418, Loss(train/val) 0.63138/0.22362. Took 0.10 sec\n",
            "Epoch 419, Loss(train/val) 0.63070/0.22364. Took 0.10 sec\n",
            "Epoch 420, Loss(train/val) 0.63375/0.22283. Took 0.10 sec\n",
            "Epoch 421, Loss(train/val) 0.61953/0.21994. Took 0.10 sec\n",
            "Epoch 422, Loss(train/val) 0.61947/0.21715. Took 0.11 sec\n",
            "Epoch 423, Loss(train/val) 0.62757/0.21560. Took 0.10 sec\n",
            "Epoch 424, Loss(train/val) 0.62781/0.21630. Took 0.09 sec\n",
            "Epoch 425, Loss(train/val) 0.62099/0.21627. Took 0.10 sec\n",
            "Epoch 426, Loss(train/val) 0.62246/0.21558. Took 0.09 sec\n",
            "Epoch 427, Loss(train/val) 0.61931/0.21677. Took 0.10 sec\n",
            "Epoch 428, Loss(train/val) 0.61704/0.21823. Took 0.10 sec\n",
            "Epoch 429, Loss(train/val) 0.62147/0.21949. Took 0.10 sec\n",
            "Epoch 430, Loss(train/val) 0.62004/0.22248. Took 0.10 sec\n",
            "Epoch 431, Loss(train/val) 0.62288/0.22169. Took 0.11 sec\n",
            "Epoch 432, Loss(train/val) 0.62226/0.22300. Took 0.09 sec\n",
            "Epoch 433, Loss(train/val) 0.62784/0.22264. Took 0.10 sec\n",
            "Epoch 434, Loss(train/val) 0.61945/0.22058. Took 0.10 sec\n",
            "Epoch 435, Loss(train/val) 0.62111/0.22132. Took 0.10 sec\n",
            "Epoch 436, Loss(train/val) 0.63338/0.21869. Took 0.10 sec\n",
            "Epoch 437, Loss(train/val) 0.62033/0.21728. Took 0.10 sec\n",
            "Epoch 438, Loss(train/val) 0.61913/0.21493. Took 0.10 sec\n",
            "Epoch 439, Loss(train/val) 0.61756/0.21392. Took 0.10 sec\n",
            "Epoch 440, Loss(train/val) 0.62256/0.21387. Took 0.10 sec\n",
            "Epoch 441, Loss(train/val) 0.61301/0.21386. Took 0.11 sec\n",
            "Epoch 442, Loss(train/val) 0.62045/0.21363. Took 0.10 sec\n",
            "Epoch 443, Loss(train/val) 0.61845/0.21471. Took 0.09 sec\n",
            "Epoch 444, Loss(train/val) 0.61697/0.21562. Took 0.10 sec\n",
            "Epoch 445, Loss(train/val) 0.61411/0.21723. Took 0.10 sec\n",
            "Epoch 446, Loss(train/val) 0.61359/0.21601. Took 0.09 sec\n",
            "Epoch 447, Loss(train/val) 0.61764/0.21780. Took 0.10 sec\n",
            "Epoch 448, Loss(train/val) 0.61762/0.21865. Took 0.10 sec\n",
            "Epoch 449, Loss(train/val) 0.61141/0.21719. Took 0.10 sec\n",
            "Epoch 450, Loss(train/val) 0.61347/0.21555. Took 0.10 sec\n",
            "Epoch 451, Loss(train/val) 0.61458/0.21435. Took 0.10 sec\n",
            "Epoch 452, Loss(train/val) 0.61074/0.21212. Took 0.11 sec\n",
            "Epoch 453, Loss(train/val) 0.61068/0.21214. Took 0.10 sec\n",
            "Epoch 454, Loss(train/val) 0.60640/0.21380. Took 0.10 sec\n",
            "Epoch 455, Loss(train/val) 0.60523/0.21390. Took 0.10 sec\n",
            "Epoch 456, Loss(train/val) 0.61492/0.21243. Took 0.10 sec\n",
            "Epoch 457, Loss(train/val) 0.60195/0.21221. Took 0.10 sec\n",
            "Epoch 458, Loss(train/val) 0.60150/0.21243. Took 0.10 sec\n",
            "Epoch 459, Loss(train/val) 0.60636/0.21283. Took 0.10 sec\n",
            "Epoch 460, Loss(train/val) 0.61069/0.21276. Took 0.10 sec\n",
            "Epoch 461, Loss(train/val) 0.60938/0.21255. Took 0.10 sec\n",
            "Epoch 462, Loss(train/val) 0.61358/0.21343. Took 0.11 sec\n",
            "Epoch 463, Loss(train/val) 0.60523/0.21906. Took 0.10 sec\n",
            "Epoch 464, Loss(train/val) 0.60980/0.22337. Took 0.09 sec\n",
            "Epoch 465, Loss(train/val) 0.60599/0.22396. Took 0.10 sec\n",
            "Epoch 466, Loss(train/val) 0.60753/0.22304. Took 0.10 sec\n",
            "Epoch 467, Loss(train/val) 0.61134/0.22154. Took 0.09 sec\n",
            "Epoch 468, Loss(train/val) 0.60297/0.22224. Took 0.10 sec\n",
            "Epoch 469, Loss(train/val) 0.60497/0.22026. Took 0.10 sec\n",
            "Epoch 470, Loss(train/val) 0.60896/0.21562. Took 0.10 sec\n",
            "Epoch 471, Loss(train/val) 0.60896/0.21504. Took 0.28 sec\n",
            "Epoch 472, Loss(train/val) 0.61525/0.21580. Took 0.50 sec\n",
            "Epoch 473, Loss(train/val) 0.60222/0.21594. Took 0.26 sec\n",
            "Epoch 474, Loss(train/val) 0.61457/0.21395. Took 0.09 sec\n",
            "Epoch 475, Loss(train/val) 0.61016/0.21551. Took 0.10 sec\n",
            "Epoch 476, Loss(train/val) 0.60529/0.22407. Took 0.11 sec\n",
            "Epoch 477, Loss(train/val) 0.59735/0.23228. Took 0.09 sec\n",
            "Epoch 478, Loss(train/val) 0.60627/0.23837. Took 0.09 sec\n",
            "Epoch 479, Loss(train/val) 0.60213/0.24046. Took 0.10 sec\n",
            "Epoch 480, Loss(train/val) 0.60124/0.23748. Took 0.10 sec\n",
            "Epoch 481, Loss(train/val) 0.60543/0.23087. Took 0.09 sec\n",
            "Epoch 482, Loss(train/val) 0.59811/0.22788. Took 0.10 sec\n",
            "Epoch 483, Loss(train/val) 0.59980/0.22437. Took 0.09 sec\n",
            "Epoch 484, Loss(train/val) 0.60157/0.22147. Took 0.10 sec\n",
            "Epoch 485, Loss(train/val) 0.59450/0.22010. Took 0.11 sec\n",
            "Epoch 486, Loss(train/val) 0.60334/0.22028. Took 0.09 sec\n",
            "Epoch 487, Loss(train/val) 0.59881/0.21890. Took 0.09 sec\n",
            "Epoch 488, Loss(train/val) 0.59573/0.21801. Took 0.10 sec\n",
            "Epoch 489, Loss(train/val) 0.59624/0.21653. Took 0.09 sec\n",
            "Epoch 490, Loss(train/val) 0.60041/0.21710. Took 0.10 sec\n",
            "Epoch 491, Loss(train/val) 0.59934/0.21895. Took 0.11 sec\n",
            "Epoch 492, Loss(train/val) 0.59704/0.21907. Took 0.10 sec\n",
            "Epoch 493, Loss(train/val) 0.59987/0.22074. Took 0.09 sec\n",
            "Epoch 494, Loss(train/val) 0.59444/0.21818. Took 0.10 sec\n",
            "Epoch 495, Loss(train/val) 0.59413/0.21617. Took 0.10 sec\n",
            "Epoch 496, Loss(train/val) 0.59047/0.21558. Took 0.10 sec\n",
            "Epoch 497, Loss(train/val) 0.59298/0.21365. Took 0.10 sec\n",
            "Epoch 498, Loss(train/val) 0.59995/0.21214. Took 0.10 sec\n",
            "Epoch 499, Loss(train/val) 0.59140/0.21167. Took 0.09 sec\n",
            "Epoch 500, Loss(train/val) 0.58850/0.21111. Took 0.10 sec\n",
            "Epoch 501, Loss(train/val) 0.60010/0.21093. Took 0.10 sec\n",
            "Epoch 502, Loss(train/val) 0.58629/0.21099. Took 0.10 sec\n",
            "Epoch 503, Loss(train/val) 0.59393/0.21080. Took 0.10 sec\n",
            "Epoch 504, Loss(train/val) 0.59196/0.21123. Took 0.09 sec\n",
            "Epoch 505, Loss(train/val) 0.59707/0.21110. Took 0.09 sec\n",
            "Epoch 506, Loss(train/val) 0.59251/0.21066. Took 0.11 sec\n",
            "Epoch 507, Loss(train/val) 0.58020/0.21038. Took 0.10 sec\n",
            "Epoch 508, Loss(train/val) 0.58428/0.21088. Took 0.09 sec\n",
            "Epoch 509, Loss(train/val) 0.58477/0.21211. Took 0.10 sec\n",
            "Epoch 510, Loss(train/val) 0.58473/0.21525. Took 0.10 sec\n",
            "Epoch 511, Loss(train/val) 0.58818/0.21633. Took 0.09 sec\n",
            "Epoch 512, Loss(train/val) 0.59463/0.21386. Took 0.10 sec\n",
            "Epoch 513, Loss(train/val) 0.58287/0.21472. Took 0.10 sec\n",
            "Epoch 514, Loss(train/val) 0.59361/0.21405. Took 0.09 sec\n",
            "Epoch 515, Loss(train/val) 0.58721/0.21350. Took 0.10 sec\n",
            "Epoch 516, Loss(train/val) 0.58042/0.21212. Took 0.10 sec\n",
            "Epoch 517, Loss(train/val) 0.58001/0.21098. Took 0.11 sec\n",
            "Epoch 518, Loss(train/val) 0.58626/0.21094. Took 0.09 sec\n",
            "Epoch 519, Loss(train/val) 0.58137/0.21153. Took 0.10 sec\n",
            "Epoch 520, Loss(train/val) 0.57529/0.21149. Took 0.10 sec\n",
            "Epoch 521, Loss(train/val) 0.59374/0.21294. Took 0.09 sec\n",
            "Epoch 522, Loss(train/val) 0.58143/0.21266. Took 0.09 sec\n",
            "Epoch 523, Loss(train/val) 0.58746/0.21409. Took 0.11 sec\n",
            "Epoch 524, Loss(train/val) 0.58713/0.21416. Took 0.09 sec\n",
            "Epoch 525, Loss(train/val) 0.58005/0.21363. Took 0.09 sec\n",
            "Epoch 526, Loss(train/val) 0.58602/0.21684. Took 0.10 sec\n",
            "Epoch 527, Loss(train/val) 0.58893/0.21965. Took 0.11 sec\n",
            "Epoch 528, Loss(train/val) 0.58440/0.22265. Took 0.09 sec\n",
            "Epoch 529, Loss(train/val) 0.58386/0.22727. Took 0.10 sec\n",
            "Epoch 530, Loss(train/val) 0.59035/0.22978. Took 0.09 sec\n",
            "Epoch 531, Loss(train/val) 0.58482/0.22805. Took 0.10 sec\n",
            "Epoch 532, Loss(train/val) 0.57817/0.22979. Took 0.10 sec\n",
            "Epoch 533, Loss(train/val) 0.58037/0.23392. Took 0.10 sec\n",
            "Epoch 534, Loss(train/val) 0.58245/0.23433. Took 0.09 sec\n",
            "Epoch 535, Loss(train/val) 0.59186/0.23500. Took 0.10 sec\n",
            "Epoch 536, Loss(train/val) 0.58580/0.22773. Took 0.10 sec\n",
            "Epoch 537, Loss(train/val) 0.58086/0.22076. Took 0.10 sec\n",
            "Epoch 538, Loss(train/val) 0.58288/0.21731. Took 0.11 sec\n",
            "Epoch 539, Loss(train/val) 0.57836/0.21413. Took 0.10 sec\n",
            "Epoch 540, Loss(train/val) 0.57487/0.21212. Took 0.09 sec\n",
            "Epoch 541, Loss(train/val) 0.57201/0.21311. Took 0.10 sec\n",
            "Epoch 542, Loss(train/val) 0.57463/0.21178. Took 0.10 sec\n",
            "Epoch 543, Loss(train/val) 0.56276/0.21127. Took 0.09 sec\n",
            "Epoch 544, Loss(train/val) 0.57536/0.21142. Took 0.11 sec\n",
            "Epoch 545, Loss(train/val) 0.57481/0.21068. Took 0.09 sec\n",
            "Epoch 546, Loss(train/val) 0.57045/0.21120. Took 0.10 sec\n",
            "Epoch 547, Loss(train/val) 0.57530/0.21304. Took 0.10 sec\n",
            "Epoch 548, Loss(train/val) 0.57695/0.21336. Took 0.11 sec\n",
            "Epoch 549, Loss(train/val) 0.57807/0.21272. Took 0.10 sec\n",
            "Epoch 550, Loss(train/val) 0.57805/0.21017. Took 0.10 sec\n",
            "Epoch 551, Loss(train/val) 0.57895/0.21049. Took 0.09 sec\n",
            "Epoch 552, Loss(train/val) 0.57149/0.20896. Took 0.10 sec\n",
            "Epoch 553, Loss(train/val) 0.55854/0.20872. Took 0.10 sec\n",
            "Epoch 554, Loss(train/val) 0.56652/0.20915. Took 0.09 sec\n",
            "Epoch 555, Loss(train/val) 0.56654/0.21041. Took 0.10 sec\n",
            "Epoch 556, Loss(train/val) 0.57629/0.21307. Took 0.10 sec\n",
            "Epoch 557, Loss(train/val) 0.57435/0.21281. Took 0.09 sec\n",
            "Epoch 558, Loss(train/val) 0.57054/0.21630. Took 0.11 sec\n",
            "Epoch 559, Loss(train/val) 0.57795/0.21564. Took 0.10 sec\n",
            "Epoch 560, Loss(train/val) 0.57595/0.21984. Took 0.09 sec\n",
            "Epoch 561, Loss(train/val) 0.57560/0.22767. Took 0.10 sec\n",
            "Epoch 562, Loss(train/val) 0.56209/0.22693. Took 0.09 sec\n",
            "Epoch 563, Loss(train/val) 0.57243/0.22437. Took 0.10 sec\n",
            "Epoch 564, Loss(train/val) 0.56404/0.22263. Took 0.10 sec\n",
            "Epoch 565, Loss(train/val) 0.56406/0.22441. Took 0.10 sec\n",
            "Epoch 566, Loss(train/val) 0.57640/0.22470. Took 0.09 sec\n",
            "Epoch 567, Loss(train/val) 0.56982/0.23028. Took 0.10 sec\n",
            "Epoch 568, Loss(train/val) 0.56717/0.23059. Took 0.09 sec\n",
            "Epoch 569, Loss(train/val) 0.56613/0.23093. Took 0.11 sec\n",
            "Epoch 570, Loss(train/val) 0.57244/0.22512. Took 0.10 sec\n",
            "Epoch 571, Loss(train/val) 0.57517/0.22411. Took 0.10 sec\n",
            "Epoch 572, Loss(train/val) 0.57084/0.21833. Took 0.12 sec\n",
            "Epoch 573, Loss(train/val) 0.56816/0.21577. Took 0.10 sec\n",
            "Epoch 574, Loss(train/val) 0.56708/0.21104. Took 0.09 sec\n",
            "Epoch 575, Loss(train/val) 0.56228/0.20921. Took 0.10 sec\n",
            "Epoch 576, Loss(train/val) 0.56921/0.20897. Took 0.09 sec\n",
            "Epoch 577, Loss(train/val) 0.55626/0.20999. Took 0.10 sec\n",
            "Epoch 578, Loss(train/val) 0.56431/0.21058. Took 0.10 sec\n",
            "Epoch 579, Loss(train/val) 0.56151/0.20887. Took 0.11 sec\n",
            "Epoch 580, Loss(train/val) 0.56024/0.20834. Took 0.09 sec\n",
            "Epoch 581, Loss(train/val) 0.56051/0.20797. Took 0.10 sec\n",
            "Epoch 582, Loss(train/val) 0.56585/0.20823. Took 0.09 sec\n",
            "Epoch 583, Loss(train/val) 0.56193/0.20910. Took 0.10 sec\n",
            "Epoch 584, Loss(train/val) 0.55794/0.20907. Took 0.10 sec\n",
            "Epoch 585, Loss(train/val) 0.55687/0.20926. Took 0.10 sec\n",
            "Epoch 586, Loss(train/val) 0.55505/0.20840. Took 0.09 sec\n",
            "Epoch 587, Loss(train/val) 0.55579/0.20847. Took 0.10 sec\n",
            "Epoch 588, Loss(train/val) 0.56341/0.20794. Took 0.09 sec\n",
            "Epoch 589, Loss(train/val) 0.55758/0.20959. Took 0.10 sec\n",
            "Epoch 590, Loss(train/val) 0.56371/0.20934. Took 0.11 sec\n",
            "Epoch 591, Loss(train/val) 0.56616/0.20935. Took 0.09 sec\n",
            "Epoch 592, Loss(train/val) 0.56619/0.20925. Took 0.09 sec\n",
            "Epoch 593, Loss(train/val) 0.54880/0.20675. Took 0.11 sec\n",
            "Epoch 594, Loss(train/val) 0.54801/0.20722. Took 0.09 sec\n",
            "Epoch 595, Loss(train/val) 0.56233/0.20724. Took 0.09 sec\n",
            "Epoch 596, Loss(train/val) 0.57124/0.20723. Took 0.10 sec\n",
            "Epoch 597, Loss(train/val) 0.56169/0.20860. Took 0.10 sec\n",
            "Epoch 598, Loss(train/val) 0.55644/0.21095. Took 0.10 sec\n",
            "Epoch 599, Loss(train/val) 0.55166/0.21367. Took 0.11 sec\n",
            "Epoch 600, Loss(train/val) 0.56666/0.22165. Took 0.10 sec\n",
            "Epoch 601, Loss(train/val) 0.55869/0.22374. Took 0.10 sec\n",
            "Epoch 602, Loss(train/val) 0.56322/0.21815. Took 0.10 sec\n",
            "Epoch 603, Loss(train/val) 0.54910/0.21642. Took 0.10 sec\n",
            "Epoch 604, Loss(train/val) 0.55404/0.21880. Took 0.09 sec\n",
            "Epoch 605, Loss(train/val) 0.55799/0.21713. Took 0.10 sec\n",
            "Epoch 606, Loss(train/val) 0.54934/0.21648. Took 0.09 sec\n",
            "Epoch 607, Loss(train/val) 0.54823/0.21728. Took 0.10 sec\n",
            "Epoch 608, Loss(train/val) 0.55974/0.21866. Took 0.10 sec\n",
            "Epoch 609, Loss(train/val) 0.55190/0.21697. Took 0.09 sec\n",
            "Epoch 610, Loss(train/val) 0.55997/0.21818. Took 0.10 sec\n",
            "Epoch 611, Loss(train/val) 0.55076/0.21504. Took 0.10 sec\n",
            "Epoch 612, Loss(train/val) 0.55117/0.21453. Took 0.09 sec\n",
            "Epoch 613, Loss(train/val) 0.53604/0.21820. Took 0.09 sec\n",
            "Epoch 614, Loss(train/val) 0.54722/0.22121. Took 0.10 sec\n",
            "Epoch 615, Loss(train/val) 0.54226/0.22633. Took 0.10 sec\n",
            "Epoch 616, Loss(train/val) 0.55332/0.22535. Took 0.09 sec\n",
            "Epoch 617, Loss(train/val) 0.55204/0.22209. Took 0.10 sec\n",
            "Epoch 618, Loss(train/val) 0.55606/0.22089. Took 0.10 sec\n",
            "Epoch 619, Loss(train/val) 0.54759/0.22187. Took 0.09 sec\n",
            "Epoch 620, Loss(train/val) 0.54672/0.22201. Took 0.10 sec\n",
            "Epoch 621, Loss(train/val) 0.55160/0.22004. Took 0.11 sec\n",
            "Epoch 622, Loss(train/val) 0.53873/0.21578. Took 0.09 sec\n",
            "Epoch 623, Loss(train/val) 0.54872/0.21740. Took 0.10 sec\n",
            "Epoch 624, Loss(train/val) 0.53971/0.22230. Took 0.10 sec\n",
            "Epoch 625, Loss(train/val) 0.55375/0.22118. Took 0.09 sec\n",
            "Epoch 626, Loss(train/val) 0.55065/0.21940. Took 0.10 sec\n",
            "Epoch 627, Loss(train/val) 0.54296/0.22026. Took 0.10 sec\n",
            "Epoch 628, Loss(train/val) 0.55441/0.21877. Took 0.09 sec\n",
            "Epoch 629, Loss(train/val) 0.55683/0.21352. Took 0.10 sec\n",
            "Epoch 630, Loss(train/val) 0.53974/0.21127. Took 0.10 sec\n",
            "Epoch 631, Loss(train/val) 0.54079/0.21091. Took 0.10 sec\n",
            "Epoch 632, Loss(train/val) 0.55090/0.21012. Took 0.10 sec\n",
            "Epoch 633, Loss(train/val) 0.54537/0.20933. Took 0.10 sec\n",
            "Epoch 634, Loss(train/val) 0.54672/0.20920. Took 0.11 sec\n",
            "Epoch 635, Loss(train/val) 0.54062/0.20836. Took 0.09 sec\n",
            "Epoch 636, Loss(train/val) 0.54272/0.20782. Took 0.09 sec\n",
            "Epoch 637, Loss(train/val) 0.53877/0.20733. Took 0.10 sec\n",
            "Epoch 638, Loss(train/val) 0.55025/0.20804. Took 0.10 sec\n",
            "Epoch 639, Loss(train/val) 0.53649/0.20822. Took 0.10 sec\n",
            "Epoch 640, Loss(train/val) 0.54683/0.20789. Took 0.10 sec\n",
            "Epoch 641, Loss(train/val) 0.54504/0.21056. Took 0.11 sec\n",
            "Epoch 642, Loss(train/val) 0.54434/0.20954. Took 0.09 sec\n",
            "Epoch 643, Loss(train/val) 0.53660/0.20739. Took 0.10 sec\n",
            "Epoch 644, Loss(train/val) 0.53233/0.20660. Took 0.09 sec\n",
            "Epoch 645, Loss(train/val) 0.54140/0.20676. Took 0.10 sec\n",
            "Epoch 646, Loss(train/val) 0.53603/0.20690. Took 0.10 sec\n",
            "Epoch 647, Loss(train/val) 0.54042/0.21051. Took 0.09 sec\n",
            "Epoch 648, Loss(train/val) 0.53627/0.21291. Took 0.09 sec\n",
            "Epoch 649, Loss(train/val) 0.53044/0.21170. Took 0.10 sec\n",
            "Epoch 650, Loss(train/val) 0.53092/0.21332. Took 0.09 sec\n",
            "Epoch 651, Loss(train/val) 0.53669/0.21365. Took 0.09 sec\n",
            "Epoch 652, Loss(train/val) 0.53851/0.21365. Took 0.11 sec\n",
            "Epoch 653, Loss(train/val) 0.53632/0.21015. Took 0.10 sec\n",
            "Epoch 654, Loss(train/val) 0.53922/0.21123. Took 0.10 sec\n",
            "Epoch 655, Loss(train/val) 0.53605/0.21331. Took 0.10 sec\n",
            "Epoch 656, Loss(train/val) 0.55477/0.21493. Took 0.09 sec\n",
            "Epoch 657, Loss(train/val) 0.53479/0.21892. Took 0.10 sec\n",
            "Epoch 658, Loss(train/val) 0.54374/0.21838. Took 0.10 sec\n",
            "Epoch 659, Loss(train/val) 0.54393/0.21327. Took 0.10 sec\n",
            "Epoch 660, Loss(train/val) 0.54360/0.21031. Took 0.09 sec\n",
            "Epoch 661, Loss(train/val) 0.53719/0.20888. Took 0.10 sec\n",
            "Epoch 662, Loss(train/val) 0.52993/0.20799. Took 0.10 sec\n",
            "Epoch 663, Loss(train/val) 0.54192/0.20784. Took 0.10 sec\n",
            "Epoch 664, Loss(train/val) 0.53304/0.20594. Took 0.11 sec\n",
            "Epoch 665, Loss(train/val) 0.53517/0.20536. Took 0.10 sec\n",
            "Epoch 666, Loss(train/val) 0.53443/0.20480. Took 0.09 sec\n",
            "Epoch 667, Loss(train/val) 0.54243/0.20491. Took 0.09 sec\n",
            "Epoch 668, Loss(train/val) 0.53397/0.20534. Took 0.10 sec\n",
            "Epoch 669, Loss(train/val) 0.53173/0.20556. Took 0.10 sec\n",
            "Epoch 670, Loss(train/val) 0.54334/0.20549. Took 0.10 sec\n",
            "Epoch 671, Loss(train/val) 0.52892/0.20666. Took 0.10 sec\n",
            "Epoch 672, Loss(train/val) 0.53254/0.21159. Took 0.10 sec\n",
            "Epoch 673, Loss(train/val) 0.53734/0.21480. Took 0.10 sec\n",
            "Epoch 674, Loss(train/val) 0.52581/0.21941. Took 0.10 sec\n",
            "Epoch 675, Loss(train/val) 0.53069/0.21637. Took 0.09 sec\n",
            "Epoch 676, Loss(train/val) 0.51829/0.21687. Took 0.11 sec\n",
            "Epoch 677, Loss(train/val) 0.52260/0.21810. Took 0.10 sec\n",
            "Epoch 678, Loss(train/val) 0.52604/0.21918. Took 0.10 sec\n",
            "Epoch 679, Loss(train/val) 0.51803/0.21894. Took 0.10 sec\n",
            "Epoch 680, Loss(train/val) 0.51811/0.21583. Took 0.10 sec\n",
            "Epoch 681, Loss(train/val) 0.52561/0.21357. Took 0.09 sec\n",
            "Epoch 682, Loss(train/val) 0.52860/0.21485. Took 0.10 sec\n",
            "Epoch 683, Loss(train/val) 0.52362/0.21397. Took 0.10 sec\n",
            "Epoch 684, Loss(train/val) 0.51940/0.21212. Took 0.10 sec\n",
            "Epoch 685, Loss(train/val) 0.52596/0.21076. Took 0.10 sec\n",
            "Epoch 686, Loss(train/val) 0.52372/0.21330. Took 0.09 sec\n",
            "Epoch 687, Loss(train/val) 0.51965/0.21480. Took 0.10 sec\n",
            "Epoch 688, Loss(train/val) 0.52850/0.22116. Took 0.10 sec\n",
            "Epoch 689, Loss(train/val) 0.52423/0.22340. Took 0.10 sec\n",
            "Epoch 690, Loss(train/val) 0.52949/0.22030. Took 0.09 sec\n",
            "Epoch 691, Loss(train/val) 0.52013/0.21974. Took 0.10 sec\n",
            "Epoch 692, Loss(train/val) 0.52722/0.22083. Took 0.09 sec\n",
            "Epoch 693, Loss(train/val) 0.52862/0.22037. Took 0.10 sec\n",
            "Epoch 694, Loss(train/val) 0.51728/0.21421. Took 0.10 sec\n",
            "Epoch 695, Loss(train/val) 0.52140/0.20975. Took 0.10 sec\n",
            "Epoch 696, Loss(train/val) 0.51988/0.20708. Took 0.10 sec\n",
            "Epoch 697, Loss(train/val) 0.51706/0.20527. Took 0.10 sec\n",
            "Epoch 698, Loss(train/val) 0.52476/0.20530. Took 0.10 sec\n",
            "Epoch 699, Loss(train/val) 0.52199/0.20514. Took 0.09 sec\n",
            "Epoch 700, Loss(train/val) 0.51780/0.20478. Took 0.10 sec\n",
            "Epoch 701, Loss(train/val) 0.52427/0.20483. Took 0.09 sec\n",
            "Epoch 702, Loss(train/val) 0.52009/0.20473. Took 0.10 sec\n",
            "Epoch 703, Loss(train/val) 0.52367/0.20491. Took 0.11 sec\n",
            "Epoch 704, Loss(train/val) 0.51668/0.20458. Took 0.10 sec\n",
            "Epoch 705, Loss(train/val) 0.51548/0.20528. Took 0.10 sec\n",
            "Epoch 706, Loss(train/val) 0.52263/0.20668. Took 0.11 sec\n",
            "Epoch 707, Loss(train/val) 0.52051/0.20591. Took 0.09 sec\n",
            "Epoch 708, Loss(train/val) 0.52168/0.20485. Took 0.10 sec\n",
            "Epoch 709, Loss(train/val) 0.51017/0.20452. Took 0.10 sec\n",
            "Epoch 710, Loss(train/val) 0.50606/0.20464. Took 0.09 sec\n",
            "Epoch 711, Loss(train/val) 0.51517/0.20476. Took 0.09 sec\n",
            "Epoch 712, Loss(train/val) 0.51171/0.20467. Took 0.10 sec\n",
            "Epoch 713, Loss(train/val) 0.51937/0.20609. Took 0.09 sec\n",
            "Epoch 714, Loss(train/val) 0.50365/0.20701. Took 0.10 sec\n",
            "Epoch 715, Loss(train/val) 0.52089/0.20445. Took 0.11 sec\n",
            "Epoch 716, Loss(train/val) 0.51449/0.20484. Took 0.09 sec\n",
            "Epoch 717, Loss(train/val) 0.50862/0.20575. Took 0.10 sec\n",
            "Epoch 718, Loss(train/val) 0.51507/0.20607. Took 0.10 sec\n",
            "Epoch 719, Loss(train/val) 0.51242/0.20943. Took 0.09 sec\n",
            "Epoch 720, Loss(train/val) 0.50849/0.20977. Took 0.10 sec\n",
            "Epoch 721, Loss(train/val) 0.51074/0.21014. Took 0.10 sec\n",
            "Epoch 722, Loss(train/val) 0.50543/0.21027. Took 0.10 sec\n",
            "Epoch 723, Loss(train/val) 0.51964/0.20775. Took 0.09 sec\n",
            "Epoch 724, Loss(train/val) 0.51889/0.20911. Took 0.11 sec\n",
            "Epoch 725, Loss(train/val) 0.52087/0.20541. Took 0.10 sec\n",
            "Epoch 726, Loss(train/val) 0.50363/0.20680. Took 0.09 sec\n",
            "Epoch 727, Loss(train/val) 0.51306/0.20931. Took 0.10 sec\n",
            "Epoch 728, Loss(train/val) 0.50558/0.20959. Took 0.10 sec\n",
            "Epoch 729, Loss(train/val) 0.51158/0.20726. Took 0.10 sec\n",
            "Epoch 730, Loss(train/val) 0.50841/0.20536. Took 0.10 sec\n",
            "Epoch 731, Loss(train/val) 0.50828/0.20787. Took 0.10 sec\n",
            "Epoch 732, Loss(train/val) 0.50726/0.21390. Took 0.10 sec\n",
            "Epoch 733, Loss(train/val) 0.51087/0.21241. Took 0.10 sec\n",
            "Epoch 734, Loss(train/val) 0.50531/0.20805. Took 0.10 sec\n",
            "Epoch 735, Loss(train/val) 0.51061/0.20391. Took 0.10 sec\n",
            "Epoch 736, Loss(train/val) 0.50135/0.20404. Took 0.10 sec\n",
            "Epoch 737, Loss(train/val) 0.50941/0.20547. Took 0.09 sec\n",
            "Epoch 738, Loss(train/val) 0.51316/0.20693. Took 0.10 sec\n",
            "Epoch 739, Loss(train/val) 0.50847/0.20456. Took 0.09 sec\n",
            "Epoch 740, Loss(train/val) 0.49479/0.20378. Took 0.10 sec\n",
            "Epoch 741, Loss(train/val) 0.50710/0.20431. Took 0.10 sec\n",
            "Epoch 742, Loss(train/val) 0.50549/0.20502. Took 0.09 sec\n",
            "Epoch 743, Loss(train/val) 0.50460/0.20505. Took 0.09 sec\n",
            "Epoch 744, Loss(train/val) 0.49745/0.20538. Took 0.10 sec\n",
            "Epoch 745, Loss(train/val) 0.49899/0.20368. Took 0.11 sec\n",
            "Epoch 746, Loss(train/val) 0.50678/0.20401. Took 0.11 sec\n",
            "Epoch 747, Loss(train/val) 0.51866/0.20380. Took 0.09 sec\n",
            "Epoch 748, Loss(train/val) 0.50977/0.20294. Took 0.09 sec\n",
            "Epoch 749, Loss(train/val) 0.51033/0.20316. Took 0.10 sec\n",
            "Epoch 750, Loss(train/val) 0.50836/0.20290. Took 0.10 sec\n",
            "Epoch 751, Loss(train/val) 0.50817/0.20315. Took 0.10 sec\n",
            "Epoch 752, Loss(train/val) 0.51301/0.20362. Took 0.10 sec\n",
            "Epoch 753, Loss(train/val) 0.50610/0.20543. Took 0.09 sec\n",
            "Epoch 754, Loss(train/val) 0.50639/0.20871. Took 0.09 sec\n",
            "Epoch 755, Loss(train/val) 0.50693/0.21134. Took 0.11 sec\n",
            "Epoch 756, Loss(train/val) 0.50544/0.21487. Took 0.10 sec\n",
            "Epoch 757, Loss(train/val) 0.49895/0.21179. Took 0.09 sec\n",
            "Epoch 758, Loss(train/val) 0.49666/0.20705. Took 0.10 sec\n",
            "Epoch 759, Loss(train/val) 0.50398/0.20510. Took 0.10 sec\n",
            "Epoch 760, Loss(train/val) 0.50140/0.20573. Took 0.10 sec\n",
            "Epoch 761, Loss(train/val) 0.49465/0.20594. Took 0.10 sec\n",
            "Epoch 762, Loss(train/val) 0.49227/0.20477. Took 0.09 sec\n",
            "Epoch 763, Loss(train/val) 0.49489/0.20471. Took 0.09 sec\n",
            "Epoch 764, Loss(train/val) 0.49210/0.20605. Took 0.10 sec\n",
            "Epoch 765, Loss(train/val) 0.49666/0.20698. Took 0.10 sec\n",
            "Epoch 766, Loss(train/val) 0.50343/0.20677. Took 0.10 sec\n",
            "Epoch 767, Loss(train/val) 0.49556/0.20660. Took 0.10 sec\n",
            "Epoch 768, Loss(train/val) 0.49264/0.20740. Took 0.09 sec\n",
            "Epoch 769, Loss(train/val) 0.49207/0.20993. Took 0.10 sec\n",
            "Epoch 770, Loss(train/val) 0.49444/0.20876. Took 0.10 sec\n",
            "Epoch 771, Loss(train/val) 0.49679/0.20946. Took 0.09 sec\n",
            "Epoch 772, Loss(train/val) 0.49640/0.21214. Took 0.10 sec\n",
            "Epoch 773, Loss(train/val) 0.50567/0.20868. Took 0.09 sec\n",
            "Epoch 774, Loss(train/val) 0.48100/0.20782. Took 0.10 sec\n",
            "Epoch 775, Loss(train/val) 0.48979/0.20323. Took 0.10 sec\n",
            "Epoch 776, Loss(train/val) 0.49462/0.20244. Took 0.11 sec\n",
            "Epoch 777, Loss(train/val) 0.49547/0.20173. Took 0.10 sec\n",
            "Epoch 778, Loss(train/val) 0.48301/0.20143. Took 0.09 sec\n",
            "Epoch 779, Loss(train/val) 0.48157/0.20111. Took 0.10 sec\n",
            "Epoch 780, Loss(train/val) 0.48524/0.20112. Took 0.10 sec\n",
            "Epoch 781, Loss(train/val) 0.50314/0.20116. Took 0.10 sec\n",
            "Epoch 782, Loss(train/val) 0.49533/0.20142. Took 0.11 sec\n",
            "Epoch 783, Loss(train/val) 0.49743/0.20152. Took 0.11 sec\n",
            "Epoch 784, Loss(train/val) 0.48022/0.20137. Took 0.10 sec\n",
            "Epoch 785, Loss(train/val) 0.49353/0.20312. Took 0.10 sec\n",
            "Epoch 786, Loss(train/val) 0.49224/0.20600. Took 0.11 sec\n",
            "Epoch 787, Loss(train/val) 0.49336/0.20894. Took 0.09 sec\n",
            "Epoch 788, Loss(train/val) 0.49513/0.21346. Took 0.09 sec\n",
            "Epoch 789, Loss(train/val) 0.50495/0.21060. Took 0.10 sec\n",
            "Epoch 790, Loss(train/val) 0.48777/0.21076. Took 0.10 sec\n",
            "Epoch 791, Loss(train/val) 0.49106/0.21151. Took 0.09 sec\n",
            "Epoch 792, Loss(train/val) 0.49177/0.21320. Took 0.10 sec\n",
            "Epoch 793, Loss(train/val) 0.49332/0.21145. Took 0.09 sec\n",
            "Epoch 794, Loss(train/val) 0.49309/0.20855. Took 0.09 sec\n",
            "Epoch 795, Loss(train/val) 0.50197/0.20821. Took 0.10 sec\n",
            "Epoch 796, Loss(train/val) 0.49399/0.20556. Took 0.12 sec\n",
            "Epoch 797, Loss(train/val) 0.48657/0.20394. Took 0.10 sec\n",
            "Epoch 798, Loss(train/val) 0.48612/0.20519. Took 0.10 sec\n",
            "Epoch 799, Loss(train/val) 0.48058/0.20672. Took 0.10 sec\n",
            "Epoch 800, Loss(train/val) 0.48676/0.20662. Took 0.10 sec\n",
            "Epoch 801, Loss(train/val) 0.48962/0.20672. Took 0.10 sec\n",
            "Epoch 802, Loss(train/val) 0.48240/0.21001. Took 0.09 sec\n",
            "Epoch 803, Loss(train/val) 0.48276/0.21286. Took 0.11 sec\n",
            "Epoch 804, Loss(train/val) 0.49314/0.21522. Took 0.09 sec\n",
            "Epoch 805, Loss(train/val) 0.48170/0.21496. Took 0.09 sec\n",
            "Epoch 806, Loss(train/val) 0.49282/0.21606. Took 0.10 sec\n",
            "Epoch 807, Loss(train/val) 0.48599/0.21398. Took 0.11 sec\n",
            "Epoch 808, Loss(train/val) 0.47798/0.20905. Took 0.10 sec\n",
            "Epoch 809, Loss(train/val) 0.47503/0.20949. Took 0.10 sec\n",
            "Epoch 810, Loss(train/val) 0.48204/0.20534. Took 0.10 sec\n",
            "Epoch 811, Loss(train/val) 0.48515/0.20317. Took 0.10 sec\n",
            "Epoch 812, Loss(train/val) 0.48385/0.20289. Took 0.09 sec\n",
            "Epoch 813, Loss(train/val) 0.47729/0.20324. Took 0.09 sec\n",
            "Epoch 814, Loss(train/val) 0.48323/0.20417. Took 0.10 sec\n",
            "Epoch 815, Loss(train/val) 0.47574/0.20431. Took 0.09 sec\n",
            "Epoch 816, Loss(train/val) 0.48662/0.20558. Took 0.10 sec\n",
            "Epoch 817, Loss(train/val) 0.48419/0.20584. Took 0.11 sec\n",
            "Epoch 818, Loss(train/val) 0.48040/0.20515. Took 0.09 sec\n",
            "Epoch 819, Loss(train/val) 0.47507/0.20647. Took 0.09 sec\n",
            "Epoch 820, Loss(train/val) 0.48282/0.20909. Took 0.10 sec\n",
            "Epoch 821, Loss(train/val) 0.48194/0.20972. Took 0.10 sec\n",
            "Epoch 822, Loss(train/val) 0.48296/0.20935. Took 0.09 sec\n",
            "Epoch 823, Loss(train/val) 0.47410/0.20588. Took 0.10 sec\n",
            "Epoch 824, Loss(train/val) 0.47797/0.20341. Took 0.10 sec\n",
            "Epoch 825, Loss(train/val) 0.49438/0.20254. Took 0.10 sec\n",
            "Epoch 826, Loss(train/val) 0.48843/0.20113. Took 0.10 sec\n",
            "Epoch 827, Loss(train/val) 0.47208/0.20117. Took 0.09 sec\n",
            "Epoch 828, Loss(train/val) 0.47383/0.20135. Took 0.11 sec\n",
            "Epoch 829, Loss(train/val) 0.48048/0.20213. Took 0.10 sec\n",
            "Epoch 830, Loss(train/val) 0.47459/0.20473. Took 0.09 sec\n",
            "Epoch 831, Loss(train/val) 0.48220/0.20614. Took 0.10 sec\n",
            "Epoch 832, Loss(train/val) 0.47746/0.20619. Took 0.10 sec\n",
            "Epoch 833, Loss(train/val) 0.48125/0.20224. Took 0.09 sec\n",
            "Epoch 834, Loss(train/val) 0.47008/0.20130. Took 0.10 sec\n",
            "Epoch 835, Loss(train/val) 0.47941/0.20070. Took 0.09 sec\n",
            "Epoch 836, Loss(train/val) 0.47380/0.19955. Took 0.09 sec\n",
            "Epoch 837, Loss(train/val) 0.47455/0.19897. Took 0.10 sec\n",
            "Epoch 838, Loss(train/val) 0.47743/0.19903. Took 0.10 sec\n",
            "Epoch 839, Loss(train/val) 0.46284/0.19934. Took 0.10 sec\n",
            "Epoch 840, Loss(train/val) 0.47498/0.19916. Took 0.11 sec\n",
            "Epoch 841, Loss(train/val) 0.46867/0.19965. Took 0.10 sec\n",
            "Epoch 842, Loss(train/val) 0.46885/0.20122. Took 0.09 sec\n",
            "Epoch 843, Loss(train/val) 0.46766/0.20177. Took 0.10 sec\n",
            "Epoch 844, Loss(train/val) 0.47125/0.20054. Took 0.10 sec\n",
            "Epoch 845, Loss(train/val) 0.47765/0.20235. Took 0.09 sec\n",
            "Epoch 846, Loss(train/val) 0.47577/0.20427. Took 0.10 sec\n",
            "Epoch 847, Loss(train/val) 0.47661/0.20591. Took 0.10 sec\n",
            "Epoch 848, Loss(train/val) 0.46166/0.20434. Took 0.09 sec\n",
            "Epoch 849, Loss(train/val) 0.47306/0.20287. Took 0.11 sec\n",
            "Epoch 850, Loss(train/val) 0.47778/0.20366. Took 0.10 sec\n",
            "Epoch 851, Loss(train/val) 0.47721/0.20505. Took 0.10 sec\n",
            "Epoch 852, Loss(train/val) 0.47137/0.20386. Took 0.10 sec\n",
            "Epoch 853, Loss(train/val) 0.46459/0.20575. Took 0.10 sec\n",
            "Epoch 854, Loss(train/val) 0.47902/0.20215. Took 0.09 sec\n",
            "Epoch 855, Loss(train/val) 0.46591/0.20068. Took 0.10 sec\n",
            "Epoch 856, Loss(train/val) 0.46819/0.19855. Took 0.10 sec\n",
            "Epoch 857, Loss(train/val) 0.47204/0.19822. Took 0.09 sec\n",
            "Epoch 858, Loss(train/val) 0.46100/0.20011. Took 0.10 sec\n",
            "Epoch 859, Loss(train/val) 0.46206/0.20198. Took 0.10 sec\n",
            "Epoch 860, Loss(train/val) 0.47279/0.20659. Took 0.11 sec\n",
            "Epoch 861, Loss(train/val) 0.45854/0.20385. Took 0.09 sec\n",
            "Epoch 862, Loss(train/val) 0.46136/0.20185. Took 0.09 sec\n",
            "Epoch 863, Loss(train/val) 0.46027/0.19837. Took 0.10 sec\n",
            "Epoch 864, Loss(train/val) 0.46076/0.19872. Took 0.09 sec\n",
            "Epoch 865, Loss(train/val) 0.45872/0.19958. Took 0.09 sec\n",
            "Epoch 866, Loss(train/val) 0.45699/0.20052. Took 0.10 sec\n",
            "Epoch 867, Loss(train/val) 0.46661/0.20187. Took 0.09 sec\n",
            "Epoch 868, Loss(train/val) 0.46746/0.20231. Took 0.10 sec\n",
            "Epoch 869, Loss(train/val) 0.45638/0.20701. Took 0.11 sec\n",
            "Epoch 870, Loss(train/val) 0.46257/0.21083. Took 0.10 sec\n",
            "Epoch 871, Loss(train/val) 0.45481/0.20789. Took 0.09 sec\n",
            "Epoch 872, Loss(train/val) 0.45805/0.20583. Took 0.10 sec\n",
            "Epoch 873, Loss(train/val) 0.45981/0.20261. Took 0.10 sec\n",
            "Epoch 874, Loss(train/val) 0.46545/0.20257. Took 0.09 sec\n",
            "Epoch 875, Loss(train/val) 0.46826/0.20301. Took 0.10 sec\n",
            "Epoch 876, Loss(train/val) 0.46141/0.20425. Took 0.10 sec\n",
            "Epoch 877, Loss(train/val) 0.45417/0.20464. Took 0.10 sec\n",
            "Epoch 878, Loss(train/val) 0.46598/0.20437. Took 0.10 sec\n",
            "Epoch 879, Loss(train/val) 0.44937/0.20242. Took 0.09 sec\n",
            "Epoch 880, Loss(train/val) 0.46153/0.20115. Took 0.10 sec\n",
            "Epoch 881, Loss(train/val) 0.45203/0.19912. Took 0.11 sec\n",
            "Epoch 882, Loss(train/val) 0.45510/0.19843. Took 0.09 sec\n",
            "Epoch 883, Loss(train/val) 0.44507/0.19907. Took 0.09 sec\n",
            "Epoch 884, Loss(train/val) 0.45953/0.20093. Took 0.10 sec\n",
            "Epoch 885, Loss(train/val) 0.46684/0.20061. Took 0.09 sec\n",
            "Epoch 886, Loss(train/val) 0.45556/0.19851. Took 0.11 sec\n",
            "Epoch 887, Loss(train/val) 0.45129/0.19864. Took 0.12 sec\n",
            "Epoch 888, Loss(train/val) 0.46306/0.19955. Took 0.09 sec\n",
            "Epoch 889, Loss(train/val) 0.45580/0.20292. Took 0.10 sec\n",
            "Epoch 890, Loss(train/val) 0.46176/0.21326. Took 0.11 sec\n",
            "Epoch 891, Loss(train/val) 0.45987/0.21837. Took 0.10 sec\n",
            "Epoch 892, Loss(train/val) 0.44282/0.22064. Took 0.09 sec\n",
            "Epoch 893, Loss(train/val) 0.45463/0.22237. Took 0.10 sec\n",
            "Epoch 894, Loss(train/val) 0.44455/0.21155. Took 0.09 sec\n",
            "Epoch 895, Loss(train/val) 0.46006/0.21024. Took 0.10 sec\n",
            "Epoch 896, Loss(train/val) 0.46417/0.20622. Took 0.10 sec\n",
            "Epoch 897, Loss(train/val) 0.45785/0.20683. Took 0.10 sec\n",
            "Epoch 898, Loss(train/val) 0.45418/0.20729. Took 0.09 sec\n",
            "Epoch 899, Loss(train/val) 0.44790/0.20915. Took 0.10 sec\n",
            "Epoch 900, Loss(train/val) 0.44205/0.20585. Took 0.11 sec\n",
            "Epoch 901, Loss(train/val) 0.47171/0.20138. Took 0.10 sec\n",
            "Epoch 902, Loss(train/val) 0.45560/0.20030. Took 0.10 sec\n",
            "Epoch 903, Loss(train/val) 0.45366/0.19957. Took 0.09 sec\n",
            "Epoch 904, Loss(train/val) 0.45242/0.19937. Took 0.10 sec\n",
            "Epoch 905, Loss(train/val) 0.45134/0.19960. Took 0.10 sec\n",
            "Epoch 906, Loss(train/val) 0.44151/0.19941. Took 0.10 sec\n",
            "Epoch 907, Loss(train/val) 0.45223/0.19856. Took 0.10 sec\n",
            "Epoch 908, Loss(train/val) 0.44507/0.19745. Took 0.10 sec\n",
            "Epoch 909, Loss(train/val) 0.45603/0.19663. Took 0.09 sec\n",
            "Epoch 910, Loss(train/val) 0.44448/0.19618. Took 0.11 sec\n",
            "Epoch 911, Loss(train/val) 0.45027/0.19838. Took 0.10 sec\n",
            "Epoch 912, Loss(train/val) 0.44683/0.20898. Took 0.10 sec\n",
            "Epoch 913, Loss(train/val) 0.43861/0.21197. Took 0.10 sec\n",
            "Epoch 914, Loss(train/val) 0.44638/0.20444. Took 0.10 sec\n",
            "Epoch 915, Loss(train/val) 0.43979/0.20499. Took 0.09 sec\n",
            "Epoch 916, Loss(train/val) 0.44471/0.20391. Took 0.10 sec\n",
            "Epoch 917, Loss(train/val) 0.44516/0.20094. Took 0.10 sec\n",
            "Epoch 918, Loss(train/val) 0.45182/0.20098. Took 0.10 sec\n",
            "Epoch 919, Loss(train/val) 0.45115/0.20331. Took 0.10 sec\n",
            "Epoch 920, Loss(train/val) 0.44053/0.20139. Took 0.10 sec\n",
            "Epoch 921, Loss(train/val) 0.44468/0.20046. Took 0.12 sec\n",
            "Epoch 922, Loss(train/val) 0.44857/0.20174. Took 0.09 sec\n",
            "Epoch 923, Loss(train/val) 0.45429/0.20272. Took 0.10 sec\n",
            "Epoch 924, Loss(train/val) 0.44635/0.20050. Took 0.10 sec\n",
            "Epoch 925, Loss(train/val) 0.44171/0.20054. Took 0.09 sec\n",
            "Epoch 926, Loss(train/val) 0.43973/0.20055. Took 0.10 sec\n",
            "Epoch 927, Loss(train/val) 0.44089/0.20104. Took 0.10 sec\n",
            "Epoch 928, Loss(train/val) 0.43755/0.19803. Took 0.09 sec\n",
            "Epoch 929, Loss(train/val) 0.43837/0.19714. Took 0.09 sec\n",
            "Epoch 930, Loss(train/val) 0.43648/0.19963. Took 0.10 sec\n",
            "Epoch 931, Loss(train/val) 0.44746/0.19941. Took 0.11 sec\n",
            "Epoch 932, Loss(train/val) 0.43960/0.19703. Took 0.10 sec\n",
            "Epoch 933, Loss(train/val) 0.43623/0.19601. Took 0.09 sec\n",
            "Epoch 934, Loss(train/val) 0.43695/0.19502. Took 0.10 sec\n",
            "Epoch 935, Loss(train/val) 0.44968/0.19545. Took 0.10 sec\n",
            "Epoch 936, Loss(train/val) 0.43398/0.19666. Took 0.09 sec\n",
            "Epoch 937, Loss(train/val) 0.43690/0.19743. Took 0.10 sec\n",
            "Epoch 938, Loss(train/val) 0.43527/0.19513. Took 0.10 sec\n",
            "Epoch 939, Loss(train/val) 0.43360/0.19458. Took 0.09 sec\n",
            "Epoch 940, Loss(train/val) 0.43995/0.19485. Took 0.09 sec\n",
            "Epoch 941, Loss(train/val) 0.44357/0.19406. Took 0.11 sec\n",
            "Epoch 942, Loss(train/val) 0.42923/0.19422. Took 0.10 sec\n",
            "Epoch 943, Loss(train/val) 0.44866/0.19461. Took 0.09 sec\n",
            "Epoch 944, Loss(train/val) 0.43024/0.19391. Took 0.10 sec\n",
            "Epoch 945, Loss(train/val) 0.42773/0.19459. Took 0.10 sec\n",
            "Epoch 946, Loss(train/val) 0.42993/0.19402. Took 0.09 sec\n",
            "Epoch 947, Loss(train/val) 0.43753/0.19409. Took 0.11 sec\n",
            "Epoch 948, Loss(train/val) 0.42191/0.19498. Took 0.10 sec\n",
            "Epoch 949, Loss(train/val) 0.44009/0.19782. Took 0.09 sec\n",
            "Epoch 950, Loss(train/val) 0.42804/0.19721. Took 0.10 sec\n",
            "Epoch 951, Loss(train/val) 0.43404/0.19573. Took 0.10 sec\n",
            "Epoch 952, Loss(train/val) 0.43247/0.19418. Took 0.10 sec\n",
            "Epoch 953, Loss(train/val) 0.42418/0.19398. Took 0.10 sec\n",
            "Epoch 954, Loss(train/val) 0.43629/0.19385. Took 0.09 sec\n",
            "Epoch 955, Loss(train/val) 0.44504/0.19427. Took 0.09 sec\n",
            "Epoch 956, Loss(train/val) 0.42424/0.19384. Took 0.10 sec\n",
            "Epoch 957, Loss(train/val) 0.43360/0.19360. Took 0.10 sec\n",
            "Epoch 958, Loss(train/val) 0.44479/0.19373. Took 0.09 sec\n",
            "Epoch 959, Loss(train/val) 0.42900/0.19409. Took 0.10 sec\n",
            "Epoch 960, Loss(train/val) 0.41773/0.19458. Took 0.10 sec\n",
            "Epoch 961, Loss(train/val) 0.42583/0.19547. Took 0.10 sec\n",
            "Epoch 962, Loss(train/val) 0.42777/0.19757. Took 0.11 sec\n",
            "Epoch 963, Loss(train/val) 0.42142/0.20142. Took 0.09 sec\n",
            "Epoch 964, Loss(train/val) 0.42581/0.20152. Took 0.10 sec\n",
            "Epoch 965, Loss(train/val) 0.42364/0.19945. Took 0.10 sec\n",
            "Epoch 966, Loss(train/val) 0.42793/0.19850. Took 0.10 sec\n",
            "Epoch 967, Loss(train/val) 0.42202/0.19735. Took 0.09 sec\n",
            "Epoch 968, Loss(train/val) 0.41780/0.19644. Took 0.10 sec\n",
            "Epoch 969, Loss(train/val) 0.43074/0.19995. Took 0.10 sec\n",
            "Epoch 970, Loss(train/val) 0.41107/0.19854. Took 0.09 sec\n",
            "Epoch 971, Loss(train/val) 0.42754/0.19584. Took 0.10 sec\n",
            "Epoch 972, Loss(train/val) 0.42181/0.19690. Took 0.10 sec\n",
            "Epoch 973, Loss(train/val) 0.43441/0.19555. Took 0.11 sec\n",
            "Epoch 974, Loss(train/val) 0.42366/0.19435. Took 0.09 sec\n",
            "Epoch 975, Loss(train/val) 0.42564/0.19419. Took 0.10 sec\n",
            "Epoch 976, Loss(train/val) 0.41918/0.19442. Took 0.10 sec\n",
            "Epoch 977, Loss(train/val) 0.41898/0.19375. Took 0.09 sec\n",
            "Epoch 978, Loss(train/val) 0.42557/0.19354. Took 0.10 sec\n",
            "Epoch 979, Loss(train/val) 0.42157/0.19404. Took 0.10 sec\n",
            "Epoch 980, Loss(train/val) 0.43392/0.19486. Took 0.09 sec\n",
            "Epoch 981, Loss(train/val) 0.42387/0.19580. Took 0.09 sec\n",
            "Epoch 982, Loss(train/val) 0.41744/0.20071. Took 0.10 sec\n",
            "Epoch 983, Loss(train/val) 0.42782/0.20555. Took 0.10 sec\n",
            "Epoch 984, Loss(train/val) 0.43014/0.20608. Took 0.09 sec\n",
            "Epoch 985, Loss(train/val) 0.41081/0.20434. Took 0.10 sec\n",
            "Epoch 986, Loss(train/val) 0.42289/0.19805. Took 0.10 sec\n",
            "Epoch 987, Loss(train/val) 0.42482/0.19702. Took 0.09 sec\n",
            "Epoch 988, Loss(train/val) 0.41785/0.19702. Took 0.10 sec\n",
            "Epoch 989, Loss(train/val) 0.42675/0.20084. Took 0.10 sec\n",
            "Epoch 990, Loss(train/val) 0.41445/0.20274. Took 0.10 sec\n",
            "Epoch 991, Loss(train/val) 0.41869/0.20584. Took 0.10 sec\n",
            "Epoch 992, Loss(train/val) 0.42591/0.20549. Took 0.11 sec\n",
            "Epoch 993, Loss(train/val) 0.41621/0.20597. Took 0.11 sec\n",
            "Epoch 994, Loss(train/val) 0.41263/0.20584. Took 0.10 sec\n",
            "Epoch 995, Loss(train/val) 0.40930/0.19830. Took 0.09 sec\n",
            "Epoch 996, Loss(train/val) 0.42156/0.19558. Took 0.10 sec\n",
            "Epoch 997, Loss(train/val) 0.42118/0.19425. Took 0.09 sec\n",
            "Epoch 998, Loss(train/val) 0.41269/0.19070. Took 0.10 sec\n",
            "Epoch 999, Loss(train/val) 0.40939/0.19055. Took 0.10 sec\n",
            "Epoch 1000, Loss(train/val) 0.41145/0.19338. Took 0.09 sec\n",
            "Epoch 1001, Loss(train/val) 0.41810/0.19398. Took 0.09 sec\n",
            "Epoch 1002, Loss(train/val) 0.42286/0.19245. Took 0.10 sec\n",
            "Epoch 1003, Loss(train/val) 0.41622/0.19056. Took 0.09 sec\n",
            "Epoch 1004, Loss(train/val) 0.42748/0.18924. Took 0.11 sec\n",
            "Epoch 1005, Loss(train/val) 0.41199/0.18959. Took 0.09 sec\n",
            "Epoch 1006, Loss(train/val) 0.41643/0.19186. Took 0.09 sec\n",
            "Epoch 1007, Loss(train/val) 0.41409/0.19462. Took 0.10 sec\n",
            "Epoch 1008, Loss(train/val) 0.41271/0.20345. Took 0.10 sec\n",
            "Epoch 1009, Loss(train/val) 0.41907/0.20970. Took 0.10 sec\n",
            "Epoch 1010, Loss(train/val) 0.40057/0.21320. Took 0.10 sec\n",
            "Epoch 1011, Loss(train/val) 0.41635/0.20557. Took 0.09 sec\n",
            "Epoch 1012, Loss(train/val) 0.40744/0.20245. Took 0.10 sec\n",
            "Epoch 1013, Loss(train/val) 0.40302/0.20085. Took 0.10 sec\n",
            "Epoch 1014, Loss(train/val) 0.40723/0.19573. Took 0.11 sec\n",
            "Epoch 1015, Loss(train/val) 0.40475/0.19685. Took 0.10 sec\n",
            "Epoch 1016, Loss(train/val) 0.41384/0.19681. Took 0.10 sec\n",
            "Epoch 1017, Loss(train/val) 0.40528/0.19454. Took 0.09 sec\n",
            "Epoch 1018, Loss(train/val) 0.41167/0.19520. Took 0.10 sec\n",
            "Epoch 1019, Loss(train/val) 0.40536/0.19214. Took 0.10 sec\n",
            "Epoch 1020, Loss(train/val) 0.40034/0.19028. Took 0.09 sec\n",
            "Epoch 1021, Loss(train/val) 0.41090/0.18897. Took 0.10 sec\n",
            "Epoch 1022, Loss(train/val) 0.41308/0.18888. Took 0.10 sec\n",
            "Epoch 1023, Loss(train/val) 0.39665/0.18907. Took 0.10 sec\n",
            "Epoch 1024, Loss(train/val) 0.40125/0.18951. Took 0.10 sec\n",
            "Epoch 1025, Loss(train/val) 0.40026/0.19139. Took 0.11 sec\n",
            "Epoch 1026, Loss(train/val) 0.41485/0.19269. Took 0.09 sec\n",
            "Epoch 1027, Loss(train/val) 0.40376/0.19351. Took 0.10 sec\n",
            "Epoch 1028, Loss(train/val) 0.39480/0.19435. Took 0.10 sec\n",
            "Epoch 1029, Loss(train/val) 0.40193/0.19418. Took 0.10 sec\n",
            "Epoch 1030, Loss(train/val) 0.40240/0.19266. Took 0.10 sec\n",
            "Epoch 1031, Loss(train/val) 0.39021/0.19273. Took 0.09 sec\n",
            "Epoch 1032, Loss(train/val) 0.39090/0.19014. Took 0.10 sec\n",
            "Epoch 1033, Loss(train/val) 0.39683/0.19001. Took 0.10 sec\n",
            "Epoch 1034, Loss(train/val) 0.39815/0.19062. Took 0.09 sec\n",
            "Epoch 1035, Loss(train/val) 0.40773/0.18764. Took 0.11 sec\n",
            "Epoch 1036, Loss(train/val) 0.40239/0.18625. Took 0.10 sec\n",
            "Epoch 1037, Loss(train/val) 0.38889/0.18659. Took 0.09 sec\n",
            "Epoch 1038, Loss(train/val) 0.40944/0.18684. Took 0.10 sec\n",
            "Epoch 1039, Loss(train/val) 0.39916/0.18789. Took 0.10 sec\n",
            "Epoch 1040, Loss(train/val) 0.38836/0.18876. Took 0.10 sec\n",
            "Epoch 1041, Loss(train/val) 0.40631/0.19067. Took 0.10 sec\n",
            "Epoch 1042, Loss(train/val) 0.41371/0.19504. Took 0.10 sec\n",
            "Epoch 1043, Loss(train/val) 0.38954/0.19623. Took 0.09 sec\n",
            "Epoch 1044, Loss(train/val) 0.40447/0.19781. Took 0.10 sec\n",
            "Epoch 1045, Loss(train/val) 0.40447/0.19529. Took 0.10 sec\n",
            "Epoch 1046, Loss(train/val) 0.39437/0.19543. Took 0.09 sec\n",
            "Epoch 1047, Loss(train/val) 0.39702/0.19438. Took 0.10 sec\n",
            "Epoch 1048, Loss(train/val) 0.39999/0.20006. Took 0.09 sec\n",
            "Epoch 1049, Loss(train/val) 0.38518/0.20186. Took 0.10 sec\n",
            "Epoch 1050, Loss(train/val) 0.38882/0.20314. Took 0.10 sec\n",
            "Epoch 1051, Loss(train/val) 0.38624/0.20426. Took 0.10 sec\n",
            "Epoch 1052, Loss(train/val) 0.39740/0.20006. Took 0.10 sec\n",
            "Epoch 1053, Loss(train/val) 0.39180/0.19272. Took 0.10 sec\n",
            "Epoch 1054, Loss(train/val) 0.38851/0.18726. Took 0.09 sec\n",
            "Epoch 1055, Loss(train/val) 0.38851/0.18583. Took 0.09 sec\n",
            "Epoch 1056, Loss(train/val) 0.38972/0.18920. Took 0.11 sec\n",
            "Epoch 1057, Loss(train/val) 0.39112/0.19113. Took 0.09 sec\n",
            "Epoch 1058, Loss(train/val) 0.38413/0.18842. Took 0.10 sec\n",
            "Epoch 1059, Loss(train/val) 0.38232/0.18636. Took 0.10 sec\n",
            "Epoch 1060, Loss(train/val) 0.41240/0.18365. Took 0.10 sec\n",
            "Epoch 1061, Loss(train/val) 0.39172/0.18361. Took 0.10 sec\n",
            "Epoch 1062, Loss(train/val) 0.38078/0.18524. Took 0.10 sec\n",
            "Epoch 1063, Loss(train/val) 0.39040/0.18635. Took 0.10 sec\n",
            "Epoch 1064, Loss(train/val) 0.40411/0.18797. Took 0.09 sec\n",
            "Epoch 1065, Loss(train/val) 0.38393/0.18877. Took 0.10 sec\n",
            "Epoch 1066, Loss(train/val) 0.38237/0.18791. Took 0.11 sec\n",
            "Epoch 1067, Loss(train/val) 0.37643/0.18691. Took 0.09 sec\n",
            "Epoch 1068, Loss(train/val) 0.39001/0.18705. Took 0.10 sec\n",
            "Epoch 1069, Loss(train/val) 0.39072/0.18498. Took 0.10 sec\n",
            "Epoch 1070, Loss(train/val) 0.39659/0.18395. Took 0.10 sec\n",
            "Epoch 1071, Loss(train/val) 0.37827/0.18361. Took 0.10 sec\n",
            "Epoch 1072, Loss(train/val) 0.39410/0.18425. Took 0.09 sec\n",
            "Epoch 1073, Loss(train/val) 0.38346/0.18363. Took 0.10 sec\n",
            "Epoch 1074, Loss(train/val) 0.38302/0.18336. Took 0.10 sec\n",
            "Epoch 1075, Loss(train/val) 0.37860/0.18368. Took 0.10 sec\n",
            "Epoch 1076, Loss(train/val) 0.37844/0.18291. Took 0.10 sec\n",
            "Epoch 1077, Loss(train/val) 0.38333/0.18237. Took 0.11 sec\n",
            "Epoch 1078, Loss(train/val) 0.38781/0.18167. Took 0.10 sec\n",
            "Epoch 1079, Loss(train/val) 0.38650/0.18085. Took 0.10 sec\n",
            "Epoch 1080, Loss(train/val) 0.39178/0.18109. Took 0.10 sec\n",
            "Epoch 1081, Loss(train/val) 0.38140/0.18113. Took 0.10 sec\n",
            "Epoch 1082, Loss(train/val) 0.38825/0.18527. Took 0.10 sec\n",
            "Epoch 1083, Loss(train/val) 0.38259/0.18775. Took 0.10 sec\n",
            "Epoch 1084, Loss(train/val) 0.38101/0.20054. Took 0.10 sec\n",
            "Epoch 1085, Loss(train/val) 0.38582/0.20278. Took 0.10 sec\n",
            "Epoch 1086, Loss(train/val) 0.36992/0.20885. Took 0.10 sec\n",
            "Epoch 1087, Loss(train/val) 0.38113/0.20296. Took 0.10 sec\n",
            "Epoch 1088, Loss(train/val) 0.40038/0.20132. Took 0.10 sec\n",
            "Epoch 1089, Loss(train/val) 0.37683/0.20761. Took 0.10 sec\n",
            "Epoch 1090, Loss(train/val) 0.37500/0.20669. Took 0.10 sec\n",
            "Epoch 1091, Loss(train/val) 0.36474/0.20118. Took 0.09 sec\n",
            "Epoch 1092, Loss(train/val) 0.37333/0.19762. Took 0.10 sec\n",
            "Epoch 1093, Loss(train/val) 0.39188/0.19719. Took 0.10 sec\n",
            "Epoch 1094, Loss(train/val) 0.37712/0.19581. Took 0.09 sec\n",
            "Epoch 1095, Loss(train/val) 0.39822/0.18776. Took 0.10 sec\n",
            "Epoch 1096, Loss(train/val) 0.39883/0.17999. Took 0.10 sec\n",
            "Epoch 1097, Loss(train/val) 0.37361/0.17956. Took 0.11 sec\n",
            "Epoch 1098, Loss(train/val) 0.37952/0.17883. Took 0.12 sec\n",
            "Epoch 1099, Loss(train/val) 0.36673/0.17996. Took 0.09 sec\n",
            "Epoch 1100, Loss(train/val) 0.36273/0.18009. Took 0.10 sec\n",
            "Epoch 1101, Loss(train/val) 0.37659/0.17970. Took 0.11 sec\n",
            "Epoch 1102, Loss(train/val) 0.36187/0.18031. Took 0.10 sec\n",
            "Epoch 1103, Loss(train/val) 0.37121/0.17979. Took 0.10 sec\n",
            "Epoch 1104, Loss(train/val) 0.36036/0.17988. Took 0.10 sec\n",
            "Epoch 1105, Loss(train/val) 0.37422/0.17940. Took 0.10 sec\n",
            "Epoch 1106, Loss(train/val) 0.36660/0.17988. Took 0.09 sec\n",
            "Epoch 1107, Loss(train/val) 0.35510/0.18117. Took 0.10 sec\n",
            "Epoch 1108, Loss(train/val) 0.36974/0.18402. Took 0.10 sec\n",
            "Epoch 1109, Loss(train/val) 0.36627/0.18436. Took 0.10 sec\n",
            "Epoch 1110, Loss(train/val) 0.35495/0.17900. Took 0.11 sec\n",
            "Epoch 1111, Loss(train/val) 0.37803/0.17771. Took 0.09 sec\n",
            "Epoch 1112, Loss(train/val) 0.36694/0.17677. Took 0.09 sec\n",
            "Epoch 1113, Loss(train/val) 0.35733/0.17750. Took 0.10 sec\n",
            "Epoch 1114, Loss(train/val) 0.37015/0.17891. Took 0.09 sec\n",
            "Epoch 1115, Loss(train/val) 0.36110/0.18217. Took 0.10 sec\n",
            "Epoch 1116, Loss(train/val) 0.36049/0.18662. Took 0.10 sec\n",
            "Epoch 1117, Loss(train/val) 0.37761/0.18970. Took 0.10 sec\n",
            "Epoch 1118, Loss(train/val) 0.35351/0.18552. Took 0.10 sec\n",
            "Epoch 1119, Loss(train/val) 0.36655/0.18099. Took 0.10 sec\n",
            "Epoch 1120, Loss(train/val) 0.35201/0.18062. Took 0.10 sec\n",
            "Epoch 1121, Loss(train/val) 0.36020/0.17845. Took 0.11 sec\n",
            "Epoch 1122, Loss(train/val) 0.35569/0.17732. Took 0.10 sec\n",
            "Epoch 1123, Loss(train/val) 0.36123/0.17644. Took 0.10 sec\n",
            "Epoch 1124, Loss(train/val) 0.35251/0.17579. Took 0.10 sec\n",
            "Epoch 1125, Loss(train/val) 0.36067/0.17484. Took 0.10 sec\n",
            "Epoch 1126, Loss(train/val) 0.35726/0.17466. Took 0.09 sec\n",
            "Epoch 1127, Loss(train/val) 0.36143/0.17406. Took 0.10 sec\n",
            "Epoch 1128, Loss(train/val) 0.34682/0.17328. Took 0.10 sec\n",
            "Epoch 1129, Loss(train/val) 0.36131/0.17308. Took 0.10 sec\n",
            "Epoch 1130, Loss(train/val) 0.35462/0.17266. Took 0.10 sec\n",
            "Epoch 1131, Loss(train/val) 0.35065/0.17284. Took 0.10 sec\n",
            "Epoch 1132, Loss(train/val) 0.35526/0.17244. Took 0.10 sec\n",
            "Epoch 1133, Loss(train/val) 0.35117/0.17337. Took 0.10 sec\n",
            "Epoch 1134, Loss(train/val) 0.35756/0.17884. Took 0.10 sec\n",
            "Epoch 1135, Loss(train/val) 0.35451/0.17968. Took 0.09 sec\n",
            "Epoch 1136, Loss(train/val) 0.35443/0.17974. Took 0.11 sec\n",
            "Epoch 1137, Loss(train/val) 0.35466/0.18549. Took 0.09 sec\n",
            "Epoch 1138, Loss(train/val) 0.34661/0.18931. Took 0.09 sec\n",
            "Epoch 1139, Loss(train/val) 0.36238/0.20077. Took 0.11 sec\n",
            "Epoch 1140, Loss(train/val) 0.34948/0.20850. Took 0.10 sec\n",
            "Epoch 1141, Loss(train/val) 0.33981/0.21306. Took 0.10 sec\n",
            "Epoch 1142, Loss(train/val) 0.34441/0.21223. Took 0.10 sec\n",
            "Epoch 1143, Loss(train/val) 0.34408/0.20233. Took 0.10 sec\n",
            "Epoch 1144, Loss(train/val) 0.34823/0.20139. Took 0.09 sec\n",
            "Epoch 1145, Loss(train/val) 0.34706/0.19046. Took 0.10 sec\n",
            "Epoch 1146, Loss(train/val) 0.33855/0.18764. Took 0.09 sec\n",
            "Epoch 1147, Loss(train/val) 0.35925/0.18938. Took 0.09 sec\n",
            "Epoch 1148, Loss(train/val) 0.35161/0.18971. Took 0.10 sec\n",
            "Epoch 1149, Loss(train/val) 0.34804/0.19116. Took 0.11 sec\n",
            "Epoch 1150, Loss(train/val) 0.34243/0.19174. Took 0.09 sec\n",
            "Epoch 1151, Loss(train/val) 0.34837/0.19075. Took 0.10 sec\n",
            "Epoch 1152, Loss(train/val) 0.34009/0.19208. Took 0.09 sec\n",
            "Epoch 1153, Loss(train/val) 0.34776/0.19068. Took 0.11 sec\n",
            "Epoch 1154, Loss(train/val) 0.34089/0.19209. Took 0.10 sec\n",
            "Epoch 1155, Loss(train/val) 0.35624/0.18003. Took 0.09 sec\n",
            "Epoch 1156, Loss(train/val) 0.34090/0.17422. Took 0.11 sec\n",
            "Epoch 1157, Loss(train/val) 0.33922/0.16954. Took 0.09 sec\n",
            "Epoch 1158, Loss(train/val) 0.33323/0.16723. Took 0.10 sec\n",
            "Epoch 1159, Loss(train/val) 0.34808/0.16666. Took 0.10 sec\n",
            "Epoch 1160, Loss(train/val) 0.34006/0.16596. Took 0.10 sec\n",
            "Epoch 1161, Loss(train/val) 0.34328/0.16667. Took 0.09 sec\n",
            "Epoch 1162, Loss(train/val) 0.33757/0.16555. Took 0.10 sec\n",
            "Epoch 1163, Loss(train/val) 0.34209/0.16530. Took 0.10 sec\n",
            "Epoch 1164, Loss(train/val) 0.34187/0.16634. Took 0.09 sec\n",
            "Epoch 1165, Loss(train/val) 0.34106/0.16561. Took 0.10 sec\n",
            "Epoch 1166, Loss(train/val) 0.34393/0.16318. Took 0.09 sec\n",
            "Epoch 1167, Loss(train/val) 0.34701/0.16221. Took 0.10 sec\n",
            "Epoch 1168, Loss(train/val) 0.33779/0.16395. Took 0.10 sec\n",
            "Epoch 1169, Loss(train/val) 0.33456/0.17119. Took 0.10 sec\n",
            "Epoch 1170, Loss(train/val) 0.32526/0.18581. Took 0.10 sec\n",
            "Epoch 1171, Loss(train/val) 0.34615/0.20016. Took 0.11 sec\n",
            "Epoch 1172, Loss(train/val) 0.32792/0.20685. Took 0.09 sec\n",
            "Epoch 1173, Loss(train/val) 0.32689/0.20385. Took 0.10 sec\n",
            "Epoch 1174, Loss(train/val) 0.33036/0.20451. Took 0.10 sec\n",
            "Epoch 1175, Loss(train/val) 0.32437/0.20604. Took 0.09 sec\n",
            "Epoch 1176, Loss(train/val) 0.31342/0.19033. Took 0.09 sec\n",
            "Epoch 1177, Loss(train/val) 0.34025/0.17042. Took 0.10 sec\n",
            "Epoch 1178, Loss(train/val) 0.32560/0.16224. Took 0.10 sec\n",
            "Epoch 1179, Loss(train/val) 0.33324/0.15934. Took 0.09 sec\n",
            "Epoch 1180, Loss(train/val) 0.31652/0.15875. Took 0.11 sec\n",
            "Epoch 1181, Loss(train/val) 0.32575/0.15800. Took 0.10 sec\n",
            "Epoch 1182, Loss(train/val) 0.32087/0.15700. Took 0.10 sec\n",
            "Epoch 1183, Loss(train/val) 0.33422/0.15687. Took 0.10 sec\n",
            "Epoch 1184, Loss(train/val) 0.34240/0.15694. Took 0.10 sec\n",
            "Epoch 1185, Loss(train/val) 0.33205/0.15889. Took 0.10 sec\n",
            "Epoch 1186, Loss(train/val) 0.31947/0.15756. Took 0.10 sec\n",
            "Epoch 1187, Loss(train/val) 0.33027/0.15507. Took 0.09 sec\n",
            "Epoch 1188, Loss(train/val) 0.31714/0.15484. Took 0.09 sec\n",
            "Epoch 1189, Loss(train/val) 0.32889/0.15543. Took 0.10 sec\n",
            "Epoch 1190, Loss(train/val) 0.31871/0.15760. Took 0.09 sec\n",
            "Epoch 1191, Loss(train/val) 0.31614/0.16906. Took 0.11 sec\n",
            "Epoch 1192, Loss(train/val) 0.32438/0.17741. Took 0.10 sec\n",
            "Epoch 1193, Loss(train/val) 0.31626/0.17389. Took 0.09 sec\n",
            "Epoch 1194, Loss(train/val) 0.33647/0.17412. Took 0.10 sec\n",
            "Epoch 1195, Loss(train/val) 0.31481/0.18125. Took 0.10 sec\n",
            "Epoch 1196, Loss(train/val) 0.31446/0.17331. Took 0.09 sec\n",
            "Epoch 1197, Loss(train/val) 0.31171/0.17099. Took 0.10 sec\n",
            "Epoch 1198, Loss(train/val) 0.33073/0.16729. Took 0.10 sec\n",
            "Epoch 1199, Loss(train/val) 0.32141/0.15982. Took 0.10 sec\n",
            "Epoch 1200, Loss(train/val) 0.31757/0.15914. Took 0.10 sec\n",
            "Epoch 1201, Loss(train/val) 0.31236/0.15655. Took 0.11 sec\n",
            "Epoch 1202, Loss(train/val) 0.30615/0.15377. Took 0.12 sec\n",
            "Epoch 1203, Loss(train/val) 0.32414/0.14910. Took 0.10 sec\n",
            "Epoch 1204, Loss(train/val) 0.30810/0.14738. Took 0.10 sec\n",
            "Epoch 1205, Loss(train/val) 0.30055/0.14747. Took 0.10 sec\n",
            "Epoch 1206, Loss(train/val) 0.32090/0.14603. Took 0.11 sec\n",
            "Epoch 1207, Loss(train/val) 0.31376/0.14563. Took 0.10 sec\n",
            "Epoch 1208, Loss(train/val) 0.30836/0.14799. Took 0.10 sec\n",
            "Epoch 1209, Loss(train/val) 0.32021/0.15108. Took 0.09 sec\n",
            "Epoch 1210, Loss(train/val) 0.32187/0.15053. Took 0.10 sec\n",
            "Epoch 1211, Loss(train/val) 0.29687/0.15185. Took 0.11 sec\n",
            "Epoch 1212, Loss(train/val) 0.30833/0.14789. Took 0.09 sec\n",
            "Epoch 1213, Loss(train/val) 0.30833/0.14356. Took 0.10 sec\n",
            "Epoch 1214, Loss(train/val) 0.30580/0.14290. Took 0.09 sec\n",
            "Epoch 1215, Loss(train/val) 0.29657/0.14272. Took 0.09 sec\n",
            "Epoch 1216, Loss(train/val) 0.29925/0.14202. Took 0.11 sec\n",
            "Epoch 1217, Loss(train/val) 0.29971/0.13998. Took 0.09 sec\n",
            "Epoch 1218, Loss(train/val) 0.31699/0.13985. Took 0.09 sec\n",
            "Epoch 1219, Loss(train/val) 0.29040/0.14023. Took 0.10 sec\n",
            "Epoch 1220, Loss(train/val) 0.31363/0.14120. Took 0.10 sec\n",
            "Epoch 1221, Loss(train/val) 0.29294/0.14091. Took 0.10 sec\n",
            "Epoch 1222, Loss(train/val) 0.30172/0.15585. Took 0.10 sec\n",
            "Epoch 1223, Loss(train/val) 0.29390/0.16500. Took 0.10 sec\n",
            "Epoch 1224, Loss(train/val) 0.28811/0.15200. Took 0.10 sec\n",
            "Epoch 1225, Loss(train/val) 0.31185/0.14934. Took 0.10 sec\n",
            "Epoch 1226, Loss(train/val) 0.29401/0.14564. Took 0.09 sec\n",
            "Epoch 1227, Loss(train/val) 0.29559/0.13771. Took 0.10 sec\n",
            "Epoch 1228, Loss(train/val) 0.30759/0.13681. Took 0.10 sec\n",
            "Epoch 1229, Loss(train/val) 0.29882/0.15176. Took 0.09 sec\n",
            "Epoch 1230, Loss(train/val) 0.29708/0.17236. Took 0.10 sec\n",
            "Epoch 1231, Loss(train/val) 0.28879/0.17665. Took 0.11 sec\n",
            "Epoch 1232, Loss(train/val) 0.31187/0.18514. Took 0.10 sec\n",
            "Epoch 1233, Loss(train/val) 0.29123/0.19779. Took 0.09 sec\n",
            "Epoch 1234, Loss(train/val) 0.28787/0.21756. Took 0.10 sec\n",
            "Epoch 1235, Loss(train/val) 0.29464/0.21353. Took 0.10 sec\n",
            "Epoch 1236, Loss(train/val) 0.28652/0.20689. Took 0.10 sec\n",
            "Epoch 1237, Loss(train/val) 0.28766/0.21681. Took 0.10 sec\n",
            "Epoch 1238, Loss(train/val) 0.27494/0.21788. Took 0.10 sec\n",
            "Epoch 1239, Loss(train/val) 0.28218/0.22648. Took 0.09 sec\n",
            "Epoch 1240, Loss(train/val) 0.29778/0.22001. Took 0.10 sec\n",
            "Epoch 1241, Loss(train/val) 0.27857/0.20391. Took 0.10 sec\n",
            "Epoch 1242, Loss(train/val) 0.28044/0.17382. Took 0.10 sec\n",
            "Epoch 1243, Loss(train/val) 0.28247/0.15031. Took 0.10 sec\n",
            "Epoch 1244, Loss(train/val) 0.27868/0.12787. Took 0.10 sec\n",
            "Epoch 1245, Loss(train/val) 0.28177/0.12857. Took 0.10 sec\n",
            "Epoch 1246, Loss(train/val) 0.27523/0.13228. Took 0.10 sec\n",
            "Epoch 1247, Loss(train/val) 0.29500/0.12918. Took 0.10 sec\n",
            "Epoch 1248, Loss(train/val) 0.29429/0.12617. Took 0.10 sec\n",
            "Epoch 1249, Loss(train/val) 0.27931/0.12398. Took 0.09 sec\n",
            "Epoch 1250, Loss(train/val) 0.26048/0.12413. Took 0.10 sec\n",
            "Epoch 1251, Loss(train/val) 0.27981/0.12445. Took 0.10 sec\n",
            "Epoch 1252, Loss(train/val) 0.27408/0.12519. Took 0.10 sec\n",
            "Epoch 1253, Loss(train/val) 0.27308/0.13466. Took 0.09 sec\n",
            "Epoch 1254, Loss(train/val) 0.28212/0.15041. Took 0.10 sec\n",
            "Epoch 1255, Loss(train/val) 0.25910/0.14968. Took 0.09 sec\n",
            "Epoch 1256, Loss(train/val) 0.29502/0.13364. Took 0.10 sec\n",
            "Epoch 1257, Loss(train/val) 0.27313/0.12889. Took 0.10 sec\n",
            "Epoch 1258, Loss(train/val) 0.26890/0.11935. Took 0.10 sec\n",
            "Epoch 1259, Loss(train/val) 0.26844/0.13060. Took 0.10 sec\n",
            "Epoch 1260, Loss(train/val) 0.27430/0.17296. Took 0.10 sec\n",
            "Epoch 1261, Loss(train/val) 0.27208/0.18573. Took 0.10 sec\n",
            "Epoch 1262, Loss(train/val) 0.26444/0.18070. Took 0.10 sec\n",
            "Epoch 1263, Loss(train/val) 0.26135/0.15337. Took 0.12 sec\n",
            "Epoch 1264, Loss(train/val) 0.26978/0.12580. Took 0.10 sec\n",
            "Epoch 1265, Loss(train/val) 0.27587/0.12021. Took 0.09 sec\n",
            "Epoch 1266, Loss(train/val) 0.29602/0.11964. Took 0.10 sec\n",
            "Epoch 1267, Loss(train/val) 0.26754/0.11640. Took 0.09 sec\n",
            "Epoch 1268, Loss(train/val) 0.27101/0.12626. Took 0.10 sec\n",
            "Epoch 1269, Loss(train/val) 0.26711/0.14891. Took 0.10 sec\n",
            "Epoch 1270, Loss(train/val) 0.26188/0.18688. Took 0.10 sec\n",
            "Epoch 1271, Loss(train/val) 0.25389/0.18152. Took 0.11 sec\n",
            "Epoch 1272, Loss(train/val) 0.26024/0.17218. Took 0.10 sec\n",
            "Epoch 1273, Loss(train/val) 0.27636/0.17872. Took 0.11 sec\n",
            "Epoch 1274, Loss(train/val) 0.26690/0.19685. Took 0.10 sec\n",
            "Epoch 1275, Loss(train/val) 0.26318/0.19011. Took 0.10 sec\n",
            "Epoch 1276, Loss(train/val) 0.25413/0.19743. Took 0.10 sec\n",
            "Epoch 1277, Loss(train/val) 0.26510/0.17371. Took 0.10 sec\n",
            "Epoch 1278, Loss(train/val) 0.26742/0.14514. Took 0.09 sec\n",
            "Epoch 1279, Loss(train/val) 0.25230/0.11500. Took 0.10 sec\n",
            "Epoch 1280, Loss(train/val) 0.26628/0.11158. Took 0.10 sec\n",
            "Epoch 1281, Loss(train/val) 0.26777/0.11451. Took 0.10 sec\n",
            "Epoch 1282, Loss(train/val) 0.24651/0.11258. Took 0.10 sec\n",
            "Epoch 1283, Loss(train/val) 0.27232/0.11761. Took 0.10 sec\n",
            "Epoch 1284, Loss(train/val) 0.26745/0.12512. Took 0.10 sec\n",
            "Epoch 1285, Loss(train/val) 0.27471/0.12620. Took 0.10 sec\n",
            "Epoch 1286, Loss(train/val) 0.26934/0.13475. Took 0.10 sec\n",
            "Epoch 1287, Loss(train/val) 0.25503/0.12214. Took 0.10 sec\n",
            "Epoch 1288, Loss(train/val) 0.25295/0.11833. Took 0.10 sec\n",
            "Epoch 1289, Loss(train/val) 0.25628/0.12611. Took 0.10 sec\n",
            "Epoch 1290, Loss(train/val) 0.25408/0.11916. Took 0.10 sec\n",
            "Epoch 1291, Loss(train/val) 0.24496/0.11520. Took 0.10 sec\n",
            "Epoch 1292, Loss(train/val) 0.25267/0.11472. Took 0.10 sec\n",
            "Epoch 1293, Loss(train/val) 0.25769/0.11593. Took 0.11 sec\n",
            "Epoch 1294, Loss(train/val) 0.24836/0.12193. Took 0.10 sec\n",
            "Epoch 1295, Loss(train/val) 0.25490/0.14838. Took 0.10 sec\n",
            "Epoch 1296, Loss(train/val) 0.27026/0.15286. Took 0.09 sec\n",
            "Epoch 1297, Loss(train/val) 0.25984/0.14402. Took 0.10 sec\n",
            "Epoch 1298, Loss(train/val) 0.26726/0.14367. Took 0.10 sec\n",
            "Epoch 1299, Loss(train/val) 0.26153/0.15380. Took 0.09 sec\n",
            "Epoch 1300, Loss(train/val) 0.25491/0.14799. Took 0.10 sec\n",
            "Epoch 1301, Loss(train/val) 0.26564/0.15029. Took 0.10 sec\n",
            "Epoch 1302, Loss(train/val) 0.24081/0.14609. Took 0.10 sec\n",
            "Epoch 1303, Loss(train/val) 0.24682/0.16777. Took 0.11 sec\n",
            "Epoch 1304, Loss(train/val) 0.25767/0.18921. Took 0.11 sec\n",
            "Epoch 1305, Loss(train/val) 0.24387/0.17504. Took 0.09 sec\n",
            "Epoch 1306, Loss(train/val) 0.23331/0.17646. Took 0.10 sec\n",
            "Epoch 1307, Loss(train/val) 0.24855/0.16094. Took 0.09 sec\n",
            "Epoch 1308, Loss(train/val) 0.22976/0.16878. Took 0.10 sec\n",
            "Epoch 1309, Loss(train/val) 0.24887/0.13005. Took 0.11 sec\n",
            "Epoch 1310, Loss(train/val) 0.23133/0.10825. Took 0.11 sec\n",
            "Epoch 1311, Loss(train/val) 0.24783/0.11317. Took 0.10 sec\n",
            "Epoch 1312, Loss(train/val) 0.23840/0.12276. Took 0.09 sec\n",
            "Epoch 1313, Loss(train/val) 0.23832/0.12445. Took 0.10 sec\n",
            "Epoch 1314, Loss(train/val) 0.23855/0.11889. Took 0.11 sec\n",
            "Epoch 1315, Loss(train/val) 0.26820/0.12398. Took 0.10 sec\n",
            "Epoch 1316, Loss(train/val) 0.25914/0.14182. Took 0.10 sec\n",
            "Epoch 1317, Loss(train/val) 0.25604/0.14718. Took 0.10 sec\n",
            "Epoch 1318, Loss(train/val) 0.24596/0.11408. Took 0.11 sec\n",
            "Epoch 1319, Loss(train/val) 0.24672/0.10461. Took 0.10 sec\n",
            "Epoch 1320, Loss(train/val) 0.23161/0.10655. Took 0.10 sec\n",
            "Epoch 1321, Loss(train/val) 0.24036/0.10800. Took 0.10 sec\n",
            "Epoch 1322, Loss(train/val) 0.25454/0.11289. Took 0.10 sec\n",
            "Epoch 1323, Loss(train/val) 0.23058/0.10585. Took 0.10 sec\n",
            "Epoch 1324, Loss(train/val) 0.21805/0.10826. Took 0.11 sec\n",
            "Epoch 1325, Loss(train/val) 0.23564/0.11005. Took 0.10 sec\n",
            "Epoch 1326, Loss(train/val) 0.24280/0.10715. Took 0.10 sec\n",
            "Epoch 1327, Loss(train/val) 0.25743/0.10799. Took 0.10 sec\n",
            "Epoch 1328, Loss(train/val) 0.23216/0.11241. Took 0.10 sec\n",
            "Epoch 1329, Loss(train/val) 0.23175/0.12569. Took 0.09 sec\n",
            "Epoch 1330, Loss(train/val) 0.24657/0.16108. Took 0.10 sec\n",
            "Epoch 1331, Loss(train/val) 0.22991/0.17435. Took 0.10 sec\n",
            "Epoch 1332, Loss(train/val) 0.24372/0.17701. Took 0.10 sec\n",
            "Epoch 1333, Loss(train/val) 0.23807/0.19188. Took 0.10 sec\n",
            "Epoch 1334, Loss(train/val) 0.23696/0.16514. Took 0.11 sec\n",
            "Epoch 1335, Loss(train/val) 0.24274/0.15098. Took 0.10 sec\n",
            "Epoch 1336, Loss(train/val) 0.23736/0.12470. Took 0.10 sec\n",
            "Epoch 1337, Loss(train/val) 0.27136/0.10422. Took 0.09 sec\n",
            "Epoch 1338, Loss(train/val) 0.22774/0.10821. Took 0.10 sec\n",
            "Epoch 1339, Loss(train/val) 0.22750/0.11402. Took 0.10 sec\n",
            "Epoch 1340, Loss(train/val) 0.22995/0.12171. Took 0.10 sec\n",
            "Epoch 1341, Loss(train/val) 0.23962/0.12903. Took 0.10 sec\n",
            "Epoch 1342, Loss(train/val) 0.23256/0.12365. Took 0.10 sec\n",
            "Epoch 1343, Loss(train/val) 0.23668/0.11435. Took 0.10 sec\n",
            "Epoch 1344, Loss(train/val) 0.23279/0.10737. Took 0.11 sec\n",
            "Epoch 1345, Loss(train/val) 0.23596/0.10991. Took 0.10 sec\n",
            "Epoch 1346, Loss(train/val) 0.25687/0.11105. Took 0.09 sec\n",
            "Epoch 1347, Loss(train/val) 0.22301/0.11337. Took 0.11 sec\n",
            "Epoch 1348, Loss(train/val) 0.22419/0.12586. Took 0.09 sec\n",
            "Epoch 1349, Loss(train/val) 0.22178/0.14181. Took 0.09 sec\n",
            "Epoch 1350, Loss(train/val) 0.23791/0.20265. Took 0.11 sec\n",
            "Epoch 1351, Loss(train/val) 0.24775/0.23214. Took 0.09 sec\n",
            "Epoch 1352, Loss(train/val) 0.24047/0.21035. Took 0.10 sec\n",
            "Epoch 1353, Loss(train/val) 0.22270/0.16308. Took 0.10 sec\n",
            "Epoch 1354, Loss(train/val) 0.24320/0.14876. Took 0.10 sec\n",
            "Epoch 1355, Loss(train/val) 0.21647/0.14072. Took 0.11 sec\n",
            "Epoch 1356, Loss(train/val) 0.23303/0.11332. Took 0.09 sec\n",
            "Epoch 1357, Loss(train/val) 0.22509/0.10444. Took 0.10 sec\n",
            "Epoch 1358, Loss(train/val) 0.23441/0.10517. Took 0.10 sec\n",
            "Epoch 1359, Loss(train/val) 0.22382/0.11109. Took 0.10 sec\n",
            "Epoch 1360, Loss(train/val) 0.22748/0.11380. Took 0.10 sec\n",
            "Epoch 1361, Loss(train/val) 0.23212/0.11191. Took 0.10 sec\n",
            "Epoch 1362, Loss(train/val) 0.22401/0.10859. Took 0.10 sec\n",
            "Epoch 1363, Loss(train/val) 0.21622/0.10219. Took 0.09 sec\n",
            "Epoch 1364, Loss(train/val) 0.23331/0.10327. Took 0.10 sec\n",
            "Epoch 1365, Loss(train/val) 0.22516/0.11161. Took 0.11 sec\n",
            "Epoch 1366, Loss(train/val) 0.22561/0.16922. Took 0.09 sec\n",
            "Epoch 1367, Loss(train/val) 0.22939/0.20126. Took 0.10 sec\n",
            "Epoch 1368, Loss(train/val) 0.22664/0.20074. Took 0.10 sec\n",
            "Epoch 1369, Loss(train/val) 0.23073/0.19341. Took 0.09 sec\n",
            "Epoch 1370, Loss(train/val) 0.21760/0.18294. Took 0.10 sec\n",
            "Epoch 1371, Loss(train/val) 0.22153/0.16899. Took 0.10 sec\n",
            "Epoch 1372, Loss(train/val) 0.23575/0.14241. Took 0.10 sec\n",
            "Epoch 1373, Loss(train/val) 0.21536/0.12438. Took 0.10 sec\n",
            "Epoch 1374, Loss(train/val) 0.21876/0.12106. Took 0.10 sec\n",
            "Epoch 1375, Loss(train/val) 0.22247/0.11155. Took 0.10 sec\n",
            "Epoch 1376, Loss(train/val) 0.22209/0.10471. Took 0.10 sec\n",
            "Epoch 1377, Loss(train/val) 0.20953/0.11074. Took 0.09 sec\n",
            "Epoch 1378, Loss(train/val) 0.23423/0.14055. Took 0.10 sec\n",
            "Epoch 1379, Loss(train/val) 0.22066/0.16069. Took 0.10 sec\n",
            "Epoch 1380, Loss(train/val) 0.21174/0.18890. Took 0.10 sec\n",
            "Epoch 1381, Loss(train/val) 0.21492/0.16971. Took 0.09 sec\n",
            "Epoch 1382, Loss(train/val) 0.21974/0.12739. Took 0.10 sec\n",
            "Epoch 1383, Loss(train/val) 0.21622/0.10354. Took 0.10 sec\n",
            "Epoch 1384, Loss(train/val) 0.21650/0.10392. Took 0.10 sec\n",
            "Epoch 1385, Loss(train/val) 0.22285/0.10643. Took 0.11 sec\n",
            "Epoch 1386, Loss(train/val) 0.23438/0.10634. Took 0.10 sec\n",
            "Epoch 1387, Loss(train/val) 0.25263/0.12101. Took 0.10 sec\n",
            "Epoch 1388, Loss(train/val) 0.22314/0.11651. Took 0.10 sec\n",
            "Epoch 1389, Loss(train/val) 0.21254/0.10739. Took 0.10 sec\n",
            "Epoch 1390, Loss(train/val) 0.24178/0.14160. Took 0.09 sec\n",
            "Epoch 1391, Loss(train/val) 0.21214/0.17153. Took 0.10 sec\n",
            "Epoch 1392, Loss(train/val) 0.22157/0.22251. Took 0.10 sec\n",
            "Epoch 1393, Loss(train/val) 0.20048/0.25270. Took 0.10 sec\n",
            "Epoch 1394, Loss(train/val) 0.21205/0.24697. Took 0.10 sec\n",
            "Epoch 1395, Loss(train/val) 0.20153/0.24598. Took 0.10 sec\n",
            "Epoch 1396, Loss(train/val) 0.22063/0.22523. Took 0.11 sec\n",
            "Epoch 1397, Loss(train/val) 0.21027/0.19680. Took 0.11 sec\n",
            "Epoch 1398, Loss(train/val) 0.21029/0.15242. Took 0.10 sec\n",
            "Epoch 1399, Loss(train/val) 0.22040/0.11376. Took 0.10 sec\n",
            "Epoch 1400, Loss(train/val) 0.21724/0.10414. Took 0.10 sec\n",
            "Epoch 1401, Loss(train/val) 0.21737/0.10415. Took 0.09 sec\n",
            "Epoch 1402, Loss(train/val) 0.22768/0.10517. Took 0.11 sec\n",
            "Epoch 1403, Loss(train/val) 0.22288/0.11006. Took 0.09 sec\n",
            "Epoch 1404, Loss(train/val) 0.23131/0.10702. Took 0.11 sec\n",
            "Epoch 1405, Loss(train/val) 0.20677/0.10963. Took 0.10 sec\n",
            "Epoch 1406, Loss(train/val) 0.21735/0.10861. Took 0.10 sec\n",
            "Epoch 1407, Loss(train/val) 0.21443/0.10705. Took 0.10 sec\n",
            "Epoch 1408, Loss(train/val) 0.19840/0.13042. Took 0.09 sec\n",
            "Epoch 1409, Loss(train/val) 0.20806/0.14759. Took 0.09 sec\n",
            "Epoch 1410, Loss(train/val) 0.20621/0.14546. Took 0.10 sec\n",
            "Epoch 1411, Loss(train/val) 0.21995/0.17601. Took 0.10 sec\n",
            "Epoch 1412, Loss(train/val) 0.20683/0.17662. Took 0.10 sec\n",
            "Epoch 1413, Loss(train/val) 0.22107/0.15896. Took 0.11 sec\n",
            "Epoch 1414, Loss(train/val) 0.21236/0.14148. Took 0.11 sec\n",
            "Epoch 1415, Loss(train/val) 0.22381/0.13171. Took 0.09 sec\n",
            "Epoch 1416, Loss(train/val) 0.20915/0.12024. Took 0.11 sec\n",
            "Epoch 1417, Loss(train/val) 0.22572/0.12137. Took 0.10 sec\n",
            "Epoch 1418, Loss(train/val) 0.24683/0.12726. Took 0.09 sec\n",
            "Epoch 1419, Loss(train/val) 0.22263/0.12881. Took 0.10 sec\n",
            "Epoch 1420, Loss(train/val) 0.21697/0.12033. Took 0.10 sec\n",
            "Epoch 1421, Loss(train/val) 0.21892/0.10453. Took 0.09 sec\n",
            "Epoch 1422, Loss(train/val) 0.20851/0.10262. Took 0.10 sec\n",
            "Epoch 1423, Loss(train/val) 0.21979/0.10229. Took 0.10 sec\n",
            "Epoch 1424, Loss(train/val) 0.19119/0.10232. Took 0.10 sec\n",
            "Epoch 1425, Loss(train/val) 0.21594/0.10248. Took 0.10 sec\n",
            "Epoch 1426, Loss(train/val) 0.21967/0.10913. Took 0.10 sec\n",
            "Epoch 1427, Loss(train/val) 0.21292/0.10899. Took 0.11 sec\n",
            "Epoch 1428, Loss(train/val) 0.21515/0.11237. Took 0.10 sec\n",
            "Epoch 1429, Loss(train/val) 0.20182/0.10302. Took 0.09 sec\n",
            "Epoch 1430, Loss(train/val) 0.22084/0.11482. Took 0.10 sec\n",
            "Epoch 1431, Loss(train/val) 0.21037/0.14753. Took 0.09 sec\n",
            "Epoch 1432, Loss(train/val) 0.20469/0.14068. Took 0.10 sec\n",
            "Epoch 1433, Loss(train/val) 0.20947/0.11558. Took 0.10 sec\n",
            "Epoch 1434, Loss(train/val) 0.21135/0.10284. Took 0.10 sec\n",
            "Epoch 1435, Loss(train/val) 0.20956/0.11596. Took 0.10 sec\n",
            "Epoch 1436, Loss(train/val) 0.21150/0.12798. Took 0.10 sec\n",
            "Epoch 1437, Loss(train/val) 0.22670/0.12907. Took 0.11 sec\n",
            "Epoch 1438, Loss(train/val) 0.21004/0.16477. Took 0.10 sec\n",
            "Epoch 1439, Loss(train/val) 0.20332/0.18611. Took 0.10 sec\n",
            "Epoch 1440, Loss(train/val) 0.20273/0.16031. Took 0.10 sec\n",
            "Epoch 1441, Loss(train/val) 0.19146/0.14009. Took 0.10 sec\n",
            "Epoch 1442, Loss(train/val) 0.23187/0.13715. Took 0.10 sec\n",
            "Epoch 1443, Loss(train/val) 0.20360/0.15220. Took 0.10 sec\n",
            "Epoch 1444, Loss(train/val) 0.20617/0.17391. Took 0.11 sec\n",
            "Epoch 1445, Loss(train/val) 0.20703/0.17415. Took 0.10 sec\n",
            "Epoch 1446, Loss(train/val) 0.21400/0.15112. Took 0.09 sec\n",
            "Epoch 1447, Loss(train/val) 0.21432/0.13171. Took 0.11 sec\n",
            "Epoch 1448, Loss(train/val) 0.22096/0.10441. Took 0.10 sec\n",
            "Epoch 1449, Loss(train/val) 0.19365/0.10275. Took 0.10 sec\n",
            "Epoch 1450, Loss(train/val) 0.20625/0.10219. Took 0.10 sec\n",
            "Epoch 1451, Loss(train/val) 0.21725/0.10176. Took 0.10 sec\n",
            "Epoch 1452, Loss(train/val) 0.19861/0.10213. Took 0.10 sec\n",
            "Epoch 1453, Loss(train/val) 0.21116/0.10209. Took 0.10 sec\n",
            "Epoch 1454, Loss(train/val) 0.20153/0.10300. Took 0.11 sec\n",
            "Epoch 1455, Loss(train/val) 0.20370/0.11276. Took 0.11 sec\n",
            "Epoch 1456, Loss(train/val) 0.20425/0.12260. Took 0.10 sec\n",
            "Epoch 1457, Loss(train/val) 0.20215/0.12894. Took 0.11 sec\n",
            "Epoch 1458, Loss(train/val) 0.20378/0.16210. Took 0.10 sec\n",
            "Epoch 1459, Loss(train/val) 0.20991/0.22728. Took 0.10 sec\n",
            "Epoch 1460, Loss(train/val) 0.20824/0.25897. Took 0.11 sec\n",
            "Epoch 1461, Loss(train/val) 0.21573/0.23918. Took 0.10 sec\n",
            "Epoch 1462, Loss(train/val) 0.21854/0.22665. Took 0.09 sec\n",
            "Epoch 1463, Loss(train/val) 0.20129/0.21094. Took 0.10 sec\n",
            "Epoch 1464, Loss(train/val) 0.19754/0.17312. Took 0.10 sec\n",
            "Epoch 1465, Loss(train/val) 0.21008/0.13748. Took 0.10 sec\n",
            "Epoch 1466, Loss(train/val) 0.20105/0.11216. Took 0.10 sec\n",
            "Epoch 1467, Loss(train/val) 0.21222/0.10175. Took 0.10 sec\n",
            "Epoch 1468, Loss(train/val) 0.22533/0.11231. Took 0.09 sec\n",
            "Epoch 1469, Loss(train/val) 0.21751/0.12020. Took 0.10 sec\n",
            "Epoch 1470, Loss(train/val) 0.23736/0.11534. Took 0.09 sec\n",
            "Epoch 1471, Loss(train/val) 0.20726/0.10495. Took 0.10 sec\n",
            "Epoch 1472, Loss(train/val) 0.18355/0.10180. Took 0.10 sec\n",
            "Epoch 1473, Loss(train/val) 0.20042/0.10195. Took 0.09 sec\n",
            "Epoch 1474, Loss(train/val) 0.20523/0.11191. Took 0.10 sec\n",
            "Epoch 1475, Loss(train/val) 0.20632/0.11471. Took 0.11 sec\n",
            "Epoch 1476, Loss(train/val) 0.22300/0.10322. Took 0.10 sec\n",
            "Epoch 1477, Loss(train/val) 0.19485/0.10221. Took 0.11 sec\n",
            "Epoch 1478, Loss(train/val) 0.21792/0.10406. Took 0.10 sec\n",
            "Epoch 1479, Loss(train/val) 0.20360/0.10347. Took 0.10 sec\n",
            "Epoch 1480, Loss(train/val) 0.19773/0.10092. Took 0.12 sec\n",
            "Epoch 1481, Loss(train/val) 0.19791/0.10364. Took 0.10 sec\n",
            "Epoch 1482, Loss(train/val) 0.20605/0.11273. Took 0.12 sec\n",
            "Epoch 1483, Loss(train/val) 0.20280/0.10724. Took 0.10 sec\n",
            "Epoch 1484, Loss(train/val) 0.20559/0.10280. Took 0.10 sec\n",
            "Epoch 1485, Loss(train/val) 0.19838/0.10523. Took 0.10 sec\n",
            "Epoch 1486, Loss(train/val) 0.19969/0.10343. Took 0.10 sec\n",
            "Epoch 1487, Loss(train/val) 0.21634/0.10638. Took 0.11 sec\n",
            "Epoch 1488, Loss(train/val) 0.19592/0.10370. Took 0.09 sec\n",
            "Epoch 1489, Loss(train/val) 0.21410/0.10351. Took 0.10 sec\n",
            "Epoch 1490, Loss(train/val) 0.21281/0.11268. Took 0.10 sec\n",
            "Epoch 1491, Loss(train/val) 0.21111/0.11899. Took 0.10 sec\n",
            "Epoch 1492, Loss(train/val) 0.20997/0.10881. Took 0.09 sec\n",
            "Epoch 1493, Loss(train/val) 0.21869/0.10187. Took 0.10 sec\n",
            "Epoch 1494, Loss(train/val) 0.23352/0.10406. Took 0.09 sec\n",
            "Epoch 1495, Loss(train/val) 0.21246/0.10436. Took 0.10 sec\n",
            "Epoch 1496, Loss(train/val) 0.19454/0.10922. Took 0.10 sec\n",
            "Epoch 1497, Loss(train/val) 0.18907/0.10140. Took 0.10 sec\n",
            "Epoch 1498, Loss(train/val) 0.20081/0.10892. Took 0.10 sec\n",
            "Epoch 1499, Loss(train/val) 0.19456/0.13389. Took 0.10 sec\n",
            "Epoch 1500, Loss(train/val) 0.21393/0.12969. Took 0.09 sec\n",
            "Epoch 1501, Loss(train/val) 0.17814/0.12418. Took 0.10 sec\n",
            "Epoch 1502, Loss(train/val) 0.18636/0.11440. Took 0.10 sec\n",
            "Epoch 1503, Loss(train/val) 0.20839/0.10231. Took 0.10 sec\n",
            "Epoch 1504, Loss(train/val) 0.19359/0.11288. Took 0.09 sec\n",
            "Epoch 1505, Loss(train/val) 0.20032/0.12985. Took 0.10 sec\n",
            "Epoch 1506, Loss(train/val) 0.20833/0.13711. Took 0.10 sec\n",
            "Epoch 1507, Loss(train/val) 0.19522/0.15627. Took 0.10 sec\n",
            "Epoch 1508, Loss(train/val) 0.20988/0.14158. Took 0.11 sec\n",
            "Epoch 1509, Loss(train/val) 0.21233/0.12559. Took 0.09 sec\n",
            "Epoch 1510, Loss(train/val) 0.19986/0.12850. Took 0.10 sec\n",
            "Epoch 1511, Loss(train/val) 0.20777/0.11693. Took 0.10 sec\n",
            "Epoch 1512, Loss(train/val) 0.20541/0.11841. Took 0.09 sec\n",
            "Epoch 1513, Loss(train/val) 0.20017/0.11416. Took 0.09 sec\n",
            "Epoch 1514, Loss(train/val) 0.24102/0.11467. Took 0.10 sec\n",
            "Epoch 1515, Loss(train/val) 0.20353/0.11027. Took 0.10 sec\n",
            "Epoch 1516, Loss(train/val) 0.19655/0.11032. Took 0.10 sec\n",
            "Epoch 1517, Loss(train/val) 0.18942/0.11035. Took 0.10 sec\n",
            "Epoch 1518, Loss(train/val) 0.20407/0.11365. Took 0.11 sec\n",
            "Epoch 1519, Loss(train/val) 0.20026/0.10252. Took 0.11 sec\n",
            "Epoch 1520, Loss(train/val) 0.21921/0.10315. Took 0.12 sec\n",
            "Epoch 1521, Loss(train/val) 0.20785/0.10878. Took 0.10 sec\n",
            "Epoch 1522, Loss(train/val) 0.19863/0.12158. Took 0.10 sec\n",
            "Epoch 1523, Loss(train/val) 0.22116/0.13430. Took 0.10 sec\n",
            "Epoch 1524, Loss(train/val) 0.20681/0.13656. Took 0.11 sec\n",
            "Epoch 1525, Loss(train/val) 0.19073/0.15260. Took 0.10 sec\n",
            "Epoch 1526, Loss(train/val) 0.20862/0.15190. Took 0.10 sec\n",
            "Epoch 1527, Loss(train/val) 0.20092/0.12833. Took 0.10 sec\n",
            "Epoch 1528, Loss(train/val) 0.19331/0.10599. Took 0.10 sec\n",
            "Epoch 1529, Loss(train/val) 0.20800/0.10433. Took 0.09 sec\n",
            "Epoch 1530, Loss(train/val) 0.20004/0.12363. Took 0.11 sec\n",
            "Epoch 1531, Loss(train/val) 0.21160/0.12140. Took 0.10 sec\n",
            "Epoch 1532, Loss(train/val) 0.19763/0.11542. Took 0.09 sec\n",
            "Epoch 1533, Loss(train/val) 0.20046/0.10324. Took 0.10 sec\n",
            "Epoch 1534, Loss(train/val) 0.20440/0.10068. Took 0.10 sec\n",
            "Epoch 1535, Loss(train/val) 0.21993/0.09906. Took 0.10 sec\n",
            "Epoch 1536, Loss(train/val) 0.19390/0.10017. Took 0.10 sec\n",
            "Epoch 1537, Loss(train/val) 0.20911/0.10956. Took 0.10 sec\n",
            "Epoch 1538, Loss(train/val) 0.19377/0.12452. Took 0.11 sec\n",
            "Epoch 1539, Loss(train/val) 0.18731/0.14965. Took 0.14 sec\n",
            "Epoch 1540, Loss(train/val) 0.19791/0.17406. Took 0.43 sec\n",
            "Epoch 1541, Loss(train/val) 0.20020/0.18567. Took 0.36 sec\n",
            "Epoch 1542, Loss(train/val) 0.20371/0.18634. Took 0.24 sec\n",
            "Epoch 1543, Loss(train/val) 0.19539/0.18666. Took 0.10 sec\n",
            "Epoch 1544, Loss(train/val) 0.19735/0.16761. Took 0.10 sec\n",
            "Epoch 1545, Loss(train/val) 0.21202/0.15823. Took 0.10 sec\n",
            "Epoch 1546, Loss(train/val) 0.20085/0.14760. Took 0.09 sec\n",
            "Epoch 1547, Loss(train/val) 0.20345/0.11556. Took 0.10 sec\n",
            "Epoch 1548, Loss(train/val) 0.22126/0.10537. Took 0.10 sec\n",
            "Epoch 1549, Loss(train/val) 0.21376/0.10071. Took 0.10 sec\n",
            "Epoch 1550, Loss(train/val) 0.21094/0.10035. Took 0.10 sec\n",
            "Epoch 1551, Loss(train/val) 0.19120/0.10286. Took 0.12 sec\n",
            "Epoch 1552, Loss(train/val) 0.22111/0.11731. Took 0.10 sec\n",
            "Epoch 1553, Loss(train/val) 0.19139/0.12539. Took 0.10 sec\n",
            "Epoch 1554, Loss(train/val) 0.21264/0.13146. Took 0.10 sec\n",
            "Epoch 1555, Loss(train/val) 0.18817/0.12973. Took 0.10 sec\n",
            "Epoch 1556, Loss(train/val) 0.19528/0.11131. Took 0.09 sec\n",
            "Epoch 1557, Loss(train/val) 0.20891/0.10675. Took 0.10 sec\n",
            "Epoch 1558, Loss(train/val) 0.19938/0.10348. Took 0.10 sec\n",
            "Epoch 1559, Loss(train/val) 0.18483/0.10102. Took 0.10 sec\n",
            "Epoch 1560, Loss(train/val) 0.24159/0.10756. Took 0.10 sec\n",
            "Epoch 1561, Loss(train/val) 0.20067/0.13798. Took 0.11 sec\n",
            "Epoch 1562, Loss(train/val) 0.18652/0.15384. Took 0.10 sec\n",
            "Epoch 1563, Loss(train/val) 0.21555/0.13126. Took 0.11 sec\n",
            "Epoch 1564, Loss(train/val) 0.19533/0.12219. Took 0.09 sec\n",
            "Epoch 1565, Loss(train/val) 0.21003/0.10622. Took 0.10 sec\n",
            "Epoch 1566, Loss(train/val) 0.19230/0.10128. Took 0.10 sec\n",
            "Epoch 1567, Loss(train/val) 0.19652/0.10141. Took 0.09 sec\n",
            "Epoch 1568, Loss(train/val) 0.19134/0.11065. Took 0.11 sec\n",
            "Epoch 1569, Loss(train/val) 0.22491/0.12865. Took 0.09 sec\n",
            "Epoch 1570, Loss(train/val) 0.20505/0.12416. Took 0.09 sec\n",
            "Epoch 1571, Loss(train/val) 0.19244/0.12133. Took 0.11 sec\n",
            "Epoch 1572, Loss(train/val) 0.18756/0.10823. Took 0.10 sec\n",
            "Epoch 1573, Loss(train/val) 0.19236/0.10168. Took 0.10 sec\n",
            "Epoch 1574, Loss(train/val) 0.21289/0.10702. Took 0.10 sec\n",
            "Epoch 1575, Loss(train/val) 0.20284/0.11909. Took 0.10 sec\n",
            "Epoch 1576, Loss(train/val) 0.19823/0.12628. Took 0.09 sec\n",
            "Epoch 1577, Loss(train/val) 0.21292/0.12153. Took 0.10 sec\n",
            "Epoch 1578, Loss(train/val) 0.19853/0.14997. Took 0.10 sec\n",
            "Epoch 1579, Loss(train/val) 0.17658/0.15584. Took 0.10 sec\n",
            "Epoch 1580, Loss(train/val) 0.22291/0.14656. Took 0.10 sec\n",
            "Epoch 1581, Loss(train/val) 0.20786/0.15185. Took 0.10 sec\n",
            "Epoch 1582, Loss(train/val) 0.20289/0.14696. Took 0.11 sec\n",
            "Epoch 1583, Loss(train/val) 0.19298/0.14532. Took 0.10 sec\n",
            "Epoch 1584, Loss(train/val) 0.19513/0.14462. Took 0.10 sec\n",
            "Epoch 1585, Loss(train/val) 0.20212/0.14257. Took 0.10 sec\n",
            "Epoch 1586, Loss(train/val) 0.19555/0.13561. Took 0.10 sec\n",
            "Epoch 1587, Loss(train/val) 0.19364/0.12187. Took 0.10 sec\n",
            "Epoch 1588, Loss(train/val) 0.20909/0.10623. Took 0.10 sec\n",
            "Epoch 1589, Loss(train/val) 0.21245/0.09990. Took 0.10 sec\n",
            "Epoch 1590, Loss(train/val) 0.19704/0.10831. Took 0.09 sec\n",
            "Epoch 1591, Loss(train/val) 0.20960/0.11510. Took 0.10 sec\n",
            "Epoch 1592, Loss(train/val) 0.19909/0.12644. Took 0.11 sec\n",
            "Epoch 1593, Loss(train/val) 0.18265/0.11978. Took 0.10 sec\n",
            "Epoch 1594, Loss(train/val) 0.18809/0.12013. Took 0.10 sec\n",
            "Epoch 1595, Loss(train/val) 0.20466/0.11770. Took 0.10 sec\n",
            "Epoch 1596, Loss(train/val) 0.19477/0.10179. Took 0.10 sec\n",
            "Epoch 1597, Loss(train/val) 0.19876/0.10281. Took 0.10 sec\n",
            "Epoch 1598, Loss(train/val) 0.21017/0.11506. Took 0.10 sec\n",
            "Epoch 1599, Loss(train/val) 0.19252/0.14391. Took 0.10 sec\n",
            "Epoch 1600, Loss(train/val) 0.18054/0.14336. Took 0.09 sec\n",
            "Epoch 1601, Loss(train/val) 0.21224/0.14543. Took 0.10 sec\n",
            "Epoch 1602, Loss(train/val) 0.20858/0.12911. Took 0.11 sec\n",
            "Epoch 1603, Loss(train/val) 0.18189/0.11724. Took 0.10 sec\n",
            "Epoch 1604, Loss(train/val) 0.20306/0.10187. Took 0.10 sec\n",
            "Epoch 1605, Loss(train/val) 0.21182/0.10513. Took 0.10 sec\n",
            "Epoch 1606, Loss(train/val) 0.20483/0.10400. Took 0.10 sec\n",
            "Epoch 1607, Loss(train/val) 0.21921/0.10469. Took 0.10 sec\n",
            "Epoch 1608, Loss(train/val) 0.19628/0.11972. Took 0.10 sec\n",
            "Epoch 1609, Loss(train/val) 0.20105/0.14277. Took 0.10 sec\n",
            "Epoch 1610, Loss(train/val) 0.20860/0.14709. Took 0.09 sec\n",
            "Epoch 1611, Loss(train/val) 0.19065/0.12941. Took 0.11 sec\n",
            "Epoch 1612, Loss(train/val) 0.20136/0.10478. Took 0.11 sec\n",
            "Epoch 1613, Loss(train/val) 0.21503/0.10160. Took 0.09 sec\n",
            "Epoch 1614, Loss(train/val) 0.20182/0.10121. Took 0.10 sec\n",
            "Epoch 1615, Loss(train/val) 0.19063/0.10546. Took 0.10 sec\n",
            "Epoch 1616, Loss(train/val) 0.20283/0.10836. Took 0.12 sec\n",
            "Epoch 1617, Loss(train/val) 0.19810/0.10707. Took 0.09 sec\n",
            "Epoch 1618, Loss(train/val) 0.21271/0.11932. Took 0.09 sec\n",
            "Epoch 1619, Loss(train/val) 0.19041/0.13680. Took 0.10 sec\n",
            "Epoch 1620, Loss(train/val) 0.20151/0.12663. Took 0.09 sec\n",
            "Epoch 1621, Loss(train/val) 0.19237/0.11342. Took 0.10 sec\n",
            "Epoch 1622, Loss(train/val) 0.20770/0.10346. Took 0.11 sec\n",
            "Epoch 1623, Loss(train/val) 0.18481/0.10553. Took 0.09 sec\n",
            "Epoch 1624, Loss(train/val) 0.20509/0.10258. Took 0.10 sec\n",
            "Epoch 1625, Loss(train/val) 0.19394/0.10325. Took 0.10 sec\n",
            "Epoch 1626, Loss(train/val) 0.18903/0.10812. Took 0.10 sec\n",
            "Epoch 1627, Loss(train/val) 0.19581/0.11566. Took 0.09 sec\n",
            "Epoch 1628, Loss(train/val) 0.19233/0.11900. Took 0.10 sec\n",
            "Epoch 1629, Loss(train/val) 0.19625/0.11406. Took 0.10 sec\n",
            "Epoch 1630, Loss(train/val) 0.21246/0.10276. Took 0.10 sec\n",
            "Epoch 1631, Loss(train/val) 0.18217/0.10116. Took 0.10 sec\n",
            "Epoch 1632, Loss(train/val) 0.19520/0.10125. Took 0.10 sec\n",
            "Epoch 1633, Loss(train/val) 0.20452/0.10676. Took 0.11 sec\n",
            "Epoch 1634, Loss(train/val) 0.19540/0.10250. Took 0.10 sec\n",
            "Epoch 1635, Loss(train/val) 0.20202/0.10103. Took 0.10 sec\n",
            "Epoch 1636, Loss(train/val) 0.19809/0.10193. Took 0.10 sec\n",
            "Epoch 1637, Loss(train/val) 0.21929/0.10134. Took 0.10 sec\n",
            "Epoch 1638, Loss(train/val) 0.18963/0.10029. Took 0.10 sec\n",
            "Epoch 1639, Loss(train/val) 0.21093/0.10155. Took 0.10 sec\n",
            "Epoch 1640, Loss(train/val) 0.19069/0.09981. Took 0.10 sec\n",
            "Epoch 1641, Loss(train/val) 0.19150/0.09915. Took 0.10 sec\n",
            "Epoch 1642, Loss(train/val) 0.20302/0.10478. Took 0.10 sec\n",
            "Epoch 1643, Loss(train/val) 0.19077/0.10404. Took 0.11 sec\n",
            "Epoch 1644, Loss(train/val) 0.21323/0.10254. Took 0.09 sec\n",
            "Epoch 1645, Loss(train/val) 0.19437/0.10903. Took 0.11 sec\n",
            "Epoch 1646, Loss(train/val) 0.19253/0.11150. Took 0.09 sec\n",
            "Epoch 1647, Loss(train/val) 0.20893/0.12044. Took 0.09 sec\n",
            "Epoch 1648, Loss(train/val) 0.18703/0.13197. Took 0.10 sec\n",
            "Epoch 1649, Loss(train/val) 0.20632/0.11648. Took 0.10 sec\n",
            "Epoch 1650, Loss(train/val) 0.19455/0.10321. Took 0.09 sec\n",
            "Epoch 1651, Loss(train/val) 0.20256/0.10078. Took 0.10 sec\n",
            "Epoch 1652, Loss(train/val) 0.20924/0.10034. Took 0.10 sec\n",
            "Epoch 1653, Loss(train/val) 0.19573/0.09945. Took 0.12 sec\n",
            "Epoch 1654, Loss(train/val) 0.19512/0.10092. Took 0.10 sec\n",
            "Epoch 1655, Loss(train/val) 0.18697/0.10076. Took 0.10 sec\n",
            "Epoch 1656, Loss(train/val) 0.19553/0.10077. Took 0.10 sec\n",
            "Epoch 1657, Loss(train/val) 0.19020/0.09987. Took 0.10 sec\n",
            "Epoch 1658, Loss(train/val) 0.19554/0.09972. Took 0.09 sec\n",
            "Epoch 1659, Loss(train/val) 0.20596/0.10149. Took 0.10 sec\n",
            "Epoch 1660, Loss(train/val) 0.21470/0.10695. Took 0.10 sec\n",
            "Epoch 1661, Loss(train/val) 0.20152/0.10676. Took 0.10 sec\n",
            "Epoch 1662, Loss(train/val) 0.20063/0.11583. Took 0.10 sec\n",
            "Epoch 1663, Loss(train/val) 0.20633/0.12038. Took 0.10 sec\n",
            "Epoch 1664, Loss(train/val) 0.20307/0.11222. Took 0.11 sec\n",
            "Epoch 1665, Loss(train/val) 0.19916/0.10069. Took 0.10 sec\n",
            "Epoch 1666, Loss(train/val) 0.19048/0.10031. Took 0.10 sec\n",
            "Epoch 1667, Loss(train/val) 0.19538/0.11537. Took 0.09 sec\n",
            "Epoch 1668, Loss(train/val) 0.20683/0.12468. Took 0.10 sec\n",
            "Epoch 1669, Loss(train/val) 0.20790/0.10959. Took 0.10 sec\n",
            "Epoch 1670, Loss(train/val) 0.19979/0.10322. Took 0.10 sec\n",
            "Epoch 1671, Loss(train/val) 0.19358/0.11205. Took 0.10 sec\n",
            "Epoch 1672, Loss(train/val) 0.19334/0.11373. Took 0.10 sec\n",
            "Epoch 1673, Loss(train/val) 0.18914/0.10045. Took 0.10 sec\n",
            "Epoch 1674, Loss(train/val) 0.19937/0.13117. Took 0.11 sec\n",
            "Epoch 1675, Loss(train/val) 0.18989/0.15686. Took 0.09 sec\n",
            "Epoch 1676, Loss(train/val) 0.19765/0.14825. Took 0.10 sec\n",
            "Epoch 1677, Loss(train/val) 0.20016/0.12376. Took 0.10 sec\n",
            "Epoch 1678, Loss(train/val) 0.19518/0.10778. Took 0.09 sec\n",
            "Epoch 1679, Loss(train/val) 0.18705/0.10180. Took 0.10 sec\n",
            "Epoch 1680, Loss(train/val) 0.19287/0.10249. Took 0.10 sec\n",
            "Epoch 1681, Loss(train/val) 0.18892/0.11821. Took 0.10 sec\n",
            "Epoch 1682, Loss(train/val) 0.18970/0.11437. Took 0.10 sec\n",
            "Epoch 1683, Loss(train/val) 0.20757/0.10542. Took 0.10 sec\n",
            "Epoch 1684, Loss(train/val) 0.19333/0.09953. Took 0.10 sec\n",
            "Epoch 1685, Loss(train/val) 0.22077/0.09986. Took 0.10 sec\n",
            "Epoch 1686, Loss(train/val) 0.18673/0.10655. Took 0.10 sec\n",
            "Epoch 1687, Loss(train/val) 0.19149/0.10708. Took 0.10 sec\n",
            "Epoch 1688, Loss(train/val) 0.20283/0.10111. Took 0.10 sec\n",
            "Epoch 1689, Loss(train/val) 0.20432/0.10282. Took 0.10 sec\n",
            "Epoch 1690, Loss(train/val) 0.19421/0.11644. Took 0.10 sec\n",
            "Epoch 1691, Loss(train/val) 0.19067/0.11895. Took 0.10 sec\n",
            "Epoch 1692, Loss(train/val) 0.18528/0.11752. Took 0.10 sec\n",
            "Epoch 1693, Loss(train/val) 0.19784/0.11035. Took 0.10 sec\n",
            "Epoch 1694, Loss(train/val) 0.19308/0.10860. Took 0.11 sec\n",
            "Epoch 1695, Loss(train/val) 0.18549/0.10561. Took 0.10 sec\n",
            "Epoch 1696, Loss(train/val) 0.19953/0.10108. Took 0.09 sec\n",
            "Epoch 1697, Loss(train/val) 0.19974/0.10672. Took 0.11 sec\n",
            "Epoch 1698, Loss(train/val) 0.21707/0.11180. Took 0.10 sec\n",
            "Epoch 1699, Loss(train/val) 0.18746/0.10923. Took 0.09 sec\n",
            "Epoch 1700, Loss(train/val) 0.18975/0.10449. Took 0.10 sec\n",
            "Epoch 1701, Loss(train/val) 0.19815/0.10111. Took 0.10 sec\n",
            "Epoch 1702, Loss(train/val) 0.19440/0.10208. Took 0.10 sec\n",
            "Epoch 1703, Loss(train/val) 0.20424/0.10785. Took 0.11 sec\n",
            "Epoch 1704, Loss(train/val) 0.18514/0.11464. Took 0.09 sec\n",
            "Epoch 1705, Loss(train/val) 0.18966/0.11971. Took 0.10 sec\n",
            "Epoch 1706, Loss(train/val) 0.20933/0.12911. Took 0.11 sec\n",
            "Epoch 1707, Loss(train/val) 0.17759/0.13238. Took 0.10 sec\n",
            "Epoch 1708, Loss(train/val) 0.21150/0.13012. Took 0.09 sec\n",
            "Epoch 1709, Loss(train/val) 0.21195/0.13415. Took 0.10 sec\n",
            "Epoch 1710, Loss(train/val) 0.18590/0.16221. Took 0.09 sec\n",
            "Epoch 1711, Loss(train/val) 0.18381/0.17596. Took 0.09 sec\n",
            "Epoch 1712, Loss(train/val) 0.17650/0.18877. Took 0.10 sec\n",
            "Epoch 1713, Loss(train/val) 0.20367/0.18086. Took 0.10 sec\n",
            "Epoch 1714, Loss(train/val) 0.19283/0.15843. Took 0.10 sec\n",
            "Epoch 1715, Loss(train/val) 0.18686/0.14916. Took 0.11 sec\n",
            "Epoch 1716, Loss(train/val) 0.20020/0.12115. Took 0.10 sec\n",
            "Epoch 1717, Loss(train/val) 0.20549/0.10412. Took 0.10 sec\n",
            "Epoch 1718, Loss(train/val) 0.20903/0.09966. Took 0.10 sec\n",
            "Epoch 1719, Loss(train/val) 0.21419/0.09949. Took 0.10 sec\n",
            "Epoch 1720, Loss(train/val) 0.19042/0.10148. Took 0.09 sec\n",
            "Epoch 1721, Loss(train/val) 0.22127/0.10354. Took 0.12 sec\n",
            "Epoch 1722, Loss(train/val) 0.19938/0.09897. Took 0.10 sec\n",
            "Epoch 1723, Loss(train/val) 0.18405/0.10643. Took 0.10 sec\n",
            "Epoch 1724, Loss(train/val) 0.20178/0.11139. Took 0.10 sec\n",
            "Epoch 1725, Loss(train/val) 0.19039/0.10826. Took 0.11 sec\n",
            "Epoch 1726, Loss(train/val) 0.19867/0.09757. Took 0.10 sec\n",
            "Epoch 1727, Loss(train/val) 0.19925/0.09692. Took 0.10 sec\n",
            "Epoch 1728, Loss(train/val) 0.19264/0.10157. Took 0.10 sec\n",
            "Epoch 1729, Loss(train/val) 0.19864/0.10386. Took 0.10 sec\n",
            "Epoch 1730, Loss(train/val) 0.20339/0.11577. Took 0.10 sec\n",
            "Epoch 1731, Loss(train/val) 0.19946/0.13013. Took 0.10 sec\n",
            "Epoch 1732, Loss(train/val) 0.18851/0.13366. Took 0.11 sec\n",
            "Epoch 1733, Loss(train/val) 0.18827/0.11364. Took 0.10 sec\n",
            "Epoch 1734, Loss(train/val) 0.20632/0.10419. Took 0.09 sec\n",
            "Epoch 1735, Loss(train/val) 0.18616/0.10174. Took 0.11 sec\n",
            "Epoch 1736, Loss(train/val) 0.20175/0.09897. Took 0.10 sec\n",
            "Epoch 1737, Loss(train/val) 0.18894/0.10994. Took 0.10 sec\n",
            "Epoch 1738, Loss(train/val) 0.19264/0.11015. Took 0.10 sec\n",
            "Epoch 1739, Loss(train/val) 0.18425/0.11482. Took 0.09 sec\n",
            "Epoch 1740, Loss(train/val) 0.21803/0.11848. Took 0.09 sec\n",
            "Epoch 1741, Loss(train/val) 0.20447/0.11451. Took 0.10 sec\n",
            "Epoch 1742, Loss(train/val) 0.18864/0.09768. Took 0.10 sec\n",
            "Epoch 1743, Loss(train/val) 0.19442/0.10969. Took 0.10 sec\n",
            "Epoch 1744, Loss(train/val) 0.19202/0.14077. Took 0.10 sec\n",
            "Epoch 1745, Loss(train/val) 0.18250/0.14185. Took 0.09 sec\n",
            "Epoch 1746, Loss(train/val) 0.19256/0.15579. Took 0.11 sec\n",
            "Epoch 1747, Loss(train/val) 0.19826/0.13837. Took 0.10 sec\n",
            "Epoch 1748, Loss(train/val) 0.19465/0.11688. Took 0.09 sec\n",
            "Epoch 1749, Loss(train/val) 0.21733/0.09815. Took 0.10 sec\n",
            "Epoch 1750, Loss(train/val) 0.19041/0.09841. Took 0.10 sec\n",
            "Epoch 1751, Loss(train/val) 0.20789/0.09945. Took 0.10 sec\n",
            "Epoch 1752, Loss(train/val) 0.18945/0.09635. Took 0.10 sec\n",
            "Epoch 1753, Loss(train/val) 0.17990/0.10042. Took 0.10 sec\n",
            "Epoch 1754, Loss(train/val) 0.18550/0.10530. Took 0.10 sec\n",
            "Epoch 1755, Loss(train/val) 0.19231/0.10240. Took 0.10 sec\n",
            "Epoch 1756, Loss(train/val) 0.20482/0.10208. Took 0.11 sec\n",
            "Epoch 1757, Loss(train/val) 0.19850/0.10774. Took 0.11 sec\n",
            "Epoch 1758, Loss(train/val) 0.20549/0.10931. Took 0.10 sec\n",
            "Epoch 1759, Loss(train/val) 0.19690/0.11669. Took 0.10 sec\n",
            "Epoch 1760, Loss(train/val) 0.21092/0.11852. Took 0.10 sec\n",
            "Epoch 1761, Loss(train/val) 0.18249/0.11412. Took 0.10 sec\n",
            "Epoch 1762, Loss(train/val) 0.20097/0.10932. Took 0.10 sec\n",
            "Epoch 1763, Loss(train/val) 0.19669/0.10381. Took 0.10 sec\n",
            "Epoch 1764, Loss(train/val) 0.18936/0.09842. Took 0.10 sec\n",
            "Epoch 1765, Loss(train/val) 0.20364/0.11010. Took 0.09 sec\n",
            "Epoch 1766, Loss(train/val) 0.18227/0.13266. Took 0.11 sec\n",
            "Epoch 1767, Loss(train/val) 0.19468/0.14511. Took 0.09 sec\n",
            "Epoch 1768, Loss(train/val) 0.17987/0.15631. Took 0.09 sec\n",
            "Epoch 1769, Loss(train/val) 0.20745/0.13682. Took 0.10 sec\n",
            "Epoch 1770, Loss(train/val) 0.20455/0.11652. Took 0.10 sec\n",
            "Epoch 1771, Loss(train/val) 0.19351/0.11165. Took 0.09 sec\n",
            "Epoch 1772, Loss(train/val) 0.20325/0.10607. Took 0.11 sec\n",
            "Epoch 1773, Loss(train/val) 0.18770/0.09760. Took 0.09 sec\n",
            "Epoch 1774, Loss(train/val) 0.17548/0.10262. Took 0.09 sec\n",
            "Epoch 1775, Loss(train/val) 0.20566/0.11519. Took 0.10 sec\n",
            "Epoch 1776, Loss(train/val) 0.19827/0.14040. Took 0.10 sec\n",
            "Epoch 1777, Loss(train/val) 0.20151/0.15447. Took 0.11 sec\n",
            "Epoch 1778, Loss(train/val) 0.18522/0.14387. Took 0.10 sec\n",
            "Epoch 1779, Loss(train/val) 0.20483/0.13299. Took 0.10 sec\n",
            "Epoch 1780, Loss(train/val) 0.18911/0.11128. Took 0.11 sec\n",
            "Epoch 1781, Loss(train/val) 0.18494/0.09728. Took 0.10 sec\n",
            "Epoch 1782, Loss(train/val) 0.20755/0.09679. Took 0.10 sec\n",
            "Epoch 1783, Loss(train/val) 0.20741/0.09689. Took 0.10 sec\n",
            "Epoch 1784, Loss(train/val) 0.20191/0.09733. Took 0.10 sec\n",
            "Epoch 1785, Loss(train/val) 0.19083/0.11587. Took 0.09 sec\n",
            "Epoch 1786, Loss(train/val) 0.17989/0.12009. Took 0.10 sec\n",
            "Epoch 1787, Loss(train/val) 0.18483/0.11701. Took 0.10 sec\n",
            "Epoch 1788, Loss(train/val) 0.18570/0.09895. Took 0.09 sec\n",
            "Epoch 1789, Loss(train/val) 0.19527/0.09637. Took 0.10 sec\n",
            "Epoch 1790, Loss(train/val) 0.19712/0.09736. Took 0.10 sec\n",
            "Epoch 1791, Loss(train/val) 0.18151/0.09791. Took 0.09 sec\n",
            "Epoch 1792, Loss(train/val) 0.19379/0.10044. Took 0.10 sec\n",
            "Epoch 1793, Loss(train/val) 0.19389/0.09822. Took 0.09 sec\n",
            "Epoch 1794, Loss(train/val) 0.18508/0.10467. Took 0.10 sec\n",
            "Epoch 1795, Loss(train/val) 0.18542/0.12781. Took 0.10 sec\n",
            "Epoch 1796, Loss(train/val) 0.18126/0.12643. Took 0.10 sec\n",
            "Epoch 1797, Loss(train/val) 0.18882/0.13008. Took 0.10 sec\n",
            "Epoch 1798, Loss(train/val) 0.19027/0.12999. Took 0.10 sec\n",
            "Epoch 1799, Loss(train/val) 0.21058/0.11644. Took 0.11 sec\n",
            "Epoch 1800, Loss(train/val) 0.19144/0.11301. Took 0.10 sec\n",
            "Epoch 1801, Loss(train/val) 0.21199/0.11285. Took 0.09 sec\n",
            "Epoch 1802, Loss(train/val) 0.19866/0.10603. Took 0.11 sec\n",
            "Epoch 1803, Loss(train/val) 0.18710/0.09925. Took 0.09 sec\n",
            "Epoch 1804, Loss(train/val) 0.18182/0.09680. Took 0.09 sec\n",
            "Epoch 1805, Loss(train/val) 0.19968/0.11286. Took 0.10 sec\n",
            "Epoch 1806, Loss(train/val) 0.19293/0.12716. Took 0.10 sec\n",
            "Epoch 1807, Loss(train/val) 0.21615/0.13686. Took 0.10 sec\n",
            "Epoch 1808, Loss(train/val) 0.20107/0.13307. Took 0.10 sec\n",
            "Epoch 1809, Loss(train/val) 0.21475/0.11886. Took 0.09 sec\n",
            "Epoch 1810, Loss(train/val) 0.18534/0.11048. Took 0.10 sec\n",
            "Epoch 1811, Loss(train/val) 0.19619/0.10252. Took 0.10 sec\n",
            "Epoch 1812, Loss(train/val) 0.18670/0.09600. Took 0.10 sec\n",
            "Epoch 1813, Loss(train/val) 0.19317/0.09793. Took 0.09 sec\n",
            "Epoch 1814, Loss(train/val) 0.20219/0.09714. Took 0.10 sec\n",
            "Epoch 1815, Loss(train/val) 0.17777/0.10028. Took 0.09 sec\n",
            "Epoch 1816, Loss(train/val) 0.19344/0.09900. Took 0.10 sec\n",
            "Epoch 1817, Loss(train/val) 0.21328/0.09656. Took 0.10 sec\n",
            "Epoch 1818, Loss(train/val) 0.19879/0.09676. Took 0.10 sec\n",
            "Epoch 1819, Loss(train/val) 0.20695/0.09599. Took 0.10 sec\n",
            "Epoch 1820, Loss(train/val) 0.17823/0.09726. Took 0.11 sec\n",
            "Epoch 1821, Loss(train/val) 0.18329/0.10789. Took 0.09 sec\n",
            "Epoch 1822, Loss(train/val) 0.19675/0.10089. Took 0.09 sec\n",
            "Epoch 1823, Loss(train/val) 0.18966/0.09743. Took 0.10 sec\n",
            "Epoch 1824, Loss(train/val) 0.22699/0.09777. Took 0.10 sec\n",
            "Epoch 1825, Loss(train/val) 0.19244/0.10326. Took 0.09 sec\n",
            "Epoch 1826, Loss(train/val) 0.18387/0.10468. Took 0.12 sec\n",
            "Epoch 1827, Loss(train/val) 0.19593/0.11029. Took 0.10 sec\n",
            "Epoch 1828, Loss(train/val) 0.20394/0.10274. Took 0.11 sec\n",
            "Epoch 1829, Loss(train/val) 0.18599/0.09875. Took 0.10 sec\n",
            "Epoch 1830, Loss(train/val) 0.20648/0.09839. Took 0.09 sec\n",
            "Epoch 1831, Loss(train/val) 0.19273/0.09972. Took 0.10 sec\n",
            "Epoch 1832, Loss(train/val) 0.18954/0.09740. Took 0.09 sec\n",
            "Epoch 1833, Loss(train/val) 0.18851/0.09792. Took 0.10 sec\n",
            "Epoch 1834, Loss(train/val) 0.18471/0.11557. Took 0.10 sec\n",
            "Epoch 1835, Loss(train/val) 0.17370/0.13310. Took 0.10 sec\n",
            "Epoch 1836, Loss(train/val) 0.20656/0.15243. Took 0.09 sec\n",
            "Epoch 1837, Loss(train/val) 0.19147/0.15321. Took 0.10 sec\n",
            "Epoch 1838, Loss(train/val) 0.20836/0.11914. Took 0.10 sec\n",
            "Epoch 1839, Loss(train/val) 0.18889/0.09693. Took 0.09 sec\n",
            "Epoch 1840, Loss(train/val) 0.19162/0.10234. Took 0.10 sec\n",
            "Epoch 1841, Loss(train/val) 0.18334/0.10529. Took 0.09 sec\n",
            "Epoch 1842, Loss(train/val) 0.20656/0.10113. Took 0.10 sec\n",
            "Epoch 1843, Loss(train/val) 0.20590/0.10481. Took 0.10 sec\n",
            "Epoch 1844, Loss(train/val) 0.18945/0.11279. Took 0.10 sec\n",
            "Epoch 1845, Loss(train/val) 0.18638/0.10347. Took 0.10 sec\n",
            "Epoch 1846, Loss(train/val) 0.18263/0.09956. Took 0.10 sec\n",
            "Epoch 1847, Loss(train/val) 0.19456/0.09688. Took 0.10 sec\n",
            "Epoch 1848, Loss(train/val) 0.23443/0.09644. Took 0.10 sec\n",
            "Epoch 1849, Loss(train/val) 0.18889/0.09836. Took 0.11 sec\n",
            "Epoch 1850, Loss(train/val) 0.19352/0.09875. Took 0.10 sec\n",
            "Epoch 1851, Loss(train/val) 0.20061/0.09789. Took 0.10 sec\n",
            "Epoch 1852, Loss(train/val) 0.19631/0.09663. Took 0.10 sec\n",
            "Epoch 1853, Loss(train/val) 0.18241/0.09945. Took 0.10 sec\n",
            "Epoch 1854, Loss(train/val) 0.18131/0.10392. Took 0.10 sec\n",
            "Epoch 1855, Loss(train/val) 0.19386/0.10128. Took 0.10 sec\n",
            "Epoch 1856, Loss(train/val) 0.19369/0.09649. Took 0.09 sec\n",
            "Epoch 1857, Loss(train/val) 0.18839/0.09722. Took 0.10 sec\n",
            "Epoch 1858, Loss(train/val) 0.18284/0.09608. Took 0.10 sec\n",
            "Epoch 1859, Loss(train/val) 0.21491/0.09889. Took 0.11 sec\n",
            "Epoch 1860, Loss(train/val) 0.17988/0.10027. Took 0.10 sec\n",
            "Epoch 1861, Loss(train/val) 0.19950/0.09663. Took 0.10 sec\n",
            "Epoch 1862, Loss(train/val) 0.21341/0.09738. Took 0.09 sec\n",
            "Epoch 1863, Loss(train/val) 0.18912/0.09681. Took 0.11 sec\n",
            "Epoch 1864, Loss(train/val) 0.17548/0.09896. Took 0.10 sec\n",
            "Epoch 1865, Loss(train/val) 0.21125/0.10213. Took 0.09 sec\n",
            "Epoch 1866, Loss(train/val) 0.19336/0.10793. Took 0.10 sec\n",
            "Epoch 1867, Loss(train/val) 0.19607/0.11133. Took 0.10 sec\n",
            "Epoch 1868, Loss(train/val) 0.19703/0.11407. Took 0.10 sec\n",
            "Epoch 1869, Loss(train/val) 0.19711/0.11029. Took 0.11 sec\n",
            "Epoch 1870, Loss(train/val) 0.19232/0.12382. Took 0.09 sec\n",
            "Epoch 1871, Loss(train/val) 0.21950/0.11219. Took 0.09 sec\n",
            "Epoch 1872, Loss(train/val) 0.18911/0.09944. Took 0.17 sec\n",
            "Epoch 1873, Loss(train/val) 0.19760/0.09579. Took 0.42 sec\n",
            "Epoch 1874, Loss(train/val) 0.19479/0.09693. Took 0.29 sec\n",
            "Epoch 1875, Loss(train/val) 0.19293/0.09851. Took 0.20 sec\n",
            "Epoch 1876, Loss(train/val) 0.19162/0.09677. Took 0.10 sec\n",
            "Epoch 1877, Loss(train/val) 0.19175/0.09598. Took 0.10 sec\n",
            "Epoch 1878, Loss(train/val) 0.20879/0.09789. Took 0.09 sec\n",
            "Epoch 1879, Loss(train/val) 0.21554/0.09426. Took 0.10 sec\n",
            "Epoch 1880, Loss(train/val) 0.20562/0.10171. Took 0.10 sec\n",
            "Epoch 1881, Loss(train/val) 0.18424/0.10695. Took 0.09 sec\n",
            "Epoch 1882, Loss(train/val) 0.18624/0.12573. Took 0.10 sec\n",
            "Epoch 1883, Loss(train/val) 0.19426/0.13322. Took 0.11 sec\n",
            "Epoch 1884, Loss(train/val) 0.18736/0.15135. Took 0.09 sec\n",
            "Epoch 1885, Loss(train/val) 0.19854/0.15414. Took 0.10 sec\n",
            "Epoch 1886, Loss(train/val) 0.21871/0.13278. Took 0.10 sec\n",
            "Epoch 1887, Loss(train/val) 0.19142/0.11509. Took 0.10 sec\n",
            "Epoch 1888, Loss(train/val) 0.18464/0.11186. Took 0.10 sec\n",
            "Epoch 1889, Loss(train/val) 0.18509/0.10092. Took 0.10 sec\n",
            "Epoch 1890, Loss(train/val) 0.19655/0.09617. Took 0.09 sec\n",
            "Epoch 1891, Loss(train/val) 0.19688/0.09616. Took 0.10 sec\n",
            "Epoch 1892, Loss(train/val) 0.17956/0.10723. Took 0.10 sec\n",
            "Epoch 1893, Loss(train/val) 0.19452/0.11583. Took 0.11 sec\n",
            "Epoch 1894, Loss(train/val) 0.19228/0.12304. Took 0.10 sec\n",
            "Epoch 1895, Loss(train/val) 0.19220/0.10458. Took 0.09 sec\n",
            "Epoch 1896, Loss(train/val) 0.19548/0.09554. Took 0.10 sec\n",
            "Epoch 1897, Loss(train/val) 0.19473/0.09510. Took 0.09 sec\n",
            "Epoch 1898, Loss(train/val) 0.19573/0.09738. Took 0.10 sec\n",
            "Epoch 1899, Loss(train/val) 0.19208/0.09994. Took 0.10 sec\n",
            "Epoch 1900, Loss(train/val) 0.19822/0.09808. Took 0.10 sec\n",
            "Epoch 1901, Loss(train/val) 0.21667/0.10960. Took 0.10 sec\n",
            "Epoch 1902, Loss(train/val) 0.19089/0.11909. Took 0.10 sec\n",
            "Epoch 1903, Loss(train/val) 0.18806/0.12916. Took 0.09 sec\n",
            "Epoch 1904, Loss(train/val) 0.17908/0.13317. Took 0.11 sec\n",
            "Epoch 1905, Loss(train/val) 0.18482/0.13054. Took 0.10 sec\n",
            "Epoch 1906, Loss(train/val) 0.19068/0.10497. Took 0.09 sec\n",
            "Epoch 1907, Loss(train/val) 0.18640/0.09757. Took 0.09 sec\n",
            "Epoch 1908, Loss(train/val) 0.17159/0.09648. Took 0.11 sec\n",
            "Epoch 1909, Loss(train/val) 0.18372/0.09464. Took 0.10 sec\n",
            "Epoch 1910, Loss(train/val) 0.19443/0.09659. Took 0.09 sec\n",
            "Epoch 1911, Loss(train/val) 0.19174/0.09978. Took 0.10 sec\n",
            "Epoch 1912, Loss(train/val) 0.18102/0.10108. Took 0.10 sec\n",
            "Epoch 1913, Loss(train/val) 0.19928/0.09672. Took 0.09 sec\n",
            "Epoch 1914, Loss(train/val) 0.17144/0.09449. Took 0.11 sec\n",
            "Epoch 1915, Loss(train/val) 0.19709/0.09479. Took 0.09 sec\n",
            "Epoch 1916, Loss(train/val) 0.19992/0.09578. Took 0.09 sec\n",
            "Epoch 1917, Loss(train/val) 0.21157/0.09717. Took 0.11 sec\n",
            "Epoch 1918, Loss(train/val) 0.19898/0.10066. Took 0.09 sec\n",
            "Epoch 1919, Loss(train/val) 0.19310/0.10185. Took 0.09 sec\n",
            "Epoch 1920, Loss(train/val) 0.18380/0.11029. Took 0.10 sec\n",
            "Epoch 1921, Loss(train/val) 0.18078/0.10570. Took 0.09 sec\n",
            "Epoch 1922, Loss(train/val) 0.22073/0.10345. Took 0.09 sec\n",
            "Epoch 1923, Loss(train/val) 0.18070/0.10125. Took 0.10 sec\n",
            "Epoch 1924, Loss(train/val) 0.22702/0.09928. Took 0.10 sec\n",
            "Epoch 1925, Loss(train/val) 0.21935/0.09690. Took 0.13 sec\n",
            "Epoch 1926, Loss(train/val) 0.19267/0.09627. Took 0.10 sec\n",
            "Epoch 1927, Loss(train/val) 0.19058/0.09552. Took 0.09 sec\n",
            "Epoch 1928, Loss(train/val) 0.19215/0.10147. Took 0.10 sec\n",
            "Epoch 1929, Loss(train/val) 0.17706/0.09708. Took 0.09 sec\n",
            "Epoch 1930, Loss(train/val) 0.19779/0.09510. Took 0.10 sec\n",
            "Epoch 1931, Loss(train/val) 0.19084/0.09461. Took 0.10 sec\n",
            "Epoch 1932, Loss(train/val) 0.17880/0.09479. Took 0.10 sec\n",
            "Epoch 1933, Loss(train/val) 0.18726/0.09640. Took 0.09 sec\n",
            "Epoch 1934, Loss(train/val) 0.18761/0.10277. Took 0.11 sec\n",
            "Epoch 1935, Loss(train/val) 0.18731/0.09945. Took 0.10 sec\n",
            "Epoch 1936, Loss(train/val) 0.18333/0.09616. Took 0.10 sec\n",
            "Epoch 1937, Loss(train/val) 0.18204/0.09451. Took 0.10 sec\n",
            "Epoch 1938, Loss(train/val) 0.18780/0.09521. Took 0.10 sec\n",
            "Epoch 1939, Loss(train/val) 0.18784/0.09543. Took 0.10 sec\n",
            "Epoch 1940, Loss(train/val) 0.17694/0.09512. Took 0.09 sec\n",
            "Epoch 1941, Loss(train/val) 0.21971/0.09998. Took 0.10 sec\n",
            "Epoch 1942, Loss(train/val) 0.19037/0.12014. Took 0.10 sec\n",
            "Epoch 1943, Loss(train/val) 0.19709/0.14810. Took 0.10 sec\n",
            "Epoch 1944, Loss(train/val) 0.19227/0.14295. Took 0.10 sec\n",
            "Epoch 1945, Loss(train/val) 0.19495/0.11943. Took 0.11 sec\n",
            "Epoch 1946, Loss(train/val) 0.18417/0.11544. Took 0.10 sec\n",
            "Epoch 1947, Loss(train/val) 0.19616/0.10788. Took 0.10 sec\n",
            "Epoch 1948, Loss(train/val) 0.19433/0.09355. Took 0.09 sec\n",
            "Epoch 1949, Loss(train/val) 0.18439/0.09348. Took 0.10 sec\n",
            "Epoch 1950, Loss(train/val) 0.17829/0.09333. Took 0.10 sec\n",
            "Epoch 1951, Loss(train/val) 0.18986/0.09407. Took 0.09 sec\n",
            "Epoch 1952, Loss(train/val) 0.17615/0.09551. Took 0.10 sec\n",
            "Epoch 1953, Loss(train/val) 0.20345/0.09852. Took 0.09 sec\n",
            "Epoch 1954, Loss(train/val) 0.18141/0.10632. Took 0.09 sec\n",
            "Epoch 1955, Loss(train/val) 0.18084/0.10633. Took 0.11 sec\n",
            "Epoch 1956, Loss(train/val) 0.18416/0.10319. Took 0.10 sec\n",
            "Epoch 1957, Loss(train/val) 0.19243/0.10112. Took 0.10 sec\n",
            "Epoch 1958, Loss(train/val) 0.18653/0.10899. Took 0.10 sec\n",
            "Epoch 1959, Loss(train/val) 0.20806/0.12089. Took 0.10 sec\n",
            "Epoch 1960, Loss(train/val) 0.20130/0.13598. Took 0.09 sec\n",
            "Epoch 1961, Loss(train/val) 0.20081/0.12747. Took 0.10 sec\n",
            "Epoch 1962, Loss(train/val) 0.17777/0.09851. Took 0.10 sec\n",
            "Epoch 1963, Loss(train/val) 0.20269/0.09376. Took 0.09 sec\n",
            "Epoch 1964, Loss(train/val) 0.20632/0.09638. Took 0.10 sec\n",
            "Epoch 1965, Loss(train/val) 0.18888/0.10334. Took 0.10 sec\n",
            "Epoch 1966, Loss(train/val) 0.19472/0.10481. Took 0.11 sec\n",
            "Epoch 1967, Loss(train/val) 0.19037/0.09430. Took 0.10 sec\n",
            "Epoch 1968, Loss(train/val) 0.18504/0.09317. Took 0.09 sec\n",
            "Epoch 1969, Loss(train/val) 0.18291/0.09440. Took 0.09 sec\n",
            "Epoch 1970, Loss(train/val) 0.20276/0.09353. Took 0.10 sec\n",
            "Epoch 1971, Loss(train/val) 0.19480/0.09480. Took 0.10 sec\n",
            "Epoch 1972, Loss(train/val) 0.21664/0.09606. Took 0.10 sec\n",
            "Epoch 1973, Loss(train/val) 0.19929/0.09534. Took 0.10 sec\n",
            "Epoch 1974, Loss(train/val) 0.18322/0.09607. Took 0.10 sec\n",
            "Epoch 1975, Loss(train/val) 0.19614/0.09618. Took 0.10 sec\n",
            "Epoch 1976, Loss(train/val) 0.22446/0.09505. Took 0.11 sec\n",
            "Epoch 1977, Loss(train/val) 0.19816/0.09683. Took 0.10 sec\n",
            "Epoch 1978, Loss(train/val) 0.19016/0.10212. Took 0.10 sec\n",
            "Epoch 1979, Loss(train/val) 0.16966/0.11255. Took 0.10 sec\n",
            "Epoch 1980, Loss(train/val) 0.18108/0.11790. Took 0.09 sec\n",
            "Epoch 1981, Loss(train/val) 0.17769/0.11119. Took 0.10 sec\n",
            "Epoch 1982, Loss(train/val) 0.19041/0.09978. Took 0.10 sec\n",
            "Epoch 1983, Loss(train/val) 0.19994/0.09435. Took 0.09 sec\n",
            "Epoch 1984, Loss(train/val) 0.17875/0.09673. Took 0.09 sec\n",
            "Epoch 1985, Loss(train/val) 0.18696/0.09926. Took 0.10 sec\n",
            "Epoch 1986, Loss(train/val) 0.17821/0.09529. Took 0.10 sec\n",
            "Epoch 1987, Loss(train/val) 0.20548/0.09978. Took 0.11 sec\n",
            "Epoch 1988, Loss(train/val) 0.19630/0.10146. Took 0.10 sec\n",
            "Epoch 1989, Loss(train/val) 0.19563/0.11016. Took 0.09 sec\n",
            "Epoch 1990, Loss(train/val) 0.19733/0.11180. Took 0.10 sec\n",
            "Epoch 1991, Loss(train/val) 0.19847/0.10834. Took 0.10 sec\n",
            "Epoch 1992, Loss(train/val) 0.20429/0.10575. Took 0.09 sec\n",
            "Epoch 1993, Loss(train/val) 0.18059/0.10522. Took 0.10 sec\n",
            "Epoch 1994, Loss(train/val) 0.20556/0.10572. Took 0.10 sec\n",
            "Epoch 1995, Loss(train/val) 0.18890/0.10725. Took 0.11 sec\n",
            "Epoch 1996, Loss(train/val) 0.21671/0.10760. Took 0.10 sec\n",
            "Epoch 1997, Loss(train/val) 0.18205/0.10717. Took 0.12 sec\n",
            "Epoch 1998, Loss(train/val) 0.17640/0.10706. Took 0.09 sec\n",
            "Epoch 1999, Loss(train/val) 0.18904/0.10004. Took 0.10 sec\n",
            "Epoch 2000, Loss(train/val) 0.19180/0.09539. Took 0.10 sec\n",
            "Epoch 2001, Loss(train/val) 0.18410/0.09619. Took 0.10 sec\n",
            "Epoch 2002, Loss(train/val) 0.19870/0.10887. Took 0.10 sec\n",
            "Epoch 2003, Loss(train/val) 0.18798/0.13546. Took 0.10 sec\n",
            "Epoch 2004, Loss(train/val) 0.20233/0.11977. Took 0.10 sec\n",
            "Epoch 2005, Loss(train/val) 0.18403/0.11773. Took 0.10 sec\n",
            "Epoch 2006, Loss(train/val) 0.18443/0.12203. Took 0.10 sec\n",
            "Epoch 2007, Loss(train/val) 0.21608/0.10746. Took 0.11 sec\n",
            "Epoch 2008, Loss(train/val) 0.19553/0.09415. Took 0.11 sec\n",
            "Epoch 2009, Loss(train/val) 0.19969/0.09305. Took 0.09 sec\n",
            "Epoch 2010, Loss(train/val) 0.18583/0.10225. Took 0.09 sec\n",
            "Epoch 2011, Loss(train/val) 0.19422/0.10595. Took 0.10 sec\n",
            "Epoch 2012, Loss(train/val) 0.17585/0.12355. Took 0.10 sec\n",
            "Epoch 2013, Loss(train/val) 0.19487/0.12871. Took 0.09 sec\n",
            "Epoch 2014, Loss(train/val) 0.17948/0.11806. Took 0.10 sec\n",
            "Epoch 2015, Loss(train/val) 0.19233/0.11703. Took 0.10 sec\n",
            "Epoch 2016, Loss(train/val) 0.20466/0.09623. Took 0.10 sec\n",
            "Epoch 2017, Loss(train/val) 0.19441/0.09217. Took 0.11 sec\n",
            "Epoch 2018, Loss(train/val) 0.18586/0.09539. Took 0.10 sec\n",
            "Epoch 2019, Loss(train/val) 0.19187/0.10330. Took 0.10 sec\n",
            "Epoch 2020, Loss(train/val) 0.18278/0.10765. Took 0.10 sec\n",
            "Epoch 2021, Loss(train/val) 0.19869/0.10216. Took 0.10 sec\n",
            "Epoch 2022, Loss(train/val) 0.19477/0.09543. Took 0.10 sec\n",
            "Epoch 2023, Loss(train/val) 0.18566/0.09491. Took 0.10 sec\n",
            "Epoch 2024, Loss(train/val) 0.22383/0.09382. Took 0.10 sec\n",
            "Epoch 2025, Loss(train/val) 0.18023/0.09377. Took 0.10 sec\n",
            "Epoch 2026, Loss(train/val) 0.18171/0.09520. Took 0.10 sec\n",
            "Epoch 2027, Loss(train/val) 0.20046/0.09641. Took 0.10 sec\n",
            "Epoch 2028, Loss(train/val) 0.17981/0.10022. Took 0.10 sec\n",
            "Epoch 2029, Loss(train/val) 0.19525/0.09436. Took 0.10 sec\n",
            "Epoch 2030, Loss(train/val) 0.20499/0.09629. Took 0.11 sec\n",
            "Epoch 2031, Loss(train/val) 0.20961/0.09625. Took 0.10 sec\n",
            "Epoch 2032, Loss(train/val) 0.19162/0.09354. Took 0.10 sec\n",
            "Epoch 2033, Loss(train/val) 0.18870/0.09382. Took 0.10 sec\n",
            "Epoch 2034, Loss(train/val) 0.19571/0.10235. Took 0.09 sec\n",
            "Epoch 2035, Loss(train/val) 0.19459/0.10026. Took 0.10 sec\n",
            "Epoch 2036, Loss(train/val) 0.19944/0.09361. Took 0.11 sec\n",
            "Epoch 2037, Loss(train/val) 0.20831/0.10318. Took 0.09 sec\n",
            "Epoch 2038, Loss(train/val) 0.20495/0.09998. Took 0.11 sec\n",
            "Epoch 2039, Loss(train/val) 0.19015/0.10741. Took 0.11 sec\n",
            "Epoch 2040, Loss(train/val) 0.17964/0.09989. Took 0.09 sec\n",
            "Epoch 2041, Loss(train/val) 0.18703/0.10244. Took 0.10 sec\n",
            "Epoch 2042, Loss(train/val) 0.17715/0.09660. Took 0.10 sec\n",
            "Epoch 2043, Loss(train/val) 0.19907/0.09281. Took 0.11 sec\n",
            "Epoch 2044, Loss(train/val) 0.19211/0.09225. Took 0.10 sec\n",
            "Epoch 2045, Loss(train/val) 0.19769/0.09193. Took 0.10 sec\n",
            "Epoch 2046, Loss(train/val) 0.20012/0.09488. Took 0.10 sec\n",
            "Epoch 2047, Loss(train/val) 0.18805/0.10090. Took 0.10 sec\n",
            "Epoch 2048, Loss(train/val) 0.18683/0.09691. Took 0.11 sec\n",
            "Epoch 2049, Loss(train/val) 0.18877/0.10224. Took 0.10 sec\n",
            "Epoch 2050, Loss(train/val) 0.18870/0.10541. Took 0.09 sec\n",
            "Epoch 2051, Loss(train/val) 0.17813/0.11665. Took 0.10 sec\n",
            "Epoch 2052, Loss(train/val) 0.17537/0.12026. Took 0.10 sec\n",
            "Epoch 2053, Loss(train/val) 0.18469/0.10910. Took 0.10 sec\n",
            "Epoch 2054, Loss(train/val) 0.18706/0.11045. Took 0.10 sec\n",
            "Epoch 2055, Loss(train/val) 0.18989/0.11680. Took 0.10 sec\n",
            "Epoch 2056, Loss(train/val) 0.18842/0.11617. Took 0.09 sec\n",
            "Epoch 2057, Loss(train/val) 0.18658/0.09977. Took 0.11 sec\n",
            "Epoch 2058, Loss(train/val) 0.19474/0.09427. Took 0.11 sec\n",
            "Epoch 2059, Loss(train/val) 0.19366/0.09418. Took 0.10 sec\n",
            "Epoch 2060, Loss(train/val) 0.21049/0.10720. Took 0.10 sec\n",
            "Epoch 2061, Loss(train/val) 0.18198/0.11426. Took 0.10 sec\n",
            "Epoch 2062, Loss(train/val) 0.17600/0.10913. Took 0.10 sec\n",
            "Epoch 2063, Loss(train/val) 0.18154/0.11880. Took 0.10 sec\n",
            "Epoch 2064, Loss(train/val) 0.19209/0.13526. Took 0.09 sec\n",
            "Epoch 2065, Loss(train/val) 0.19362/0.13466. Took 0.10 sec\n",
            "Epoch 2066, Loss(train/val) 0.19448/0.11800. Took 0.10 sec\n",
            "Epoch 2067, Loss(train/val) 0.19262/0.10474. Took 0.10 sec\n",
            "Epoch 2068, Loss(train/val) 0.18226/0.10032. Took 0.11 sec\n",
            "Epoch 2069, Loss(train/val) 0.20683/0.09312. Took 0.10 sec\n",
            "Epoch 2070, Loss(train/val) 0.19195/0.09311. Took 0.09 sec\n",
            "Epoch 2071, Loss(train/val) 0.18233/0.09986. Took 0.09 sec\n",
            "Epoch 2072, Loss(train/val) 0.18703/0.11058. Took 0.10 sec\n",
            "Epoch 2073, Loss(train/val) 0.19110/0.10827. Took 0.09 sec\n",
            "Epoch 2074, Loss(train/val) 0.18390/0.10166. Took 0.10 sec\n",
            "Epoch 2075, Loss(train/val) 0.19322/0.09257. Took 0.10 sec\n",
            "Epoch 2076, Loss(train/val) 0.19025/0.09743. Took 0.09 sec\n",
            "Epoch 2077, Loss(train/val) 0.22517/0.10251. Took 0.09 sec\n",
            "Epoch 2078, Loss(train/val) 0.19044/0.10797. Took 0.11 sec\n",
            "Epoch 2079, Loss(train/val) 0.18269/0.10831. Took 0.11 sec\n",
            "Epoch 2080, Loss(train/val) 0.18270/0.10187. Took 0.10 sec\n",
            "Epoch 2081, Loss(train/val) 0.17734/0.10056. Took 0.10 sec\n",
            "Epoch 2082, Loss(train/val) 0.17800/0.11385. Took 0.10 sec\n",
            "Epoch 2083, Loss(train/val) 0.19345/0.12085. Took 0.10 sec\n",
            "Epoch 2084, Loss(train/val) 0.18687/0.10347. Took 0.09 sec\n",
            "Epoch 2085, Loss(train/val) 0.18792/0.09914. Took 0.09 sec\n",
            "Epoch 2086, Loss(train/val) 0.22808/0.09476. Took 0.11 sec\n",
            "Epoch 2087, Loss(train/val) 0.20061/0.09214. Took 0.10 sec\n",
            "Epoch 2088, Loss(train/val) 0.17791/0.09284. Took 0.09 sec\n",
            "Epoch 2089, Loss(train/val) 0.19102/0.10369. Took 0.11 sec\n",
            "Epoch 2090, Loss(train/val) 0.18271/0.10663. Took 0.10 sec\n",
            "Epoch 2091, Loss(train/val) 0.20025/0.10352. Took 0.10 sec\n",
            "Epoch 2092, Loss(train/val) 0.18352/0.09291. Took 0.10 sec\n",
            "Epoch 2093, Loss(train/val) 0.18524/0.09882. Took 0.10 sec\n",
            "Epoch 2094, Loss(train/val) 0.18377/0.09441. Took 0.09 sec\n",
            "Epoch 2095, Loss(train/val) 0.21090/0.09018. Took 0.10 sec\n",
            "Epoch 2096, Loss(train/val) 0.16776/0.08907. Took 0.10 sec\n",
            "Epoch 2097, Loss(train/val) 0.19804/0.08979. Took 0.09 sec\n",
            "Epoch 2098, Loss(train/val) 0.19495/0.10163. Took 0.10 sec\n",
            "Epoch 2099, Loss(train/val) 0.19961/0.13301. Took 0.11 sec\n",
            "Epoch 2100, Loss(train/val) 0.18117/0.16797. Took 0.09 sec\n",
            "Epoch 2101, Loss(train/val) 0.19428/0.17731. Took 0.11 sec\n",
            "Epoch 2102, Loss(train/val) 0.19307/0.15352. Took 0.09 sec\n",
            "Epoch 2103, Loss(train/val) 0.20250/0.12137. Took 0.09 sec\n",
            "Epoch 2104, Loss(train/val) 0.18991/0.11891. Took 0.10 sec\n",
            "Epoch 2105, Loss(train/val) 0.18064/0.10892. Took 0.10 sec\n",
            "Epoch 2106, Loss(train/val) 0.18788/0.09680. Took 0.10 sec\n",
            "Epoch 2107, Loss(train/val) 0.18873/0.09049. Took 0.10 sec\n",
            "Epoch 2108, Loss(train/val) 0.18411/0.09014. Took 0.09 sec\n",
            "Epoch 2109, Loss(train/val) 0.19256/0.09244. Took 0.10 sec\n",
            "Epoch 2110, Loss(train/val) 0.17966/0.09443. Took 0.11 sec\n",
            "Epoch 2111, Loss(train/val) 0.19375/0.09425. Took 0.10 sec\n",
            "Epoch 2112, Loss(train/val) 0.19649/0.09112. Took 0.10 sec\n",
            "Epoch 2113, Loss(train/val) 0.17605/0.09367. Took 0.10 sec\n",
            "Epoch 2114, Loss(train/val) 0.19417/0.09062. Took 0.09 sec\n",
            "Epoch 2115, Loss(train/val) 0.17653/0.09061. Took 0.09 sec\n",
            "Epoch 2116, Loss(train/val) 0.19167/0.09166. Took 0.10 sec\n",
            "Epoch 2117, Loss(train/val) 0.18913/0.09836. Took 0.09 sec\n",
            "Epoch 2118, Loss(train/val) 0.18244/0.09254. Took 0.10 sec\n",
            "Epoch 2119, Loss(train/val) 0.19039/0.09164. Took 0.10 sec\n",
            "Epoch 2120, Loss(train/val) 0.18565/0.09166. Took 0.11 sec\n",
            "Epoch 2121, Loss(train/val) 0.19258/0.09148. Took 0.10 sec\n",
            "Epoch 2122, Loss(train/val) 0.20291/0.09333. Took 0.10 sec\n",
            "Epoch 2123, Loss(train/val) 0.21152/0.09124. Took 0.10 sec\n",
            "Epoch 2124, Loss(train/val) 0.18723/0.09234. Took 0.10 sec\n",
            "Epoch 2125, Loss(train/val) 0.18445/0.09607. Took 0.10 sec\n",
            "Epoch 2126, Loss(train/val) 0.17537/0.11703. Took 0.10 sec\n",
            "Epoch 2127, Loss(train/val) 0.17780/0.12361. Took 0.10 sec\n",
            "Epoch 2128, Loss(train/val) 0.17979/0.13192. Took 0.10 sec\n",
            "Epoch 2129, Loss(train/val) 0.18001/0.12435. Took 0.10 sec\n",
            "Epoch 2130, Loss(train/val) 0.20108/0.10983. Took 0.11 sec\n",
            "Epoch 2131, Loss(train/val) 0.18269/0.09238. Took 0.10 sec\n",
            "Epoch 2132, Loss(train/val) 0.18390/0.08963. Took 0.09 sec\n",
            "Epoch 2133, Loss(train/val) 0.17422/0.08941. Took 0.10 sec\n",
            "Epoch 2134, Loss(train/val) 0.17753/0.08883. Took 0.09 sec\n",
            "Epoch 2135, Loss(train/val) 0.17724/0.08886. Took 0.11 sec\n",
            "Epoch 2136, Loss(train/val) 0.18172/0.09213. Took 0.11 sec\n",
            "Epoch 2137, Loss(train/val) 0.18177/0.09343. Took 0.09 sec\n",
            "Epoch 2138, Loss(train/val) 0.18419/0.09034. Took 0.10 sec\n",
            "Epoch 2139, Loss(train/val) 0.20006/0.08864. Took 0.10 sec\n",
            "Epoch 2140, Loss(train/val) 0.19319/0.10511. Took 0.09 sec\n",
            "Epoch 2141, Loss(train/val) 0.19436/0.14268. Took 0.11 sec\n",
            "Epoch 2142, Loss(train/val) 0.19093/0.17577. Took 0.10 sec\n",
            "Epoch 2143, Loss(train/val) 0.19810/0.19318. Took 0.10 sec\n",
            "Epoch 2144, Loss(train/val) 0.16855/0.18447. Took 0.10 sec\n",
            "Epoch 2145, Loss(train/val) 0.20504/0.17800. Took 0.09 sec\n",
            "Epoch 2146, Loss(train/val) 0.20659/0.15328. Took 0.10 sec\n",
            "Epoch 2147, Loss(train/val) 0.17696/0.12862. Took 0.10 sec\n",
            "Epoch 2148, Loss(train/val) 0.19135/0.09806. Took 0.10 sec\n",
            "Epoch 2149, Loss(train/val) 0.19048/0.09102. Took 0.10 sec\n",
            "Epoch 2150, Loss(train/val) 0.17772/0.09499. Took 0.10 sec\n",
            "Epoch 2151, Loss(train/val) 0.18256/0.11158. Took 0.11 sec\n",
            "Epoch 2152, Loss(train/val) 0.19664/0.11896. Took 0.10 sec\n",
            "Epoch 2153, Loss(train/val) 0.21113/0.13450. Took 0.10 sec\n",
            "Epoch 2154, Loss(train/val) 0.18497/0.15180. Took 0.09 sec\n",
            "Epoch 2155, Loss(train/val) 0.19708/0.13989. Took 0.11 sec\n",
            "Epoch 2156, Loss(train/val) 0.21020/0.10671. Took 0.10 sec\n",
            "Epoch 2157, Loss(train/val) 0.19373/0.09007. Took 0.10 sec\n",
            "Epoch 2158, Loss(train/val) 0.18154/0.10742. Took 0.10 sec\n",
            "Epoch 2159, Loss(train/val) 0.18423/0.14808. Took 0.10 sec\n",
            "Epoch 2160, Loss(train/val) 0.18262/0.16760. Took 0.10 sec\n",
            "Epoch 2161, Loss(train/val) 0.19605/0.16876. Took 0.12 sec\n",
            "Epoch 2162, Loss(train/val) 0.18077/0.15738. Took 0.10 sec\n",
            "Epoch 2163, Loss(train/val) 0.18462/0.13890. Took 0.10 sec\n",
            "Epoch 2164, Loss(train/val) 0.18580/0.12213. Took 0.10 sec\n",
            "Epoch 2165, Loss(train/val) 0.18194/0.10149. Took 0.09 sec\n",
            "Epoch 2166, Loss(train/val) 0.18589/0.09081. Took 0.10 sec\n",
            "Epoch 2167, Loss(train/val) 0.17285/0.09058. Took 0.10 sec\n",
            "Epoch 2168, Loss(train/val) 0.18795/0.09763. Took 0.10 sec\n",
            "Epoch 2169, Loss(train/val) 0.18466/0.10618. Took 0.10 sec\n",
            "Epoch 2170, Loss(train/val) 0.18760/0.10312. Took 0.10 sec\n",
            "Epoch 2171, Loss(train/val) 0.19093/0.09179. Took 0.11 sec\n",
            "Epoch 2172, Loss(train/val) 0.20725/0.09096. Took 0.10 sec\n",
            "Epoch 2173, Loss(train/val) 0.19169/0.09464. Took 0.11 sec\n",
            "Epoch 2174, Loss(train/val) 0.18747/0.10910. Took 0.09 sec\n",
            "Epoch 2175, Loss(train/val) 0.18708/0.13187. Took 0.10 sec\n",
            "Epoch 2176, Loss(train/val) 0.17999/0.13857. Took 0.10 sec\n",
            "Epoch 2177, Loss(train/val) 0.19546/0.13789. Took 0.10 sec\n",
            "Epoch 2178, Loss(train/val) 0.18083/0.15522. Took 0.11 sec\n",
            "Epoch 2179, Loss(train/val) 0.20482/0.15190. Took 0.10 sec\n",
            "Epoch 2180, Loss(train/val) 0.19576/0.12290. Took 0.09 sec\n",
            "Epoch 2181, Loss(train/val) 0.18690/0.09684. Took 0.10 sec\n",
            "Epoch 2182, Loss(train/val) 0.18493/0.08976. Took 0.11 sec\n",
            "Epoch 2183, Loss(train/val) 0.19411/0.10052. Took 0.10 sec\n",
            "Epoch 2184, Loss(train/val) 0.21499/0.11490. Took 0.10 sec\n",
            "Epoch 2185, Loss(train/val) 0.18718/0.12189. Took 0.09 sec\n",
            "Epoch 2186, Loss(train/val) 0.17939/0.11678. Took 0.10 sec\n",
            "Epoch 2187, Loss(train/val) 0.18717/0.11792. Took 0.10 sec\n",
            "Epoch 2188, Loss(train/val) 0.17794/0.10578. Took 0.10 sec\n",
            "Epoch 2189, Loss(train/val) 0.18261/0.09528. Took 0.10 sec\n",
            "Epoch 2190, Loss(train/val) 0.18977/0.09143. Took 0.10 sec\n",
            "Epoch 2191, Loss(train/val) 0.18873/0.08980. Took 0.09 sec\n",
            "Epoch 2192, Loss(train/val) 0.18501/0.09354. Took 0.12 sec\n",
            "Epoch 2193, Loss(train/val) 0.17013/0.10167. Took 0.10 sec\n",
            "Epoch 2194, Loss(train/val) 0.17776/0.10460. Took 0.10 sec\n",
            "Epoch 2195, Loss(train/val) 0.19678/0.09573. Took 0.10 sec\n",
            "Epoch 2196, Loss(train/val) 0.19843/0.09039. Took 0.10 sec\n",
            "Epoch 2197, Loss(train/val) 0.19290/0.09099. Took 0.09 sec\n",
            "Epoch 2198, Loss(train/val) 0.18269/0.08980. Took 0.10 sec\n",
            "Epoch 2199, Loss(train/val) 0.20358/0.09288. Took 0.10 sec\n",
            "Epoch 2200, Loss(train/val) 0.18222/0.09527. Took 0.10 sec\n",
            "Epoch 2201, Loss(train/val) 0.21551/0.09170. Took 0.11 sec\n",
            "Epoch 2202, Loss(train/val) 0.19079/0.09314. Took 0.11 sec\n",
            "Epoch 2203, Loss(train/val) 0.20327/0.10413. Took 0.10 sec\n",
            "Epoch 2204, Loss(train/val) 0.17495/0.10125. Took 0.09 sec\n",
            "Epoch 2205, Loss(train/val) 0.19161/0.10710. Took 0.10 sec\n",
            "Epoch 2206, Loss(train/val) 0.21063/0.11085. Took 0.10 sec\n",
            "Epoch 2207, Loss(train/val) 0.18549/0.10729. Took 0.10 sec\n",
            "Epoch 2208, Loss(train/val) 0.17780/0.09555. Took 0.10 sec\n",
            "Epoch 2209, Loss(train/val) 0.18128/0.08880. Took 0.10 sec\n",
            "Epoch 2210, Loss(train/val) 0.20489/0.09151. Took 0.09 sec\n",
            "Epoch 2211, Loss(train/val) 0.18638/0.09527. Took 0.10 sec\n",
            "Epoch 2212, Loss(train/val) 0.20350/0.09455. Took 0.11 sec\n",
            "Epoch 2213, Loss(train/val) 0.18389/0.09286. Took 0.10 sec\n",
            "Epoch 2214, Loss(train/val) 0.17604/0.09073. Took 0.09 sec\n",
            "Epoch 2215, Loss(train/val) 0.21017/0.08825. Took 0.11 sec\n",
            "Epoch 2216, Loss(train/val) 0.17988/0.08956. Took 0.10 sec\n",
            "Epoch 2217, Loss(train/val) 0.19191/0.08825. Took 0.10 sec\n",
            "Epoch 2218, Loss(train/val) 0.21952/0.09056. Took 0.10 sec\n",
            "Epoch 2219, Loss(train/val) 0.19023/0.08815. Took 0.10 sec\n",
            "Epoch 2220, Loss(train/val) 0.18498/0.08767. Took 0.10 sec\n",
            "Epoch 2221, Loss(train/val) 0.18477/0.08875. Took 0.10 sec\n",
            "Epoch 2222, Loss(train/val) 0.18783/0.10522. Took 0.10 sec\n",
            "Epoch 2223, Loss(train/val) 0.19013/0.12728. Took 0.10 sec\n",
            "Epoch 2224, Loss(train/val) 0.19238/0.14145. Took 0.10 sec\n",
            "Epoch 2225, Loss(train/val) 0.18616/0.14503. Took 0.09 sec\n",
            "Epoch 2226, Loss(train/val) 0.20327/0.13081. Took 0.10 sec\n",
            "Epoch 2227, Loss(train/val) 0.19948/0.11066. Took 0.10 sec\n",
            "Epoch 2228, Loss(train/val) 0.18625/0.09881. Took 0.10 sec\n",
            "Epoch 2229, Loss(train/val) 0.18406/0.08675. Took 0.11 sec\n",
            "Epoch 2230, Loss(train/val) 0.19885/0.09259. Took 0.10 sec\n",
            "Epoch 2231, Loss(train/val) 0.18532/0.09000. Took 0.09 sec\n",
            "Epoch 2232, Loss(train/val) 0.18444/0.08763. Took 0.10 sec\n",
            "Epoch 2233, Loss(train/val) 0.19434/0.08723. Took 0.10 sec\n",
            "Epoch 2234, Loss(train/val) 0.19955/0.08702. Took 0.10 sec\n",
            "Epoch 2235, Loss(train/val) 0.17440/0.08736. Took 0.10 sec\n",
            "Epoch 2236, Loss(train/val) 0.17476/0.08702. Took 0.10 sec\n",
            "Epoch 2237, Loss(train/val) 0.17698/0.08808. Took 0.10 sec\n",
            "Epoch 2238, Loss(train/val) 0.19511/0.08803. Took 0.11 sec\n",
            "Epoch 2239, Loss(train/val) 0.17525/0.08833. Took 0.10 sec\n",
            "Epoch 2240, Loss(train/val) 0.17508/0.08926. Took 0.09 sec\n",
            "Epoch 2241, Loss(train/val) 0.19369/0.08955. Took 0.10 sec\n",
            "Epoch 2242, Loss(train/val) 0.17728/0.09476. Took 0.10 sec\n",
            "Epoch 2243, Loss(train/val) 0.18695/0.08944. Took 0.13 sec\n",
            "Epoch 2244, Loss(train/val) 0.20615/0.09263. Took 0.09 sec\n",
            "Epoch 2245, Loss(train/val) 0.19326/0.09921. Took 0.10 sec\n",
            "Epoch 2246, Loss(train/val) 0.17702/0.09852. Took 0.10 sec\n",
            "Epoch 2247, Loss(train/val) 0.17507/0.09448. Took 0.09 sec\n",
            "Epoch 2248, Loss(train/val) 0.19279/0.08965. Took 0.10 sec\n",
            "Epoch 2249, Loss(train/val) 0.17559/0.08911. Took 0.10 sec\n",
            "Epoch 2250, Loss(train/val) 0.19262/0.08890. Took 0.09 sec\n",
            "Epoch 2251, Loss(train/val) 0.18827/0.08876. Took 0.10 sec\n",
            "Epoch 2252, Loss(train/val) 0.17912/0.08863. Took 0.10 sec\n",
            "Epoch 2253, Loss(train/val) 0.17981/0.09050. Took 0.10 sec\n",
            "Epoch 2254, Loss(train/val) 0.16314/0.09390. Took 0.09 sec\n",
            "Epoch 2255, Loss(train/val) 0.18193/0.09064. Took 0.10 sec\n",
            "Epoch 2256, Loss(train/val) 0.18859/0.09655. Took 0.10 sec\n",
            "Epoch 2257, Loss(train/val) 0.19710/0.09456. Took 0.10 sec\n",
            "Epoch 2258, Loss(train/val) 0.18400/0.08782. Took 0.10 sec\n",
            "Epoch 2259, Loss(train/val) 0.17099/0.08880. Took 0.09 sec\n",
            "Epoch 2260, Loss(train/val) 0.17585/0.08975. Took 0.09 sec\n",
            "Epoch 2261, Loss(train/val) 0.17532/0.09139. Took 0.11 sec\n",
            "Epoch 2262, Loss(train/val) 0.21513/0.08971. Took 0.09 sec\n",
            "Epoch 2263, Loss(train/val) 0.18067/0.10387. Took 0.10 sec\n",
            "Epoch 2264, Loss(train/val) 0.18734/0.12713. Took 0.10 sec\n",
            "Epoch 2265, Loss(train/val) 0.19050/0.10933. Took 0.10 sec\n",
            "Epoch 2266, Loss(train/val) 0.19853/0.10701. Took 0.10 sec\n",
            "Epoch 2267, Loss(train/val) 0.18419/0.10215. Took 0.10 sec\n",
            "Epoch 2268, Loss(train/val) 0.19928/0.09566. Took 0.10 sec\n",
            "Epoch 2269, Loss(train/val) 0.19022/0.09437. Took 0.10 sec\n",
            "Epoch 2270, Loss(train/val) 0.17347/0.09068. Took 0.10 sec\n",
            "Epoch 2271, Loss(train/val) 0.18275/0.10648. Took 0.09 sec\n",
            "Epoch 2272, Loss(train/val) 0.17920/0.10462. Took 0.10 sec\n",
            "Epoch 2273, Loss(train/val) 0.19232/0.09325. Took 0.11 sec\n",
            "Epoch 2274, Loss(train/val) 0.19861/0.08855. Took 0.10 sec\n",
            "Epoch 2275, Loss(train/val) 0.17498/0.09183. Took 0.09 sec\n",
            "Epoch 2276, Loss(train/val) 0.17380/0.09218. Took 0.10 sec\n",
            "Epoch 2277, Loss(train/val) 0.18807/0.09264. Took 0.10 sec\n",
            "Epoch 2278, Loss(train/val) 0.21458/0.08885. Took 0.10 sec\n",
            "Epoch 2279, Loss(train/val) 0.17606/0.09148. Took 0.10 sec\n",
            "Epoch 2280, Loss(train/val) 0.18304/0.09581. Took 0.09 sec\n",
            "Epoch 2281, Loss(train/val) 0.18345/0.08866. Took 0.09 sec\n",
            "Epoch 2282, Loss(train/val) 0.21884/0.08762. Took 0.10 sec\n",
            "Epoch 2283, Loss(train/val) 0.18375/0.08854. Took 0.09 sec\n",
            "Epoch 2284, Loss(train/val) 0.17456/0.08736. Took 0.10 sec\n",
            "Epoch 2285, Loss(train/val) 0.17581/0.08948. Took 0.10 sec\n",
            "Epoch 2286, Loss(train/val) 0.17852/0.08913. Took 0.10 sec\n",
            "Epoch 2287, Loss(train/val) 0.17953/0.09176. Took 0.10 sec\n",
            "Epoch 2288, Loss(train/val) 0.19437/0.09837. Took 0.10 sec\n",
            "Epoch 2289, Loss(train/val) 0.18123/0.09977. Took 0.10 sec\n",
            "Epoch 2290, Loss(train/val) 0.18298/0.09546. Took 0.10 sec\n",
            "Epoch 2291, Loss(train/val) 0.19537/0.09914. Took 0.10 sec\n",
            "Epoch 2292, Loss(train/val) 0.18485/0.09274. Took 0.10 sec\n",
            "Epoch 2293, Loss(train/val) 0.19645/0.10227. Took 0.10 sec\n",
            "Epoch 2294, Loss(train/val) 0.18963/0.10594. Took 0.12 sec\n",
            "Epoch 2295, Loss(train/val) 0.18056/0.11775. Took 0.09 sec\n",
            "Epoch 2296, Loss(train/val) 0.18667/0.11574. Took 0.10 sec\n",
            "Epoch 2297, Loss(train/val) 0.18300/0.09323. Took 0.10 sec\n",
            "Epoch 2298, Loss(train/val) 0.18391/0.09122. Took 0.10 sec\n",
            "Epoch 2299, Loss(train/val) 0.18874/0.09264. Took 0.11 sec\n",
            "Epoch 2300, Loss(train/val) 0.17558/0.08658. Took 0.10 sec\n",
            "Epoch 2301, Loss(train/val) 0.20672/0.09018. Took 0.09 sec\n",
            "Epoch 2302, Loss(train/val) 0.20183/0.09897. Took 0.10 sec\n",
            "Epoch 2303, Loss(train/val) 0.17218/0.13966. Took 0.10 sec\n",
            "Epoch 2304, Loss(train/val) 0.19160/0.18311. Took 0.11 sec\n",
            "Epoch 2305, Loss(train/val) 0.19290/0.20866. Took 0.10 sec\n",
            "Epoch 2306, Loss(train/val) 0.17827/0.21579. Took 0.10 sec\n",
            "Epoch 2307, Loss(train/val) 0.17669/0.19504. Took 0.10 sec\n",
            "Epoch 2308, Loss(train/val) 0.18025/0.14230. Took 0.10 sec\n",
            "Epoch 2309, Loss(train/val) 0.18647/0.09990. Took 0.10 sec\n",
            "Epoch 2310, Loss(train/val) 0.17934/0.08912. Took 0.10 sec\n",
            "Epoch 2311, Loss(train/val) 0.19067/0.10326. Took 0.10 sec\n",
            "Epoch 2312, Loss(train/val) 0.18756/0.11035. Took 0.10 sec\n",
            "Epoch 2313, Loss(train/val) 0.17869/0.10530. Took 0.10 sec\n",
            "Epoch 2314, Loss(train/val) 0.18712/0.09023. Took 0.10 sec\n",
            "Epoch 2315, Loss(train/val) 0.21322/0.08729. Took 0.10 sec\n",
            "Epoch 2316, Loss(train/val) 0.18270/0.08719. Took 0.10 sec\n",
            "Epoch 2317, Loss(train/val) 0.17096/0.08776. Took 0.09 sec\n",
            "Epoch 2318, Loss(train/val) 0.18020/0.08658. Took 0.10 sec\n",
            "Epoch 2319, Loss(train/val) 0.21500/0.08735. Took 0.10 sec\n",
            "Epoch 2320, Loss(train/val) 0.16861/0.08816. Took 0.09 sec\n",
            "Epoch 2321, Loss(train/val) 0.20318/0.08722. Took 0.10 sec\n",
            "Epoch 2322, Loss(train/val) 0.19256/0.08912. Took 0.10 sec\n",
            "Epoch 2323, Loss(train/val) 0.19424/0.09654. Took 0.09 sec\n",
            "Epoch 2324, Loss(train/val) 0.19384/0.11371. Took 0.10 sec\n",
            "Epoch 2325, Loss(train/val) 0.18354/0.12191. Took 0.10 sec\n",
            "Epoch 2326, Loss(train/val) 0.19199/0.12074. Took 0.09 sec\n",
            "Epoch 2327, Loss(train/val) 0.17423/0.11162. Took 0.10 sec\n",
            "Epoch 2328, Loss(train/val) 0.18570/0.10250. Took 0.10 sec\n",
            "Epoch 2329, Loss(train/val) 0.18121/0.10488. Took 0.09 sec\n",
            "Epoch 2330, Loss(train/val) 0.16804/0.10896. Took 0.11 sec\n",
            "Epoch 2331, Loss(train/val) 0.18518/0.10276. Took 0.09 sec\n",
            "Epoch 2332, Loss(train/val) 0.17875/0.08946. Took 0.10 sec\n",
            "Epoch 2333, Loss(train/val) 0.17546/0.08806. Took 0.10 sec\n",
            "Epoch 2334, Loss(train/val) 0.18087/0.08896. Took 0.09 sec\n",
            "Epoch 2335, Loss(train/val) 0.17125/0.08925. Took 0.10 sec\n",
            "Epoch 2336, Loss(train/val) 0.18094/0.09589. Took 0.10 sec\n",
            "Epoch 2337, Loss(train/val) 0.18707/0.09427. Took 0.09 sec\n",
            "Epoch 2338, Loss(train/val) 0.18605/0.08825. Took 0.09 sec\n",
            "Epoch 2339, Loss(train/val) 0.17906/0.08980. Took 0.11 sec\n",
            "Epoch 2340, Loss(train/val) 0.17408/0.08738. Took 0.09 sec\n",
            "Epoch 2341, Loss(train/val) 0.18064/0.08676. Took 0.09 sec\n",
            "Epoch 2342, Loss(train/val) 0.17872/0.08667. Took 0.10 sec\n",
            "Epoch 2343, Loss(train/val) 0.18966/0.08714. Took 0.09 sec\n",
            "Epoch 2344, Loss(train/val) 0.19316/0.09006. Took 0.09 sec\n",
            "Epoch 2345, Loss(train/val) 0.19230/0.10008. Took 0.11 sec\n",
            "Epoch 2346, Loss(train/val) 0.19605/0.11180. Took 0.10 sec\n",
            "Epoch 2347, Loss(train/val) 0.18023/0.14620. Took 0.09 sec\n",
            "Epoch 2348, Loss(train/val) 0.18428/0.16082. Took 0.13 sec\n",
            "Epoch 2349, Loss(train/val) 0.18778/0.14933. Took 0.10 sec\n",
            "Epoch 2350, Loss(train/val) 0.18280/0.12894. Took 0.09 sec\n",
            "Epoch 2351, Loss(train/val) 0.17460/0.10004. Took 0.10 sec\n",
            "Epoch 2352, Loss(train/val) 0.20227/0.08767. Took 0.10 sec\n",
            "Epoch 2353, Loss(train/val) 0.18159/0.08887. Took 0.10 sec\n",
            "Epoch 2354, Loss(train/val) 0.18863/0.09570. Took 0.10 sec\n",
            "Epoch 2355, Loss(train/val) 0.19045/0.11956. Took 0.10 sec\n",
            "Epoch 2356, Loss(train/val) 0.17175/0.12846. Took 0.10 sec\n",
            "Epoch 2357, Loss(train/val) 0.17940/0.11194. Took 0.11 sec\n",
            "Epoch 2358, Loss(train/val) 0.18095/0.09264. Took 0.10 sec\n",
            "Epoch 2359, Loss(train/val) 0.18944/0.08790. Took 0.09 sec\n",
            "Epoch 2360, Loss(train/val) 0.17812/0.09106. Took 0.10 sec\n",
            "Epoch 2361, Loss(train/val) 0.18277/0.09054. Took 0.10 sec\n",
            "Epoch 2362, Loss(train/val) 0.20941/0.08739. Took 0.10 sec\n",
            "Epoch 2363, Loss(train/val) 0.19362/0.08964. Took 0.10 sec\n",
            "Epoch 2364, Loss(train/val) 0.18871/0.09805. Took 0.09 sec\n",
            "Epoch 2365, Loss(train/val) 0.19067/0.09101. Took 0.09 sec\n",
            "Epoch 2366, Loss(train/val) 0.19134/0.08699. Took 0.11 sec\n",
            "Epoch 2367, Loss(train/val) 0.17284/0.08677. Took 0.09 sec\n",
            "Epoch 2368, Loss(train/val) 0.21979/0.08618. Took 0.09 sec\n",
            "Epoch 2369, Loss(train/val) 0.18874/0.08591. Took 0.10 sec\n",
            "Epoch 2370, Loss(train/val) 0.18106/0.08732. Took 0.09 sec\n",
            "Epoch 2371, Loss(train/val) 0.19619/0.09126. Took 0.09 sec\n",
            "Epoch 2372, Loss(train/val) 0.19987/0.10692. Took 0.10 sec\n",
            "Epoch 2373, Loss(train/val) 0.18011/0.10462. Took 0.10 sec\n",
            "Epoch 2374, Loss(train/val) 0.18697/0.09517. Took 0.09 sec\n",
            "Epoch 2375, Loss(train/val) 0.20481/0.08803. Took 0.10 sec\n",
            "Epoch 2376, Loss(train/val) 0.17094/0.08609. Took 0.11 sec\n",
            "Epoch 2377, Loss(train/val) 0.17971/0.09436. Took 0.10 sec\n",
            "Epoch 2378, Loss(train/val) 0.18883/0.10861. Took 0.10 sec\n",
            "Epoch 2379, Loss(train/val) 0.21815/0.12433. Took 0.10 sec\n",
            "Epoch 2380, Loss(train/val) 0.18228/0.12138. Took 0.11 sec\n",
            "Epoch 2381, Loss(train/val) 0.18942/0.10884. Took 0.10 sec\n",
            "Epoch 2382, Loss(train/val) 0.18750/0.09183. Took 0.10 sec\n",
            "Epoch 2383, Loss(train/val) 0.17947/0.08767. Took 0.10 sec\n",
            "Epoch 2384, Loss(train/val) 0.19206/0.08687. Took 0.10 sec\n",
            "Epoch 2385, Loss(train/val) 0.19729/0.08646. Took 0.09 sec\n",
            "Epoch 2386, Loss(train/val) 0.18010/0.08878. Took 0.10 sec\n",
            "Epoch 2387, Loss(train/val) 0.19488/0.08692. Took 0.11 sec\n",
            "Epoch 2388, Loss(train/val) 0.16919/0.08847. Took 0.12 sec\n",
            "Epoch 2389, Loss(train/val) 0.19874/0.08899. Took 0.15 sec\n",
            "Epoch 2390, Loss(train/val) 0.19345/0.08577. Took 0.15 sec\n",
            "Epoch 2391, Loss(train/val) 0.18416/0.08540. Took 0.16 sec\n",
            "Epoch 2392, Loss(train/val) 0.18379/0.08539. Took 0.15 sec\n",
            "Epoch 2393, Loss(train/val) 0.16464/0.08529. Took 0.16 sec\n",
            "Epoch 2394, Loss(train/val) 0.18630/0.08735. Took 0.16 sec\n",
            "Epoch 2395, Loss(train/val) 0.17634/0.08910. Took 0.15 sec\n",
            "Epoch 2396, Loss(train/val) 0.18265/0.08641. Took 0.16 sec\n",
            "Epoch 2397, Loss(train/val) 0.18186/0.08652. Took 0.15 sec\n",
            "Epoch 2398, Loss(train/val) 0.19118/0.09465. Took 0.16 sec\n",
            "Epoch 2399, Loss(train/val) 0.18382/0.10588. Took 0.15 sec\n",
            "Epoch 2400, Loss(train/val) 0.17728/0.11041. Took 0.17 sec\n",
            "Epoch 2401, Loss(train/val) 0.18044/0.10755. Took 0.16 sec\n",
            "Epoch 2402, Loss(train/val) 0.17563/0.09972. Took 0.16 sec\n",
            "Epoch 2403, Loss(train/val) 0.18870/0.09325. Took 0.16 sec\n",
            "Epoch 2404, Loss(train/val) 0.18911/0.09021. Took 0.16 sec\n",
            "Epoch 2405, Loss(train/val) 0.18849/0.08618. Took 0.15 sec\n",
            "Epoch 2406, Loss(train/val) 0.17977/0.08587. Took 0.11 sec\n",
            "Epoch 2407, Loss(train/val) 0.17779/0.08502. Took 0.11 sec\n",
            "Epoch 2408, Loss(train/val) 0.17221/0.08821. Took 0.11 sec\n",
            "Epoch 2409, Loss(train/val) 0.18453/0.09272. Took 0.10 sec\n",
            "Epoch 2410, Loss(train/val) 0.18309/0.08970. Took 0.09 sec\n",
            "Epoch 2411, Loss(train/val) 0.18124/0.08793. Took 0.11 sec\n",
            "Epoch 2412, Loss(train/val) 0.17615/0.08493. Took 0.10 sec\n",
            "Epoch 2413, Loss(train/val) 0.16644/0.08525. Took 0.09 sec\n",
            "Epoch 2414, Loss(train/val) 0.18608/0.08557. Took 0.10 sec\n",
            "Epoch 2415, Loss(train/val) 0.17397/0.08713. Took 0.09 sec\n",
            "Epoch 2416, Loss(train/val) 0.17933/0.08573. Took 0.09 sec\n",
            "Epoch 2417, Loss(train/val) 0.20865/0.08620. Took 0.10 sec\n",
            "Epoch 2418, Loss(train/val) 0.17889/0.08751. Took 0.10 sec\n",
            "Epoch 2419, Loss(train/val) 0.22481/0.08660. Took 0.10 sec\n",
            "Epoch 2420, Loss(train/val) 0.17322/0.08602. Took 0.10 sec\n",
            "Epoch 2421, Loss(train/val) 0.18945/0.09612. Took 0.09 sec\n",
            "Epoch 2422, Loss(train/val) 0.19982/0.11807. Took 0.10 sec\n",
            "Epoch 2423, Loss(train/val) 0.18706/0.12396. Took 0.09 sec\n",
            "Epoch 2424, Loss(train/val) 0.17413/0.12450. Took 0.10 sec\n",
            "Epoch 2425, Loss(train/val) 0.18295/0.13130. Took 0.10 sec\n",
            "Epoch 2426, Loss(train/val) 0.17963/0.11591. Took 0.10 sec\n",
            "Epoch 2427, Loss(train/val) 0.17695/0.09211. Took 0.10 sec\n",
            "Epoch 2428, Loss(train/val) 0.16410/0.08547. Took 0.11 sec\n",
            "Epoch 2429, Loss(train/val) 0.18794/0.09442. Took 0.10 sec\n",
            "Epoch 2430, Loss(train/val) 0.18361/0.12606. Took 0.10 sec\n",
            "Epoch 2431, Loss(train/val) 0.20284/0.14710. Took 0.10 sec\n",
            "Epoch 2432, Loss(train/val) 0.18650/0.14523. Took 0.10 sec\n",
            "Epoch 2433, Loss(train/val) 0.17104/0.12386. Took 0.10 sec\n",
            "Epoch 2434, Loss(train/val) 0.18616/0.10367. Took 0.10 sec\n",
            "Epoch 2435, Loss(train/val) 0.17270/0.08880. Took 0.09 sec\n",
            "Epoch 2436, Loss(train/val) 0.17013/0.08608. Took 0.09 sec\n",
            "Epoch 2437, Loss(train/val) 0.16060/0.08714. Took 0.10 sec\n",
            "Epoch 2438, Loss(train/val) 0.20171/0.08740. Took 0.10 sec\n",
            "Epoch 2439, Loss(train/val) 0.17315/0.08895. Took 0.11 sec\n",
            "Epoch 2440, Loss(train/val) 0.17126/0.08921. Took 0.10 sec\n",
            "Epoch 2441, Loss(train/val) 0.17776/0.08836. Took 0.10 sec\n",
            "Epoch 2442, Loss(train/val) 0.19169/0.08683. Took 0.10 sec\n",
            "Epoch 2443, Loss(train/val) 0.17321/0.09071. Took 0.12 sec\n",
            "Epoch 2444, Loss(train/val) 0.17448/0.09802. Took 0.10 sec\n",
            "Epoch 2445, Loss(train/val) 0.17774/0.10157. Took 0.10 sec\n",
            "Epoch 2446, Loss(train/val) 0.17890/0.09775. Took 0.09 sec\n",
            "Epoch 2447, Loss(train/val) 0.17999/0.10109. Took 0.10 sec\n",
            "Epoch 2448, Loss(train/val) 0.17242/0.09165. Took 0.10 sec\n",
            "Epoch 2449, Loss(train/val) 0.17896/0.08472. Took 0.12 sec\n",
            "Epoch 2450, Loss(train/val) 0.17132/0.08946. Took 0.10 sec\n",
            "Epoch 2451, Loss(train/val) 0.17720/0.09810. Took 0.10 sec\n",
            "Epoch 2452, Loss(train/val) 0.18485/0.09889. Took 0.10 sec\n",
            "Epoch 2453, Loss(train/val) 0.19583/0.09860. Took 0.10 sec\n",
            "Epoch 2454, Loss(train/val) 0.18320/0.09152. Took 0.10 sec\n",
            "Epoch 2455, Loss(train/val) 0.18213/0.08587. Took 0.10 sec\n",
            "Epoch 2456, Loss(train/val) 0.19133/0.08459. Took 0.10 sec\n",
            "Epoch 2457, Loss(train/val) 0.18878/0.08737. Took 0.10 sec\n",
            "Epoch 2458, Loss(train/val) 0.18908/0.10946. Took 0.10 sec\n",
            "Epoch 2459, Loss(train/val) 0.19803/0.10595. Took 0.11 sec\n",
            "Epoch 2460, Loss(train/val) 0.18243/0.09141. Took 0.10 sec\n",
            "Epoch 2461, Loss(train/val) 0.18372/0.08579. Took 0.10 sec\n",
            "Epoch 2462, Loss(train/val) 0.18226/0.08578. Took 0.11 sec\n",
            "Epoch 2463, Loss(train/val) 0.17906/0.08592. Took 0.10 sec\n",
            "Epoch 2464, Loss(train/val) 0.17800/0.08603. Took 0.10 sec\n",
            "Epoch 2465, Loss(train/val) 0.18171/0.08523. Took 0.10 sec\n",
            "Epoch 2466, Loss(train/val) 0.18913/0.08577. Took 0.09 sec\n",
            "Epoch 2467, Loss(train/val) 0.16395/0.09462. Took 0.10 sec\n",
            "Epoch 2468, Loss(train/val) 0.18642/0.11591. Took 0.11 sec\n",
            "Epoch 2469, Loss(train/val) 0.16910/0.11706. Took 0.11 sec\n",
            "Epoch 2470, Loss(train/val) 0.17431/0.11776. Took 0.10 sec\n",
            "Epoch 2471, Loss(train/val) 0.18990/0.10595. Took 0.10 sec\n",
            "Epoch 2472, Loss(train/val) 0.17955/0.09260. Took 0.09 sec\n",
            "Epoch 2473, Loss(train/val) 0.19521/0.08513. Took 0.10 sec\n",
            "Epoch 2474, Loss(train/val) 0.19525/0.09455. Took 0.10 sec\n",
            "Epoch 2475, Loss(train/val) 0.18734/0.10119. Took 0.09 sec\n",
            "Epoch 2476, Loss(train/val) 0.17836/0.09675. Took 0.10 sec\n",
            "Epoch 2477, Loss(train/val) 0.18882/0.09315. Took 0.10 sec\n",
            "Epoch 2478, Loss(train/val) 0.17324/0.09259. Took 0.10 sec\n",
            "Epoch 2479, Loss(train/val) 0.18035/0.09067. Took 0.11 sec\n",
            "Epoch 2480, Loss(train/val) 0.18467/0.08416. Took 0.10 sec\n",
            "Epoch 2481, Loss(train/val) 0.18244/0.08438. Took 0.10 sec\n",
            "Epoch 2482, Loss(train/val) 0.18109/0.08625. Took 0.10 sec\n",
            "Epoch 2483, Loss(train/val) 0.17455/0.08795. Took 0.10 sec\n",
            "Epoch 2484, Loss(train/val) 0.17778/0.08556. Took 0.09 sec\n",
            "Epoch 2485, Loss(train/val) 0.18332/0.09003. Took 0.10 sec\n",
            "Epoch 2486, Loss(train/val) 0.18062/0.09963. Took 0.10 sec\n",
            "Epoch 2487, Loss(train/val) 0.19527/0.10298. Took 0.10 sec\n",
            "Epoch 2488, Loss(train/val) 0.17562/0.09557. Took 0.11 sec\n",
            "Epoch 2489, Loss(train/val) 0.18960/0.09600. Took 0.11 sec\n",
            "Epoch 2490, Loss(train/val) 0.17535/0.09638. Took 0.10 sec\n",
            "Epoch 2491, Loss(train/val) 0.17526/0.08571. Took 0.10 sec\n",
            "Epoch 2492, Loss(train/val) 0.20318/0.08682. Took 0.09 sec\n",
            "Epoch 2493, Loss(train/val) 0.17078/0.11028. Took 0.10 sec\n",
            "Epoch 2494, Loss(train/val) 0.17652/0.11003. Took 0.10 sec\n",
            "Epoch 2495, Loss(train/val) 0.18570/0.09392. Took 0.09 sec\n",
            "Epoch 2496, Loss(train/val) 0.17483/0.09245. Took 0.10 sec\n",
            "Epoch 2497, Loss(train/val) 0.16090/0.09278. Took 0.10 sec\n",
            "Epoch 2498, Loss(train/val) 0.18821/0.09050. Took 0.10 sec\n",
            "Epoch 2499, Loss(train/val) 0.19227/0.10295. Took 0.10 sec\n",
            "Epoch 2500, Loss(train/val) 0.18816/0.12329. Took 0.10 sec\n",
            "Epoch 2501, Loss(train/val) 0.17987/0.12325. Took 0.10 sec\n",
            "Epoch 2502, Loss(train/val) 0.21237/0.09991. Took 0.10 sec\n",
            "Epoch 2503, Loss(train/val) 0.20138/0.08747. Took 0.10 sec\n",
            "Epoch 2504, Loss(train/val) 0.20023/0.08746. Took 0.09 sec\n",
            "Epoch 2505, Loss(train/val) 0.17719/0.09484. Took 0.10 sec\n",
            "Epoch 2506, Loss(train/val) 0.19023/0.10454. Took 0.10 sec\n",
            "Epoch 2507, Loss(train/val) 0.20657/0.09540. Took 0.10 sec\n",
            "Epoch 2508, Loss(train/val) 0.16721/0.08948. Took 0.10 sec\n",
            "Epoch 2509, Loss(train/val) 0.18818/0.08469. Took 0.10 sec\n",
            "Epoch 2510, Loss(train/val) 0.17955/0.08473. Took 0.10 sec\n",
            "Epoch 2511, Loss(train/val) 0.19306/0.08415. Took 0.10 sec\n",
            "Epoch 2512, Loss(train/val) 0.19046/0.08943. Took 0.09 sec\n",
            "Epoch 2513, Loss(train/val) 0.18345/0.09764. Took 0.09 sec\n",
            "Epoch 2514, Loss(train/val) 0.17631/0.09376. Took 0.10 sec\n",
            "Epoch 2515, Loss(train/val) 0.17957/0.09791. Took 0.10 sec\n",
            "Epoch 2516, Loss(train/val) 0.17791/0.10459. Took 0.09 sec\n",
            "Epoch 2517, Loss(train/val) 0.16776/0.09507. Took 0.10 sec\n",
            "Epoch 2518, Loss(train/val) 0.20653/0.09597. Took 0.10 sec\n",
            "Epoch 2519, Loss(train/val) 0.17942/0.09183. Took 0.10 sec\n",
            "Epoch 2520, Loss(train/val) 0.17972/0.08500. Took 0.11 sec\n",
            "Epoch 2521, Loss(train/val) 0.17262/0.08273. Took 0.10 sec\n",
            "Epoch 2522, Loss(train/val) 0.20373/0.08286. Took 0.10 sec\n",
            "Epoch 2523, Loss(train/val) 0.19512/0.08255. Took 0.11 sec\n",
            "Epoch 2524, Loss(train/val) 0.19359/0.08468. Took 0.09 sec\n",
            "Epoch 2525, Loss(train/val) 0.16925/0.08852. Took 0.10 sec\n",
            "Epoch 2526, Loss(train/val) 0.18284/0.09434. Took 0.10 sec\n",
            "Epoch 2527, Loss(train/val) 0.17254/0.09958. Took 0.09 sec\n",
            "Epoch 2528, Loss(train/val) 0.19068/0.09418. Took 0.10 sec\n",
            "Epoch 2529, Loss(train/val) 0.18892/0.09377. Took 0.10 sec\n",
            "Epoch 2530, Loss(train/val) 0.17698/0.09350. Took 0.10 sec\n",
            "Epoch 2531, Loss(train/val) 0.18006/0.10610. Took 0.10 sec\n",
            "Epoch 2532, Loss(train/val) 0.19379/0.11002. Took 0.10 sec\n",
            "Epoch 2533, Loss(train/val) 0.19588/0.10631. Took 0.10 sec\n",
            "Epoch 2534, Loss(train/val) 0.17756/0.10249. Took 0.10 sec\n",
            "Epoch 2535, Loss(train/val) 0.18620/0.08831. Took 0.10 sec\n",
            "Epoch 2536, Loss(train/val) 0.18373/0.08603. Took 0.10 sec\n",
            "Epoch 2537, Loss(train/val) 0.17584/0.08601. Took 0.10 sec\n",
            "Epoch 2538, Loss(train/val) 0.22250/0.09385. Took 0.10 sec\n",
            "Epoch 2539, Loss(train/val) 0.18740/0.10853. Took 0.09 sec\n",
            "Epoch 2540, Loss(train/val) 0.18382/0.11388. Took 0.09 sec\n",
            "Epoch 2541, Loss(train/val) 0.18625/0.10603. Took 0.12 sec\n",
            "Epoch 2542, Loss(train/val) 0.16596/0.09514. Took 0.10 sec\n",
            "Epoch 2543, Loss(train/val) 0.17801/0.08639. Took 0.10 sec\n",
            "Epoch 2544, Loss(train/val) 0.17393/0.08371. Took 0.10 sec\n",
            "Epoch 2545, Loss(train/val) 0.20198/0.08556. Took 0.09 sec\n",
            "Epoch 2546, Loss(train/val) 0.19760/0.08430. Took 0.10 sec\n",
            "Epoch 2547, Loss(train/val) 0.20208/0.08324. Took 0.10 sec\n",
            "Epoch 2548, Loss(train/val) 0.18389/0.08936. Took 0.09 sec\n",
            "Epoch 2549, Loss(train/val) 0.19430/0.10403. Took 0.10 sec\n",
            "Epoch 2550, Loss(train/val) 0.19507/0.12245. Took 0.10 sec\n",
            "Epoch 2551, Loss(train/val) 0.17639/0.12609. Took 0.12 sec\n",
            "Epoch 2552, Loss(train/val) 0.17485/0.11882. Took 0.10 sec\n",
            "Epoch 2553, Loss(train/val) 0.19790/0.10102. Took 0.09 sec\n",
            "Epoch 2554, Loss(train/val) 0.17382/0.08869. Took 0.10 sec\n",
            "Epoch 2555, Loss(train/val) 0.17242/0.08448. Took 0.10 sec\n",
            "Epoch 2556, Loss(train/val) 0.18605/0.08480. Took 0.09 sec\n",
            "Epoch 2557, Loss(train/val) 0.18886/0.08467. Took 0.09 sec\n",
            "Epoch 2558, Loss(train/val) 0.19508/0.08412. Took 0.10 sec\n",
            "Epoch 2559, Loss(train/val) 0.17933/0.08954. Took 0.09 sec\n",
            "Epoch 2560, Loss(train/val) 0.18670/0.10037. Took 0.10 sec\n",
            "Epoch 2561, Loss(train/val) 0.17990/0.11343. Took 0.11 sec\n",
            "Epoch 2562, Loss(train/val) 0.17973/0.10175. Took 0.10 sec\n",
            "Epoch 2563, Loss(train/val) 0.18597/0.09585. Took 0.10 sec\n",
            "Epoch 2564, Loss(train/val) 0.18488/0.08813. Took 0.10 sec\n",
            "Epoch 2565, Loss(train/val) 0.18178/0.08407. Took 0.09 sec\n",
            "Epoch 2566, Loss(train/val) 0.19098/0.08771. Took 0.09 sec\n",
            "Epoch 2567, Loss(train/val) 0.17932/0.09647. Took 0.11 sec\n",
            "Epoch 2568, Loss(train/val) 0.17326/0.10106. Took 0.10 sec\n",
            "Epoch 2569, Loss(train/val) 0.17116/0.10140. Took 0.10 sec\n",
            "Epoch 2570, Loss(train/val) 0.17375/0.09276. Took 0.10 sec\n",
            "Epoch 2571, Loss(train/val) 0.18588/0.09499. Took 0.12 sec\n",
            "Epoch 2572, Loss(train/val) 0.19063/0.09414. Took 0.10 sec\n",
            "Epoch 2573, Loss(train/val) 0.17508/0.08442. Took 0.10 sec\n",
            "Epoch 2574, Loss(train/val) 0.18101/0.08542. Took 0.10 sec\n",
            "Epoch 2575, Loss(train/val) 0.18257/0.08273. Took 0.10 sec\n",
            "Epoch 2576, Loss(train/val) 0.16703/0.08846. Took 0.10 sec\n",
            "Epoch 2577, Loss(train/val) 0.17405/0.09398. Took 0.09 sec\n",
            "Epoch 2578, Loss(train/val) 0.16947/0.08921. Took 0.10 sec\n",
            "Epoch 2579, Loss(train/val) 0.16923/0.08196. Took 0.10 sec\n",
            "Epoch 2580, Loss(train/val) 0.17934/0.08298. Took 0.10 sec\n",
            "Epoch 2581, Loss(train/val) 0.17343/0.09080. Took 0.10 sec\n",
            "Epoch 2582, Loss(train/val) 0.18554/0.10985. Took 0.11 sec\n",
            "Epoch 2583, Loss(train/val) 0.18258/0.10725. Took 0.10 sec\n",
            "Epoch 2584, Loss(train/val) 0.18766/0.08997. Took 0.10 sec\n",
            "Epoch 2585, Loss(train/val) 0.16635/0.08465. Took 0.10 sec\n",
            "Epoch 2586, Loss(train/val) 0.18188/0.08335. Took 0.10 sec\n",
            "Epoch 2587, Loss(train/val) 0.16822/0.08720. Took 0.10 sec\n",
            "Epoch 2588, Loss(train/val) 0.17638/0.09027. Took 0.10 sec\n",
            "Epoch 2589, Loss(train/val) 0.20810/0.08860. Took 0.11 sec\n",
            "Epoch 2590, Loss(train/val) 0.18895/0.08458. Took 0.10 sec\n",
            "Epoch 2591, Loss(train/val) 0.18388/0.08619. Took 0.09 sec\n",
            "Epoch 2592, Loss(train/val) 0.19385/0.08948. Took 0.11 sec\n",
            "Epoch 2593, Loss(train/val) 0.17332/0.10242. Took 0.11 sec\n",
            "Epoch 2594, Loss(train/val) 0.18221/0.11442. Took 0.09 sec\n",
            "Epoch 2595, Loss(train/val) 0.17589/0.12273. Took 0.10 sec\n",
            "Epoch 2596, Loss(train/val) 0.17551/0.10780. Took 0.09 sec\n",
            "Epoch 2597, Loss(train/val) 0.19482/0.09366. Took 0.11 sec\n",
            "Epoch 2598, Loss(train/val) 0.18819/0.08884. Took 0.09 sec\n",
            "Epoch 2599, Loss(train/val) 0.18573/0.08855. Took 0.09 sec\n",
            "Epoch 2600, Loss(train/val) 0.17657/0.09516. Took 0.10 sec\n",
            "Epoch 2601, Loss(train/val) 0.17594/0.10726. Took 0.10 sec\n",
            "Epoch 2602, Loss(train/val) 0.18690/0.09455. Took 0.10 sec\n",
            "Epoch 2603, Loss(train/val) 0.18763/0.08349. Took 0.10 sec\n",
            "Epoch 2604, Loss(train/val) 0.17413/0.08425. Took 0.10 sec\n",
            "Epoch 2605, Loss(train/val) 0.17788/0.08615. Took 0.09 sec\n",
            "Epoch 2606, Loss(train/val) 0.18266/0.08559. Took 0.10 sec\n",
            "Epoch 2607, Loss(train/val) 0.17962/0.08856. Took 0.10 sec\n",
            "Epoch 2608, Loss(train/val) 0.20349/0.08860. Took 0.10 sec\n",
            "Epoch 2609, Loss(train/val) 0.18033/0.08316. Took 0.10 sec\n",
            "Epoch 2610, Loss(train/val) 0.17609/0.08721. Took 0.10 sec\n",
            "Epoch 2611, Loss(train/val) 0.18660/0.08714. Took 0.09 sec\n",
            "Epoch 2612, Loss(train/val) 0.17881/0.09142. Took 0.12 sec\n",
            "Epoch 2613, Loss(train/val) 0.21798/0.09034. Took 0.10 sec\n",
            "Epoch 2614, Loss(train/val) 0.17837/0.08763. Took 0.09 sec\n",
            "Epoch 2615, Loss(train/val) 0.18847/0.08574. Took 0.10 sec\n",
            "Epoch 2616, Loss(train/val) 0.21383/0.08362. Took 0.10 sec\n",
            "Epoch 2617, Loss(train/val) 0.16717/0.08568. Took 0.09 sec\n",
            "Epoch 2618, Loss(train/val) 0.17344/0.08522. Took 0.10 sec\n",
            "Epoch 2619, Loss(train/val) 0.17808/0.08371. Took 0.10 sec\n",
            "Epoch 2620, Loss(train/val) 0.21555/0.08307. Took 0.10 sec\n",
            "Epoch 2621, Loss(train/val) 0.19091/0.08291. Took 0.10 sec\n",
            "Epoch 2622, Loss(train/val) 0.19864/0.08287. Took 0.10 sec\n",
            "Epoch 2623, Loss(train/val) 0.18088/0.08755. Took 0.11 sec\n",
            "Epoch 2624, Loss(train/val) 0.18594/0.08928. Took 0.09 sec\n",
            "Epoch 2625, Loss(train/val) 0.19114/0.08815. Took 0.09 sec\n",
            "Epoch 2626, Loss(train/val) 0.19191/0.08378. Took 0.10 sec\n",
            "Epoch 2627, Loss(train/val) 0.17711/0.09026. Took 0.10 sec\n",
            "Epoch 2628, Loss(train/val) 0.19256/0.10830. Took 0.10 sec\n",
            "Epoch 2629, Loss(train/val) 0.20317/0.13559. Took 0.10 sec\n",
            "Epoch 2630, Loss(train/val) 0.18181/0.13839. Took 0.10 sec\n",
            "Epoch 2631, Loss(train/val) 0.18388/0.11851. Took 0.10 sec\n",
            "Epoch 2632, Loss(train/val) 0.18258/0.09696. Took 0.10 sec\n",
            "Epoch 2633, Loss(train/val) 0.19120/0.08614. Took 0.11 sec\n",
            "Epoch 2634, Loss(train/val) 0.17601/0.08277. Took 0.10 sec\n",
            "Epoch 2635, Loss(train/val) 0.18755/0.08323. Took 0.10 sec\n",
            "Epoch 2636, Loss(train/val) 0.17708/0.08465. Took 0.10 sec\n",
            "Epoch 2637, Loss(train/val) 0.19314/0.08516. Took 0.11 sec\n",
            "Epoch 2638, Loss(train/val) 0.17254/0.08357. Took 0.09 sec\n",
            "Epoch 2639, Loss(train/val) 0.18708/0.08343. Took 0.09 sec\n",
            "Epoch 2640, Loss(train/val) 0.18893/0.08518. Took 0.10 sec\n",
            "Epoch 2641, Loss(train/val) 0.19513/0.08809. Took 0.11 sec\n",
            "Epoch 2642, Loss(train/val) 0.18359/0.09376. Took 0.10 sec\n",
            "Epoch 2643, Loss(train/val) 0.18679/0.09180. Took 0.10 sec\n",
            "Epoch 2644, Loss(train/val) 0.20586/0.09431. Took 0.10 sec\n",
            "Epoch 2645, Loss(train/val) 0.18760/0.08897. Took 0.10 sec\n",
            "Epoch 2646, Loss(train/val) 0.17709/0.08520. Took 0.10 sec\n",
            "Epoch 2647, Loss(train/val) 0.18037/0.08405. Took 0.10 sec\n",
            "Epoch 2648, Loss(train/val) 0.18343/0.08299. Took 0.09 sec\n",
            "Epoch 2649, Loss(train/val) 0.18310/0.08320. Took 0.10 sec\n",
            "Epoch 2650, Loss(train/val) 0.17821/0.08367. Took 0.10 sec\n",
            "Epoch 2651, Loss(train/val) 0.17448/0.08754. Took 0.09 sec\n",
            "Epoch 2652, Loss(train/val) 0.16915/0.08973. Took 0.10 sec\n",
            "Epoch 2653, Loss(train/val) 0.17182/0.09627. Took 0.11 sec\n",
            "Epoch 2654, Loss(train/val) 0.18394/0.10066. Took 0.10 sec\n",
            "Epoch 2655, Loss(train/val) 0.18530/0.09375. Took 0.09 sec\n",
            "Epoch 2656, Loss(train/val) 0.19069/0.08630. Took 0.12 sec\n",
            "Epoch 2657, Loss(train/val) 0.17269/0.08392. Took 0.10 sec\n",
            "Epoch 2658, Loss(train/val) 0.17297/0.08807. Took 0.10 sec\n",
            "Epoch 2659, Loss(train/val) 0.20678/0.08520. Took 0.11 sec\n",
            "Epoch 2660, Loss(train/val) 0.20022/0.09763. Took 0.10 sec\n",
            "Epoch 2661, Loss(train/val) 0.17423/0.12339. Took 0.10 sec\n",
            "Epoch 2662, Loss(train/val) 0.18428/0.13092. Took 0.10 sec\n",
            "Epoch 2663, Loss(train/val) 0.19021/0.10804. Took 0.10 sec\n",
            "Epoch 2664, Loss(train/val) 0.20085/0.10146. Took 0.09 sec\n",
            "Epoch 2665, Loss(train/val) 0.19441/0.10000. Took 0.10 sec\n",
            "Epoch 2666, Loss(train/val) 0.17239/0.11351. Took 0.09 sec\n",
            "Epoch 2667, Loss(train/val) 0.18221/0.12594. Took 0.10 sec\n",
            "Epoch 2668, Loss(train/val) 0.19176/0.12464. Took 0.10 sec\n",
            "Epoch 2669, Loss(train/val) 0.17579/0.10220. Took 0.10 sec\n",
            "Epoch 2670, Loss(train/val) 0.17820/0.08930. Took 0.09 sec\n",
            "Epoch 2671, Loss(train/val) 0.19400/0.08389. Took 0.10 sec\n",
            "Epoch 2672, Loss(train/val) 0.17197/0.08396. Took 0.09 sec\n",
            "Epoch 2673, Loss(train/val) 0.17790/0.08458. Took 0.09 sec\n",
            "Epoch 2674, Loss(train/val) 0.19143/0.08366. Took 0.11 sec\n",
            "Epoch 2675, Loss(train/val) 0.17839/0.08345. Took 0.10 sec\n",
            "Epoch 2676, Loss(train/val) 0.17369/0.08448. Took 0.10 sec\n",
            "Epoch 2677, Loss(train/val) 0.17233/0.08790. Took 0.10 sec\n",
            "Epoch 2678, Loss(train/val) 0.17425/0.09169. Took 0.10 sec\n",
            "Epoch 2679, Loss(train/val) 0.17215/0.08864. Took 0.11 sec\n",
            "Epoch 2680, Loss(train/val) 0.17549/0.08499. Took 0.09 sec\n",
            "Epoch 2681, Loss(train/val) 0.18690/0.08364. Took 0.10 sec\n",
            "Epoch 2682, Loss(train/val) 0.18993/0.09415. Took 0.10 sec\n",
            "Epoch 2683, Loss(train/val) 0.18044/0.09587. Took 0.10 sec\n",
            "Epoch 2684, Loss(train/val) 0.17698/0.09747. Took 0.11 sec\n",
            "Epoch 2685, Loss(train/val) 0.17801/0.10289. Took 0.10 sec\n",
            "Epoch 2686, Loss(train/val) 0.19513/0.08645. Took 0.10 sec\n",
            "Epoch 2687, Loss(train/val) 0.19016/0.08411. Took 0.10 sec\n",
            "Epoch 2688, Loss(train/val) 0.20436/0.08787. Took 0.10 sec\n",
            "Epoch 2689, Loss(train/val) 0.17295/0.08362. Took 0.10 sec\n",
            "Epoch 2690, Loss(train/val) 0.17260/0.08373. Took 0.10 sec\n",
            "Epoch 2691, Loss(train/val) 0.18123/0.08198. Took 0.10 sec\n",
            "Epoch 2692, Loss(train/val) 0.17199/0.08281. Took 0.09 sec\n",
            "Epoch 2693, Loss(train/val) 0.17920/0.09460. Took 0.10 sec\n",
            "Epoch 2694, Loss(train/val) 0.17245/0.10812. Took 0.10 sec\n",
            "Epoch 2695, Loss(train/val) 0.18014/0.11779. Took 0.09 sec\n",
            "Epoch 2696, Loss(train/val) 0.17024/0.11118. Took 0.11 sec\n",
            "Epoch 2697, Loss(train/val) 0.21219/0.10287. Took 0.10 sec\n",
            "Epoch 2698, Loss(train/val) 0.17401/0.09632. Took 0.09 sec\n",
            "Epoch 2699, Loss(train/val) 0.20386/0.08954. Took 0.10 sec\n",
            "Epoch 2700, Loss(train/val) 0.17712/0.08348. Took 0.09 sec\n",
            "Epoch 2701, Loss(train/val) 0.17431/0.08723. Took 0.09 sec\n",
            "Epoch 2702, Loss(train/val) 0.17566/0.08730. Took 0.10 sec\n",
            "Epoch 2703, Loss(train/val) 0.17844/0.08966. Took 0.10 sec\n",
            "Epoch 2704, Loss(train/val) 0.18759/0.08849. Took 0.11 sec\n",
            "Epoch 2705, Loss(train/val) 0.16780/0.08346. Took 0.10 sec\n",
            "Epoch 2706, Loss(train/val) 0.17701/0.08166. Took 0.09 sec\n",
            "Epoch 2707, Loss(train/val) 0.17119/0.08159. Took 0.11 sec\n",
            "Epoch 2708, Loss(train/val) 0.17638/0.08190. Took 0.09 sec\n",
            "Epoch 2709, Loss(train/val) 0.18572/0.08246. Took 0.10 sec\n",
            "Epoch 2710, Loss(train/val) 0.16455/0.08238. Took 0.10 sec\n",
            "Epoch 2711, Loss(train/val) 0.17619/0.08403. Took 0.10 sec\n",
            "Epoch 2712, Loss(train/val) 0.18045/0.08552. Took 0.10 sec\n",
            "Epoch 2713, Loss(train/val) 0.18292/0.08753. Took 0.10 sec\n",
            "Epoch 2714, Loss(train/val) 0.17266/0.08412. Took 0.09 sec\n",
            "Epoch 2715, Loss(train/val) 0.18219/0.08460. Took 0.10 sec\n",
            "Epoch 2716, Loss(train/val) 0.19246/0.08816. Took 0.10 sec\n",
            "Epoch 2717, Loss(train/val) 0.17335/0.08491. Took 0.09 sec\n",
            "Epoch 2718, Loss(train/val) 0.16863/0.08280. Took 0.09 sec\n",
            "Epoch 2719, Loss(train/val) 0.20128/0.08293. Took 0.10 sec\n",
            "Epoch 2720, Loss(train/val) 0.22052/0.08326. Took 0.09 sec\n",
            "Epoch 2721, Loss(train/val) 0.17355/0.08359. Took 0.09 sec\n",
            "Epoch 2722, Loss(train/val) 0.18005/0.08690. Took 0.11 sec\n",
            "Epoch 2723, Loss(train/val) 0.17958/0.09382. Took 0.10 sec\n",
            "Epoch 2724, Loss(train/val) 0.18967/0.08402. Took 0.09 sec\n",
            "Epoch 2725, Loss(train/val) 0.19149/0.08353. Took 0.11 sec\n",
            "Epoch 2726, Loss(train/val) 0.17964/0.08444. Took 0.10 sec\n",
            "Epoch 2727, Loss(train/val) 0.18858/0.08820. Took 0.09 sec\n",
            "Epoch 2728, Loss(train/val) 0.16838/0.09204. Took 0.11 sec\n",
            "Epoch 2729, Loss(train/val) 0.17062/0.09311. Took 0.10 sec\n",
            "Epoch 2730, Loss(train/val) 0.17756/0.08376. Took 0.09 sec\n",
            "Epoch 2731, Loss(train/val) 0.17267/0.08290. Took 0.10 sec\n",
            "Epoch 2732, Loss(train/val) 0.17117/0.08919. Took 0.10 sec\n",
            "Epoch 2733, Loss(train/val) 0.17852/0.08887. Took 0.10 sec\n",
            "Epoch 2734, Loss(train/val) 0.19088/0.08693. Took 0.10 sec\n",
            "Epoch 2735, Loss(train/val) 0.17522/0.08420. Took 0.10 sec\n",
            "Epoch 2736, Loss(train/val) 0.18475/0.08523. Took 0.09 sec\n",
            "Epoch 2737, Loss(train/val) 0.18719/0.08988. Took 0.11 sec\n",
            "Epoch 2738, Loss(train/val) 0.23267/0.09266. Took 0.09 sec\n",
            "Epoch 2739, Loss(train/val) 0.18602/0.09289. Took 0.09 sec\n",
            "Epoch 2740, Loss(train/val) 0.18131/0.09163. Took 0.11 sec\n",
            "Epoch 2741, Loss(train/val) 0.19820/0.09435. Took 0.10 sec\n",
            "Epoch 2742, Loss(train/val) 0.17641/0.08538. Took 0.10 sec\n",
            "Epoch 2743, Loss(train/val) 0.19091/0.08257. Took 0.10 sec\n",
            "Epoch 2744, Loss(train/val) 0.18029/0.08369. Took 0.10 sec\n",
            "Epoch 2745, Loss(train/val) 0.17059/0.08314. Took 0.09 sec\n",
            "Epoch 2746, Loss(train/val) 0.17708/0.08409. Took 0.11 sec\n",
            "Epoch 2747, Loss(train/val) 0.18969/0.08306. Took 0.10 sec\n",
            "Epoch 2748, Loss(train/val) 0.18739/0.08967. Took 0.10 sec\n",
            "Epoch 2749, Loss(train/val) 0.19393/0.08513. Took 0.10 sec\n",
            "Epoch 2750, Loss(train/val) 0.17599/0.08915. Took 0.10 sec\n",
            "Epoch 2751, Loss(train/val) 0.19734/0.08601. Took 0.09 sec\n",
            "Epoch 2752, Loss(train/val) 0.17587/0.08711. Took 0.11 sec\n",
            "Epoch 2753, Loss(train/val) 0.17520/0.08544. Took 0.10 sec\n",
            "Epoch 2754, Loss(train/val) 0.18022/0.08865. Took 0.10 sec\n",
            "Epoch 2755, Loss(train/val) 0.21121/0.08455. Took 0.10 sec\n",
            "Epoch 2756, Loss(train/val) 0.18302/0.08499. Took 0.11 sec\n",
            "Epoch 2757, Loss(train/val) 0.18146/0.08974. Took 0.10 sec\n",
            "Epoch 2758, Loss(train/val) 0.18431/0.09251. Took 0.11 sec\n",
            "Epoch 2759, Loss(train/val) 0.17861/0.09553. Took 0.09 sec\n",
            "Epoch 2760, Loss(train/val) 0.19901/0.08879. Took 0.10 sec\n",
            "Epoch 2761, Loss(train/val) 0.18312/0.08499. Took 0.12 sec\n",
            "Epoch 2762, Loss(train/val) 0.17695/0.08850. Took 0.11 sec\n",
            "Epoch 2763, Loss(train/val) 0.18219/0.09407. Took 0.10 sec\n",
            "Epoch 2764, Loss(train/val) 0.18216/0.08741. Took 0.09 sec\n",
            "Epoch 2765, Loss(train/val) 0.19103/0.08543. Took 0.10 sec\n",
            "Epoch 2766, Loss(train/val) 0.16802/0.09671. Took 0.10 sec\n",
            "Epoch 2767, Loss(train/val) 0.18791/0.09220. Took 0.11 sec\n",
            "Epoch 2768, Loss(train/val) 0.17442/0.09046. Took 0.09 sec\n",
            "Epoch 2769, Loss(train/val) 0.18637/0.08741. Took 0.10 sec\n",
            "Epoch 2770, Loss(train/val) 0.18177/0.08353. Took 0.10 sec\n",
            "Epoch 2771, Loss(train/val) 0.16531/0.08332. Took 0.09 sec\n",
            "Epoch 2772, Loss(train/val) 0.17044/0.08377. Took 0.09 sec\n",
            "Epoch 2773, Loss(train/val) 0.18713/0.08401. Took 0.10 sec\n",
            "Epoch 2774, Loss(train/val) 0.17533/0.08500. Took 0.10 sec\n",
            "Epoch 2775, Loss(train/val) 0.20887/0.08556. Took 0.09 sec\n",
            "Epoch 2776, Loss(train/val) 0.18778/0.08252. Took 0.11 sec\n",
            "Epoch 2777, Loss(train/val) 0.18186/0.08982. Took 0.10 sec\n",
            "Epoch 2778, Loss(train/val) 0.17014/0.10374. Took 0.09 sec\n",
            "Epoch 2779, Loss(train/val) 0.17376/0.11830. Took 0.10 sec\n",
            "Epoch 2780, Loss(train/val) 0.17821/0.11853. Took 0.09 sec\n",
            "Epoch 2781, Loss(train/val) 0.17118/0.11914. Took 0.10 sec\n",
            "Epoch 2782, Loss(train/val) 0.20097/0.12072. Took 0.10 sec\n",
            "Epoch 2783, Loss(train/val) 0.17074/0.10921. Took 0.11 sec\n",
            "Epoch 2784, Loss(train/val) 0.17724/0.08574. Took 0.09 sec\n",
            "Epoch 2785, Loss(train/val) 0.19329/0.08462. Took 0.10 sec\n",
            "Epoch 2786, Loss(train/val) 0.19121/0.08470. Took 0.10 sec\n",
            "Epoch 2787, Loss(train/val) 0.18545/0.08899. Took 0.10 sec\n",
            "Epoch 2788, Loss(train/val) 0.17356/0.08250. Took 0.11 sec\n",
            "Epoch 2789, Loss(train/val) 0.16426/0.08233. Took 0.09 sec\n",
            "Epoch 2790, Loss(train/val) 0.17249/0.08234. Took 0.10 sec\n",
            "Epoch 2791, Loss(train/val) 0.17306/0.08253. Took 0.10 sec\n",
            "Epoch 2792, Loss(train/val) 0.18031/0.08157. Took 0.10 sec\n",
            "Epoch 2793, Loss(train/val) 0.16033/0.08091. Took 0.10 sec\n",
            "Epoch 2794, Loss(train/val) 0.17216/0.08116. Took 0.11 sec\n",
            "Epoch 2795, Loss(train/val) 0.16647/0.08312. Took 0.10 sec\n",
            "Epoch 2796, Loss(train/val) 0.17657/0.08473. Took 0.10 sec\n",
            "Epoch 2797, Loss(train/val) 0.19265/0.08868. Took 0.12 sec\n",
            "Epoch 2798, Loss(train/val) 0.18750/0.09795. Took 0.09 sec\n",
            "Epoch 2799, Loss(train/val) 0.18245/0.09967. Took 0.10 sec\n",
            "Epoch 2800, Loss(train/val) 0.16896/0.09294. Took 0.10 sec\n",
            "Epoch 2801, Loss(train/val) 0.16680/0.08958. Took 0.10 sec\n",
            "Epoch 2802, Loss(train/val) 0.17872/0.09469. Took 0.10 sec\n",
            "Epoch 2803, Loss(train/val) 0.18116/0.09848. Took 0.11 sec\n",
            "Epoch 2804, Loss(train/val) 0.19353/0.09258. Took 0.10 sec\n",
            "Epoch 2805, Loss(train/val) 0.17844/0.08402. Took 0.10 sec\n",
            "Epoch 2806, Loss(train/val) 0.18628/0.08279. Took 0.10 sec\n",
            "Epoch 2807, Loss(train/val) 0.18023/0.08391. Took 0.11 sec\n",
            "Epoch 2808, Loss(train/val) 0.18195/0.08495. Took 0.10 sec\n",
            "Epoch 2809, Loss(train/val) 0.17188/0.08621. Took 0.11 sec\n",
            "Epoch 2810, Loss(train/val) 0.17676/0.08670. Took 0.09 sec\n",
            "Epoch 2811, Loss(train/val) 0.19075/0.08328. Took 0.10 sec\n",
            "Epoch 2812, Loss(train/val) 0.22337/0.08207. Took 0.09 sec\n",
            "Epoch 2813, Loss(train/val) 0.17803/0.08188. Took 0.10 sec\n",
            "Epoch 2814, Loss(train/val) 0.17685/0.08571. Took 0.10 sec\n",
            "Epoch 2815, Loss(train/val) 0.17376/0.09587. Took 0.09 sec\n",
            "Epoch 2816, Loss(train/val) 0.18544/0.09326. Took 0.10 sec\n",
            "Epoch 2817, Loss(train/val) 0.18403/0.09089. Took 0.11 sec\n",
            "Epoch 2818, Loss(train/val) 0.18256/0.08960. Took 0.10 sec\n",
            "Epoch 2819, Loss(train/val) 0.18815/0.08871. Took 0.10 sec\n",
            "Epoch 2820, Loss(train/val) 0.17713/0.08250. Took 0.10 sec\n",
            "Epoch 2821, Loss(train/val) 0.18270/0.08287. Took 0.10 sec\n",
            "Epoch 2822, Loss(train/val) 0.17598/0.08646. Took 0.10 sec\n",
            "Epoch 2823, Loss(train/val) 0.17948/0.08514. Took 0.10 sec\n",
            "Epoch 2824, Loss(train/val) 0.19378/0.08092. Took 0.10 sec\n",
            "Epoch 2825, Loss(train/val) 0.19228/0.08015. Took 0.10 sec\n",
            "Epoch 2826, Loss(train/val) 0.18147/0.08283. Took 0.10 sec\n",
            "Epoch 2827, Loss(train/val) 0.18137/0.08593. Took 0.11 sec\n",
            "Epoch 2828, Loss(train/val) 0.16757/0.08089. Took 0.10 sec\n",
            "Epoch 2829, Loss(train/val) 0.18156/0.08077. Took 0.10 sec\n",
            "Epoch 2830, Loss(train/val) 0.19720/0.08401. Took 0.09 sec\n",
            "Epoch 2831, Loss(train/val) 0.18669/0.08833. Took 0.11 sec\n",
            "Epoch 2832, Loss(train/val) 0.16953/0.08312. Took 0.09 sec\n",
            "Epoch 2833, Loss(train/val) 0.21552/0.08178. Took 0.10 sec\n",
            "Epoch 2834, Loss(train/val) 0.20095/0.08731. Took 0.10 sec\n",
            "Epoch 2835, Loss(train/val) 0.18149/0.09146. Took 0.09 sec\n",
            "Epoch 2836, Loss(train/val) 0.18856/0.08906. Took 0.10 sec\n",
            "Epoch 2837, Loss(train/val) 0.18747/0.08119. Took 0.11 sec\n",
            "Epoch 2838, Loss(train/val) 0.17279/0.08456. Took 0.10 sec\n",
            "Epoch 2839, Loss(train/val) 0.18674/0.10879. Took 0.10 sec\n",
            "Epoch 2840, Loss(train/val) 0.17790/0.13437. Took 0.10 sec\n",
            "Epoch 2841, Loss(train/val) 0.16997/0.13951. Took 0.10 sec\n",
            "Epoch 2842, Loss(train/val) 0.18816/0.12747. Took 0.10 sec\n",
            "Epoch 2843, Loss(train/val) 0.17975/0.12641. Took 0.10 sec\n",
            "Epoch 2844, Loss(train/val) 0.19637/0.10811. Took 0.09 sec\n",
            "Epoch 2845, Loss(train/val) 0.19348/0.09245. Took 0.09 sec\n",
            "Epoch 2846, Loss(train/val) 0.19227/0.08645. Took 0.11 sec\n",
            "Epoch 2847, Loss(train/val) 0.17442/0.08230. Took 0.10 sec\n",
            "Epoch 2848, Loss(train/val) 0.17663/0.09977. Took 0.11 sec\n",
            "Epoch 2849, Loss(train/val) 0.17756/0.11109. Took 0.10 sec\n",
            "Epoch 2850, Loss(train/val) 0.18919/0.11721. Took 0.09 sec\n",
            "Epoch 2851, Loss(train/val) 0.17748/0.11412. Took 0.11 sec\n",
            "Epoch 2852, Loss(train/val) 0.18428/0.11229. Took 0.09 sec\n",
            "Epoch 2853, Loss(train/val) 0.17923/0.11098. Took 0.10 sec\n",
            "Epoch 2854, Loss(train/val) 0.19920/0.11603. Took 0.10 sec\n",
            "Epoch 2855, Loss(train/val) 0.17721/0.09065. Took 0.09 sec\n",
            "Epoch 2856, Loss(train/val) 0.17730/0.08255. Took 0.09 sec\n",
            "Epoch 2857, Loss(train/val) 0.18799/0.08985. Took 0.11 sec\n",
            "Epoch 2858, Loss(train/val) 0.18496/0.10211. Took 0.11 sec\n",
            "Epoch 2859, Loss(train/val) 0.18154/0.10275. Took 0.10 sec\n",
            "Epoch 2860, Loss(train/val) 0.18719/0.08944. Took 0.10 sec\n",
            "Epoch 2861, Loss(train/val) 0.18792/0.08343. Took 0.10 sec\n",
            "Epoch 2862, Loss(train/val) 0.19688/0.08738. Took 0.10 sec\n",
            "Epoch 2863, Loss(train/val) 0.18517/0.09294. Took 0.10 sec\n",
            "Epoch 2864, Loss(train/val) 0.17601/0.09523. Took 0.10 sec\n",
            "Epoch 2865, Loss(train/val) 0.18533/0.08681. Took 0.10 sec\n",
            "Epoch 2866, Loss(train/val) 0.18425/0.08506. Took 0.10 sec\n",
            "Epoch 2867, Loss(train/val) 0.17328/0.08448. Took 0.10 sec\n",
            "Epoch 2868, Loss(train/val) 0.18487/0.08214. Took 0.11 sec\n",
            "Epoch 2869, Loss(train/val) 0.19434/0.08660. Took 0.12 sec\n",
            "Epoch 2870, Loss(train/val) 0.17428/0.08868. Took 0.10 sec\n",
            "Epoch 2871, Loss(train/val) 0.18654/0.10141. Took 0.10 sec\n",
            "Epoch 2872, Loss(train/val) 0.18277/0.10762. Took 0.10 sec\n",
            "Epoch 2873, Loss(train/val) 0.17737/0.08784. Took 0.10 sec\n",
            "Epoch 2874, Loss(train/val) 0.19428/0.08192. Took 0.10 sec\n",
            "Epoch 2875, Loss(train/val) 0.17864/0.08865. Took 0.09 sec\n",
            "Epoch 2876, Loss(train/val) 0.18365/0.09162. Took 0.10 sec\n",
            "Epoch 2877, Loss(train/val) 0.18714/0.08670. Took 0.10 sec\n",
            "Epoch 2878, Loss(train/val) 0.18483/0.07918. Took 0.11 sec\n",
            "Epoch 2879, Loss(train/val) 0.18467/0.08271. Took 0.10 sec\n",
            "Epoch 2880, Loss(train/val) 0.17434/0.08564. Took 0.10 sec\n",
            "Epoch 2881, Loss(train/val) 0.19269/0.08131. Took 0.10 sec\n",
            "Epoch 2882, Loss(train/val) 0.19054/0.08522. Took 0.10 sec\n",
            "Epoch 2883, Loss(train/val) 0.17651/0.08702. Took 0.09 sec\n",
            "Epoch 2884, Loss(train/val) 0.18347/0.09154. Took 0.10 sec\n",
            "Epoch 2885, Loss(train/val) 0.17500/0.08875. Took 0.09 sec\n",
            "Epoch 2886, Loss(train/val) 0.17245/0.09468. Took 0.10 sec\n",
            "Epoch 2887, Loss(train/val) 0.16143/0.09511. Took 0.11 sec\n",
            "Epoch 2888, Loss(train/val) 0.17827/0.09495. Took 0.10 sec\n",
            "Epoch 2889, Loss(train/val) 0.17783/0.10904. Took 0.10 sec\n",
            "Epoch 2890, Loss(train/val) 0.17419/0.11989. Took 0.10 sec\n",
            "Epoch 2891, Loss(train/val) 0.18380/0.12306. Took 0.10 sec\n",
            "Epoch 2892, Loss(train/val) 0.19224/0.10650. Took 0.10 sec\n",
            "Epoch 2893, Loss(train/val) 0.21886/0.09238. Took 0.10 sec\n",
            "Epoch 2894, Loss(train/val) 0.21067/0.08556. Took 0.11 sec\n",
            "Epoch 2895, Loss(train/val) 0.17775/0.08838. Took 0.10 sec\n",
            "Epoch 2896, Loss(train/val) 0.19371/0.08023. Took 0.10 sec\n",
            "Epoch 2897, Loss(train/val) 0.17747/0.08594. Took 0.11 sec\n",
            "Epoch 2898, Loss(train/val) 0.18549/0.08825. Took 0.10 sec\n",
            "Epoch 2899, Loss(train/val) 0.18239/0.08850. Took 0.11 sec\n",
            "Epoch 2900, Loss(train/val) 0.19234/0.08500. Took 0.10 sec\n",
            "Epoch 2901, Loss(train/val) 0.16132/0.08054. Took 0.10 sec\n",
            "Epoch 2902, Loss(train/val) 0.18566/0.08837. Took 0.10 sec\n",
            "Epoch 2903, Loss(train/val) 0.16947/0.09711. Took 0.09 sec\n",
            "Epoch 2904, Loss(train/val) 0.17493/0.08853. Took 0.09 sec\n",
            "Epoch 2905, Loss(train/val) 0.18839/0.08293. Took 0.11 sec\n",
            "Epoch 2906, Loss(train/val) 0.18271/0.08222. Took 0.09 sec\n",
            "Epoch 2907, Loss(train/val) 0.18678/0.09937. Took 0.10 sec\n",
            "Epoch 2908, Loss(train/val) 0.19518/0.10180. Took 0.10 sec\n",
            "Epoch 2909, Loss(train/val) 0.18212/0.10645. Took 0.11 sec\n",
            "Epoch 2910, Loss(train/val) 0.19272/0.09444. Took 0.10 sec\n",
            "Epoch 2911, Loss(train/val) 0.18430/0.08672. Took 0.10 sec\n",
            "Epoch 2912, Loss(train/val) 0.17818/0.08711. Took 0.10 sec\n",
            "Epoch 2913, Loss(train/val) 0.20623/0.09043. Took 0.11 sec\n",
            "Epoch 2914, Loss(train/val) 0.17416/0.08567. Took 0.09 sec\n",
            "Epoch 2915, Loss(train/val) 0.19560/0.08329. Took 0.10 sec\n",
            "Epoch 2916, Loss(train/val) 0.19125/0.08293. Took 0.10 sec\n",
            "Epoch 2917, Loss(train/val) 0.19659/0.08129. Took 0.10 sec\n",
            "Epoch 2918, Loss(train/val) 0.18767/0.08397. Took 0.10 sec\n",
            "Epoch 2919, Loss(train/val) 0.18177/0.08278. Took 0.11 sec\n",
            "Epoch 2920, Loss(train/val) 0.19810/0.08539. Took 0.10 sec\n",
            "Epoch 2921, Loss(train/val) 0.18413/0.08988. Took 0.10 sec\n",
            "Epoch 2922, Loss(train/val) 0.17928/0.09030. Took 0.10 sec\n",
            "Epoch 2923, Loss(train/val) 0.16846/0.09140. Took 0.09 sec\n",
            "Epoch 2924, Loss(train/val) 0.17733/0.09663. Took 0.10 sec\n",
            "Epoch 2925, Loss(train/val) 0.16324/0.08753. Took 0.10 sec\n",
            "Epoch 2926, Loss(train/val) 0.17766/0.08389. Took 0.10 sec\n",
            "Epoch 2927, Loss(train/val) 0.21849/0.08184. Took 0.10 sec\n",
            "Epoch 2928, Loss(train/val) 0.18639/0.09599. Took 0.10 sec\n",
            "Epoch 2929, Loss(train/val) 0.18457/0.11462. Took 0.11 sec\n",
            "Epoch 2930, Loss(train/val) 0.18213/0.11077. Took 0.10 sec\n",
            "Epoch 2931, Loss(train/val) 0.17748/0.09223. Took 0.10 sec\n",
            "Epoch 2932, Loss(train/val) 0.20207/0.08291. Took 0.10 sec\n",
            "Epoch 2933, Loss(train/val) 0.17627/0.08298. Took 0.10 sec\n",
            "Epoch 2934, Loss(train/val) 0.18481/0.08271. Took 0.09 sec\n",
            "Epoch 2935, Loss(train/val) 0.18104/0.08370. Took 0.10 sec\n",
            "Epoch 2936, Loss(train/val) 0.17397/0.08102. Took 0.10 sec\n",
            "Epoch 2937, Loss(train/val) 0.16391/0.08151. Took 0.10 sec\n",
            "Epoch 2938, Loss(train/val) 0.17980/0.08444. Took 0.10 sec\n",
            "Epoch 2939, Loss(train/val) 0.17679/0.08967. Took 0.11 sec\n",
            "Epoch 2940, Loss(train/val) 0.16842/0.09656. Took 0.09 sec\n",
            "Epoch 2941, Loss(train/val) 0.17751/0.09085. Took 0.10 sec\n",
            "Epoch 2942, Loss(train/val) 0.17045/0.08389. Took 0.11 sec\n",
            "Epoch 2943, Loss(train/val) 0.16362/0.08472. Took 0.10 sec\n",
            "Epoch 2944, Loss(train/val) 0.16878/0.08797. Took 0.10 sec\n",
            "Epoch 2945, Loss(train/val) 0.17294/0.08708. Took 0.10 sec\n",
            "Epoch 2946, Loss(train/val) 0.19738/0.09258. Took 0.10 sec\n",
            "Epoch 2947, Loss(train/val) 0.17492/0.10048. Took 0.10 sec\n",
            "Epoch 2948, Loss(train/val) 0.16574/0.10303. Took 0.11 sec\n",
            "Epoch 2949, Loss(train/val) 0.18078/0.10503. Took 0.10 sec\n",
            "Epoch 2950, Loss(train/val) 0.17326/0.10208. Took 0.09 sec\n",
            "Epoch 2951, Loss(train/val) 0.17304/0.08806. Took 0.10 sec\n",
            "Epoch 2952, Loss(train/val) 0.20176/0.09380. Took 0.10 sec\n",
            "Epoch 2953, Loss(train/val) 0.17895/0.08747. Took 0.10 sec\n",
            "Epoch 2954, Loss(train/val) 0.18087/0.08852. Took 0.10 sec\n",
            "Epoch 2955, Loss(train/val) 0.17794/0.09085. Took 0.10 sec\n",
            "Epoch 2956, Loss(train/val) 0.18714/0.10214. Took 0.10 sec\n",
            "Epoch 2957, Loss(train/val) 0.17059/0.11237. Took 0.10 sec\n",
            "Epoch 2958, Loss(train/val) 0.18835/0.13018. Took 0.10 sec\n",
            "Epoch 2959, Loss(train/val) 0.18334/0.15024. Took 0.10 sec\n",
            "Epoch 2960, Loss(train/val) 0.18509/0.17979. Took 0.11 sec\n",
            "Epoch 2961, Loss(train/val) 0.17678/0.19647. Took 0.10 sec\n",
            "Epoch 2962, Loss(train/val) 0.20703/0.15541. Took 0.10 sec\n",
            "Epoch 2963, Loss(train/val) 0.18256/0.11337. Took 0.10 sec\n",
            "Epoch 2964, Loss(train/val) 0.17698/0.08147. Took 0.10 sec\n",
            "Epoch 2965, Loss(train/val) 0.17821/0.10280. Took 0.11 sec\n",
            "Epoch 2966, Loss(train/val) 0.20761/0.14448. Took 0.10 sec\n",
            "Epoch 2967, Loss(train/val) 0.17404/0.16059. Took 0.10 sec\n",
            "Epoch 2968, Loss(train/val) 0.17502/0.13193. Took 0.11 sec\n",
            "Epoch 2969, Loss(train/val) 0.18207/0.09750. Took 0.09 sec\n",
            "Epoch 2970, Loss(train/val) 0.17758/0.08080. Took 0.11 sec\n",
            "Epoch 2971, Loss(train/val) 0.18421/0.08044. Took 0.10 sec\n",
            "Epoch 2972, Loss(train/val) 0.18144/0.08167. Took 0.10 sec\n",
            "Epoch 2973, Loss(train/val) 0.17251/0.08167. Took 0.10 sec\n",
            "Epoch 2974, Loss(train/val) 0.18556/0.08215. Took 0.10 sec\n",
            "Epoch 2975, Loss(train/val) 0.16985/0.07961. Took 0.12 sec\n",
            "Epoch 2976, Loss(train/val) 0.17351/0.07983. Took 0.10 sec\n",
            "Epoch 2977, Loss(train/val) 0.16211/0.08541. Took 0.11 sec\n",
            "Epoch 2978, Loss(train/val) 0.21039/0.09513. Took 0.11 sec\n",
            "Epoch 2979, Loss(train/val) 0.17708/0.11400. Took 0.10 sec\n",
            "Epoch 2980, Loss(train/val) 0.16304/0.12836. Took 0.12 sec\n",
            "Epoch 2981, Loss(train/val) 0.18107/0.12109. Took 0.10 sec\n",
            "Epoch 2982, Loss(train/val) 0.20270/0.10128. Took 0.10 sec\n",
            "Epoch 2983, Loss(train/val) 0.19238/0.09287. Took 0.09 sec\n",
            "Epoch 2984, Loss(train/val) 0.16500/0.08636. Took 0.12 sec\n",
            "Epoch 2985, Loss(train/val) 0.17846/0.08399. Took 0.10 sec\n",
            "Epoch 2986, Loss(train/val) 0.15808/0.08301. Took 0.10 sec\n",
            "Epoch 2987, Loss(train/val) 0.20237/0.08520. Took 0.10 sec\n",
            "Epoch 2988, Loss(train/val) 0.17073/0.09340. Took 0.10 sec\n",
            "Epoch 2989, Loss(train/val) 0.17840/0.10426. Took 0.10 sec\n",
            "Epoch 2990, Loss(train/val) 0.18923/0.11217. Took 0.11 sec\n",
            "Epoch 2991, Loss(train/val) 0.17659/0.09958. Took 0.10 sec\n",
            "Epoch 2992, Loss(train/val) 0.16293/0.10201. Took 0.10 sec\n",
            "Epoch 2993, Loss(train/val) 0.18158/0.11073. Took 0.10 sec\n",
            "Epoch 2994, Loss(train/val) 0.21037/0.09274. Took 0.10 sec\n",
            "Epoch 2995, Loss(train/val) 0.16943/0.08234. Took 0.10 sec\n",
            "Epoch 2996, Loss(train/val) 0.18021/0.08107. Took 0.10 sec\n",
            "Epoch 2997, Loss(train/val) 0.16704/0.08353. Took 0.11 sec\n",
            "Epoch 2998, Loss(train/val) 0.17919/0.09444. Took 0.11 sec\n",
            "Epoch 2999, Loss(train/val) 0.17117/0.11868. Took 0.10 sec\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x2592 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}